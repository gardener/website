<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Docs | Gardener</title><meta name=description content><meta property="og:title" content="Docs"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Docs"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Docs"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.c9d1d44a766d2318f4e814a91c5c78ed.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Docs</h1><div class=content></div></div><div class=td-content><h1 id=pg-caee8ca71ae55bd8c49bf6bc76717b23>1 - Gardener</h1><div class=lead>The core component providing the extension API server of your Kubernetes cluster</div><h1 id=gardenerhttpsgardenercloud><a href=https://gardener.cloud>Gardener</a></h1><p><img src=/__resources/gardener-large_d39ce1.png alt="Gardener Logo"></p><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://kubernetes.slack.com/messages/gardener><img src="https://img.shields.io/badge/slack-gardener-brightgreen.svg?logo=slack" alt="Slack channel #gardener"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener><img src=https://goreportcard.com/badge/github.com/gardener/gardener alt="Go Report Card"></a>
<a href=https://godoc.org/github.com/gardener/gardener><img src=https://godoc.org/github.com/gardener/gardener?status.svg alt=GoDoc></a>
<a href=https://bestpractices.coreinfrastructure.org/projects/1822><img src=https://bestpractices.coreinfrastructure.org/projects/1822/badge alt="CII Best Practices"></a></p><p>Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.</p><p>Gardener is 100% Kubernetes-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG Cluster Lifecycle</a>&rsquo;s <a href=https://github.com/kubernetes-sigs/cluster-api#cluster-api>Cluster API</a> that only harmonizes how to get to clusters, while <a href=/docs/gardener/api-reference/core/#shoot>Gardener&rsquo;s Cluster API</a> goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.</p><p>In 2020, SIG Cluster Lifecycle&rsquo;s Cluster API made a huge step forward with <a href=https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/><code>v1alpha3</code></a> and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle&rsquo;s Cluster API, please see <a href=/docs/gardener/concepts/cluster-api/>here</a>.</p><p>Gardener&rsquo;s main principle is to <strong>leverage Kubernetes concepts for all of its tasks</strong>.</p><p>In essence, Gardener is an <a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/>extension API server</a> that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called <strong>garden</strong> cluster) in order to use them for the management of end-user Kubernetes clusters (which are called <strong>shoot</strong> clusters). These shoot clusters are described via <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>declarative cluster specifications</a> which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.</p><p>To accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called <em>control plane</em> components are hosted in Kubernetes clusters themselves (which are called <strong>seed</strong> clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for &ldquo;day-2 operations&rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.</p><p>Gardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:</p><ul><li>Kubernetes API Server = Gardener API Server</li><li>Kubernetes Controller Manager = Gardener Controller Manager</li><li>Kubernetes Scheduler = Gardener Scheduler</li><li>Kubelet = Gardenlet</li><li>Node = Seed cluster</li><li>Pod = Shoot cluster</li></ul><p>Please find more information regarding the concepts and a detailed description of the architecture in our <a href=/docs/gardener/concepts/architecture/>Gardener Wiki</a> and our blog posts on kubernetes.io: <a href=https://kubernetes.io/blog/2018/05/17/gardener>Gardener - the Kubernetes Botanist (17.5.2018)</a> and <a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update>Gardener Project Update (2.12.2019)</a>.</p><hr><h2 id=k8s-conformance-test-coverage-img-srchttpsrawgithubusercontentcomcncfartworkmasterprojectskubernetescertified-kubernetesversionlesscolorcertified-kubernetes-colorsvg-altcertified-kubernetes-logo-width50-alignright>K8s Conformance Test Coverage <img src=https://raw.githubusercontent.com/cncf/artwork/master/projects/kubernetes/certified-kubernetes/versionless/color/certified-kubernetes-color.svg alt="certified kubernetes logo" width=50 align=right></h2><p>Gardener takes part in the <a href=https://www.cncf.io/certification/software-conformance/>Certified Kubernetes Conformance Program</a> to attest its compatibility with the K8s conformance testsuite. Currently Gardener is certified for K8s versions up to v1.25, see <a href="https://docs.google.com/spreadsheets/d/1uF9BoDzzisHSQemXHIKegMhuythuq_GL3N1mlUUK2h0/edit#gid=0&range=126:127">the conformance spreadsheet</a>.</p><p>Continuous conformance test results of the latest stable Gardener release are uploaded regularly to the CNCF test grid:</p><table><thead><tr><th>Provider/K8s</th><th>v1.26</th><th>v1.25</th><th>v1.24</th><th>v1.23</th><th>v1.22</th><th>v1.21</th><th>v1.20</th></tr></thead><tbody><tr><td><strong>AWS</strong></td><td>N/A</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20AWS/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20AWS/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20AWS/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20AWS/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20AWS/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20AWS/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td><strong>Azure</strong></td><td>N/A</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Azure/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Azure/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20Azure/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Azure/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Azure/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Azure/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td><strong>GCP</strong></td><td>N/A</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20GCE/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20GCE/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20GCE/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20GCE/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20GCE/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20GCE/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td><strong>OpenStack</strong></td><td>N/A</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20OpenStack/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20OpenStack/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20OpenStack/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20OpenStack/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20OpenStack/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20OpenStack/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td><strong>Alicloud</strong></td><td>N/A</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td><strong>Equinix Metal</strong></td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td><strong>vSphere</strong></td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr></tbody></table><p>Get an overview of the test results at <a href=https://testgrid.k8s.io/conformance-gardener>testgrid</a>.</p><h2 id=start-using-or-developing-the-gardener-locally>Start using or developing the Gardener locally</h2><p>See our documentation in the <code>/docs</code> repository, please <a href=https://github.com/gardener/gardener/blob/master/docs/README.md>find the index here</a>.</p><h2 id=setting-up-your-own-gardener-landscape-in-the-cloud>Setting up your own Gardener landscape in the Cloud</h2><p>The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. You can do this with our <a href=https://github.com/gardener/gardener/tree/master/charts/gardener>Gardener Helm Chart</a>.</p><p>Alternatively you can use our <a href=https://github.com/gardener/garden-setup>garden setup</a> project to create a fully configured Gardener landscape which also includes our <a href=https://github.com/gardener/dashboard>Gardener Dashboard</a>.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome!</p><p>All channels for getting in touch or learning about our project are listed under the <a href=https://gardener.cloud/docs/contribute/#community>community</a> section. We are cordially inviting interested parties to join our <a href=https://gardener.cloud/docs/contribute/#bi-weekly-meetings>bi-weekly meetings</a>.</p><p>Please report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as <a href=https://github.com/gardener/gardener/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn More!</h2><p>Please find further resources about our project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a>.</li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://news.sap.com/2018/11/hasso-plattner-founders-award-finalist-profile-project-gardener/>SAP news article about &ldquo;Project Gardener&rdquo;</a></li><li><a href=https://www.sap-tv.com/video/40962/gardener-planting-the-seeds-of-success-in-the-cloud>Introduction movie: &ldquo;Gardener - Planting the Seeds of Success in the Cloud&rdquo;</a></li><li><a href="https://www.youtube.com/watch?v=bfw22WPg99A">&ldquo;Thinking Cloud Native&rdquo; talk at EclipseCon 2018</a></li><li><a href=https://blogs.sap.com/2018/07/26/showcase-of-gardener-at-oscon/>Blog - &ldquo;Showcase of Gardener at OSCON 2018&rdquo;</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a8efdcbcfad5d117ab83dc78a59d1180>1.1 - API Reference</h1><h1 id=gardener-api-reference>Gardener API Reference</h1><ul><li><a href=/docs/gardener/api-reference/authentication/><code>authentication.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/core/><code>core.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/extensions/><code>extensions.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/operations/><code>operations.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/resources/><code>resources.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/seedmanagement/><code>seedmanagement.gardener.cloud</code> API Group</a></li><li><a href=/docs/gardener/api-reference/settings/><code>settings.gardener.cloud</code> API Group</a></li></ul></div><div class=td-content><h1 id=pg-fa900f2cc6e9837e1344861d6c05eae9>1.1.1 - Authentication</h1><p>Packages:</p><ul><li><a href=#authentication.gardener.cloud%2fv1alpha1>authentication.gardener.cloud/v1alpha1</a></li></ul><h2 id=authentication.gardener.cloud/v1alpha1>authentication.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is a version of the API.</p></p>Resource Types:<ul><li><a href=#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest>AdminKubeconfigRequest</a></li></ul><h3 id=authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest>AdminKubeconfigRequest</h3><p><p>AdminKubeconfigRequest can be used to request a kubeconfig with admin credentials
for a Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>authentication.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>AdminKubeconfigRequest</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestSpec>AdminKubeconfigRequestSpec</a></em></td><td><p>Spec is the specification of the AdminKubeconfigRequest.</p><br><br><table><tr><td><code>expirationSeconds</code></br><em>int64</em></td><td><em>(Optional)</em><p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the &lsquo;expirationTimestamp&rsquo; field in a response.
Defaults to 1 hour.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestStatus>AdminKubeconfigRequestStatus</a></em></td><td><p>Status is the status of the AdminKubeconfigRequest.</p></td></tr></tbody></table><h3 id=authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestSpec>AdminKubeconfigRequestSpec</h3><p>(<em>Appears on:</em>
<a href=#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest>AdminKubeconfigRequest</a>)</p><p><p>AdminKubeconfigRequestSpec contains the expiration time of the kubeconfig.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>expirationSeconds</code></br><em>int64</em></td><td><em>(Optional)</em><p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the &lsquo;expirationTimestamp&rsquo; field in a response.
Defaults to 1 hour.</p></td></tr></tbody></table><h3 id=authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestStatus>AdminKubeconfigRequestStatus</h3><p>(<em>Appears on:</em>
<a href=#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest>AdminKubeconfigRequest</a>)</p><p><p>AdminKubeconfigRequestStatus is the status of the AdminKubeconfigRequest containing
the kubeconfig and expiration of the credential.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kubeconfig</code></br><em>[]byte</em></td><td><p>Kubeconfig contains the kubeconfig with cluster-admin privileges for the shoot cluster.</p></td></tr><tr><td><code>expirationTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>ExpirationTimestamp is the expiration timestamp of the returned credential.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-209c498ead72c3692859e3c0e9e817cb>1.1.2 - Core</h1><p>Packages:</p><ul><li><a href=#core.gardener.cloud%2fv1beta1>core.gardener.cloud/v1beta1</a></li></ul><h2 id=core.gardener.cloud/v1beta1>core.gardener.cloud/v1beta1</h2><p><p>Package v1beta1 is a version of the API.</p></p>Resource Types:<ul><li><a href=#core.gardener.cloud/v1beta1.BackupBucket>BackupBucket</a></li><li><a href=#core.gardener.cloud/v1beta1.BackupEntry>BackupEntry</a></li><li><a href=#core.gardener.cloud/v1beta1.CloudProfile>CloudProfile</a></li><li><a href=#core.gardener.cloud/v1beta1.ControllerDeployment>ControllerDeployment</a></li><li><a href=#core.gardener.cloud/v1beta1.ControllerInstallation>ControllerInstallation</a></li><li><a href=#core.gardener.cloud/v1beta1.ControllerRegistration>ControllerRegistration</a></li><li><a href=#core.gardener.cloud/v1beta1.ExposureClass>ExposureClass</a></li><li><a href=#core.gardener.cloud/v1beta1.Project>Project</a></li><li><a href=#core.gardener.cloud/v1beta1.Quota>Quota</a></li><li><a href=#core.gardener.cloud/v1beta1.SecretBinding>SecretBinding</a></li><li><a href=#core.gardener.cloud/v1beta1.Seed>Seed</a></li><li><a href=#core.gardener.cloud/v1beta1.Shoot>Shoot</a></li><li><a href=#core.gardener.cloud/v1beta1.ShootState>ShootState</a></li></ul><h3 id=core.gardener.cloud/v1beta1.BackupBucket>BackupBucket</h3><p><p>BackupBucket holds details about backup bucket</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>BackupBucket</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupBucketSpec>BackupBucketSpec</a></em></td><td><p>Specification of the Backup Bucket.</p><br><br><table><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupBucketProvider>BackupBucketProvider</a></em></td><td><p>Provider holds the details of cloud provider of the object store. This field is immutable.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to BackupBucket resource.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName holds the name of the seed allocated to BackupBucket for running controller.
This field is immutable.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupBucketStatus>BackupBucketStatus</a></em></td><td><p>Most recently observed status of the Backup Bucket.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupEntry>BackupEntry</h3><p><p>BackupEntry holds details about shoot backup.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>BackupEntry</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupEntrySpec>BackupEntrySpec</a></em></td><td><em>(Optional)</em><p>Spec contains the specification of the Backup Entry.</p><br><br><table><tr><td><code>bucketName</code></br><em>string</em></td><td><p>BucketName is the name of backup bucket for this Backup Entry.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName holds the name of the seed to which this BackupEntry is scheduled</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupEntryStatus>BackupEntryStatus</a></em></td><td><em>(Optional)</em><p>Status contains the most recently observed status of the Backup Entry.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CloudProfile>CloudProfile</h3><p><p>CloudProfile represents certain properties about a provider environment.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>CloudProfile</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a></em></td><td><em>(Optional)</em><p>Spec defines the provider environment properties.</p><br><br><table><tr><td><code>caBundle</code></br><em>string</em></td><td><em>(Optional)</em><p>CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesSettings>KubernetesSettings</a></em></td><td><p>Kubernetes contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.</p></td></tr><tr><td><code>machineImages</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineImage>[]MachineImage</a></em></td><td><p>MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.</p></td></tr><tr><td><code>machineTypes</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineType>[]MachineType</a></em></td><td><p>MachineTypes contains constraints regarding allowed values for machine types in the ‘workers’ block in the Shoot specification.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig contains provider-specific configuration for the profile.</p></td></tr><tr><td><code>regions</code></br><em><a href=#core.gardener.cloud/v1beta1.Region>[]Region</a></em></td><td><p>Regions contains constraints regarding allowed values for regions and zones.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector contains an optional list of labels on <code>Seed</code> resources that marks those seeds whose shoots may use this provider profile.
An empty list means that all seeds of the same provider type are supported.
This is useful for environments that are of the same type (like openstack) but may have different “instances”/landscapes.
Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider
type of the seed must match the shoot’s provider.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the name of the provider.</p></td></tr><tr><td><code>volumeTypes</code></br><em><a href=#core.gardener.cloud/v1beta1.VolumeType>[]VolumeType</a></em></td><td><em>(Optional)</em><p>VolumeTypes contains constraints regarding allowed values for volume types in the ‘workers’ block in the Shoot specification.</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerDeployment>ControllerDeployment</h3><p><p>ControllerDeployment contains information about how this controller is deployed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ControllerDeployment</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the deployment type.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>ProviderConfig contains type-specific configuration. It contains assets that deploy the controller.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerInstallation>ControllerInstallation</h3><p><p>ControllerInstallation represents an installation request for an external controller.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ControllerInstallation</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerInstallationSpec>ControllerInstallationSpec</a></em></td><td><p>Spec contains the specification of this installation.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>registrationRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>RegistrationRef is used to reference a ControllerRegistration resource.
The name field of the RegistrationRef is immutable.</p></td></tr><tr><td><code>seedRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>SeedRef is used to reference a Seed resource. The name field of the SeedRef is immutable.</p></td></tr><tr><td><code>deploymentRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><em>(Optional)</em><p>DeploymentRef is used to reference a ControllerDeployment resource.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerInstallationStatus>ControllerInstallationStatus</a></em></td><td><p>Status contains the status of this installation.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerRegistration>ControllerRegistration</h3><p><p>ControllerRegistration represents a registration of an external controller.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ControllerRegistration</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerRegistrationSpec>ControllerRegistrationSpec</a></em></td><td><p>Spec contains the specification of this registration.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResource>[]ControllerResource</a></em></td><td><em>(Optional)</em><p>Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, …) and their actual types
(aws-route53, gcp, auditlog, …).</p></td></tr><tr><td><code>deployment</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerRegistrationDeployment>ControllerRegistrationDeployment</a></em></td><td><em>(Optional)</em><p>Deployment contains information for how this controller is deployed.</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ExposureClass>ExposureClass</h3><p><p>ExposureClass represents a control plane endpoint exposure strategy.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ExposureClass</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>handler</code></br><em>string</em></td><td><p>Handler is the name of the handler which applies the control plane endpoint exposure strategy.
This field is immutable.</p></td></tr><tr><td><code>scheduling</code></br><em><a href=#core.gardener.cloud/v1beta1.ExposureClassScheduling>ExposureClassScheduling</a></em></td><td><em>(Optional)</em><p>Scheduling holds information how to select applicable Seed’s for ExposureClass usage.
This field is immutable.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Project>Project</h3><p><p>Project holds certain properties about a Gardener project.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Project</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectSpec>ProjectSpec</a></em></td><td><em>(Optional)</em><p>Spec defines the project properties.</p><br><br><table><tr><td><code>createdBy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#subject-v1-rbac>Kubernetes rbac/v1.Subject</a></em></td><td><em>(Optional)</em><p>CreatedBy is a subject representing a user name, an email address, or any other identifier of a user
who created the project. This field is immutable.</p></td></tr><tr><td><code>description</code></br><em>string</em></td><td><em>(Optional)</em><p>Description is a human-readable description of what the project is used for.</p></td></tr><tr><td><code>owner</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#subject-v1-rbac>Kubernetes rbac/v1.Subject</a></em></td><td><em>(Optional)</em><p>Owner is a subject representing a user name, an email address, or any other identifier of a user owning
the project.
IMPORTANT: Be aware that this field will be removed in the <code>v1</code> version of this API in favor of the <code>owner</code>
role. The only way to change the owner will be by moving the <code>owner</code> role. In this API version the only way
to change the owner is to use this field.
TODO: Remove this field in favor of the <code>owner</code> role in <code>v1</code>.</p></td></tr><tr><td><code>purpose</code></br><em>string</em></td><td><em>(Optional)</em><p>Purpose is a human-readable explanation of the project’s purpose.</p></td></tr><tr><td><code>members</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectMember>[]ProjectMember</a></em></td><td><em>(Optional)</em><p>Members is a list of subjects representing a user name, an email address, or any other identifier of a user,
group, or service account that has a certain role.</p></td></tr><tr><td><code>namespace</code></br><em>string</em></td><td><em>(Optional)</em><p>Namespace is the name of the namespace that has been created for the Project object.
A nil value means that Gardener will determine the name of the namespace.
This field is immutable.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectTolerations>ProjectTolerations</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on seed clusters.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectStatus>ProjectStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the Project.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Quota>Quota</h3><p><p>Quota represents a quota on resources consumed by shoot clusters either per project or per provider secret.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Quota</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.QuotaSpec>QuotaSpec</a></em></td><td><em>(Optional)</em><p>Spec defines the Quota constraints.</p><br><br><table><tr><td><code>clusterLifetimeDays</code></br><em>int32</em></td><td><em>(Optional)</em><p>ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.</p></td></tr><tr><td><code>metrics</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Metrics is a list of resources which will be put under constraints.</p></td></tr><tr><td><code>scope</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>Scope is the scope of the Quota object, either ‘project’ or ‘secret’. This field is immutable.</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SecretBinding>SecretBinding</h3><p><p>SecretBinding represents a binding to a secret in the same or another namespace.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>SecretBinding</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret object in the same or another namespace.
This field is immutable.</p></td></tr><tr><td><code>quotas</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>[]Kubernetes core/v1.ObjectReference</a></em></td><td><em>(Optional)</em><p>Quotas is a list of references to Quota objects in the same or another namespace.
This field is immutable.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.SecretBindingProvider>SecretBindingProvider</a></em></td><td><em>(Optional)</em><p>Provider defines the provider type of the SecretBinding.
This field is immutable.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Seed>Seed</h3><p><p>Seed represents an installation request for an external controller.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Seed</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a></em></td><td><p>Spec contains the specification of this installation.</p><br><br><table><tr><td><code>backup</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedBackup>SeedBackup</a></em></td><td><em>(Optional)</em><p>Backup holds the object store configuration for the backups of shoot (currently only etcd).
If it is not specified, then there won’t be any backups taken for shoots associated with this seed.
If backup field is present in seed, then backups of the etcd from shoot control plane will be stored
under the configured object store.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedDNS>SeedDNS</a></em></td><td><p>DNS contains DNS-relevant information about this seed cluster.</p></td></tr><tr><td><code>networks</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</a></em></td><td><p>Networks defines the pod, service and worker network of the Seed cluster.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedProvider>SeedProvider</a></em></td><td><p>Provider defines the provider type and region for this Seed cluster.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes
cluster to be registered as Seed.</p></td></tr><tr><td><code>taints</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedTaint>[]SeedTaint</a></em></td><td><em>(Optional)</em><p>Taints describes taints on the seed.</p></td></tr><tr><td><code>volume</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedVolume>SeedVolume</a></em></td><td><em>(Optional)</em><p>Volume contains settings for persistentvolumes created in the seed cluster.</p></td></tr><tr><td><code>settings</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a></em></td><td><em>(Optional)</em><p>Settings contains certain settings for this seed cluster.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#core.gardener.cloud/v1beta1.Ingress>Ingress</a></em></td><td><em>(Optional)</em><p>Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedStatus>SeedStatus</a></em></td><td><p>Status contains the status of this installation.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Shoot>Shoot</h3><p><p>Shoot represents a Shoot cluster created and managed by Gardener.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Shoot</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a></em></td><td><em>(Optional)</em><p>Specification of the Shoot cluster.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>addons</code></br><em><a href=#core.gardener.cloud/v1beta1.Addons>Addons</a></em></td><td><em>(Optional)</em><p>Addons contains information about enabled/disabled addons and their configuration.</p></td></tr><tr><td><code>cloudProfileName</code></br><em>string</em></td><td><p>CloudProfileName is a name of a CloudProfile object. This field is immutable.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.DNS>DNS</a></em></td><td><em>(Optional)</em><p>DNS contains information about the DNS settings of the Shoot.</p></td></tr><tr><td><code>extensions</code></br><em><a href=#core.gardener.cloud/v1beta1.Extension>[]Extension</a></em></td><td><em>(Optional)</em><p>Extensions contain type and provider information for Shoot extensions.</p></td></tr><tr><td><code>hibernation</code></br><em><a href=#core.gardener.cloud/v1beta1.Hibernation>Hibernation</a></em></td><td><em>(Optional)</em><p>Hibernation contains information whether the Shoot is suspended or not.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a></em></td><td><p>Kubernetes contains the version and configuration settings of the control plane components.</p></td></tr><tr><td><code>networking</code></br><em><a href=#core.gardener.cloud/v1beta1.Networking>Networking</a></em></td><td><p>Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.</p></td></tr><tr><td><code>maintenance</code></br><em><a href=#core.gardener.cloud/v1beta1.Maintenance>Maintenance</a></em></td><td><em>(Optional)</em><p>Maintenance contains information about the time window for maintenance operations and which
operations should be performed.</p></td></tr><tr><td><code>monitoring</code></br><em><a href=#core.gardener.cloud/v1beta1.Monitoring>Monitoring</a></em></td><td><em>(Optional)</em><p>Monitoring contains information about custom monitoring configurations for the shoot.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.Provider>Provider</a></em></td><td><p>Provider contains all provider-specific and provider-relevant information.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootPurpose>ShootPurpose</a></em></td><td><em>(Optional)</em><p>Purpose is the purpose class for this cluster.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is a name of a region. This field is immutable.</p></td></tr><tr><td><code>secretBindingName</code></br><em>string</em></td><td><p>SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret.
The credentials inside the provider secret will be used to create the shoot in the respective account.
This field is immutable.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed cluster that runs the control plane of the Shoot.
This field is immutable when the SeedChange feature gate is disabled.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.NamedResourceReference>[]NamedResourceReference</a></em></td><td><em>(Optional)</em><p>Resources holds a list of named resource references that can be referred to in extension configs by their names.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on seed clusters.</p></td></tr><tr><td><code>exposureClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy.
This field is immutable.</p></td></tr><tr><td><code>systemComponents</code></br><em><a href=#core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</a></em></td><td><em>(Optional)</em><p>SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.</p></td></tr><tr><td><code>controlPlane</code></br><em><a href=#core.gardener.cloud/v1beta1.ControlPlane>ControlPlane</a></em></td><td><em>(Optional)</em><p>ControlPlane contains general settings for the control plane of the shoot.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the Shoot cluster.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootState>ShootState</h3><p><p>ShootState contains a snapshot of the Shoot’s state required to migrate the Shoot’s control plane to a new Seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>core.gardener.cloud/v1beta1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ShootState</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootStateSpec>ShootStateSpec</a></em></td><td><em>(Optional)</em><p>Specification of the ShootState.</p><br><br><table><tr><td><code>gardener</code></br><em><a href=#core.gardener.cloud/v1beta1.GardenerResourceData>[]GardenerResourceData</a></em></td><td><em>(Optional)</em><p>Gardener holds the data required to generate resources deployed by the gardenlet</p></td></tr><tr><td><code>extensions</code></br><em><a href=#core.gardener.cloud/v1beta1.ExtensionResourceState>[]ExtensionResourceState</a></em></td><td><em>(Optional)</em><p>Extensions holds the state of custom resources reconciled by extension controllers in the seed</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.ResourceData>[]ResourceData</a></em></td><td><em>(Optional)</em><p>Resources holds the data of resources referred to by extension controller states</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Addon>Addon</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubernetesDashboard>KubernetesDashboard</a>,
<a href=#core.gardener.cloud/v1beta1.NginxIngress>NginxIngress</a>)</p><p><p>Addon allows enabling or disabling a specific addon and is used to derive from.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled indicates whether the addon is enabled or not.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Addons>Addons</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Addons is a collection of configuration for specific addons which are managed by the Gardener.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kubernetesDashboard</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesDashboard>KubernetesDashboard</a></em></td><td><em>(Optional)</em><p>KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.</p></td></tr><tr><td><code>nginxIngress</code></br><em><a href=#core.gardener.cloud/v1beta1.NginxIngress>NginxIngress</a></em></td><td><em>(Optional)</em><p>NginxIngress holds configuration settings for the nginx-ingress addon.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.AdmissionPlugin>AdmissionPlugin</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the plugin.</p></td></tr><tr><td><code>config</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>Config is the configuration of the plugin.</p></td></tr><tr><td><code>disabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>Disabled specifies whether this plugin should be disabled.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Alerting>Alerting</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Monitoring>Monitoring</a>)</p><p><p>Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>emailReceivers</code></br><em>[]string</em></td><td><em>(Optional)</em><p>MonitoringEmailReceivers is a list of recipients for alerts</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.AuditConfig>AuditConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>AuditConfig contains settings for audit of the api server</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>auditPolicy</code></br><em><a href=#core.gardener.cloud/v1beta1.AuditPolicy>AuditPolicy</a></em></td><td><em>(Optional)</em><p>AuditPolicy contains configuration settings for audit policy of the kube-apiserver.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.AuditPolicy>AuditPolicy</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.AuditConfig>AuditConfig</a>)</p><p><p>AuditPolicy contains audit policy for kube-apiserver</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>configMapRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><em>(Optional)</em><p>ConfigMapRef is a reference to a ConfigMap object in the same namespace,
which contains the audit policy for the kube-apiserver.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.AvailabilityZone>AvailabilityZone</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Region>Region</a>)</p><p><p>AvailabilityZone is an availability zone.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is an availability zone name.</p></td></tr><tr><td><code>unavailableMachineTypes</code></br><em>[]string</em></td><td><em>(Optional)</em><p>UnavailableMachineTypes is a list of machine type names that are not availability in this zone.</p></td></tr><tr><td><code>unavailableVolumeTypes</code></br><em>[]string</em></td><td><em>(Optional)</em><p>UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupBucketProvider>BackupBucketProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupBucketSpec>BackupBucketSpec</a>)</p><p><p>BackupBucketProvider holds the details of cloud provider of the object store.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of provider.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of the bucket.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupBucketSpec>BackupBucketSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupBucket>BackupBucket</a>)</p><p><p>BackupBucketSpec is the specification of a Backup Bucket.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.BackupBucketProvider>BackupBucketProvider</a></em></td><td><p>Provider holds the details of cloud provider of the object store. This field is immutable.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to BackupBucket resource.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName holds the name of the seed allocated to BackupBucket for running controller.
This field is immutable.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupBucketStatus>BackupBucketStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupBucket>BackupBucket</a>)</p><p><p>BackupBucketStatus holds the most recently observed status of the Backup Bucket.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>providerStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderStatus is the configuration passed to BackupBucket resource.</p></td></tr><tr><td><code>lastOperation</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperation>LastOperation</a></em></td><td><em>(Optional)</em><p>LastOperation holds information about the last operation on the BackupBucket.</p></td></tr><tr><td><code>lastError</code></br><em><a href=#core.gardener.cloud/v1beta1.LastError>LastError</a></em></td><td><em>(Optional)</em><p>LastError holds information about the last occurred error during an operation.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the
BackupBucket’s generation, which is updated on mutation by the API Server.</p></td></tr><tr><td><code>generatedSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>GeneratedSecretRef is reference to the secret generated by backup bucket, which
will have object store specific credentials.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupEntrySpec>BackupEntrySpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupEntry>BackupEntry</a>)</p><p><p>BackupEntrySpec is the specification of a Backup Entry.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>bucketName</code></br><em>string</em></td><td><p>BucketName is the name of backup bucket for this Backup Entry.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName holds the name of the seed to which this BackupEntry is scheduled</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.BackupEntryStatus>BackupEntryStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupEntry>BackupEntry</a>)</p><p><p>BackupEntryStatus holds the most recently observed status of the Backup Entry.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>lastOperation</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperation>LastOperation</a></em></td><td><em>(Optional)</em><p>LastOperation holds information about the last operation on the BackupEntry.</p></td></tr><tr><td><code>lastError</code></br><em><a href=#core.gardener.cloud/v1beta1.LastError>LastError</a></em></td><td><em>(Optional)</em><p>LastError holds information about the last occurred error during an operation.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the
BackupEntry’s generation, which is updated on mutation by the API Server.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed to which this BackupEntry is currently scheduled. This field is populated
at the beginning of a create/reconcile operation. It is used when moving the BackupEntry between seeds.</p></td></tr><tr><td><code>migrationStartTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>MigrationStartTime is the time when a migration to a different seed was initiated.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CARotation>CARotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>CARotation contains information about the certificate authority credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></br><em><a href=#core.gardener.cloud/v1beta1.CredentialsRotationPhase>CredentialsRotationPhase</a></em></td><td><p>Phase describes the phase of the certificate authority credential rotation.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the certificate authority credential rotation was successfully
completed.</p></td></tr><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the certificate authority credential rotation was initiated.</p></td></tr><tr><td><code>lastInitiationFinishedTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationFinishedTime is the recent time when the certificate authority credential rotation initiation was
completed.</p></td></tr><tr><td><code>lastCompletionTriggeredTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTriggeredTime is the recent time when the certificate authority credential rotation completion was
triggered.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CRI>CRI</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.MachineImageVersion>MachineImageVersion</a>,
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>CRI contains information about the Container Runtimes.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em><a href=#core.gardener.cloud/v1beta1.CRIName>CRIName</a></em></td><td><p>The name of the CRI library. Supported values are <code>docker</code> and <code>containerd</code>.</p></td></tr><tr><td><code>containerRuntimes</code></br><em><a href=#core.gardener.cloud/v1beta1.ContainerRuntime>[]ContainerRuntime</a></em></td><td><em>(Optional)</em><p>ContainerRuntimes is the list of the required container runtimes supported for a worker pool.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CRIName>CRIName
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CRI>CRI</a>)</p><p><p>CRIName is a type alias for the CRI name string.</p></p><h3 id=core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfile>CloudProfile</a>)</p><p><p>CloudProfileSpec is the specification of a CloudProfile.
It must contain exactly one of its defined keys.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>caBundle</code></br><em>string</em></td><td><em>(Optional)</em><p>CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesSettings>KubernetesSettings</a></em></td><td><p>Kubernetes contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.</p></td></tr><tr><td><code>machineImages</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineImage>[]MachineImage</a></em></td><td><p>MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.</p></td></tr><tr><td><code>machineTypes</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineType>[]MachineType</a></em></td><td><p>MachineTypes contains constraints regarding allowed values for machine types in the ‘workers’ block in the Shoot specification.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig contains provider-specific configuration for the profile.</p></td></tr><tr><td><code>regions</code></br><em><a href=#core.gardener.cloud/v1beta1.Region>[]Region</a></em></td><td><p>Regions contains constraints regarding allowed values for regions and zones.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector contains an optional list of labels on <code>Seed</code> resources that marks those seeds whose shoots may use this provider profile.
An empty list means that all seeds of the same provider type are supported.
This is useful for environments that are of the same type (like openstack) but may have different “instances”/landscapes.
Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider
type of the seed must match the shoot’s provider.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the name of the provider.</p></td></tr><tr><td><code>volumeTypes</code></br><em><a href=#core.gardener.cloud/v1beta1.VolumeType>[]VolumeType</a></em></td><td><em>(Optional)</em><p>VolumeTypes contains constraints regarding allowed values for volume types in the ‘workers’ block in the Shoot specification.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ClusterAutoscaler>ClusterAutoscaler</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>scaleDownDelayAfterAdd</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).</p></td></tr><tr><td><code>scaleDownDelayAfterDelete</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (default: 0 secs).</p></td></tr><tr><td><code>scaleDownDelayAfterFailure</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).</p></td></tr><tr><td><code>scaleDownUnneededTime</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).</p></td></tr><tr><td><code>scaleDownUtilizationThreshold</code></br><em>float64</em></td><td><em>(Optional)</em><p>ScaleDownUtilizationThreshold defines the threshold in fraction (0.0 - 1.0) under which a node is being removed (default: 0.5).</p></td></tr><tr><td><code>scanInterval</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).</p></td></tr><tr><td><code>expander</code></br><em><a href=#core.gardener.cloud/v1beta1.ExpanderMode>ExpanderMode</a></em></td><td><em>(Optional)</em><p>Expander defines the algorithm to use during scale up (default: least-waste).
See: <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders</a>.</p></td></tr><tr><td><code>maxNodeProvisionTime</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MaxNodeProvisionTime defines how long CA waits for node to be provisioned (default: 20 mins).</p></td></tr><tr><td><code>maxGracefulTerminationSeconds</code></br><em>int32</em></td><td><em>(Optional)</em><p>MaxGracefulTerminationSeconds is the number of seconds CA waits for pod termination when trying to scale down a node (default: 600).</p></td></tr><tr><td><code>ignoreTaints</code></br><em>[]string</em></td><td><em>(Optional)</em><p>IgnoreTaints specifies a list of taint keys to ignore in node templates when considering to scale a node group.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Condition>Condition</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerInstallationStatus>ControllerInstallationStatus</a>,
<a href=#core.gardener.cloud/v1beta1.SeedStatus>SeedStatus</a>,
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>Condition holds the information about the state of a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=#core.gardener.cloud/v1beta1.ConditionType>ConditionType</a></em></td><td><p>Type of the condition.</p></td></tr><tr><td><code>status</code></br><em><a href=#core.gardener.cloud/v1beta1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition was updated.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>message</code></br><em>string</em></td><td><p>A human readable message indicating details about the transition.</p></td></tr><tr><td><code>codes</code></br><em><a href=#core.gardener.cloud/v1beta1.ErrorCode>[]ErrorCode</a></em></td><td><em>(Optional)</em><p>Well-defined error codes in case the condition reports a problem.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ConditionStatus>ConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Condition>Condition</a>)</p><p><p>ConditionStatus is the status of a condition.</p></p><h3 id=core.gardener.cloud/v1beta1.ConditionType>ConditionType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Condition>Condition</a>)</p><p><p>ConditionType is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.ContainerRuntime>ContainerRuntime</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CRI>CRI</a>)</p><p><p>ContainerRuntime contains information about worker’s available container runtime</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of the Container Runtime.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to container runtime resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControlPlane>ControlPlane</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>ControlPlane holds information about the general settings for the control plane of a shoot.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>highAvailability</code></br><em><a href=#core.gardener.cloud/v1beta1.HighAvailability>HighAvailability</a></em></td><td><em>(Optional)</em><p>HighAvailability holds the configuration settings for high availability of the
control plane of a shoot.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerDeploymentPolicy>ControllerDeploymentPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerRegistrationDeployment>ControllerRegistrationDeployment</a>)</p><p><p>ControllerDeploymentPolicy is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.ControllerInstallationSpec>ControllerInstallationSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerInstallation>ControllerInstallation</a>)</p><p><p>ControllerInstallationSpec is the specification of a ControllerInstallation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>registrationRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>RegistrationRef is used to reference a ControllerRegistration resource.
The name field of the RegistrationRef is immutable.</p></td></tr><tr><td><code>seedRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>SeedRef is used to reference a Seed resource. The name field of the SeedRef is immutable.</p></td></tr><tr><td><code>deploymentRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><em>(Optional)</em><p>DeploymentRef is used to reference a ControllerDeployment resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerInstallationStatus>ControllerInstallationStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerInstallation>ControllerInstallation</a>)</p><p><p>ControllerInstallationStatus is the status of a ControllerInstallation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=#core.gardener.cloud/v1beta1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a ControllerInstallations’s current state.</p></td></tr><tr><td><code>providerStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderStatus contains type-specific status.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerRegistrationDeployment>ControllerRegistrationDeployment</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerRegistrationSpec>ControllerRegistrationSpec</a>)</p><p><p>ControllerRegistrationDeployment contains information for how this controller is deployed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>policy</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerDeploymentPolicy>ControllerDeploymentPolicy</a></em></td><td><em>(Optional)</em><p>Policy controls how the controller is deployed. It defaults to ‘OnDemand’.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be
considered for a deployment.
An empty list means that all seeds are selected.</p></td></tr><tr><td><code>deploymentRefs</code></br><em><a href=#core.gardener.cloud/v1beta1.DeploymentRef>[]DeploymentRef</a></em></td><td><em>(Optional)</em><p>DeploymentRefs holds references to <code>ControllerDeployments</code>. Only one element is support now.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerRegistrationSpec>ControllerRegistrationSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerRegistration>ControllerRegistration</a>)</p><p><p>ControllerRegistrationSpec is the specification of a ControllerRegistration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResource>[]ControllerResource</a></em></td><td><em>(Optional)</em><p>Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, …) and their actual types
(aws-route53, gcp, auditlog, …).</p></td></tr><tr><td><code>deployment</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerRegistrationDeployment>ControllerRegistrationDeployment</a></em></td><td><em>(Optional)</em><p>Deployment contains information for how this controller is deployed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerResource>ControllerResource</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerRegistrationSpec>ControllerRegistrationSpec</a>)</p><p><p>ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, …) and the actual type for this
kind (aws-route53, gcp, auditlog, …).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind is the resource kind, for example “OperatingSystemConfig”.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the resource type, for example “coreos” or “ubuntu”.</p></td></tr><tr><td><code>globallyEnabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.
This field is defaulted to false when kind is “Extension”.</p></td></tr><tr><td><code>reconcileTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.
This field is defaulted to 3m0s when kind is “Extension”.</p></td></tr><tr><td><code>primary</code></br><em>bool</em></td><td><em>(Optional)</em><p>Primary determines if the controller backed by this ControllerRegistration is responsible for the extension
resource’s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type
combination. This field is immutable.</p></td></tr><tr><td><code>lifecycle</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResourceLifecycle>ControllerResourceLifecycle</a></em></td><td><em>(Optional)</em><p>Lifecycle defines a strategy that determines when different operations on a ControllerResource should be performed.
This field is defaulted in the following way when kind is “Extension”.
Reconcile: “AfterKubeAPIServer”
Delete: “BeforeKubeAPIServer”
Migrate: “BeforeKubeAPIServer”</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerResourceLifecycle>ControllerResourceLifecycle</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerResource>ControllerResource</a>)</p><p><p>ControllerResourceLifecycle defines the lifecycle of a controller resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>reconcile</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResourceLifecycleStrategy>ControllerResourceLifecycleStrategy</a></em></td><td><em>(Optional)</em><p>Reconcile defines the strategy during reconciliation.</p></td></tr><tr><td><code>delete</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResourceLifecycleStrategy>ControllerResourceLifecycleStrategy</a></em></td><td><em>(Optional)</em><p>Delete defines the strategy during deletion.</p></td></tr><tr><td><code>migrate</code></br><em><a href=#core.gardener.cloud/v1beta1.ControllerResourceLifecycleStrategy>ControllerResourceLifecycleStrategy</a></em></td><td><em>(Optional)</em><p>Migrate defines the strategy during migration.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ControllerResourceLifecycleStrategy>ControllerResourceLifecycleStrategy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerResourceLifecycle>ControllerResourceLifecycle</a>)</p><p><p>ControllerResourceLifecycleStrategy is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.CoreDNS>CoreDNS</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</a>)</p><p><p>CoreDNS contains the settings of the Core DNS components running in the data plane of the Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>autoscaling</code></br><em><a href=#core.gardener.cloud/v1beta1.CoreDNSAutoscaling>CoreDNSAutoscaling</a></em></td><td><em>(Optional)</em><p>Autoscaling contains the settings related to autoscaling of the Core DNS components running in the data plane of the Shoot cluster.</p></td></tr><tr><td><code>rewriting</code></br><em><a href=#core.gardener.cloud/v1beta1.CoreDNSRewriting>CoreDNSRewriting</a></em></td><td><em>(Optional)</em><p>Rewriting contains the setting related to rewriting of requests, which are obviously incorrect due to the unnecessary application of the search path.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CoreDNSAutoscaling>CoreDNSAutoscaling</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CoreDNS>CoreDNS</a>)</p><p><p>CoreDNSAutoscaling contains the settings related to autoscaling of the Core DNS components running in the data plane of the Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>mode</code></br><em><a href=#core.gardener.cloud/v1beta1.CoreDNSAutoscalingMode>CoreDNSAutoscalingMode</a></em></td><td><p>The mode of the autoscaling to be used for the Core DNS components running in the data plane of the Shoot cluster.
Supported values are <code>horizontal</code> and <code>cluster-proportional</code>.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CoreDNSAutoscalingMode>CoreDNSAutoscalingMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CoreDNSAutoscaling>CoreDNSAutoscaling</a>)</p><p><p>CoreDNSAutoscalingMode is a type alias for the Core DNS autoscaling mode string.</p></p><h3 id=core.gardener.cloud/v1beta1.CoreDNSRewriting>CoreDNSRewriting</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CoreDNS>CoreDNS</a>)</p><p><p>CoreDNSRewriting contains the setting related to rewriting requests, which are obviously incorrect due to the unnecessary application of the search path.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>commonSuffixes</code></br><em>[]string</em></td><td><em>(Optional)</em><p>CommonSuffixes are expected to be the suffix of a fully qualified domain name. Each suffix should contain at least one or two dots (‘.’) to prevent accidental clashes.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.CredentialsRotationPhase>CredentialsRotationPhase
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CARotation>CARotation</a>,
<a href=#core.gardener.cloud/v1beta1.ETCDEncryptionKeyRotation>ETCDEncryptionKeyRotation</a>,
<a href=#core.gardener.cloud/v1beta1.ServiceAccountKeyRotation>ServiceAccountKeyRotation</a>)</p><p><p>CredentialsRotationPhase is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.DNS>DNS</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>DNS holds information about the provider, the hosted zone id and the domain.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>domain</code></br><em>string</em></td><td><em>(Optional)</em><p>Domain is the external available domain of the Shoot cluster. This domain will be written into the
kubeconfig that is handed out to end-users. This field is immutable.</p></td></tr><tr><td><code>providers</code></br><em><a href=#core.gardener.cloud/v1beta1.DNSProvider>[]DNSProvider</a></em></td><td><em>(Optional)</em><p>Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if
not a default domain is used.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.DNSIncludeExclude>DNSIncludeExclude</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.DNSProvider>DNSProvider</a>,
<a href=#core.gardener.cloud/v1beta1.SeedDNSProvider>SeedDNSProvider</a>)</p><p><p>DNSIncludeExclude contains information about which domains shall be included/excluded.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>include</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Include is a list of domains that shall be included.</p></td></tr><tr><td><code>exclude</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Exclude is a list of domains that shall be excluded.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.DNSProvider>DNSProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.DNS>DNS</a>)</p><p><p>DNSProvider contains information about a DNS provider.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>domains</code></br><em><a href=#core.gardener.cloud/v1beta1.DNSIncludeExclude>DNSIncludeExclude</a></em></td><td><em>(Optional)</em><p>Domains contains information about which domains shall be included/excluded for this provider.</p></td></tr><tr><td><code>primary</code></br><em>bool</em></td><td><em>(Optional)</em><p>Primary indicates that this DNSProvider is used for shoot related domains.</p></td></tr><tr><td><code>secretName</code></br><em>string</em></td><td><em>(Optional)</em><p>SecretName is a name of a secret containing credentials for the stated domain and the
provider. When not specified, the Gardener will use the cloud provider credentials referenced
by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override
this behavior, i.e. forcing the Gardener to only look into the given secret.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><em>(Optional)</em><p>Type is the DNS provider type.</p></td></tr><tr><td><code>zones</code></br><em><a href=#core.gardener.cloud/v1beta1.DNSIncludeExclude>DNSIncludeExclude</a></em></td><td><em>(Optional)</em><p>Zones contains information about which hosted zones shall be included/excluded for this provider.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.DataVolume>DataVolume</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>DataVolume contains information about a data volume.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the volume to make it referencable.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><em>(Optional)</em><p>Type is the type of the volume.</p></td></tr><tr><td><code>size</code></br><em>string</em></td><td><p>VolumeSize is the size of the volume.</p></td></tr><tr><td><code>encrypted</code></br><em>bool</em></td><td><em>(Optional)</em><p>Encrypted determines if the volume should be encrypted.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.DeploymentRef>DeploymentRef</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControllerRegistrationDeployment>ControllerRegistrationDeployment</a>)</p><p><p>DeploymentRef contains information about <code>ControllerDeployment</code> references.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the <code>ControllerDeployment</code> that is being referred to.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ETCDEncryptionKeyRotation>ETCDEncryptionKeyRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>ETCDEncryptionKeyRotation contains information about the ETCD encryption key credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></br><em><a href=#core.gardener.cloud/v1beta1.CredentialsRotationPhase>CredentialsRotationPhase</a></em></td><td><p>Phase describes the phase of the ETCD encryption key credential rotation.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the ETCD encryption key credential rotation was successfully
completed.</p></td></tr><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the ETCD encryption key credential rotation was initiated.</p></td></tr><tr><td><code>lastInitiationFinishedTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationFinishedTime is the recent time when the certificate authority credential rotation initiation was
completed.</p></td></tr><tr><td><code>lastCompletionTriggeredTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTriggeredTime is the recent time when the certificate authority credential rotation completion was
triggered.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ErrorCode>ErrorCode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Condition>Condition</a>,
<a href=#core.gardener.cloud/v1beta1.LastError>LastError</a>)</p><p><p>ErrorCode is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.ExpanderMode>ExpanderMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ClusterAutoscaler>ClusterAutoscaler</a>)</p><p><p>ExpanderMode is type used for Expander values</p></p><h3 id=core.gardener.cloud/v1beta1.ExpirableVersion>ExpirableVersion</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubernetesSettings>KubernetesSettings</a>,
<a href=#core.gardener.cloud/v1beta1.MachineImageVersion>MachineImageVersion</a>)</p><p><p>ExpirableVersion contains a version and an expiration date.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the version identifier.</p></td></tr><tr><td><code>expirationDate</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>ExpirationDate defines the time at which this version expires.</p></td></tr><tr><td><code>classification</code></br><em><a href=#core.gardener.cloud/v1beta1.VersionClassification>VersionClassification</a></em></td><td><em>(Optional)</em><p>Classification defines the state of a version (preview, supported, deprecated)</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ExposureClassScheduling>ExposureClassScheduling</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ExposureClass>ExposureClass</a>)</p><p><p>ExposureClassScheduling holds information to select applicable Seed’s for ExposureClass usage.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector is an optional label selector for Seed’s which are suitable to use the ExposureClass.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on Seed clusters.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Extension>Extension</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Extension contains type and provider information for Shoot extensions.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of the extension resource.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to extension resource.</p></td></tr><tr><td><code>disabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>Disabled allows to disable extensions that were marked as ‘globally enabled’ by Gardener administrators.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ExtensionResourceState>ExtensionResourceState</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStateSpec>ShootStateSpec</a>)</p><p><p>ExtensionResourceState contains the kind of the extension custom resource and its last observed state in the Shoot’s
namespace on the Seed cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind (type) of the extension custom resource</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><em>(Optional)</em><p>Name of the extension custom resource</p></td></tr><tr><td><code>purpose</code></br><em>string</em></td><td><em>(Optional)</em><p>Purpose of the extension custom resource</p></td></tr><tr><td><code>state</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>State of the extension resource</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.NamedResourceReference>[]NamedResourceReference</a></em></td><td><em>(Optional)</em><p>Resources holds a list of named resource references that can be referred to in the state by their names.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.FailureTolerance>FailureTolerance</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.HighAvailability>HighAvailability</a>)</p><p><p>FailureTolerance describes information about failure tolerance level of a highly available resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=#core.gardener.cloud/v1beta1.FailureToleranceType>FailureToleranceType</a></em></td><td><p>Type specifies the type of failure that the highly available resource can tolerate</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.FailureToleranceType>FailureToleranceType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.FailureTolerance>FailureTolerance</a>)</p><p><p>FailureToleranceType specifies the type of failure that a highly available
shoot control plane that can tolerate.</p></p><h3 id=core.gardener.cloud/v1beta1.Gardener>Gardener</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedStatus>SeedStatus</a>,
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>Gardener holds the information about the Gardener version that operated a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></br><em>string</em></td><td><p>ID is the Docker container id of the Gardener which last acted on a resource.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the hostname (pod name) of the Gardener which last acted on a resource.</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the version of the Gardener which last acted on a resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.GardenerResourceData>GardenerResourceData</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStateSpec>ShootStateSpec</a>)</p><p><p>GardenerResourceData holds the data which is used to generate resources, deployed in the Shoot’s control plane.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the object required to generate resources</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type of the object</p></td></tr><tr><td><code>data</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Data contains the payload required to generate resources</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels are labels of the object</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Hibernation>Hibernation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Hibernation contains information whether the Shoot is suspended or not.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot’s desired state is to be hibernated.
If it is false or nil, the Shoot’s desired state is to be awakened.</p></td></tr><tr><td><code>schedules</code></br><em><a href=#core.gardener.cloud/v1beta1.HibernationSchedule>[]HibernationSchedule</a></em></td><td><em>(Optional)</em><p>Schedules determine the hibernation schedules.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.HibernationSchedule>HibernationSchedule</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Hibernation>Hibernation</a>)</p><p><p>HibernationSchedule determines the hibernation schedule of a Shoot.
A Shoot will be regularly hibernated at each start time and will be woken up at each end time.
Start or End can be omitted, though at least one of each has to be specified.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>start</code></br><em>string</em></td><td><em>(Optional)</em><p>Start is a Cron spec at which time a Shoot will be hibernated.</p></td></tr><tr><td><code>end</code></br><em>string</em></td><td><em>(Optional)</em><p>End is a Cron spec at which time a Shoot will be woken up.</p></td></tr><tr><td><code>location</code></br><em>string</em></td><td><em>(Optional)</em><p>Location is the time location in which both start and and shall be evaluated.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.HighAvailability>HighAvailability</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ControlPlane>ControlPlane</a>)</p><p><p>HighAvailability specifies the configuration settings for high availability for a resource. Typical
usages could be to configure HA for shoot control plane or for seed system components.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>failureTolerance</code></br><em><a href=#core.gardener.cloud/v1beta1.FailureTolerance>FailureTolerance</a></em></td><td><p>FailureTolerance holds information about failure tolerance level of a highly available resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.HorizontalPodAutoscalerConfig>HorizontalPodAutoscalerConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeControllerManagerConfig>KubeControllerManagerConfig</a>)</p><p><p>HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.
Note: Descriptions were taken from the Kubernetes documentation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>cpuInitializationPeriod</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>The period after which a ready pod transition is considered to be the first.</p></td></tr><tr><td><code>downscaleStabilization</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>The configurable window at which the controller will choose the highest recommendation for autoscaling.</p></td></tr><tr><td><code>initialReadinessDelay</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>The configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.</p></td></tr><tr><td><code>syncPeriod</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>The period for syncing the number of pods in horizontal pod autoscaler.</p></td></tr><tr><td><code>tolerance</code></br><em>float64</em></td><td><em>(Optional)</em><p>The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.IPFamily>IPFamily
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Networking>Networking</a>,
<a href=#core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</a>)</p><p><p>IPFamily is a type for specifying an IP protocol version to use in Gardener clusters.</p></p><h3 id=core.gardener.cloud/v1beta1.Ingress>Ingress</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>Ingress configures the Ingress specific settings of the Seed cluster</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>domain</code></br><em>string</em></td><td><p>Domain specifies the IngressDomain of the Seed cluster pointing to the ingress controller endpoint. It will be used
to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable.</p></td></tr><tr><td><code>controller</code></br><em><a href=#core.gardener.cloud/v1beta1.IngressController>IngressController</a></em></td><td><p>Controller configures a Gardener managed Ingress Controller listening on the ingressDomain</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.IngressController>IngressController</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Ingress>Ingress</a>)</p><p><p>IngressController enables a Gardener managed Ingress Controller listening on the ingressDomain</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind defines which kind of IngressController to use, for example <code>nginx</code></p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig specifies infrastructure specific configuration for the ingressController</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>KubeAPIServerConfig contains configuration settings for the kube-apiserver.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>KubernetesConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</a></em></td><td><p>(Members of <code>KubernetesConfig</code> are embedded into this type.)</p></td></tr><tr><td><code>admissionPlugins</code></br><em><a href=#core.gardener.cloud/v1beta1.AdmissionPlugin>[]AdmissionPlugin</a></em></td><td><em>(Optional)</em><p>AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding
configuration.</p></td></tr><tr><td><code>apiAudiences</code></br><em>[]string</em></td><td><em>(Optional)</em><p>APIAudiences are the identifiers of the API. The service account token authenticator will
validate that tokens used against the API are bound to at least one of these audiences.
Defaults to [“kubernetes”].</p></td></tr><tr><td><code>auditConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.AuditConfig>AuditConfig</a></em></td><td><em>(Optional)</em><p>AuditConfig contains configuration settings for the audit of the kube-apiserver.</p></td></tr><tr><td><code>enableBasicAuthentication</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.</p><p>Deprecated: basic authentication has been removed in Kubernetes v1.19+. The field is no-op and will be removed in a future version.</p></td></tr><tr><td><code>oidcConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.OIDCConfig>OIDCConfig</a></em></td><td><em>(Optional)</em><p>OIDCConfig contains configuration settings for the OIDC provider.</p></td></tr><tr><td><code>runtimeConfig</code></br><em>map[string]bool</em></td><td><em>(Optional)</em><p>RuntimeConfig contains information about enabled or disabled APIs.</p></td></tr><tr><td><code>serviceAccountConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.ServiceAccountConfig>ServiceAccountConfig</a></em></td><td><em>(Optional)</em><p>ServiceAccountConfig contains configuration settings for the service account handling
of the kube-apiserver.</p></td></tr><tr><td><code>watchCacheSizes</code></br><em><a href=#core.gardener.cloud/v1beta1.WatchCacheSizes>WatchCacheSizes</a></em></td><td><em>(Optional)</em><p>WatchCacheSizes contains configuration of the API server’s watch cache sizes.
Configuring these flags might be useful for large-scale Shoot clusters with a lot of parallel update requests
and a lot of watching controllers (e.g. large ManagedSeed clusters). When the API server’s watch cache’s
capacity is too small to cope with the amount of update requests and watchers for a particular resource, it
might happen that controller watches are permanently stopped with <code>too old resource version</code> errors.
Starting from kubernetes v1.19, the API server’s watch cache size is adapted dynamically and setting the watch
cache size flags will have no effect, except when setting it to 0 (which disables the watch cache).</p></td></tr><tr><td><code>requests</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeAPIServerRequests>KubeAPIServerRequests</a></em></td><td><em>(Optional)</em><p>Requests contains configuration for request-specific settings for the kube-apiserver.</p></td></tr><tr><td><code>enableAnonymousAuthentication</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableAnonymousAuthentication defines whether anonymous requests to the secure port
of the API server should be allowed (flag <code>--anonymous-auth</code>).
See: <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></p></td></tr><tr><td><code>eventTTL</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EventTTL controls the amount of time to retain events.
Defaults to 1h.</p></td></tr><tr><td><code>logging</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeAPIServerLogging>KubeAPIServerLogging</a></em></td><td><em>(Optional)</em><p>Logging contains configuration for the log level and HTTP access logs.</p></td></tr><tr><td><code>defaultNotReadyTolerationSeconds</code></br><em>int64</em></td><td><em>(Optional)</em><p>DefaultNotReadyTolerationSeconds indicates the tolerationSeconds of the toleration for notReady:NoExecute
that is added by default to every pod that does not already have such a toleration (flag <code>--default-not-ready-toleration-seconds</code>).
The field has effect only when the <code>DefaultTolerationSeconds</code> admission plugin is enabled.
Defaults to 300.</p></td></tr><tr><td><code>defaultUnreachableTolerationSeconds</code></br><em>int64</em></td><td><em>(Optional)</em><p>DefaultUnreachableTolerationSeconds indicates the tolerationSeconds of the toleration for unreachable:NoExecute
that is added by default to every pod that does not already have such a toleration (flag <code>--default-unreachable-toleration-seconds</code>).
The field has effect only when the <code>DefaultTolerationSeconds</code> admission plugin is enabled.
Defaults to 300.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeAPIServerLogging>KubeAPIServerLogging</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>KubeAPIServerLogging contains configuration for the logs level and http access logs</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>verbosity</code></br><em>int32</em></td><td><em>(Optional)</em><p>Verbosity is the kube-apiserver log verbosity level
Defaults to 2.</p></td></tr><tr><td><code>httpAccessVerbosity</code></br><em>int32</em></td><td><em>(Optional)</em><p>HTTPAccessVerbosity is the kube-apiserver access logs level</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeAPIServerRequests>KubeAPIServerRequests</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>KubeAPIServerRequests contains configuration for request-specific settings for the kube-apiserver.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>maxNonMutatingInflight</code></br><em>int32</em></td><td><em>(Optional)</em><p>MaxNonMutatingInflight is the maximum number of non-mutating requests in flight at a given time. When the server
exceeds this, it rejects requests.</p></td></tr><tr><td><code>maxMutatingInflight</code></br><em>int32</em></td><td><em>(Optional)</em><p>MaxMutatingInflight is the maximum number of mutating requests in flight at a given time. When the server
exceeds this, it rejects requests.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeControllerManagerConfig>KubeControllerManagerConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>KubernetesConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</a></em></td><td><p>(Members of <code>KubernetesConfig</code> are embedded into this type.)</p></td></tr><tr><td><code>horizontalPodAutoscaler</code></br><em><a href=#core.gardener.cloud/v1beta1.HorizontalPodAutoscalerConfig>HorizontalPodAutoscalerConfig</a></em></td><td><em>(Optional)</em><p>HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.</p></td></tr><tr><td><code>nodeCIDRMaskSize</code></br><em>int32</em></td><td><em>(Optional)</em><p>NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24). This field is immutable.</p></td></tr><tr><td><code>podEvictionTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>PodEvictionTimeout defines the grace period for deleting pods on failed nodes. Defaults to 2m.</p><p>Deprecated: The corresponding kube-controller-manager flag <code>--pod-eviction-timeout</code> is deprecated
in favor of the kube-apiserver flags <code>--default-not-ready-toleration-seconds</code> and <code>--default-unreachable-toleration-seconds</code>.
The <code>--pod-eviction-timeout</code> flag does not have effect when the taint besed eviction is enabled. The taint
based eviction is beta (enabled by default) since Kubernetes 1.13 and GA since Kubernetes 1.18. Hence,
instead of setting this field, set the <code>spec.kubernetes.kubeAPIServer.defaultNotReadyTolerationSeconds</code> and
<code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code>.</p></td></tr><tr><td><code>nodeMonitorGracePeriod</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>NodeMonitorGracePeriod defines the grace period before an unresponsive node is marked unhealthy.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeProxyConfig>KubeProxyConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>KubeProxyConfig contains configuration settings for the kube-proxy.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>KubernetesConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</a></em></td><td><p>(Members of <code>KubernetesConfig</code> are embedded into this type.)</p></td></tr><tr><td><code>mode</code></br><em><a href=#core.gardener.cloud/v1beta1.ProxyMode>ProxyMode</a></em></td><td><em>(Optional)</em><p>Mode specifies which proxy mode to use.
defaults to IPTables.</p></td></tr><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>Enabled indicates whether kube-proxy should be deployed or not.
Depending on the networking extensions switching kube-proxy off might be rejected. Consulting the respective documentation of the used networking extension is recommended before using this field.
defaults to true if not specified.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeSchedulerConfig>KubeSchedulerConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>KubeSchedulerConfig contains configuration settings for the kube-scheduler.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>KubernetesConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</a></em></td><td><p>(Members of <code>KubernetesConfig</code> are embedded into this type.)</p></td></tr><tr><td><code>kubeMaxPDVols</code></br><em>string</em></td><td><em>(Optional)</em><p>KubeMaxPDVols allows to configure the <code>KUBE_MAX_PD_VOLS</code> environment variable for the kube-scheduler.
Please find more information here: <a href=https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits>https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits</a>
Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware
of all the side-effects and consequences when changing it.</p></td></tr><tr><td><code>profile</code></br><em><a href=#core.gardener.cloud/v1beta1.SchedulingProfile>SchedulingProfile</a></em></td><td><em>(Optional)</em><p>Profile configures the scheduling profile for the cluster.
If not specified, the used profile is “balanced” (provides the default kube-scheduler behavior).</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>,
<a href=#core.gardener.cloud/v1beta1.WorkerKubernetes>WorkerKubernetes</a>)</p><p><p>KubeletConfig contains configuration settings for the kubelet.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>KubernetesConfig</code></br><em><a href=#core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</a></em></td><td><p>(Members of <code>KubernetesConfig</code> are embedded into this type.)</p></td></tr><tr><td><code>cpuCFSQuota</code></br><em>bool</em></td><td><em>(Optional)</em><p>CPUCFSQuota allows you to disable/enable CPU throttling for Pods.</p></td></tr><tr><td><code>cpuManagerPolicy</code></br><em>string</em></td><td><em>(Optional)</em><p>CPUManagerPolicy allows to set alternative CPU management policies (default: none).</p></td></tr><tr><td><code>evictionHard</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigEviction>KubeletConfigEviction</a></em></td><td><em>(Optional)</em><p>EvictionHard describes a set of eviction thresholds (e.g. memory.available&lt;1Gi) that if met would trigger a Pod eviction.
Default:
memory.available: “100Mi/1Gi/5%”
nodefs.available: “5%”
nodefs.inodesFree: “5%”
imagefs.available: “5%”
imagefs.inodesFree: “5%”</p></td></tr><tr><td><code>evictionMaxPodGracePeriod</code></br><em>int32</em></td><td><em>(Optional)</em><p>EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met.
Default: 90</p></td></tr><tr><td><code>evictionMinimumReclaim</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigEvictionMinimumReclaim>KubeletConfigEvictionMinimumReclaim</a></em></td><td><em>(Optional)</em><p>EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure.
Default: 0 for each resource</p></td></tr><tr><td><code>evictionPressureTransitionPeriod</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition.
Default: 4m0s</p></td></tr><tr><td><code>evictionSoft</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigEviction>KubeletConfigEviction</a></em></td><td><em>(Optional)</em><p>EvictionSoft describes a set of eviction thresholds (e.g. memory.available&lt;1.5Gi) that if met over a corresponding grace period would trigger a Pod eviction.
Default:
memory.available: “200Mi/1.5Gi/10%”
nodefs.available: “10%”
nodefs.inodesFree: “10%”
imagefs.available: “10%”
imagefs.inodesFree: “10%”</p></td></tr><tr><td><code>evictionSoftGracePeriod</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigEvictionSoftGracePeriod>KubeletConfigEvictionSoftGracePeriod</a></em></td><td><em>(Optional)</em><p>EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction.
Default:
memory.available: 1m30s
nodefs.available: 1m30s
nodefs.inodesFree: 1m30s
imagefs.available: 1m30s
imagefs.inodesFree: 1m30s</p></td></tr><tr><td><code>maxPods</code></br><em>int32</em></td><td><em>(Optional)</em><p>MaxPods is the maximum number of Pods that are allowed by the Kubelet.
Default: 110</p></td></tr><tr><td><code>podPidsLimit</code></br><em>int64</em></td><td><em>(Optional)</em><p>PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.</p></td></tr><tr><td><code>imagePullProgressDeadline</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled.
Default: 1m</p></td></tr><tr><td><code>failSwapOn</code></br><em>bool</em></td><td><em>(Optional)</em><p>FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).</p></td></tr><tr><td><code>kubeReserved</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigReserved>KubeletConfigReserved</a></em></td><td><em>(Optional)</em><p>KubeReserved is the configuration for resources reserved for kubernetes node components (mainly kubelet and container runtime).
When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.
Default: cpu=80m,memory=1Gi,pid=20k</p></td></tr><tr><td><code>systemReserved</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfigReserved>KubeletConfigReserved</a></em></td><td><em>(Optional)</em><p>SystemReserved is the configuration for resources reserved for system processes not managed by kubernetes (e.g. journald).
When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.</p></td></tr><tr><td><code>imageGCHighThresholdPercent</code></br><em>int32</em></td><td><em>(Optional)</em><p>ImageGCHighThresholdPercent describes the percent of the disk usage which triggers image garbage collection.
Default: 50</p></td></tr><tr><td><code>imageGCLowThresholdPercent</code></br><em>int32</em></td><td><em>(Optional)</em><p>ImageGCLowThresholdPercent describes the percent of the disk to which garbage collection attempts to free.
Default: 40</p></td></tr><tr><td><code>serializeImagePulls</code></br><em>bool</em></td><td><em>(Optional)</em><p>SerializeImagePulls describes whether the images are pulled one at a time.
Default: true</p></td></tr><tr><td><code>registryPullQPS</code></br><em>int32</em></td><td><em>(Optional)</em><p>RegistryPullQPS is the limit of registry pulls per second. The value must not be a negative number.
Setting it to 0 means no limit.
Default: 5</p></td></tr><tr><td><code>registryBurst</code></br><em>int32</em></td><td><em>(Optional)</em><p>RegistryBurst is the maximum size of bursty pulls, temporarily allows pulls to burst to this number,
while still not exceeding registryPullQPS. The value must not be a negative number.
Only used if registryPullQPS is greater than 0.
Default: 10</p></td></tr><tr><td><code>seccompDefault</code></br><em>bool</em></td><td><em>(Optional)</em><p>SeccompDefault enables the use of <code>RuntimeDefault</code> as the default seccomp profile for all workloads.
This requires the corresponding SeccompDefault feature gate to be enabled as well.
This field is only available for Kubernetes v1.25 or later.</p></td></tr><tr><td><code>containerLogMaxSize</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>A quantity defines the maximum size of the container log file before it is rotated. For example: “5Mi” or “256Ki”.
Default: 100Mi</p></td></tr><tr><td><code>containerLogMaxFiles</code></br><em>int32</em></td><td><em>(Optional)</em><p>Maximum number of container log files that can be present for a container.</p></td></tr><tr><td><code>protectKernelDefaults</code></br><em>bool</em></td><td><em>(Optional)</em><p>ProtectKernelDefaults ensures that the kernel tunables are equal to the kubelet defaults.
Defaults to true for Kubernetes v1.26 or later.</p></td></tr><tr><td><code>streamingConnectionIdleTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>StreamingConnectionIdleTimeout is the maximum time a streaming connection can be idle before the connection is automatically closed.
This field cannot be set lower than “30s” or greater than “4h”.
Default:
“4h” for Kubernetes &lt; v1.26.
“5m” for Kubernetes >= v1.26.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeletConfigEviction>KubeletConfigEviction</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a>)</p><p><p>KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>memoryAvailable</code></br><em>string</em></td><td><em>(Optional)</em><p>MemoryAvailable is the threshold for the free memory on the host server.</p></td></tr><tr><td><code>imageFSAvailable</code></br><em>string</em></td><td><em>(Optional)</em><p>ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).</p></td></tr><tr><td><code>imageFSInodesFree</code></br><em>string</em></td><td><em>(Optional)</em><p>ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.</p></td></tr><tr><td><code>nodeFSAvailable</code></br><em>string</em></td><td><em>(Optional)</em><p>NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).</p></td></tr><tr><td><code>nodeFSInodesFree</code></br><em>string</em></td><td><em>(Optional)</em><p>NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeletConfigEvictionMinimumReclaim>KubeletConfigEvictionMinimumReclaim</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a>)</p><p><p>KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>memoryAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>MemoryAvailable is the threshold for the memory reclaim on the host server.</p></td></tr><tr><td><code>imageFSAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).</p></td></tr><tr><td><code>imageFSInodesFree</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.</p></td></tr><tr><td><code>nodeFSAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).</p></td></tr><tr><td><code>nodeFSInodesFree</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeletConfigEvictionSoftGracePeriod>KubeletConfigEvictionSoftGracePeriod</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a>)</p><p><p>KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>memoryAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.</p></td></tr><tr><td><code>imageFSAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.</p></td></tr><tr><td><code>imageFSInodesFree</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.</p></td></tr><tr><td><code>nodeFSAvailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.</p></td></tr><tr><td><code>nodeFSInodesFree</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubeletConfigReserved>KubeletConfigReserved</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a>)</p><p><p>KubeletConfigReserved contains reserved resources for daemons</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>cpu</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>CPU is the reserved cpu.</p></td></tr><tr><td><code>memory</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>Memory is the reserved memory.</p></td></tr><tr><td><code>ephemeralStorage</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>EphemeralStorage is the reserved ephemeral-storage.</p></td></tr><tr><td><code>pid</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>PID is the reserved process-ids.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Kubernetes contains the version and configuration variables for the Shoot control plane.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>allowPrivilegedContainers</code></br><em>bool</em></td><td><em>(Optional)</em><p>AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot.
Defaults to true for Kubernetes versions below v1.25. Unusable for Kubernetes versions v1.25 and higher.</p></td></tr><tr><td><code>clusterAutoscaler</code></br><em><a href=#core.gardener.cloud/v1beta1.ClusterAutoscaler>ClusterAutoscaler</a></em></td><td><em>(Optional)</em><p>ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.</p></td></tr><tr><td><code>kubeAPIServer</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a></em></td><td><em>(Optional)</em><p>KubeAPIServer contains configuration settings for the kube-apiserver.</p></td></tr><tr><td><code>kubeControllerManager</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeControllerManagerConfig>KubeControllerManagerConfig</a></em></td><td><em>(Optional)</em><p>KubeControllerManager contains configuration settings for the kube-controller-manager.</p></td></tr><tr><td><code>kubeScheduler</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeSchedulerConfig>KubeSchedulerConfig</a></em></td><td><em>(Optional)</em><p>KubeScheduler contains configuration settings for the kube-scheduler.</p></td></tr><tr><td><code>kubeProxy</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeProxyConfig>KubeProxyConfig</a></em></td><td><em>(Optional)</em><p>KubeProxy contains configuration settings for the kube-proxy.</p></td></tr><tr><td><code>kubelet</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a></em></td><td><em>(Optional)</em><p>Kubelet contains configuration settings for the kubelet.</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the semantic Kubernetes version to use for the Shoot cluster.</p></td></tr><tr><td><code>verticalPodAutoscaler</code></br><em><a href=#core.gardener.cloud/v1beta1.VerticalPodAutoscaler>VerticalPodAutoscaler</a></em></td><td><em>(Optional)</em><p>VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.</p></td></tr><tr><td><code>enableStaticTokenKubeconfig</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableStaticTokenKubeconfig indicates whether static token kubeconfig secret will be created for the Shoot cluster.
Defaults to true for Shoots with Kubernetes versions &lt; 1.26. Defaults to false for Shoots with Kubernetes versions >= 1.26.
Starting Kubernetes 1.27 the field will be locked to false.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubernetesConfig>KubernetesConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>,
<a href=#core.gardener.cloud/v1beta1.KubeControllerManagerConfig>KubeControllerManagerConfig</a>,
<a href=#core.gardener.cloud/v1beta1.KubeProxyConfig>KubeProxyConfig</a>,
<a href=#core.gardener.cloud/v1beta1.KubeSchedulerConfig>KubeSchedulerConfig</a>,
<a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a>)</p><p><p>KubernetesConfig contains common configuration fields for the control plane components.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>featureGates</code></br><em>map[string]bool</em></td><td><em>(Optional)</em><p>FeatureGates contains information about enabled feature gates.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubernetesDashboard>KubernetesDashboard</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Addons>Addons</a>)</p><p><p>KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Addon</code></br><em><a href=#core.gardener.cloud/v1beta1.Addon>Addon</a></em></td><td><p>(Members of <code>Addon</code> are embedded into this type.)</p></td></tr><tr><td><code>authenticationMode</code></br><em>string</em></td><td><em>(Optional)</em><p>AuthenticationMode defines the authentication mode for the kubernetes-dashboard.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.KubernetesSettings>KubernetesSettings</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>)</p><p><p>KubernetesSettings contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>versions</code></br><em><a href=#core.gardener.cloud/v1beta1.ExpirableVersion>[]ExpirableVersion</a></em></td><td><em>(Optional)</em><p>Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.LastError>LastError</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupBucketStatus>BackupBucketStatus</a>,
<a href=#core.gardener.cloud/v1beta1.BackupEntryStatus>BackupEntryStatus</a>,
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>LastError indicates the last occurred error for an operation on a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></br><em>string</em></td><td><p>A human readable message indicating details about the last error.</p></td></tr><tr><td><code>taskID</code></br><em>string</em></td><td><em>(Optional)</em><p>ID of the task which caused this last error</p></td></tr><tr><td><code>codes</code></br><em><a href=#core.gardener.cloud/v1beta1.ErrorCode>[]ErrorCode</a></em></td><td><em>(Optional)</em><p>Well-defined error codes of the last error(s).</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>Last time the error was reported</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.LastMaintenance>LastMaintenance</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>LastMaintenance holds information about a maintenance operation on the Shoot.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></br><em>string</em></td><td><p>A human-readable message containing details about the operations performed in the last maintenance.</p></td></tr><tr><td><code>triggeredTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>TriggeredTime is the time when maintenance was triggered.</p></td></tr><tr><td><code>state</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperationState>LastOperationState</a></em></td><td><p>Status of the last maintenance operation, one of Processing, Succeeded, Error.</p></td></tr><tr><td><code>failureReason</code></br><em>string</em></td><td><em>(Optional)</em><p>FailureReason holds the information about the last maintenance operation failure reason.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.LastOperation>LastOperation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.BackupBucketStatus>BackupBucketStatus</a>,
<a href=#core.gardener.cloud/v1beta1.BackupEntryStatus>BackupEntryStatus</a>,
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>LastOperation indicates the type and the state of the last operation, along with a description
message and a progress indicator.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></br><em>string</em></td><td><p>A human readable message indicating details about the last operation.</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the operation state transitioned from one to another.</p></td></tr><tr><td><code>progress</code></br><em>int32</em></td><td><p>The progress in percentage (0-100) of the last operation.</p></td></tr><tr><td><code>state</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperationState>LastOperationState</a></em></td><td><p>Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.</p></td></tr><tr><td><code>type</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperationType>LastOperationType</a></em></td><td><p>Type of the last operation, one of Create, Reconcile, Delete, Migrate, Restore.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.LastOperationState>LastOperationState
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.LastMaintenance>LastMaintenance</a>,
<a href=#core.gardener.cloud/v1beta1.LastOperation>LastOperation</a>)</p><p><p>LastOperationState is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.LastOperationType>LastOperationType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.LastOperation>LastOperation</a>)</p><p><p>LastOperationType is a string alias.</p></p><h3 id=core.gardener.cloud/v1beta1.Machine>Machine</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>Machine contains information about the machine type and image.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the machine type of the worker group.</p></td></tr><tr><td><code>image</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootMachineImage>ShootMachineImage</a></em></td><td><em>(Optional)</em><p>Image holds information about the machine image to use for all nodes of this pool. It will default to the
latest version of the first image stated in the referenced CloudProfile if no value has been provided.</p></td></tr><tr><td><code>architecture</code></br><em>string</em></td><td><em>(Optional)</em><p>Architecture is CPU architecture of machines in this worker pool.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MachineControllerManagerSettings>MachineControllerManagerSettings</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>machineDrainTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineDrainTimeout is the period after which machine is forcefully deleted.</p></td></tr><tr><td><code>machineHealthTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineHealthTimeout is the period after which machine is declared failed.</p></td></tr><tr><td><code>machineCreationTimeout</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineCreationTimeout is the period after which creation of the machine is declared failed.</p></td></tr><tr><td><code>maxEvictRetries</code></br><em>int32</em></td><td><em>(Optional)</em><p>MaxEvictRetries are the number of eviction retries on a pod after which drain is declared failed, and forceful deletion is triggered.</p></td></tr><tr><td><code>nodeConditions</code></br><em>[]string</em></td><td><em>(Optional)</em><p>NodeConditions are the set of conditions if set to true for the period of MachineHealthTimeout, machine will be declared failed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MachineImage>MachineImage</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>)</p><p><p>MachineImage defines the name and multiple versions of the machine image in any environment.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the image.</p></td></tr><tr><td><code>versions</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineImageVersion>[]MachineImageVersion</a></em></td><td><p>Versions contains versions, expiration dates and container runtimes of the machine image</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MachineImageVersion>MachineImageVersion</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.MachineImage>MachineImage</a>)</p><p><p>MachineImageVersion is an expirable version with list of supported container runtimes and interfaces</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ExpirableVersion</code></br><em><a href=#core.gardener.cloud/v1beta1.ExpirableVersion>ExpirableVersion</a></em></td><td><p>(Members of <code>ExpirableVersion</code> are embedded into this type.)</p></td></tr><tr><td><code>cri</code></br><em><a href=#core.gardener.cloud/v1beta1.CRI>[]CRI</a></em></td><td><em>(Optional)</em><p>CRI list of supported container runtime and interfaces supported by this version</p></td></tr><tr><td><code>architectures</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Architectures is the list of CPU architectures of the machine image in this version.</p></td></tr><tr><td><code>kubeletVersionConstraint</code></br><em>string</em></td><td><em>(Optional)</em><p>KubeletVersionConstraint is a constraint describing the supported kubelet versions by the machine image in this version.
If the field is not specified, it is assumed that the machine image in this version supports all kubelet versions.
Examples:
- ‘>= 1.26’ - supports only kubelet versions greater than or equal to 1.26
- ‘&lt; 1.26’ - supports only kubelet versions less than 1.26</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MachineType>MachineType</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>)</p><p><p>MachineType contains certain properties of a machine type.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>cpu</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><p>CPU is the number of CPUs for this machine type.</p></td></tr><tr><td><code>gpu</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><p>GPU is the number of GPUs for this machine type.</p></td></tr><tr><td><code>memory</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><p>Memory is the amount of memory for this machine type.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the machine type.</p></td></tr><tr><td><code>storage</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineTypeStorage>MachineTypeStorage</a></em></td><td><em>(Optional)</em><p>Storage is the amount of storage associated with the root volume of this machine type.</p></td></tr><tr><td><code>usable</code></br><em>bool</em></td><td><em>(Optional)</em><p>Usable defines if the machine type can be used for shoot clusters.</p></td></tr><tr><td><code>architecture</code></br><em>string</em></td><td><em>(Optional)</em><p>Architecture is the CPU architecture of this machine type.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MachineTypeStorage>MachineTypeStorage</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.MachineType>MachineType</a>)</p><p><p>MachineTypeStorage is the amount of storage associated with the root volume of this machine type.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></br><em>string</em></td><td><p>Class is the class of the storage type.</p></td></tr><tr><td><code>size</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>StorageSize is the storage size.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of the storage.</p></td></tr><tr><td><code>minSize</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>MinSize is the minimal supported storage size.
This overrides any other common minimum size configuration from <code>spec.volumeTypes[*].minSize</code>.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Maintenance>Maintenance</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Maintenance contains information about the time window for maintenance operations and which
operations should be performed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>autoUpdate</code></br><em><a href=#core.gardener.cloud/v1beta1.MaintenanceAutoUpdate>MaintenanceAutoUpdate</a></em></td><td><em>(Optional)</em><p>AutoUpdate contains information about which constraints should be automatically updated.</p></td></tr><tr><td><code>timeWindow</code></br><em><a href=#core.gardener.cloud/v1beta1.MaintenanceTimeWindow>MaintenanceTimeWindow</a></em></td><td><em>(Optional)</em><p>TimeWindow contains information about the time window for maintenance operations.</p></td></tr><tr><td><code>confineSpecUpdateRollout</code></br><em>bool</em></td><td><em>(Optional)</em><p>ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately.
Instead, they are rolled out during the shoot’s maintenance time window. There is one exception that will trigger
an immediate roll out which is changes to the Spec.Hibernation.Enabled field.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MaintenanceAutoUpdate>MaintenanceAutoUpdate</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Maintenance>Maintenance</a>)</p><p><p>MaintenanceAutoUpdate contains information about which constraints should be automatically updated.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kubernetesVersion</code></br><em>bool</em></td><td><p>KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).</p></td></tr><tr><td><code>machineImageVersion</code></br><em>bool</em></td><td><p>MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.MaintenanceTimeWindow>MaintenanceTimeWindow</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Maintenance>Maintenance</a>)</p><p><p>MaintenanceTimeWindow contains information about the time window for maintenance operations.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>begin</code></br><em>string</em></td><td><p>Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. “220000+0100”.
If not present, a random value will be computed.</p></td></tr><tr><td><code>end</code></br><em>string</em></td><td><p>End is the end of the time window in the format HHMMSS+ZONE, e.g. “220000+0100”.
If not present, the value will be computed based on the “Begin” value.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Monitoring>Monitoring</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Monitoring contains information about the monitoring configuration for the shoot.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>alerting</code></br><em><a href=#core.gardener.cloud/v1beta1.Alerting>Alerting</a></em></td><td><em>(Optional)</em><p>Alerting contains information about the alerting configuration for the shoot cluster.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.NamedResourceReference>NamedResourceReference</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ExtensionResourceState>ExtensionResourceState</a>,
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>NamedResourceReference is a named reference to a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the resource reference.</p></td></tr><tr><td><code>resourceRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#crossversionobjectreference-v1-autoscaling>Kubernetes autoscaling/v1.CrossVersionObjectReference</a></em></td><td><p>ResourceRef is a reference to a resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Networking>Networking</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Networking defines networking parameters for the shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type identifies the type of the networking plugin. This field is immutable.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to network resource.</p></td></tr><tr><td><code>pods</code></br><em>string</em></td><td><em>(Optional)</em><p>Pods is the CIDR of the pod network. This field is immutable.</p></td></tr><tr><td><code>nodes</code></br><em>string</em></td><td><em>(Optional)</em><p>Nodes is the CIDR of the entire node network.
This field is immutable if the feature gate MutableShootSpecNetworkingNodes is disabled.</p></td></tr><tr><td><code>services</code></br><em>string</em></td><td><em>(Optional)</em><p>Services is the CIDR of the service network. This field is immutable.</p></td></tr><tr><td><code>ipFamilies</code></br><em><a href=#core.gardener.cloud/v1beta1.IPFamily>[]IPFamily</a></em></td><td><em>(Optional)</em><p>IPFamilies specifies the IP protocol versions to use for shoot networking. This field is immutable.
See <a href=/docs/gardener/usage/ipv6/>https://github.com/gardener/gardener/blob/master/docs/usage/ipv6.md</a>.
Defaults to [“IPv4”].</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.NginxIngress>NginxIngress</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Addons>Addons</a>)</p><p><p>NginxIngress describes configuration values for the nginx-ingress addon.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Addon</code></br><em><a href=#core.gardener.cloud/v1beta1.Addon>Addon</a></em></td><td><p>(Members of <code>Addon</code> are embedded into this type.)</p></td></tr><tr><td><code>loadBalancerSourceRanges</code></br><em>[]string</em></td><td><em>(Optional)</em><p>LoadBalancerSourceRanges is list of allowed IP sources for NginxIngress</p></td></tr><tr><td><code>config</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Config contains custom configuration for the nginx-ingress-controller configuration.
See <a href=https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options>https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options</a></p></td></tr><tr><td><code>externalTrafficPolicy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceexternaltrafficpolicytype-v1-core>Kubernetes core/v1.ServiceExternalTrafficPolicyType</a></em></td><td><em>(Optional)</em><p>ExternalTrafficPolicy controls the <code>.spec.externalTrafficPolicy</code> value of the load balancer <code>Service</code>
exposing the nginx-ingress. Defaults to <code>Cluster</code>.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.NodeLocalDNS>NodeLocalDNS</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</a>)</p><p><p>NodeLocalDNS contains the settings of the node local DNS components running in the data plane of the Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled indicates whether node local DNS is enabled or not.</p></td></tr><tr><td><code>forceTCPToClusterDNS</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceTCPToClusterDNS indicates whether the connection from the node local DNS to the cluster DNS (Core DNS) will be forced to TCP or not.
Default, if unspecified, is to enforce TCP.</p></td></tr><tr><td><code>forceTCPToUpstreamDNS</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceTCPToUpstreamDNS indicates whether the connection from the node local DNS to the upstream DNS (infrastructure DNS) will be forced to TCP or not.
Default, if unspecified, is to enforce TCP.</p></td></tr><tr><td><code>disableForwardToUpstreamDNS</code></br><em>bool</em></td><td><em>(Optional)</em><p>DisableForwardToUpstreamDNS indicates whether requests from node local DNS to upstream DNS should be disabled.
Default, if unspecified, is to forward requests for external domains to upstream DNS</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.OIDCConfig>OIDCConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>OIDCConfig contains configuration settings for the OIDC provider.
Note: Descriptions were taken from the Kubernetes documentation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>caBundle</code></br><em>string</em></td><td><em>(Optional)</em><p>If set, the OpenID server’s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host’s root CA set will be used.</p></td></tr><tr><td><code>clientAuthentication</code></br><em><a href=#core.gardener.cloud/v1beta1.OpenIDConnectClientAuthentication>OpenIDConnectClientAuthentication</a></em></td><td><em>(Optional)</em><p>ClientAuthentication can optionally contain client configuration used for kubeconfig generation.</p></td></tr><tr><td><code>clientID</code></br><em>string</em></td><td><em>(Optional)</em><p>The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.</p></td></tr><tr><td><code>groupsClaim</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.</p></td></tr><tr><td><code>groupsPrefix</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.</p></td></tr><tr><td><code>issuerURL</code></br><em>string</em></td><td><em>(Optional)</em><p>The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).</p></td></tr><tr><td><code>requiredClaims</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.</p></td></tr><tr><td><code>signingAlgs</code></br><em>[]string</em></td><td><em>(Optional)</em><p>List of allowed JOSE asymmetric signing algorithms. JWTs with a ‘alg’ header value not in this list will be rejected. Values are defined by RFC 7518 <a href=https://tools.ietf.org/html/rfc7518#section-3.1>https://tools.ietf.org/html/rfc7518#section-3.1</a></p></td></tr><tr><td><code>usernameClaim</code></br><em>string</em></td><td><em>(Optional)</em><p>The OpenID claim to use as the user name. Note that claims other than the default (‘sub’) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default “sub”)</p></td></tr><tr><td><code>usernamePrefix</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, all usernames will be prefixed with this value. If not provided, username claims other than ‘email’ are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value ‘-’.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.OpenIDConnectClientAuthentication>OpenIDConnectClientAuthentication</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.OIDCConfig>OIDCConfig</a>)</p><p><p>OpenIDConnectClientAuthentication contains configuration for OIDC clients.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>extraConfig</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Extra configuration added to kubeconfig’s auth-provider.
Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token</p></td></tr><tr><td><code>secret</code></br><em>string</em></td><td><em>(Optional)</em><p>The client Secret for the OpenID Connect client.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ProjectMember>ProjectMember</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ProjectSpec>ProjectSpec</a>)</p><p><p>ProjectMember is a member of a project.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Subject</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#subject-v1-rbac>Kubernetes rbac/v1.Subject</a></em></td><td><p>(Members of <code>Subject</code> are embedded into this type.)</p><p>Subject is representing a user name, an email address, or any other identifier of a user, group, or service
account that has a certain role.</p></td></tr><tr><td><code>role</code></br><em>string</em></td><td><p>Role represents the role of this member.
IMPORTANT: Be aware that this field will be removed in the <code>v1</code> version of this API in favor of the <code>roles</code>
list.
TODO: Remove this field in favor of the <code>roles</code> list in <code>v1</code>.</p></td></tr><tr><td><code>roles</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Roles represents the list of roles of this member.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ProjectPhase>ProjectPhase
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ProjectStatus>ProjectStatus</a>)</p><p><p>ProjectPhase is a label for the condition of a project at the current time.</p></p><h3 id=core.gardener.cloud/v1beta1.ProjectSpec>ProjectSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Project>Project</a>)</p><p><p>ProjectSpec is the specification of a Project.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>createdBy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#subject-v1-rbac>Kubernetes rbac/v1.Subject</a></em></td><td><em>(Optional)</em><p>CreatedBy is a subject representing a user name, an email address, or any other identifier of a user
who created the project. This field is immutable.</p></td></tr><tr><td><code>description</code></br><em>string</em></td><td><em>(Optional)</em><p>Description is a human-readable description of what the project is used for.</p></td></tr><tr><td><code>owner</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#subject-v1-rbac>Kubernetes rbac/v1.Subject</a></em></td><td><em>(Optional)</em><p>Owner is a subject representing a user name, an email address, or any other identifier of a user owning
the project.
IMPORTANT: Be aware that this field will be removed in the <code>v1</code> version of this API in favor of the <code>owner</code>
role. The only way to change the owner will be by moving the <code>owner</code> role. In this API version the only way
to change the owner is to use this field.
TODO: Remove this field in favor of the <code>owner</code> role in <code>v1</code>.</p></td></tr><tr><td><code>purpose</code></br><em>string</em></td><td><em>(Optional)</em><p>Purpose is a human-readable explanation of the project’s purpose.</p></td></tr><tr><td><code>members</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectMember>[]ProjectMember</a></em></td><td><em>(Optional)</em><p>Members is a list of subjects representing a user name, an email address, or any other identifier of a user,
group, or service account that has a certain role.</p></td></tr><tr><td><code>namespace</code></br><em>string</em></td><td><em>(Optional)</em><p>Namespace is the name of the namespace that has been created for the Project object.
A nil value means that Gardener will determine the name of the namespace.
This field is immutable.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectTolerations>ProjectTolerations</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on seed clusters.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ProjectStatus>ProjectStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Project>Project</a>)</p><p><p>ProjectStatus holds the most recently observed status of the project.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this project.</p></td></tr><tr><td><code>phase</code></br><em><a href=#core.gardener.cloud/v1beta1.ProjectPhase>ProjectPhase</a></em></td><td><p>Phase is the current phase of the project.</p></td></tr><tr><td><code>staleSinceTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>StaleSinceTimestamp contains the timestamp when the project was first discovered to be stale/unused.</p></td></tr><tr><td><code>staleAutoDeleteTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>StaleAutoDeleteTimestamp contains the timestamp when the project will be garbage-collected/automatically deleted
because it’s stale/unused.</p></td></tr><tr><td><code>lastActivityTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastActivityTimestamp contains the timestamp from the last activity performed in this project.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ProjectTolerations>ProjectTolerations</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ProjectSpec>ProjectSpec</a>)</p><p><p>ProjectTolerations contains the tolerations for taints on seed clusters.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>defaults</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Defaults contains a list of tolerations that are added to the shoots in this project by default.</p></td></tr><tr><td><code>whitelist</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note
that this list may only be added by users having the <code>spec-tolerations-whitelist</code> verb for project resources.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Provider>Provider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Provider contains provider-specific information that are handed-over to the provider-specific
extension controller.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of the provider. This field is immutable.</p></td></tr><tr><td><code>controlPlaneConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete
definition in the documentation of your provider extension.</p></td></tr><tr><td><code>infrastructureConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete
definition in the documentation of your provider extension.</p></td></tr><tr><td><code>workers</code></br><em><a href=#core.gardener.cloud/v1beta1.Worker>[]Worker</a></em></td><td><p>Workers is a list of worker groups.</p></td></tr><tr><td><code>workersSettings</code></br><em><a href=#core.gardener.cloud/v1beta1.WorkersSettings>WorkersSettings</a></em></td><td><em>(Optional)</em><p>WorkersSettings contains settings for all workers.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ProxyMode>ProxyMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeProxyConfig>KubeProxyConfig</a>)</p><p><p>ProxyMode available in Linux platform: ‘userspace’ (older, going to be EOL), ‘iptables’
(newer, faster), ‘ipvs’ (newest, better in performance and scalability).
As of now only ‘iptables’ and ‘ipvs’ is supported by Gardener.
In Linux platform, if the iptables proxy is selected, regardless of how, but the system’s kernel or iptables versions are
insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to ‘ipvs’,
and the fall back path is firstly iptables and then userspace.</p></p><h3 id=core.gardener.cloud/v1beta1.QuotaSpec>QuotaSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Quota>Quota</a>)</p><p><p>QuotaSpec is the specification of a Quota.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>clusterLifetimeDays</code></br><em>int32</em></td><td><em>(Optional)</em><p>ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.</p></td></tr><tr><td><code>metrics</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Metrics is a list of resources which will be put under constraints.</p></td></tr><tr><td><code>scope</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>Scope is the scope of the Quota object, either ‘project’ or ‘secret’. This field is immutable.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Region>Region</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>)</p><p><p>Region contains certain properties of a region.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is a region name.</p></td></tr><tr><td><code>zones</code></br><em><a href=#core.gardener.cloud/v1beta1.AvailabilityZone>[]AvailabilityZone</a></em></td><td><em>(Optional)</em><p>Zones is a list of availability zones in this region.</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region.
It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt
quality, reliability, access restrictions, etc.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ResourceData>ResourceData</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStateSpec>ShootStateSpec</a>)</p><p><p>ResourceData holds the data of a resource referred to by an extension controller state.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>CrossVersionObjectReference</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#crossversionobjectreference-v1-autoscaling>Kubernetes autoscaling/v1.CrossVersionObjectReference</a></em></td><td><p>(Members of <code>CrossVersionObjectReference</code> are embedded into this type.)</p></td></tr><tr><td><code>data</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Data of the resource</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ResourceWatchCacheSize>ResourceWatchCacheSize</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.WatchCacheSizes>WatchCacheSizes</a>)</p><p><p>ResourceWatchCacheSize contains configuration of the API server’s watch cache size for one specific resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiGroup</code></br><em>string</em></td><td><em>(Optional)</em><p>APIGroup is the API group of the resource for which the watch cache size should be configured.
An unset value is used to specify the legacy core API (e.g. for <code>secrets</code>).</p></td></tr><tr><td><code>resource</code></br><em>string</em></td><td><p>Resource is the name of the resource for which the watch cache size should be configured
(in lowercase plural form, e.g. <code>secrets</code>).</p></td></tr><tr><td><code>size</code></br><em>int32</em></td><td><p>CacheSize specifies the watch cache size that should be configured for the specified resource.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SSHAccess>SSHAccess</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.WorkersSettings>WorkersSettings</a>)</p><p><p>SSHAccess contains settings regarding ssh access to the worker nodes.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled indicates whether the SSH access to the worker nodes is ensured to be enabled or disabled in systemd.
Defaults to true.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SchedulingProfile>SchedulingProfile
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeSchedulerConfig>KubeSchedulerConfig</a>)</p><p><p>SchedulingProfile is a string alias used for scheduling profile values.</p></p><h3 id=core.gardener.cloud/v1beta1.SecretBindingProvider>SecretBindingProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SecretBinding>SecretBinding</a>)</p><p><p>SecretBindingProvider defines the provider type of the SecretBinding.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the type of the provider.</p><p>For backwards compatibility, the field can contain multiple providers separated by a comma.
However the usage of single SecretBinding (hence Secret) for different cloud providers is strongly discouraged.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedBackup>SeedBackup</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedBackup contains the object store configuration for backups for shoot (currently only etcd).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>provider</code></br><em>string</em></td><td><p>Provider is a provider name. This field is immutable.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to BackupBucket resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><em>(Optional)</em><p>Region is a region name. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a Secret object containing the cloud provider credentials for
the object store where backups should be stored. It should have enough privileges to manipulate
the objects as well as buckets.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedDNS>SeedDNS</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedDNS contains DNS-relevant information about this seed cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ingressDomain</code></br><em>string</em></td><td><em>(Optional)</em><p>IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used
to construct ingress URLs for system applications running in Shoot clusters. This field is immutable.
Deprecated: This field is deprecated and will be removed in a future version of Gardener. Use spec.ingress.domain instead.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedDNSProvider>SeedDNSProvider</a></em></td><td><em>(Optional)</em><p>Provider configures a DNSProvider</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedDNSProvider>SeedDNSProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedDNS>SeedDNS</a>)</p><p><p>SeedDNSProvider configures a DNSProvider for Seeds</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type describes the type of the dns-provider, for example <code>aws-route53</code></p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a Secret object containing cloud provider credentials used for registering external domains.</p></td></tr><tr><td><code>domains</code></br><em><a href=#core.gardener.cloud/v1beta1.DNSIncludeExclude>DNSIncludeExclude</a></em></td><td><em>(Optional)</em><p>Domains contains information about which domains shall be included/excluded for this provider.
Deprecated: This field is deprecated and will be removed in a future version of Gardener.</p></td></tr><tr><td><code>zones</code></br><em><a href=#core.gardener.cloud/v1beta1.DNSIncludeExclude>DNSIncludeExclude</a></em></td><td><em>(Optional)</em><p>Zones contains information about which hosted zones shall be included/excluded for this provider.
Deprecated: This field is deprecated and will be removed in a future version of Gardener.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>nodes</code></br><em>string</em></td><td><em>(Optional)</em><p>Nodes is the CIDR of the node network. This field is immutable.</p></td></tr><tr><td><code>pods</code></br><em>string</em></td><td><p>Pods is the CIDR of the pod network. This field is immutable.</p></td></tr><tr><td><code>services</code></br><em>string</em></td><td><p>Services is the CIDR of the service network. This field is immutable.</p></td></tr><tr><td><code>shootDefaults</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootNetworks>ShootNetworks</a></em></td><td><em>(Optional)</em><p>ShootDefaults contains the default networks CIDRs for shoots.</p></td></tr><tr><td><code>blockCIDRs</code></br><em>[]string</em></td><td><em>(Optional)</em><p>BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running
in the seed cluster.</p></td></tr><tr><td><code>ipFamilies</code></br><em><a href=#core.gardener.cloud/v1beta1.IPFamily>[]IPFamily</a></em></td><td><em>(Optional)</em><p>IPFamilies specifies the IP protocol versions to use for seed networking. This field is immutable.
See <a href=/docs/gardener/usage/ipv6/>https://github.com/gardener/gardener/blob/master/docs/usage/ipv6.md</a>.
Defaults to [“IPv4”].</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedProvider>SeedProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedProvider defines the provider-specific information of this Seed cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type is the name of the provider.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the configuration passed to Seed resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is a name of a region.</p></td></tr><tr><td><code>zones</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Zones is the list of availability zones the seed cluster is deployed to.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>,
<a href=#core.gardener.cloud/v1beta1.ExposureClassScheduling>ExposureClassScheduling</a>,
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>SeedSelector contains constraints for selecting seed to be usable for shoots using a profile</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>LabelSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>(Members of <code>LabelSelector</code> are embedded into this type.)</p><em>(Optional)</em><p>LabelSelector is optional and can be used to select seeds by their label settings</p></td></tr><tr><td><code>providerTypes</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Providers is optional and can be used by restricting seeds by their provider type. ‘*’ can be used to enable seeds regardless of their provider type.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdog>SeedSettingDependencyWatchdog</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingDependencyWatchdog controls the dependency-watchdog settings for the seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>endpoint</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdogEndpoint>SeedSettingDependencyWatchdogEndpoint</a></em></td><td><em>(Optional)</em><p>Endpoint controls the endpoint settings for the dependency-watchdog for the seed.</p></td></tr><tr><td><code>probe</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdogProbe>SeedSettingDependencyWatchdogProbe</a></em></td><td><em>(Optional)</em><p>Probe controls the probe settings for the dependency-watchdog for the seed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdogEndpoint>SeedSettingDependencyWatchdogEndpoint</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdog>SeedSettingDependencyWatchdog</a>)</p><p><p>SeedSettingDependencyWatchdogEndpoint controls the endpoint settings for the dependency-watchdog for the seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled controls whether the endpoint controller of the dependency-watchdog should be enabled. This controller
helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in
CrashLoopBackoff status and restarting them once their dependants become ready and available again.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdogProbe>SeedSettingDependencyWatchdogProbe</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdog>SeedSettingDependencyWatchdog</a>)</p><p><p>SeedSettingDependencyWatchdogProbe controls the probe settings for the dependency-watchdog for the seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled controls whether the probe controller of the dependency-watchdog should be enabled. This controller
scales down the kube-controller-manager of shoot clusters in case their respective kube-apiserver is not
reachable via its external ingress in order to avoid melt-down situations.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingExcessCapacityReservation>SeedSettingExcessCapacityReservation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled controls whether the excess capacity reservation should be enabled.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingLoadBalancerServices>SeedSettingLoadBalancerServices</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the
seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations is a map of annotations that will be injected/merged into every load balancer service object.</p></td></tr><tr><td><code>externalTrafficPolicy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceexternaltrafficpolicytype-v1-core>Kubernetes core/v1.ServiceExternalTrafficPolicyType</a></em></td><td><em>(Optional)</em><p>ExternalTrafficPolicy describes how nodes distribute service traffic they
receive on one of the service’s “externally-facing” addresses.
Defaults to “Cluster”.</p></td></tr><tr><td><code>zones</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingLoadBalancerServicesZones>[]SeedSettingLoadBalancerServicesZones</a></em></td><td><em>(Optional)</em><p>Zones controls settings, which are specific to the single-zone load balancers in a multi-zonal setup.
Can be empty for single-zone seeds. Each specified zone has to relate to one of the zones in seed.spec.provider.zones.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingLoadBalancerServicesZones>SeedSettingLoadBalancerServicesZones</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettingLoadBalancerServices>SeedSettingLoadBalancerServices</a>)</p><p><p>SeedSettingLoadBalancerServicesZones controls settings, which are specific to the single-zone load balancers in a
multi-zonal setup.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the zone as specified in seed.spec.provider.zones.</p></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations is a map of annotations that will be injected/merged into the zone-specific load balancer service object.</p></td></tr><tr><td><code>externalTrafficPolicy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceexternaltrafficpolicytype-v1-core>Kubernetes core/v1.ServiceExternalTrafficPolicyType</a></em></td><td><em>(Optional)</em><p>ExternalTrafficPolicy describes how nodes distribute service traffic they
receive on one of the service’s “externally-facing” addresses.
Defaults to “Cluster”.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingOwnerChecks>SeedSettingOwnerChecks</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingOwnerChecks controls certain owner checks settings for shoots scheduled on this seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled controls whether owner checks are enabled for shoots scheduled on this seed. It
is enabled by default because it is a prerequisite for control plane migration.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingScheduling>SeedSettingScheduling</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingScheduling controls settings for scheduling decisions for the seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>visible</code></br><em>bool</em></td><td><p>Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds
are not considered by the scheduler.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettingVerticalPodAutoscaler>SeedSettingVerticalPodAutoscaler</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a>)</p><p><p>SeedSettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the
seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled controls whether the VPA components shall be deployed into the garden namespace in the seed cluster. It
is enabled by default because Gardener heavily relies on a VPA being deployed. You should only disable this if
your seed cluster already has another, manually/custom managed VPA deployment.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedSettings contains certain settings for this seed cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>excessCapacityReservation</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingExcessCapacityReservation>SeedSettingExcessCapacityReservation</a></em></td><td><em>(Optional)</em><p>ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.</p></td></tr><tr><td><code>scheduling</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingScheduling>SeedSettingScheduling</a></em></td><td><em>(Optional)</em><p>Scheduling controls settings for scheduling decisions for the seed.</p></td></tr><tr><td><code>loadBalancerServices</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingLoadBalancerServices>SeedSettingLoadBalancerServices</a></em></td><td><em>(Optional)</em><p>LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.</p></td></tr><tr><td><code>verticalPodAutoscaler</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingVerticalPodAutoscaler>SeedSettingVerticalPodAutoscaler</a></em></td><td><em>(Optional)</em><p>VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.</p></td></tr><tr><td><code>ownerChecks</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingOwnerChecks>SeedSettingOwnerChecks</a></em></td><td><em>(Optional)</em><p>SeedSettingOwnerChecks controls certain owner checks settings for shoots scheduled on this seed.</p></td></tr><tr><td><code>dependencyWatchdog</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettingDependencyWatchdog>SeedSettingDependencyWatchdog</a></em></td><td><em>(Optional)</em><p>DependencyWatchdog controls certain settings for the dependency-watchdog components deployed in the seed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Seed>Seed</a>,
<a href=#core.gardener.cloud/v1beta1.SeedTemplate>SeedTemplate</a>)</p><p><p>SeedSpec is the specification of a Seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>backup</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedBackup>SeedBackup</a></em></td><td><em>(Optional)</em><p>Backup holds the object store configuration for the backups of shoot (currently only etcd).
If it is not specified, then there won’t be any backups taken for shoots associated with this seed.
If backup field is present in seed, then backups of the etcd from shoot control plane will be stored
under the configured object store.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedDNS>SeedDNS</a></em></td><td><p>DNS contains DNS-relevant information about this seed cluster.</p></td></tr><tr><td><code>networks</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</a></em></td><td><p>Networks defines the pod, service and worker network of the Seed cluster.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedProvider>SeedProvider</a></em></td><td><p>Provider defines the provider type and region for this Seed cluster.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes
cluster to be registered as Seed.</p></td></tr><tr><td><code>taints</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedTaint>[]SeedTaint</a></em></td><td><em>(Optional)</em><p>Taints describes taints on the seed.</p></td></tr><tr><td><code>volume</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedVolume>SeedVolume</a></em></td><td><em>(Optional)</em><p>Volume contains settings for persistentvolumes created in the seed cluster.</p></td></tr><tr><td><code>settings</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a></em></td><td><em>(Optional)</em><p>Settings contains certain settings for this seed cluster.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#core.gardener.cloud/v1beta1.Ingress>Ingress</a></em></td><td><em>(Optional)</em><p>Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedStatus>SeedStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Seed>Seed</a>)</p><p><p>SeedStatus is the status of a Seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>gardener</code></br><em><a href=#core.gardener.cloud/v1beta1.Gardener>Gardener</a></em></td><td><em>(Optional)</em><p>Gardener holds information about the Gardener which last acted on the Shoot.</p></td></tr><tr><td><code>kubernetesVersion</code></br><em>string</em></td><td><em>(Optional)</em><p>KubernetesVersion is the Kubernetes version of the seed cluster.</p></td></tr><tr><td><code>conditions</code></br><em><a href=#core.gardener.cloud/v1beta1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a Seed’s current state.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the
Seed’s generation, which is updated on mutation by the API Server.</p></td></tr><tr><td><code>clusterIdentity</code></br><em>string</em></td><td><em>(Optional)</em><p>ClusterIdentity is the identity of the Seed cluster. This field is immutable.</p></td></tr><tr><td><code>capacity</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><em>(Optional)</em><p>Capacity represents the total resources of a seed.</p></td></tr><tr><td><code>allocatable</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><em>(Optional)</em><p>Allocatable represents the resources of a seed that are available for scheduling.
Defaults to Capacity.</p></td></tr><tr><td><code>clientCertificateExpirationTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>ClientCertificateExpirationTimestamp is the timestamp at which gardenlet’s client certificate expires.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedTaint>SeedTaint</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedTaint describes a taint on a seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>key</code></br><em>string</em></td><td><p>Key is the taint key to be applied to a seed.</p></td></tr><tr><td><code>value</code></br><em>string</em></td><td><em>(Optional)</em><p>Value is the taint value corresponding to the taint key.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedTemplate>SeedTemplate</h3><p><p>SeedTemplate is a template for creating a Seed object.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the Seed.</p><br><br><table><tr><td><code>backup</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedBackup>SeedBackup</a></em></td><td><em>(Optional)</em><p>Backup holds the object store configuration for the backups of shoot (currently only etcd).
If it is not specified, then there won’t be any backups taken for shoots associated with this seed.
If backup field is present in seed, then backups of the etcd from shoot control plane will be stored
under the configured object store.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedDNS>SeedDNS</a></em></td><td><p>DNS contains DNS-relevant information about this seed cluster.</p></td></tr><tr><td><code>networks</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</a></em></td><td><p>Networks defines the pod, service and worker network of the Seed cluster.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedProvider>SeedProvider</a></em></td><td><p>Provider defines the provider type and region for this Seed cluster.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes
cluster to be registered as Seed.</p></td></tr><tr><td><code>taints</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedTaint>[]SeedTaint</a></em></td><td><em>(Optional)</em><p>Taints describes taints on the seed.</p></td></tr><tr><td><code>volume</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedVolume>SeedVolume</a></em></td><td><em>(Optional)</em><p>Volume contains settings for persistentvolumes created in the seed cluster.</p></td></tr><tr><td><code>settings</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSettings>SeedSettings</a></em></td><td><em>(Optional)</em><p>Settings contains certain settings for this seed cluster.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#core.gardener.cloud/v1beta1.Ingress>Ingress</a></em></td><td><em>(Optional)</em><p>Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedVolume>SeedVolume</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedSpec>SeedSpec</a>)</p><p><p>SeedVolume contains settings for persistentvolumes created in the seed cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>minimumSize</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>MinimumSize defines the minimum size that should be used for PVCs in the seed.</p></td></tr><tr><td><code>providers</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedVolumeProvider>[]SeedVolumeProvider</a></em></td><td><em>(Optional)</em><p>Providers is a list of storage class provisioner types for the seed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SeedVolumeProvider>SeedVolumeProvider</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedVolume>SeedVolume</a>)</p><p><p>SeedVolumeProvider is a storage class provisioner type.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>purpose</code></br><em>string</em></td><td><p>Purpose is the purpose of this provider.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the storage class provisioner type.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ServiceAccountConfig>ServiceAccountConfig</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>ServiceAccountConfig is the kube-apiserver configuration for service accounts.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>issuer</code></br><em>string</em></td><td><em>(Optional)</em><p>Issuer is the identifier of the service account token issuer. The issuer will assert this
identifier in “iss” claim of issued tokens. This value is used to generate new service account tokens.
This value is a string or URI. Defaults to URI of the API server.</p></td></tr><tr><td><code>extendTokenExpiration</code></br><em>bool</em></td><td><em>(Optional)</em><p>ExtendTokenExpiration turns on projected service account expiration extension during token generation, which
helps safe transition from legacy token to bound service account token feature. If this flag is enabled,
admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition,
ignoring value of service-account-max-token-expiration.</p></td></tr><tr><td><code>maxTokenExpiration</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MaxTokenExpiration is the maximum validity duration of a token created by the service account token issuer. If an
otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued
with a validity duration of this value.
This field must be within [30d,90d].</p></td></tr><tr><td><code>acceptedIssuers</code></br><em>[]string</em></td><td><em>(Optional)</em><p>AcceptedIssuers is an additional set of issuers that are used to determine which service account tokens are accepted.
These values are not used to generate new service account tokens. Only useful when service account tokens are also
issued by another external system or a change of the current issuer that is used for generating tokens is being performed.
This field is only available for Kubernetes v1.22 or later.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ServiceAccountKeyRotation>ServiceAccountKeyRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>ServiceAccountKeyRotation contains information about the service account key credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></br><em><a href=#core.gardener.cloud/v1beta1.CredentialsRotationPhase>CredentialsRotationPhase</a></em></td><td><p>Phase describes the phase of the service account key credential rotation.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the service account key credential rotation was successfully
completed.</p></td></tr><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the service account key credential rotation was initiated.</p></td></tr><tr><td><code>lastInitiationFinishedTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationFinishedTime is the recent time when the certificate authority credential rotation initiation was
completed.</p></td></tr><tr><td><code>lastCompletionTriggeredTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTriggeredTime is the recent time when the certificate authority credential rotation completion was
triggered.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootAdvertisedAddress>ShootAdvertisedAddress</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>ShootAdvertisedAddress contains information for the shoot’s Kube API server.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the advertised address. e.g. external</p></td></tr><tr><td><code>url</code></br><em>string</em></td><td><p>The URL of the API Server. e.g. <a href=https://api.foo.bar>https://api.foo.bar</a> or <a href=https://1.2.3.4>https://1.2.3.4</a></p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootCredentials>ShootCredentials</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</a>)</p><p><p>ShootCredentials contains information about the shoot credentials.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>rotation</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a></em></td><td><em>(Optional)</em><p>Rotation contains information about the credential rotations.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentials>ShootCredentials</a>)</p><p><p>ShootCredentialsRotation contains information about the rotation of credentials.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>certificateAuthorities</code></br><em><a href=#core.gardener.cloud/v1beta1.CARotation>CARotation</a></em></td><td><em>(Optional)</em><p>CertificateAuthorities contains information about the certificate authority credential rotation.</p></td></tr><tr><td><code>kubeconfig</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootKubeconfigRotation>ShootKubeconfigRotation</a></em></td><td><em>(Optional)</em><p>Kubeconfig contains information about the kubeconfig credential rotation.</p></td></tr><tr><td><code>sshKeypair</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootSSHKeypairRotation>ShootSSHKeypairRotation</a></em></td><td><em>(Optional)</em><p>SSHKeypair contains information about the ssh-keypair credential rotation.</p></td></tr><tr><td><code>observability</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootObservabilityRotation>ShootObservabilityRotation</a></em></td><td><em>(Optional)</em><p>Observability contains information about the observability credential rotation.</p></td></tr><tr><td><code>serviceAccountKey</code></br><em><a href=#core.gardener.cloud/v1beta1.ServiceAccountKeyRotation>ServiceAccountKeyRotation</a></em></td><td><em>(Optional)</em><p>ServiceAccountKey contains information about the service account key credential rotation.</p></td></tr><tr><td><code>etcdEncryptionKey</code></br><em><a href=#core.gardener.cloud/v1beta1.ETCDEncryptionKeyRotation>ETCDEncryptionKeyRotation</a></em></td><td><em>(Optional)</em><p>ETCDEncryptionKey contains information about the ETCD encryption key credential rotation.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootKubeconfigRotation>ShootKubeconfigRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>ShootKubeconfigRotation contains information about the kubeconfig credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the kubeconfig credential rotation was initiated.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the kubeconfig credential rotation was successfully completed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootMachineImage>ShootMachineImage</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Machine>Machine</a>)</p><p><p>ShootMachineImage defines the name and the version of the shoot’s machine image in any environment. Has to be
defined in the respective CloudProfile.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the image.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the shoot’s individual configuration passed to an extension resource.</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><em>(Optional)</em><p>Version is the version of the shoot’s image.
If version is not provided, it will be defaulted to the latest version from the CloudProfile.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootNetworks>ShootNetworks</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.SeedNetworks>SeedNetworks</a>)</p><p><p>ShootNetworks contains the default networks CIDRs for shoots.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>pods</code></br><em>string</em></td><td><em>(Optional)</em><p>Pods is the CIDR of the pod network.</p></td></tr><tr><td><code>services</code></br><em>string</em></td><td><em>(Optional)</em><p>Services is the CIDR of the service network.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootObservabilityRotation>ShootObservabilityRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>ShootObservabilityRotation contains information about the observability credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the observability credential rotation was initiated.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the observability credential rotation was successfully completed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootPurpose>ShootPurpose
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>ShootPurpose is a type alias for string.</p></p><h3 id=core.gardener.cloud/v1beta1.ShootSSHKeypairRotation>ShootSSHKeypairRotation</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootCredentialsRotation>ShootCredentialsRotation</a>)</p><p><p>ShootSSHKeypairRotation contains information about the ssh-keypair credential rotation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>lastInitiationTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastInitiationTime is the most recent time when the ssh-keypair credential rotation was initiated.</p></td></tr><tr><td><code>lastCompletionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastCompletionTime is the most recent time when the ssh-keypair credential rotation was successfully completed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Shoot>Shoot</a>,
<a href=#core.gardener.cloud/v1beta1.ShootTemplate>ShootTemplate</a>)</p><p><p>ShootSpec is the specification of a Shoot.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>addons</code></br><em><a href=#core.gardener.cloud/v1beta1.Addons>Addons</a></em></td><td><em>(Optional)</em><p>Addons contains information about enabled/disabled addons and their configuration.</p></td></tr><tr><td><code>cloudProfileName</code></br><em>string</em></td><td><p>CloudProfileName is a name of a CloudProfile object. This field is immutable.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.DNS>DNS</a></em></td><td><em>(Optional)</em><p>DNS contains information about the DNS settings of the Shoot.</p></td></tr><tr><td><code>extensions</code></br><em><a href=#core.gardener.cloud/v1beta1.Extension>[]Extension</a></em></td><td><em>(Optional)</em><p>Extensions contain type and provider information for Shoot extensions.</p></td></tr><tr><td><code>hibernation</code></br><em><a href=#core.gardener.cloud/v1beta1.Hibernation>Hibernation</a></em></td><td><em>(Optional)</em><p>Hibernation contains information whether the Shoot is suspended or not.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a></em></td><td><p>Kubernetes contains the version and configuration settings of the control plane components.</p></td></tr><tr><td><code>networking</code></br><em><a href=#core.gardener.cloud/v1beta1.Networking>Networking</a></em></td><td><p>Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.</p></td></tr><tr><td><code>maintenance</code></br><em><a href=#core.gardener.cloud/v1beta1.Maintenance>Maintenance</a></em></td><td><em>(Optional)</em><p>Maintenance contains information about the time window for maintenance operations and which
operations should be performed.</p></td></tr><tr><td><code>monitoring</code></br><em><a href=#core.gardener.cloud/v1beta1.Monitoring>Monitoring</a></em></td><td><em>(Optional)</em><p>Monitoring contains information about custom monitoring configurations for the shoot.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.Provider>Provider</a></em></td><td><p>Provider contains all provider-specific and provider-relevant information.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootPurpose>ShootPurpose</a></em></td><td><em>(Optional)</em><p>Purpose is the purpose class for this cluster.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is a name of a region. This field is immutable.</p></td></tr><tr><td><code>secretBindingName</code></br><em>string</em></td><td><p>SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret.
The credentials inside the provider secret will be used to create the shoot in the respective account.
This field is immutable.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed cluster that runs the control plane of the Shoot.
This field is immutable when the SeedChange feature gate is disabled.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.NamedResourceReference>[]NamedResourceReference</a></em></td><td><em>(Optional)</em><p>Resources holds a list of named resource references that can be referred to in extension configs by their names.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on seed clusters.</p></td></tr><tr><td><code>exposureClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy.
This field is immutable.</p></td></tr><tr><td><code>systemComponents</code></br><em><a href=#core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</a></em></td><td><em>(Optional)</em><p>SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.</p></td></tr><tr><td><code>controlPlane</code></br><em><a href=#core.gardener.cloud/v1beta1.ControlPlane>ControlPlane</a></em></td><td><em>(Optional)</em><p>ControlPlane contains general settings for the control plane of the shoot.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootStateSpec>ShootStateSpec</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootState>ShootState</a>)</p><p><p>ShootStateSpec is the specification of the ShootState.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>gardener</code></br><em><a href=#core.gardener.cloud/v1beta1.GardenerResourceData>[]GardenerResourceData</a></em></td><td><em>(Optional)</em><p>Gardener holds the data required to generate resources deployed by the gardenlet</p></td></tr><tr><td><code>extensions</code></br><em><a href=#core.gardener.cloud/v1beta1.ExtensionResourceState>[]ExtensionResourceState</a></em></td><td><em>(Optional)</em><p>Extensions holds the state of custom resources reconciled by extension controllers in the seed</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.ResourceData>[]ResourceData</a></em></td><td><em>(Optional)</em><p>Resources holds the data of resources referred to by extension controller states</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootStatus>ShootStatus</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Shoot>Shoot</a>)</p><p><p>ShootStatus holds the most recently observed status of the Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=#core.gardener.cloud/v1beta1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a Shoots’s current state.</p></td></tr><tr><td><code>constraints</code></br><em><a href=#core.gardener.cloud/v1beta1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Constraints represents conditions of a Shoot’s current state that constraint some operations on it.</p></td></tr><tr><td><code>gardener</code></br><em><a href=#core.gardener.cloud/v1beta1.Gardener>Gardener</a></em></td><td><p>Gardener holds information about the Gardener which last acted on the Shoot.</p></td></tr><tr><td><code>hibernated</code></br><em>bool</em></td><td><p>IsHibernated indicates whether the Shoot is currently hibernated.</p></td></tr><tr><td><code>lastOperation</code></br><em><a href=#core.gardener.cloud/v1beta1.LastOperation>LastOperation</a></em></td><td><em>(Optional)</em><p>LastOperation holds information about the last operation on the Shoot.</p></td></tr><tr><td><code>lastErrors</code></br><em><a href=#core.gardener.cloud/v1beta1.LastError>[]LastError</a></em></td><td><em>(Optional)</em><p>LastErrors holds information about the last occurred error(s) during an operation.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the
Shoot’s generation, which is updated on mutation by the API Server.</p></td></tr><tr><td><code>retryCycleStartTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation
must be retried until we give up).</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written
after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.</p></td></tr><tr><td><code>technicalID</code></br><em>string</em></td><td><p>TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and
basically everything that is related to this particular Shoot. This field is immutable.</p></td></tr><tr><td><code>uid</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/types#UID>k8s.io/apimachinery/pkg/types.UID</a></em></td><td><p>UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters.
It is used to compute unique hashes. This field is immutable.</p></td></tr><tr><td><code>clusterIdentity</code></br><em>string</em></td><td><em>(Optional)</em><p>ClusterIdentity is the identity of the Shoot cluster. This field is immutable.</p></td></tr><tr><td><code>advertisedAddresses</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootAdvertisedAddress>[]ShootAdvertisedAddress</a></em></td><td><em>(Optional)</em><p>List of addresses on which the Kube API server can be reached.</p></td></tr><tr><td><code>migrationStartTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>MigrationStartTime is the time when a migration to a different seed was initiated.</p></td></tr><tr><td><code>credentials</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootCredentials>ShootCredentials</a></em></td><td><em>(Optional)</em><p>Credentials contains information about the shoot credentials.</p></td></tr><tr><td><code>lastHibernationTriggerTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastHibernationTriggerTime indicates the last time when the hibernation controller
managed to change the hibernation settings of the cluster</p></td></tr><tr><td><code>lastMaintenance</code></br><em><a href=#core.gardener.cloud/v1beta1.LastMaintenance>LastMaintenance</a></em></td><td><em>(Optional)</em><p>LastMaintenance holds information about the last maintenance operations on the Shoot.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.ShootTemplate>ShootTemplate</h3><p><p>ShootTemplate is a template for creating a Shoot object.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the Shoot.</p><br><br><table><tr><td><code>addons</code></br><em><a href=#core.gardener.cloud/v1beta1.Addons>Addons</a></em></td><td><em>(Optional)</em><p>Addons contains information about enabled/disabled addons and their configuration.</p></td></tr><tr><td><code>cloudProfileName</code></br><em>string</em></td><td><p>CloudProfileName is a name of a CloudProfile object. This field is immutable.</p></td></tr><tr><td><code>dns</code></br><em><a href=#core.gardener.cloud/v1beta1.DNS>DNS</a></em></td><td><em>(Optional)</em><p>DNS contains information about the DNS settings of the Shoot.</p></td></tr><tr><td><code>extensions</code></br><em><a href=#core.gardener.cloud/v1beta1.Extension>[]Extension</a></em></td><td><em>(Optional)</em><p>Extensions contain type and provider information for Shoot extensions.</p></td></tr><tr><td><code>hibernation</code></br><em><a href=#core.gardener.cloud/v1beta1.Hibernation>Hibernation</a></em></td><td><em>(Optional)</em><p>Hibernation contains information whether the Shoot is suspended or not.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a></em></td><td><p>Kubernetes contains the version and configuration settings of the control plane components.</p></td></tr><tr><td><code>networking</code></br><em><a href=#core.gardener.cloud/v1beta1.Networking>Networking</a></em></td><td><p>Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.</p></td></tr><tr><td><code>maintenance</code></br><em><a href=#core.gardener.cloud/v1beta1.Maintenance>Maintenance</a></em></td><td><em>(Optional)</em><p>Maintenance contains information about the time window for maintenance operations and which
operations should be performed.</p></td></tr><tr><td><code>monitoring</code></br><em><a href=#core.gardener.cloud/v1beta1.Monitoring>Monitoring</a></em></td><td><em>(Optional)</em><p>Monitoring contains information about custom monitoring configurations for the shoot.</p></td></tr><tr><td><code>provider</code></br><em><a href=#core.gardener.cloud/v1beta1.Provider>Provider</a></em></td><td><p>Provider contains all provider-specific and provider-relevant information.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#core.gardener.cloud/v1beta1.ShootPurpose>ShootPurpose</a></em></td><td><em>(Optional)</em><p>Purpose is the purpose class for this cluster.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is a name of a region. This field is immutable.</p></td></tr><tr><td><code>secretBindingName</code></br><em>string</em></td><td><p>SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret.
The credentials inside the provider secret will be used to create the shoot in the respective account.
This field is immutable.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed cluster that runs the control plane of the Shoot.
This field is immutable when the SeedChange feature gate is disabled.</p></td></tr><tr><td><code>seedSelector</code></br><em><a href=#core.gardener.cloud/v1beta1.SeedSelector>SeedSelector</a></em></td><td><em>(Optional)</em><p>SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.</p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.NamedResourceReference>[]NamedResourceReference</a></em></td><td><em>(Optional)</em><p>Resources holds a list of named resource references that can be referred to in extension configs by their names.</p></td></tr><tr><td><code>tolerations</code></br><em><a href=#core.gardener.cloud/v1beta1.Toleration>[]Toleration</a></em></td><td><em>(Optional)</em><p>Tolerations contains the tolerations for taints on seed clusters.</p></td></tr><tr><td><code>exposureClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy.
This field is immutable.</p></td></tr><tr><td><code>systemComponents</code></br><em><a href=#core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</a></em></td><td><em>(Optional)</em><p>SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.</p></td></tr><tr><td><code>controlPlane</code></br><em><a href=#core.gardener.cloud/v1beta1.ControlPlane>ControlPlane</a></em></td><td><em>(Optional)</em><p>ControlPlane contains general settings for the control plane of the shoot.</p></td></tr></table></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.SystemComponents>SystemComponents</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>coreDNS</code></br><em><a href=#core.gardener.cloud/v1beta1.CoreDNS>CoreDNS</a></em></td><td><em>(Optional)</em><p>CoreDNS contains the settings of the Core DNS components running in the data plane of the Shoot cluster.</p></td></tr><tr><td><code>nodeLocalDNS</code></br><em><a href=#core.gardener.cloud/v1beta1.NodeLocalDNS>NodeLocalDNS</a></em></td><td><em>(Optional)</em><p>NodeLocalDNS contains the settings of the node local DNS components running in the data plane of the Shoot cluster.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Toleration>Toleration</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ExposureClassScheduling>ExposureClassScheduling</a>,
<a href=#core.gardener.cloud/v1beta1.ProjectTolerations>ProjectTolerations</a>,
<a href=#core.gardener.cloud/v1beta1.ShootSpec>ShootSpec</a>)</p><p><p>Toleration is a toleration for a seed taint.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>key</code></br><em>string</em></td><td><p>Key is the toleration key to be applied to a project or shoot.</p></td></tr><tr><td><code>value</code></br><em>string</em></td><td><em>(Optional)</em><p>Value is the toleration value corresponding to the toleration key.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.VersionClassification>VersionClassification
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.ExpirableVersion>ExpirableVersion</a>)</p><p><p>VersionClassification is the logical state of a version.</p></p><h3 id=core.gardener.cloud/v1beta1.VerticalPodAutoscaler>VerticalPodAutoscaler</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Kubernetes>Kubernetes</a>)</p><p><p>VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled specifies whether the Kubernetes VPA shall be enabled for the shoot cluster.</p></td></tr><tr><td><code>evictAfterOOMThreshold</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EvictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given
threshold since its start and if it has only one container (default: 10m0s).</p></td></tr><tr><td><code>evictionRateBurst</code></br><em>int32</em></td><td><em>(Optional)</em><p>EvictionRateBurst defines the burst of pods that can be evicted (default: 1)</p></td></tr><tr><td><code>evictionRateLimit</code></br><em>float64</em></td><td><em>(Optional)</em><p>EvictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will
disable the rate limiter (default: -1).</p></td></tr><tr><td><code>evictionTolerance</code></br><em>float64</em></td><td><em>(Optional)</em><p>EvictionTolerance defines the fraction of replica count that can be evicted for update in case more than one
pod can be evicted (default: 0.5).</p></td></tr><tr><td><code>recommendationMarginFraction</code></br><em>float64</em></td><td><em>(Optional)</em><p>RecommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request
(default: 0.15).</p></td></tr><tr><td><code>updaterInterval</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>UpdaterInterval is the interval how often the updater should run (default: 1m0s).</p></td></tr><tr><td><code>recommenderInterval</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>RecommenderInterval is the interval how often metrics should be fetched (default: 1m0s).</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Volume>Volume</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>Volume contains information about the volume type, size, and encryption.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><em>(Optional)</em><p>Name of the volume to make it referencable.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><em>(Optional)</em><p>Type is the type of the volume.</p></td></tr><tr><td><code>size</code></br><em>string</em></td><td><p>VolumeSize is the size of the volume.</p></td></tr><tr><td><code>encrypted</code></br><em>bool</em></td><td><em>(Optional)</em><p>Encrypted determines if the volume should be encrypted.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.VolumeType>VolumeType</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.CloudProfileSpec>CloudProfileSpec</a>)</p><p><p>VolumeType contains certain properties of a volume type.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></br><em>string</em></td><td><p>Class is the class of the volume type.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the volume type.</p></td></tr><tr><td><code>usable</code></br><em>bool</em></td><td><em>(Optional)</em><p>Usable defines if the volume type can be used for shoot clusters.</p></td></tr><tr><td><code>minSize</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/api/resource#Quantity>k8s.io/apimachinery/pkg/api/resource.Quantity</a></em></td><td><em>(Optional)</em><p>MinSize is the minimal supported storage size.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.WatchCacheSizes>WatchCacheSizes</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.KubeAPIServerConfig>KubeAPIServerConfig</a>)</p><p><p>WatchCacheSizes contains configuration of the API server’s watch cache sizes.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>default</code></br><em>int32</em></td><td><em>(Optional)</em><p>Default configures the default watch cache size of the kube-apiserver
(flag <code>--default-watch-cache-size</code>, defaults to 100).
See: <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></p></td></tr><tr><td><code>resources</code></br><em><a href=#core.gardener.cloud/v1beta1.ResourceWatchCacheSize>[]ResourceWatchCacheSize</a></em></td><td><em>(Optional)</em><p>Resources configures the watch cache size of the kube-apiserver per resource
(flag <code>--watch-cache-sizes</code>).
See: <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.Worker>Worker</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Provider>Provider</a>)</p><p><p>Worker is the base definition of a worker group.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations is a map of key/value pairs for annotations for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>caBundle</code></br><em>string</em></td><td><em>(Optional)</em><p>CABundle is a certificate bundle which will be installed onto every machine of this worker pool.</p></td></tr><tr><td><code>cri</code></br><em><a href=#core.gardener.cloud/v1beta1.CRI>CRI</a></em></td><td><em>(Optional)</em><p>CRI contains configurations of CRI support of every machine in the worker pool.
Defaults to a CRI with name <code>containerd</code> when the Kubernetes version of the <code>Shoot</code> is >= 1.22.</p></td></tr><tr><td><code>kubernetes</code></br><em><a href=#core.gardener.cloud/v1beta1.WorkerKubernetes>WorkerKubernetes</a></em></td><td><em>(Optional)</em><p>Kubernetes contains configuration for Kubernetes components related to this worker pool.</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels is a map of key/value pairs for labels for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the worker group.</p></td></tr><tr><td><code>machine</code></br><em><a href=#core.gardener.cloud/v1beta1.Machine>Machine</a></em></td><td><p>Machine contains information about the machine type and image.</p></td></tr><tr><td><code>maximum</code></br><em>int32</em></td><td><p>Maximum is the maximum number of VMs to create.</p></td></tr><tr><td><code>minimum</code></br><em>int32</em></td><td><p>Minimum is the minimum number of VMs to create.</p></td></tr><tr><td><code>maxSurge</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>MaxSurge is maximum number of VMs that are created during an update.</p></td></tr><tr><td><code>maxUnavailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>MaxUnavailable is the maximum number of VMs that can be unavailable during an update.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the provider-specific configuration for this worker pool.</p></td></tr><tr><td><code>taints</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#taint-v1-core>[]Kubernetes core/v1.Taint</a></em></td><td><em>(Optional)</em><p>Taints is a list of taints for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>volume</code></br><em><a href=#core.gardener.cloud/v1beta1.Volume>Volume</a></em></td><td><em>(Optional)</em><p>Volume contains information about the volume type and size.</p></td></tr><tr><td><code>dataVolumes</code></br><em><a href=#core.gardener.cloud/v1beta1.DataVolume>[]DataVolume</a></em></td><td><em>(Optional)</em><p>DataVolumes contains a list of additional worker volumes.</p></td></tr><tr><td><code>kubeletDataVolumeName</code></br><em>string</em></td><td><em>(Optional)</em><p>KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.</p></td></tr><tr><td><code>zones</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional
as not every provider may support availability zones.</p></td></tr><tr><td><code>systemComponents</code></br><em><a href=#core.gardener.cloud/v1beta1.WorkerSystemComponents>WorkerSystemComponents</a></em></td><td><em>(Optional)</em><p>SystemComponents contains configuration for system components related to this worker pool</p></td></tr><tr><td><code>machineControllerManager</code></br><em><a href=#core.gardener.cloud/v1beta1.MachineControllerManagerSettings>MachineControllerManagerSettings</a></em></td><td><em>(Optional)</em><p>MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.WorkerKubernetes>WorkerKubernetes</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kubelet</code></br><em><a href=#core.gardener.cloud/v1beta1.KubeletConfig>KubeletConfig</a></em></td><td><em>(Optional)</em><p>Kubelet contains configuration settings for all kubelets of this worker pool.
If set, all <code>spec.kubernetes.kubelet</code> settings will be overwritten for this worker pool (no merge of settings).</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><em>(Optional)</em><p>Version is the semantic Kubernetes version to use for the Kubelet in this Worker Group.
If not specified the kubelet version is derived from the global shoot cluster kubernetes version.
version must be equal or lower than the version of the shoot kubernetes version.
Only one minor version difference to other worker groups and global kubernetes version is allowed.</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.WorkerSystemComponents>WorkerSystemComponents</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Worker>Worker</a>)</p><p><p>WorkerSystemComponents contains configuration for system components related to this worker pool</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>allow</code></br><em>bool</em></td><td><p>Allow determines whether the pool should be allowed to host system components or not (defaults to true)</p></td></tr></tbody></table><h3 id=core.gardener.cloud/v1beta1.WorkersSettings>WorkersSettings</h3><p>(<em>Appears on:</em>
<a href=#core.gardener.cloud/v1beta1.Provider>Provider</a>)</p><p><p>WorkersSettings contains settings for all workers.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>sshAccess</code></br><em><a href=#core.gardener.cloud/v1beta1.SSHAccess>SSHAccess</a></em></td><td><em>(Optional)</em><p>SSHAccess contains settings regarding ssh access to the worker nodes.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-fca1695658a108c8072b581668195464>1.1.3 - Extensions</h1><p>Packages:</p><ul><li><a href=#extensions.gardener.cloud%2fv1alpha1>extensions.gardener.cloud/v1alpha1</a></li></ul><h2 id=extensions.gardener.cloud/v1alpha1>extensions.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is the v1alpha1 version of the API.</p></p>Resource Types:<ul><li><a href=#extensions.gardener.cloud/v1alpha1.BackupBucket>BackupBucket</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.BackupEntry>BackupEntry</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Bastion>Bastion</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Cluster>Cluster</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntime>ContainerRuntime</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.DNSRecord>DNSRecord</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Extension>Extension</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Infrastructure>Infrastructure</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Network>Network</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfig>OperatingSystemConfig</a></li><li><a href=#extensions.gardener.cloud/v1alpha1.Worker>Worker</a></li></ul><h3 id=extensions.gardener.cloud/v1alpha1.BackupBucket>BackupBucket</h3><p><p>BackupBucket is a specification for backup bucket.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>BackupBucket</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BackupBucketSpec>BackupBucketSpec</a></em></td><td><p>Specification of the BackupBucket.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this bucket. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BackupBucketStatus>BackupBucketStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BackupEntry>BackupEntry</h3><p><p>BackupEntry is a specification for backup Entry.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>BackupEntry</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BackupEntrySpec>BackupEntrySpec</a></em></td><td><p>Specification of the BackupEntry.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>backupBucketProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>BackupBucketProviderStatus contains the provider status that has
been generated by the controller responsible for the <code>BackupBucket</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this Entry. This field is immutable.</p></td></tr><tr><td><code>bucketName</code></br><em>string</em></td><td><p>BucketName is the name of backup bucket for this Backup Entry.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BackupEntryStatus>BackupEntryStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Bastion>Bastion</h3><p><p>Bastion is a bastion or jump host that is dynamically created
to provide SSH access to shoot nodes.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Bastion</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</a></em></td><td><p>Spec is the specification of this Bastion.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>userData</code></br><em>[]byte</em></td><td><p>UserData is the base64-encoded user data for the bastion instance. This should
contain code to provision the SSH key on the bastion instance.
This field is immutable.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BastionIngressPolicy>[]BastionIngressPolicy</a></em></td><td><p>Ingress controls from where the created bastion host should be reachable.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BastionStatus>BastionStatus</a></em></td><td><em>(Optional)</em><p>Status is the bastion’s status.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Cluster>Cluster</h3><p><p>Cluster is a specification for a Cluster resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Cluster</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ClusterSpec>ClusterSpec</a></em></td><td><br><br><table><tr><td><code>cloudProfile</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>CloudProfile is a raw extension field that contains the cloudprofile resource referenced
by the shoot that has to be reconciled.</p></td></tr><tr><td><code>seed</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Seed is a raw extension field that contains the seed resource referenced by the shoot that
has to be reconciled.</p></td></tr><tr><td><code>shoot</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Shoot is a raw extension field that contains the shoot resource that has to be reconciled.</p></td></tr></table></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ContainerRuntime>ContainerRuntime</h3><p><p>ContainerRuntime is a specification for a container runtime resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ContainerRuntime</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeSpec>ContainerRuntimeSpec</a></em></td><td><p>Specification of the ContainerRuntime.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>binaryPath</code></br><em>string</em></td><td><p>BinaryPath is the Worker’s machine path where container runtime extensions should copy the binaries to.</p></td></tr><tr><td><code>workerPool</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeWorkerPool>ContainerRuntimeWorkerPool</a></em></td><td><p>WorkerPool identifies the worker pool of the Shoot.
For each worker pool and type, Gardener deploys a ContainerRuntime CRD.</p></td></tr><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeStatus>ContainerRuntimeStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</h3><p><p>ControlPlane is a specification for a ControlPlane resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ControlPlane</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ControlPlaneSpec>ControlPlaneSpec</a></em></td><td><p>Specification of the ControlPlane.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.Purpose>Purpose</a></em></td><td><em>(Optional)</em><p>Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.
This field is immutable.</p></td></tr><tr><td><code>infrastructureProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>InfrastructureProviderStatus contains the provider status that has
been generated by the controller responsible for the <code>Infrastructure</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this control plane. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ControlPlaneStatus>ControlPlaneStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DNSRecord>DNSRecord</h3><p><p>DNSRecord is a specification for a DNSRecord resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>DNSRecord</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DNSRecordSpec>DNSRecordSpec</a></em></td><td><p>Specification of the DNSRecord.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><em>(Optional)</em><p>Region is the region of this DNS record. If not specified, the region specified in SecretRef will be used.
If that is also not specified, the extension controller will use its default region.</p></td></tr><tr><td><code>zone</code></br><em>string</em></td><td><em>(Optional)</em><p>Zone is the DNS hosted zone of this DNS record. If not specified, it will be determined automatically by
getting all hosted zones of the account and searching for the longest zone name that is a suffix of Name.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the fully qualified domain name, e.g. “api.<shoot domain>”. This field is immutable.</p></td></tr><tr><td><code>recordType</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DNSRecordType>DNSRecordType</a></em></td><td><p>RecordType is the DNS record type. Only A, CNAME, and TXT records are currently supported. This field is immutable.</p></td></tr><tr><td><code>values</code></br><em>[]string</em></td><td><p>Values is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.</p></td></tr><tr><td><code>ttl</code></br><em>int64</em></td><td><em>(Optional)</em><p>TTL is the time to live in seconds. Defaults to 120.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DNSRecordStatus>DNSRecordStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Extension>Extension</h3><p><p>Extension is a specification for a Extension resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Extension</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ExtensionSpec>ExtensionSpec</a></em></td><td><p>Specification of the Extension.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ExtensionStatus>ExtensionStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Infrastructure>Infrastructure</h3><p><p>Infrastructure is a specification for cloud provider infrastructure.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Infrastructure</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.InfrastructureSpec>InfrastructureSpec</a></em></td><td><p>Specification of the Infrastructure.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this infrastructure. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider credentials.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>[]byte</em></td><td><em>(Optional)</em><p>SSHPublicKey is the public SSH key that should be used with this infrastructure.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.InfrastructureStatus>InfrastructureStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Network>Network</h3><p><p>Network is the specification for cluster networking.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Network</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.NetworkSpec>NetworkSpec</a></em></td><td><p>Specification of the Network.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>podCIDR</code></br><em>string</em></td><td><p>PodCIDR defines the CIDR that will be used for pods. This field is immutable.</p></td></tr><tr><td><code>serviceCIDR</code></br><em>string</em></td><td><p>ServiceCIDR defines the CIDR that will be used for services. This field is immutable.</p></td></tr><tr><td><code>ipFamilies</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.IPFamily>[]IPFamily</a></em></td><td><em>(Optional)</em><p>IPFamilies specifies the IP protocol versions to use for shoot networking. This field is immutable.
See <a href=/docs/gardener/usage/ipv6/>https://github.com/gardener/gardener/blob/master/docs/usage/ipv6.md</a></p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.NetworkStatus>NetworkStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.OperatingSystemConfig>OperatingSystemConfig</h3><p><p>OperatingSystemConfig is a specification for a OperatingSystemConfig resource</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>OperatingSystemConfig</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a></em></td><td><p>Specification of the OperatingSystemConfig.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>criConfig</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.CRIConfig>CRIConfig</a></em></td><td><em>(Optional)</em><p>CRI config is a structure contains configurations of the CRI library</p></td></tr><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigPurpose>OperatingSystemConfigPurpose</a></em></td><td><p>Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it
gets sent to the <code>Worker</code> extension controller to bootstrap a VM, or it is downloaded by the
cloud-config-downloader script already running on a bootstrapped VM.
This field is immutable.</p></td></tr><tr><td><code>reloadConfigFilePath</code></br><em>string</em></td><td><em>(Optional)</em><p>ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers
are asked to use it when determining the .status.command of this resource. For example, if for CoreOS
the reload-path might be “/var/lib/config”; then the controller shall set .status.command to
“/usr/bin/coreos-cloudinit –from-file=/var/lib/config”.</p></td></tr><tr><td><code>units</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.Unit>[]Unit</a></em></td><td><em>(Optional)</em><p>Units is a list of unit for the operating system configuration (usually, a systemd unit).</p></td></tr><tr><td><code>files</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.File>[]File</a></em></td><td><em>(Optional)</em><p>Files is a list of files that should get written to the host’s file system.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigStatus>OperatingSystemConfigStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Worker>Worker</h3><p><p>Worker is a specification for a Worker resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Worker</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.WorkerSpec>WorkerSpec</a></em></td><td><p>Specification of the Worker.
If the object’s deletion timestamp is set, this field is immutable.</p><br><br><table><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>infrastructureProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>InfrastructureProviderStatus is a raw extension field that contains the provider status that has
been generated by the controller responsible for the <code>Infrastructure</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the name of the region where the worker pool should be deployed to. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>[]byte</em></td><td><em>(Optional)</em><p>SSHPublicKey is the public SSH key that should be used with these workers.</p></td></tr><tr><td><code>pools</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>[]WorkerPool</a></em></td><td><p>Pools is a list of worker pools.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BackupBucketSpec>BackupBucketSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupBucket>BackupBucket</a>)</p><p><p>BackupBucketSpec is the spec for an BackupBucket resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this bucket. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BackupBucketStatus>BackupBucketStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupBucket>BackupBucket</a>)</p><p><p>BackupBucketStatus is the status for an BackupBucket resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>generatedSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>GeneratedSecretRef is reference to the secret generated by backup bucket, which
will have object store specific credentials.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BackupEntrySpec>BackupEntrySpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupEntry>BackupEntry</a>)</p><p><p>BackupEntrySpec is the spec for an BackupEntry resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>backupBucketProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>BackupBucketProviderStatus contains the provider status that has
been generated by the controller responsible for the <code>BackupBucket</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this Entry. This field is immutable.</p></td></tr><tr><td><code>bucketName</code></br><em>string</em></td><td><p>BucketName is the name of backup bucket for this Backup Entry.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the credentials to access object store.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BackupEntryStatus>BackupEntryStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupEntry>BackupEntry</a>)</p><p><p>BackupEntryStatus is the status for an BackupEntry resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BastionIngressPolicy>BastionIngressPolicy</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</a>)</p><p><p>BastionIngressPolicy represents an ingress policy for SSH bastion hosts.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ipBlock</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#ipblock-v1-networking>Kubernetes networking/v1.IPBlock</a></em></td><td><p>IPBlock defines an IP block that is allowed to access the bastion.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Bastion>Bastion</a>)</p><p><p>BastionSpec contains the specification for an SSH bastion host.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>userData</code></br><em>[]byte</em></td><td><p>UserData is the base64-encoded user data for the bastion instance. This should
contain code to provision the SSH key on the bastion instance.
This field is immutable.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.BastionIngressPolicy>[]BastionIngressPolicy</a></em></td><td><p>Ingress controls from where the created bastion host should be reachable.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.BastionStatus>BastionStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Bastion>Bastion</a>)</p><p><p>BastionStatus holds the most recently observed status of the Bastion.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>ingress</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#loadbalanceringress-v1-core>Kubernetes core/v1.LoadBalancerIngress</a></em></td><td><em>(Optional)</em><p>Ingress is the external IP and/or hostname of the bastion host.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.CRIConfig>CRIConfig</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a>)</p><p><p>CRIConfig contains configurations of the CRI library.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.CRIName>CRIName</a></em></td><td><p>Name is a mandatory string containing the name of the CRI library. Supported values are <code>docker</code> and <code>containerd</code>.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.CRIName>CRIName
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.CRIConfig>CRIConfig</a>)</p><p><p>CRIName is a type alias for the CRI name string.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.CloudConfig>CloudConfig</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigStatus>OperatingSystemConfigStatus</a>)</p><p><p>CloudConfig contains the generated output for the given operating system
config spec. It contains a reference to a secret as the result may contain confidential data.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the actual result of the generated cloud config.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ClusterSpec>ClusterSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Cluster>Cluster</a>)</p><p><p>ClusterSpec is the spec for a Cluster resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>cloudProfile</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>CloudProfile is a raw extension field that contains the cloudprofile resource referenced
by the shoot that has to be reconciled.</p></td></tr><tr><td><code>seed</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Seed is a raw extension field that contains the seed resource referenced by the shoot that
has to be reconciled.</p></td></tr><tr><td><code>shoot</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Shoot is a raw extension field that contains the shoot resource that has to be reconciled.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ContainerRuntimeSpec>ContainerRuntimeSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntime>ContainerRuntime</a>)</p><p><p>ContainerRuntimeSpec is the spec for a ContainerRuntime resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>binaryPath</code></br><em>string</em></td><td><p>BinaryPath is the Worker’s machine path where container runtime extensions should copy the binaries to.</p></td></tr><tr><td><code>workerPool</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeWorkerPool>ContainerRuntimeWorkerPool</a></em></td><td><p>WorkerPool identifies the worker pool of the Shoot.
For each worker pool and type, Gardener deploys a ContainerRuntime CRD.</p></td></tr><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ContainerRuntimeStatus>ContainerRuntimeStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntime>ContainerRuntime</a>)</p><p><p>ContainerRuntimeStatus is the status for a ContainerRuntime resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ContainerRuntimeWorkerPool>ContainerRuntimeWorkerPool</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeSpec>ContainerRuntimeSpec</a>)</p><p><p>ContainerRuntimeWorkerPool identifies a Shoot worker pool by its name and selector.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name specifies the name of the worker pool the container runtime should be available for.
This field is immutable.</p></td></tr><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>Selector is the label selector used by the extension to match the nodes belonging to the worker pool.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ControlPlaneSpec>ControlPlaneSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</a>)</p><p><p>ControlPlaneSpec is the spec of a ControlPlane resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.Purpose>Purpose</a></em></td><td><em>(Optional)</em><p>Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.
This field is immutable.</p></td></tr><tr><td><code>infrastructureProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>InfrastructureProviderStatus contains the provider status that has
been generated by the controller responsible for the <code>Infrastructure</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this control plane. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ControlPlaneStatus>ControlPlaneStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</a>)</p><p><p>ControlPlaneStatus is the status of a ControlPlane resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DNSRecordSpec>DNSRecordSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.DNSRecord>DNSRecord</a>)</p><p><p>DNSRecordSpec is the spec of a DNSRecord resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><em>(Optional)</em><p>Region is the region of this DNS record. If not specified, the region specified in SecretRef will be used.
If that is also not specified, the extension controller will use its default region.</p></td></tr><tr><td><code>zone</code></br><em>string</em></td><td><em>(Optional)</em><p>Zone is the DNS hosted zone of this DNS record. If not specified, it will be determined automatically by
getting all hosted zones of the account and searching for the longest zone name that is a suffix of Name.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the fully qualified domain name, e.g. “api.<shoot domain>”. This field is immutable.</p></td></tr><tr><td><code>recordType</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DNSRecordType>DNSRecordType</a></em></td><td><p>RecordType is the DNS record type. Only A, CNAME, and TXT records are currently supported. This field is immutable.</p></td></tr><tr><td><code>values</code></br><em>[]string</em></td><td><p>Values is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.</p></td></tr><tr><td><code>ttl</code></br><em>int64</em></td><td><em>(Optional)</em><p>TTL is the time to live in seconds. Defaults to 120.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DNSRecordStatus>DNSRecordStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.DNSRecord>DNSRecord</a>)</p><p><p>DNSRecordStatus is the status of a DNSRecord resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>zone</code></br><em>string</em></td><td><em>(Optional)</em><p>Zone is the DNS hosted zone of this DNS record.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DNSRecordType>DNSRecordType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.DNSRecordSpec>DNSRecordSpec</a>)</p><p><p>DNSRecordType is a string alias.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.DataVolume>DataVolume</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>WorkerPool</a>)</p><p><p>DataVolume contains information about a data volume.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the volume to make it referencable.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><em>(Optional)</em><p>Type is the type of the volume.</p></td></tr><tr><td><code>size</code></br><em>string</em></td><td><p>Size is the of the root volume.</p></td></tr><tr><td><code>encrypted</code></br><em>bool</em></td><td><em>(Optional)</em><p>Encrypted determines if the volume should be encrypted.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupBucketSpec>BackupBucketSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.BackupEntrySpec>BackupEntrySpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeSpec>ContainerRuntimeSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ControlPlaneSpec>ControlPlaneSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.DNSRecordSpec>DNSRecordSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ExtensionSpec>ExtensionSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.InfrastructureSpec>InfrastructureSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.NetworkSpec>NetworkSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a>,
<a href=#extensions.gardener.cloud/v1alpha1.WorkerSpec>WorkerSpec</a>)</p><p><p>DefaultSpec contains common status fields for every extension resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em>string</em></td><td><p>Type contains the instance of the resource’s kind.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is the provider specific configuration.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.BackupBucketStatus>BackupBucketStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.BackupEntryStatus>BackupEntryStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.BastionStatus>BastionStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ContainerRuntimeStatus>ContainerRuntimeStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ControlPlaneStatus>ControlPlaneStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.DNSRecordStatus>DNSRecordStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.ExtensionStatus>ExtensionStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.InfrastructureStatus>InfrastructureStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.NetworkStatus>NetworkStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigStatus>OperatingSystemConfigStatus</a>,
<a href=#extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</a>)</p><p><p>DefaultStatus contains common status fields for every extension resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>providerStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderStatus contains provider-specific status.</p></td></tr><tr><td><code>conditions</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Condition>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a Seed’s current state.</p></td></tr><tr><td><code>lastError</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.LastError>github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError</a></em></td><td><em>(Optional)</em><p>LastError holds information about the last occurred error during an operation.</p></td></tr><tr><td><code>lastOperation</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.LastOperation>github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation</a></em></td><td><em>(Optional)</em><p>LastOperation holds information about the last operation on the resource.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>state</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>State can be filled by the operating controller with what ever data it needs.</p></td></tr><tr><td><code>resources</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.NamedResourceReference>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference</a></em></td><td><em>(Optional)</em><p>Resources holds a list of named resource references that can be referred to in the state by their names.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.DropIn>DropIn</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Unit>Unit</a>)</p><p><p>DropIn is a drop-in configuration for a systemd unit.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the drop-in.</p></td></tr><tr><td><code>content</code></br><em>string</em></td><td><p>Content is the content of the drop-in.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ExtensionSpec>ExtensionSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Extension>Extension</a>)</p><p><p>ExtensionSpec is the spec for a Extension resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.ExtensionStatus>ExtensionStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Extension>Extension</a>)</p><p><p>ExtensionStatus is the status for a Extension resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.File>File</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a>)</p><p><p>File is a file that should get written to the host’s file system. The content can either be inlined or
referenced from a secret in the same namespace.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>path</code></br><em>string</em></td><td><p>Path is the path of the file system where the file should get written to.</p></td></tr><tr><td><code>permissions</code></br><em>int32</em></td><td><em>(Optional)</em><p>Permissions describes with which permissions the file should get written to the file system.
Should be defaulted to octal 0644.</p></td></tr><tr><td><code>content</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.FileContent>FileContent</a></em></td><td><p>Content describe the file’s content.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.FileCodecID>FileCodecID
(<code>string</code> alias)</p></h3><p><p>FileCodecID is the id of a FileCodec for cloud-init scripts.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.FileContent>FileContent</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.File>File</a>)</p><p><p>FileContent can either reference a secret or contain inline configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>secretRef</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.FileContentSecretRef>FileContentSecretRef</a></em></td><td><em>(Optional)</em><p>SecretRef is a struct that contains information about the referenced secret.</p></td></tr><tr><td><code>inline</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.FileContentInline>FileContentInline</a></em></td><td><em>(Optional)</em><p>Inline is a struct that contains information about the inlined data.</p></td></tr><tr><td><code>transmitUnencoded</code></br><em>bool</em></td><td><em>(Optional)</em><p>TransmitUnencoded set to true will ensure that the os-extension does not encode the file content when sent to the node.
This for example can be used to manipulate the clear-text content before it reaches the node.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.FileContentInline>FileContentInline</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.FileContent>FileContent</a>)</p><p><p>FileContentInline contains keys for inlining a file content’s data and encoding.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>encoding</code></br><em>string</em></td><td><p>Encoding is the file’s encoding (e.g. base64).</p></td></tr><tr><td><code>data</code></br><em>string</em></td><td><p>Data is the file’s data.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.FileContentSecretRef>FileContentSecretRef</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.FileContent>FileContent</a>)</p><p><p>FileContentSecretRef contains keys for referencing a file content’s data from a secret in the same namespace.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the secret.</p></td></tr><tr><td><code>dataKey</code></br><em>string</em></td><td><p>DataKey is the key in the secret’s <code>.data</code> field that should be read.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.IPFamily>IPFamily
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.NetworkSpec>NetworkSpec</a>)</p><p><p>IPFamily is a type for specifying an IP protocol version to use in Gardener clusters.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.InfrastructureSpec>InfrastructureSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Infrastructure>Infrastructure</a>)</p><p><p>InfrastructureSpec is the spec for an Infrastructure resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the region of this infrastructure. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider credentials.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>[]byte</em></td><td><em>(Optional)</em><p>SSHPublicKey is the public SSH key that should be used with this infrastructure.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.InfrastructureStatus>InfrastructureStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Infrastructure>Infrastructure</a>)</p><p><p>InfrastructureStatus is the status for an Infrastructure resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>nodesCIDR</code></br><em>string</em></td><td><em>(Optional)</em><p>NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller.
This might be needed in environments in which the CIDR for the network for the shoot worker node cannot
be statically defined in the Shoot resource but must be computed dynamically.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.MachineDeployment>MachineDeployment</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</a>)</p><p><p>MachineDeployment is a created machine deployment.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the <code>MachineDeployment</code> resource.</p></td></tr><tr><td><code>minimum</code></br><em>int32</em></td><td><p>Minimum is the minimum number for this machine deployment.</p></td></tr><tr><td><code>maximum</code></br><em>int32</em></td><td><p>Maximum is the maximum number for this machine deployment.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.MachineImage>MachineImage</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>WorkerPool</a>)</p><p><p>MachineImage contains logical information about the name and the version of the machie image that
should be used. The logical information must be mapped to the provider-specific information (e.g.,
AMIs, …) by the provider itself.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the logical name of the machine image.</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the version of the machine image.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.NetworkSpec>NetworkSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Network>Network</a>)</p><p><p>NetworkSpec is the spec for an Network resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>podCIDR</code></br><em>string</em></td><td><p>PodCIDR defines the CIDR that will be used for pods. This field is immutable.</p></td></tr><tr><td><code>serviceCIDR</code></br><em>string</em></td><td><p>ServiceCIDR defines the CIDR that will be used for services. This field is immutable.</p></td></tr><tr><td><code>ipFamilies</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.IPFamily>[]IPFamily</a></em></td><td><em>(Optional)</em><p>IPFamilies specifies the IP protocol versions to use for shoot networking. This field is immutable.
See <a href=/docs/gardener/usage/ipv6/>https://github.com/gardener/gardener/blob/master/docs/usage/ipv6.md</a></p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.NetworkStatus>NetworkStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Network>Network</a>)</p><p><p>NetworkStatus is the status for an Network resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.NodeTemplate>NodeTemplate</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>WorkerPool</a>)</p><p><p>NodeTemplate contains information about the expected node properties.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>capacity</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Capacity represents the expected Node capacity.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Object>Object</h3><p><p>Object is an extension object resource.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.OperatingSystemConfigPurpose>OperatingSystemConfigPurpose
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a>)</p><p><p>OperatingSystemConfigPurpose is a string alias.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfig>OperatingSystemConfig</a>)</p><p><p>OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>criConfig</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.CRIConfig>CRIConfig</a></em></td><td><em>(Optional)</em><p>CRI config is a structure contains configurations of the CRI library</p></td></tr><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>purpose</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigPurpose>OperatingSystemConfigPurpose</a></em></td><td><p>Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it
gets sent to the <code>Worker</code> extension controller to bootstrap a VM, or it is downloaded by the
cloud-config-downloader script already running on a bootstrapped VM.
This field is immutable.</p></td></tr><tr><td><code>reloadConfigFilePath</code></br><em>string</em></td><td><em>(Optional)</em><p>ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers
are asked to use it when determining the .status.command of this resource. For example, if for CoreOS
the reload-path might be “/var/lib/config”; then the controller shall set .status.command to
“/usr/bin/coreos-cloudinit –from-file=/var/lib/config”.</p></td></tr><tr><td><code>units</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.Unit>[]Unit</a></em></td><td><em>(Optional)</em><p>Units is a list of unit for the operating system configuration (usually, a systemd unit).</p></td></tr><tr><td><code>files</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.File>[]File</a></em></td><td><em>(Optional)</em><p>Files is a list of files that should get written to the host’s file system.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.OperatingSystemConfigStatus>OperatingSystemConfigStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfig>OperatingSystemConfig</a>)</p><p><p>OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>cloudConfig</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.CloudConfig>CloudConfig</a></em></td><td><em>(Optional)</em><p>CloudConfig is a structure for containing the generated output for the given operating system
config spec. It contains a reference to a secret as the result may contain confidential data.</p></td></tr><tr><td><code>command</code></br><em>string</em></td><td><em>(Optional)</em><p>Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g.
“/usr/bin/reload-cloud-config -from-file=<path>”. The <path>is optionally provided by Gardener
in the .spec.reloadConfigFilePath field.</p></td></tr><tr><td><code>units</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Units is a list of systemd unit names that are part of the generated Cloud Config and shall be
restarted when a new version has been downloaded.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Purpose>Purpose
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.ControlPlaneSpec>ControlPlaneSpec</a>)</p><p><p>Purpose is a string alias.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.Spec>Spec</h3><p><p>Spec is the spec section of an Object.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.Status>Status</h3><p><p>Status is the status of an Object.</p></p><h3 id=extensions.gardener.cloud/v1alpha1.Unit>Unit</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.OperatingSystemConfigSpec>OperatingSystemConfigSpec</a>)</p><p><p>Unit is a unit for the operating system configuration (usually, a systemd unit).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of a unit.</p></td></tr><tr><td><code>command</code></br><em>string</em></td><td><em>(Optional)</em><p>Command is the unit’s command.</p></td></tr><tr><td><code>enable</code></br><em>bool</em></td><td><em>(Optional)</em><p>Enable describes whether the unit is enabled or not.</p></td></tr><tr><td><code>content</code></br><em>string</em></td><td><em>(Optional)</em><p>Content is the unit’s content.</p></td></tr><tr><td><code>dropIns</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DropIn>[]DropIn</a></em></td><td><em>(Optional)</em><p>DropIns is a list of drop-ins for this unit.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.Volume>Volume</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>WorkerPool</a>)</p><p><p>Volume contains information about the root disks that should be used for worker pools.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><em>(Optional)</em><p>Name of the volume to make it referencable.</p></td></tr><tr><td><code>type</code></br><em>string</em></td><td><em>(Optional)</em><p>Type is the type of the volume.</p></td></tr><tr><td><code>size</code></br><em>string</em></td><td><p>Size is the of the root volume.</p></td></tr><tr><td><code>encrypted</code></br><em>bool</em></td><td><em>(Optional)</em><p>Encrypted determines if the volume should be encrypted.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.WorkerPool>WorkerPool</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.WorkerSpec>WorkerSpec</a>)</p><p><p>WorkerPool is the definition of a specific worker pool.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>machineType</code></br><em>string</em></td><td><p>MachineType contains information about the machine type that should be used for this worker pool.</p></td></tr><tr><td><code>maximum</code></br><em>int32</em></td><td><p>Maximum is the maximum size of the worker pool.</p></td></tr><tr><td><code>maxSurge</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><p>MaxSurge is maximum number of VMs that are created during an update.</p></td></tr><tr><td><code>maxUnavailable</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><p>MaxUnavailable is the maximum number of VMs that can be unavailable during an update.</p></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations is a map of key/value pairs for annotations for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels is a map of key/value pairs for labels for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>taints</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#taint-v1-core>[]Kubernetes core/v1.Taint</a></em></td><td><em>(Optional)</em><p>Taints is a list of taints for all the <code>Node</code> objects in this worker pool.</p></td></tr><tr><td><code>machineImage</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.MachineImage>MachineImage</a></em></td><td><p>MachineImage contains logical information about the name and the version of the machie image that
should be used. The logical information must be mapped to the provider-specific information (e.g.,
AMIs, …) by the provider itself.</p></td></tr><tr><td><code>minimum</code></br><em>int32</em></td><td><p>Minimum is the minimum size of the worker pool.</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of this worker pool.</p></td></tr><tr><td><code>providerConfig</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>ProviderConfig is a provider specific configuration for the worker pool.</p></td></tr><tr><td><code>userData</code></br><em>[]byte</em></td><td><p>UserData is a base64-encoded string that contains the data that is sent to the provider’s APIs
when a new machine/VM that is part of this worker pool shall be spawned.</p></td></tr><tr><td><code>volume</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.Volume>Volume</a></em></td><td><em>(Optional)</em><p>Volume contains information about the root disks that should be used for this worker pool.</p></td></tr><tr><td><code>dataVolumes</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DataVolume>[]DataVolume</a></em></td><td><em>(Optional)</em><p>DataVolumes contains a list of additional worker volumes.</p></td></tr><tr><td><code>kubeletDataVolumeName</code></br><em>string</em></td><td><em>(Optional)</em><p>KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.</p></td></tr><tr><td><code>zones</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Zones contains information about availability zones for this worker pool.</p></td></tr><tr><td><code>machineControllerManager</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.MachineControllerManagerSettings>github.com/gardener/gardener/pkg/apis/core/v1beta1.MachineControllerManagerSettings</a></em></td><td><em>(Optional)</em><p>MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.</p></td></tr><tr><td><code>kubernetesVersion</code></br><em>string</em></td><td><em>(Optional)</em><p>KubernetesVersion is the kubernetes version in this worker pool</p></td></tr><tr><td><code>nodeTemplate</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.NodeTemplate>NodeTemplate</a></em></td><td><em>(Optional)</em><p>NodeTemplate contains resource information of the machine which is used by Cluster Autoscaler to generate nodeTemplate during scaling a nodeGroup from zero</p></td></tr><tr><td><code>architecture</code></br><em>string</em></td><td><em>(Optional)</em><p>Architecture is the CPU architecture of the worker pool machines and machine image.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.WorkerSpec>WorkerSpec</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Worker>Worker</a>)</p><p><p>WorkerSpec is the spec for a Worker resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultSpec</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultSpec>DefaultSpec</a></em></td><td><p>(Members of <code>DefaultSpec</code> are embedded into this type.)</p><p>DefaultSpec is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>infrastructureProviderStatus</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>InfrastructureProviderStatus is a raw extension field that contains the provider status that has
been generated by the controller responsible for the <code>Infrastructure</code> resource.</p></td></tr><tr><td><code>region</code></br><em>string</em></td><td><p>Region is the name of the region where the worker pool should be deployed to. This field is immutable.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a secret that contains the cloud provider specific credentials.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>[]byte</em></td><td><em>(Optional)</em><p>SSHPublicKey is the public SSH key that should be used with these workers.</p></td></tr><tr><td><code>pools</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.WorkerPool>[]WorkerPool</a></em></td><td><p>Pools is a list of worker pools.</p></td></tr></tbody></table><h3 id=extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</h3><p>(<em>Appears on:</em>
<a href=#extensions.gardener.cloud/v1alpha1.Worker>Worker</a>)</p><p><p>WorkerStatus is the status for a Worker resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>DefaultStatus</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.DefaultStatus>DefaultStatus</a></em></td><td><p>(Members of <code>DefaultStatus</code> are embedded into this type.)</p><p>DefaultStatus is a structure containing common fields used by all extension resources.</p></td></tr><tr><td><code>machineDeployments</code></br><em><a href=#extensions.gardener.cloud/v1alpha1.MachineDeployment>[]MachineDeployment</a></em></td><td><p>MachineDeployments is a list of created machine deployments. It will be used to e.g. configure
the cluster-autoscaler properly.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-bf08e7ef74854170c72dba9d178a3302>1.1.4 - Operations</h1><p>Packages:</p><ul><li><a href=#operations.gardener.cloud%2fv1alpha1>operations.gardener.cloud/v1alpha1</a></li></ul><h2 id=operations.gardener.cloud/v1alpha1>operations.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is a version of the API.</p></p>Resource Types:<ul><li><a href=#operations.gardener.cloud/v1alpha1.Bastion>Bastion</a></li></ul><h3 id=operations.gardener.cloud/v1alpha1.Bastion>Bastion</h3><p><p>Bastion holds details about an SSH bastion for a shoot cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>operations.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>Bastion</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#operations.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</a></em></td><td><p>Specification of the Bastion.</p><br><br><table><tr><td><code>shootRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#localobjectreference-v1-core>Kubernetes core/v1.LocalObjectReference</a></em></td><td><p>ShootRef defines the target shoot for a Bastion. The name field of the ShootRef is immutable.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed to which this Bastion is currently scheduled. This field is populated
at the beginning of a create/reconcile operation.</p></td></tr><tr><td><code>providerType</code></br><em>string</em></td><td><em>(Optional)</em><p>ProviderType is cloud provider used by the referenced Shoot.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>string</em></td><td><p>SSHPublicKey is the user’s public key. This field is immutable.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#operations.gardener.cloud/v1alpha1.BastionIngressPolicy>[]BastionIngressPolicy</a></em></td><td><p>Ingress controls from where the created bastion host should be reachable.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#operations.gardener.cloud/v1alpha1.BastionStatus>BastionStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the Bastion.</p></td></tr></tbody></table><h3 id=operations.gardener.cloud/v1alpha1.BastionIngressPolicy>BastionIngressPolicy</h3><p>(<em>Appears on:</em>
<a href=#operations.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</a>)</p><p><p>BastionIngressPolicy represents an ingress policy for SSH bastion hosts.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ipBlock</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#ipblock-v1-networking>Kubernetes networking/v1.IPBlock</a></em></td><td><p>IPBlock defines an IP block that is allowed to access the bastion.</p></td></tr></tbody></table><h3 id=operations.gardener.cloud/v1alpha1.BastionSpec>BastionSpec</h3><p>(<em>Appears on:</em>
<a href=#operations.gardener.cloud/v1alpha1.Bastion>Bastion</a>)</p><p><p>BastionSpec is the specification of a Bastion.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>shootRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#localobjectreference-v1-core>Kubernetes core/v1.LocalObjectReference</a></em></td><td><p>ShootRef defines the target shoot for a Bastion. The name field of the ShootRef is immutable.</p></td></tr><tr><td><code>seedName</code></br><em>string</em></td><td><em>(Optional)</em><p>SeedName is the name of the seed to which this Bastion is currently scheduled. This field is populated
at the beginning of a create/reconcile operation.</p></td></tr><tr><td><code>providerType</code></br><em>string</em></td><td><em>(Optional)</em><p>ProviderType is cloud provider used by the referenced Shoot.</p></td></tr><tr><td><code>sshPublicKey</code></br><em>string</em></td><td><p>SSHPublicKey is the user’s public key. This field is immutable.</p></td></tr><tr><td><code>ingress</code></br><em><a href=#operations.gardener.cloud/v1alpha1.BastionIngressPolicy>[]BastionIngressPolicy</a></em></td><td><p>Ingress controls from where the created bastion host should be reachable.</p></td></tr></tbody></table><h3 id=operations.gardener.cloud/v1alpha1.BastionStatus>BastionStatus</h3><p>(<em>Appears on:</em>
<a href=#operations.gardener.cloud/v1alpha1.Bastion>Bastion</a>)</p><p><p>BastionStatus holds the most recently observed status of the Bastion.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ingress</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#loadbalanceringress-v1-core>Kubernetes core/v1.LoadBalancerIngress</a></em></td><td><em>(Optional)</em><p>Ingress holds the public IP and/or hostname of the bastion instance.</p></td></tr><tr><td><code>conditions</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1alpha1.Condition>[]github.com/gardener/gardener/pkg/apis/core/v1alpha1.Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a Bastion’s current state.</p></td></tr><tr><td><code>lastHeartbeatTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>LastHeartbeatTimestamp is the time when the bastion was last marked as
not to be deleted. When this is set, the ExpirationTimestamp is advanced
as well.</p></td></tr><tr><td><code>expirationTimestamp</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>ExpirationTimestamp is the time after which a Bastion is supposed to be
garbage collected.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this Bastion. It corresponds to the
Bastion’s generation, which is updated on mutation by the API Server.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-97ca1fbc87b4264d8485190bd50e2c5d>1.1.5 - Operator</h1><p>Packages:</p><ul><li><a href=#operator.gardener.cloud%2fv1alpha1>operator.gardener.cloud/v1alpha1</a></li></ul><h2 id=operator.gardener.cloud/v1alpha1>operator.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 contains the configuration of the Gardener Operator.</p></p>Resource Types:<ul></ul><h3 id=operator.gardener.cloud/v1alpha1.Backup>Backup</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.ETCDMain>ETCDMain</a>)</p><p><p>Backup contains the object store configuration for backups for the virtual garden etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>provider</code></br><em>string</em></td><td><p>Provider is a provider name. This field is immutable.</p></td></tr><tr><td><code>bucketName</code></br><em>string</em></td><td><p>BucketName is the name of the backup bucket.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where
backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</a>)</p><p><p>ControlPlane holds information about the general settings for the control plane of the virtual garden cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>highAvailability</code></br><em><a href=#operator.gardener.cloud/v1alpha1.HighAvailability>HighAvailability</a></em></td><td><em>(Optional)</em><p>HighAvailability holds the configuration settings for high availability settings.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.Credentials>Credentials</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.GardenStatus>GardenStatus</a>)</p><p><p>Credentials contains information about the virtual garden cluster credentials.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>rotation</code></br><em><a href=#operator.gardener.cloud/v1alpha1.CredentialsRotation>CredentialsRotation</a></em></td><td><em>(Optional)</em><p>Rotation contains information about the credential rotations.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.CredentialsRotation>CredentialsRotation</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.Credentials>Credentials</a>)</p><p><p>CredentialsRotation contains information about the rotation of credentials.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>certificateAuthorities</code></br><em>github.com/gardener/gardener/pkg/apis/core/v1beta1.CARotation</em></td><td><em>(Optional)</em><p>CertificateAuthorities contains information about the certificate authority credential rotation.</p></td></tr><tr><td><code>serviceAccountKey</code></br><em>github.com/gardener/gardener/pkg/apis/core/v1beta1.ServiceAccountKeyRotation</em></td><td><em>(Optional)</em><p>ServiceAccountKey contains information about the service account key credential rotation.</p></td></tr><tr><td><code>etcdEncryptionKey</code></br><em>github.com/gardener/gardener/pkg/apis/core/v1beta1.ETCDEncryptionKeyRotation</em></td><td><em>(Optional)</em><p>ETCDEncryptionKey contains information about the ETCD encryption key credential rotation.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.ETCD>ETCD</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</a>)</p><p><p>ETCD contains configuration for the etcds of the virtual garden cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>main</code></br><em><a href=#operator.gardener.cloud/v1alpha1.ETCDMain>ETCDMain</a></em></td><td><em>(Optional)</em><p>Main contains configuration for the main etcd.</p></td></tr><tr><td><code>events</code></br><em><a href=#operator.gardener.cloud/v1alpha1.ETCDEvents>ETCDEvents</a></em></td><td><em>(Optional)</em><p>Events contains configuration for the events etcd.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.ETCDEvents>ETCDEvents</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.ETCD>ETCD</a>)</p><p><p>ETCDEvents contains configuration for the events etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>storage</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Storage>Storage</a></em></td><td><em>(Optional)</em><p>Storage contains storage configuration.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.ETCDMain>ETCDMain</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.ETCD>ETCD</a>)</p><p><p>ETCDMain contains configuration for the main etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>backup</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Backup>Backup</a></em></td><td><em>(Optional)</em><p>Backup contains the object store configuration for backups for the virtual garden etcd.</p></td></tr><tr><td><code>storage</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Storage>Storage</a></em></td><td><em>(Optional)</em><p>Storage contains storage configuration.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.Garden>Garden</h3><p><p>Garden describes a list of gardens.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#operator.gardener.cloud/v1alpha1.GardenSpec>GardenSpec</a></em></td><td><p>Spec contains the specification of this garden.</p><br><br><table><tr><td><code>runtimeCluster</code></br><em><a href=#operator.gardener.cloud/v1alpha1.RuntimeCluster>RuntimeCluster</a></em></td><td><p>RuntimeCluster contains configuration for the runtime cluster.</p></td></tr><tr><td><code>virtualCluster</code></br><em><a href=#operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</a></em></td><td><p>VirtualCluster contains configuration for the virtual cluster.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#operator.gardener.cloud/v1alpha1.GardenStatus>GardenStatus</a></em></td><td><p>Status contains the status of this garden.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.GardenSpec>GardenSpec</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.Garden>Garden</a>)</p><p><p>GardenSpec contains the specification of a garden environment.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>runtimeCluster</code></br><em><a href=#operator.gardener.cloud/v1alpha1.RuntimeCluster>RuntimeCluster</a></em></td><td><p>RuntimeCluster contains configuration for the runtime cluster.</p></td></tr><tr><td><code>virtualCluster</code></br><em><a href=#operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</a></em></td><td><p>VirtualCluster contains configuration for the virtual cluster.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.GardenStatus>GardenStatus</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.Garden>Garden</a>)</p><p><p>GardenStatus is the status of a garden environment.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>gardener</code></br><em>github.com/gardener/gardener/pkg/apis/core/v1beta1.Gardener</em></td><td><em>(Optional)</em><p>Gardener holds information about the Gardener which last acted on the Garden.</p></td></tr><tr><td><code>conditions</code></br><em>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition</em></td><td><p>Conditions is a list of conditions.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>credentials</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Credentials>Credentials</a></em></td><td><em>(Optional)</em><p>Credentials contains information about the virtual garden cluster credentials.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.HighAvailability>HighAvailability</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</a>)</p><p><p>HighAvailability specifies the configuration settings for high availability for a resource.</p></p><h3 id=operator.gardener.cloud/v1alpha1.Maintenance>Maintenance</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</a>)</p><p><p>Maintenance contains information about the time window for maintenance operations.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>timeWindow</code></br><em>github.com/gardener/gardener/pkg/apis/core/v1beta1.MaintenanceTimeWindow</em></td><td><p>TimeWindow contains information about the time window for maintenance operations.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.Provider>Provider</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.RuntimeCluster>RuntimeCluster</a>)</p><p><p>Provider defines the provider-specific information for this cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>zones</code></br><em>[]string</em></td><td><em>(Optional)</em><p>Zones is the list of availability zones the cluster is deployed to.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.RuntimeCluster>RuntimeCluster</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.GardenSpec>GardenSpec</a>)</p><p><p>RuntimeCluster contains configuration for the runtime cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>provider</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Provider>Provider</a></em></td><td><p>Provider defines the provider-specific information for this cluster.</p></td></tr><tr><td><code>settings</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Settings>Settings</a></em></td><td><em>(Optional)</em><p>Settings contains certain settings for this cluster.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.SettingLoadBalancerServices>SettingLoadBalancerServices</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.Settings>Settings</a>)</p><p><p>SettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the
runtime cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations is a map of annotations that will be injected/merged into every load balancer service object.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.SettingVerticalPodAutoscaler>SettingVerticalPodAutoscaler</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.Settings>Settings</a>)</p><p><p>SettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the
seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>Enabled controls whether the VPA components shall be deployed into this cluster. It is true by default because
the operator (and Gardener) heavily rely on a VPA being deployed. You should only disable this if your runtime
cluster already has another, manually/custom managed VPA deployment. If this is not the case, but you still
disable it, then reconciliation will fail.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.Settings>Settings</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.RuntimeCluster>RuntimeCluster</a>)</p><p><p>Settings contains certain settings for this cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>loadBalancerServices</code></br><em><a href=#operator.gardener.cloud/v1alpha1.SettingLoadBalancerServices>SettingLoadBalancerServices</a></em></td><td><em>(Optional)</em><p>LoadBalancerServices controls certain settings for services of type load balancer that are created in the runtime
cluster.</p></td></tr><tr><td><code>verticalPodAutoscaler</code></br><em><a href=#operator.gardener.cloud/v1alpha1.SettingVerticalPodAutoscaler>SettingVerticalPodAutoscaler</a></em></td><td><em>(Optional)</em><p>VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the
cluster.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.Storage>Storage</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.ETCDEvents>ETCDEvents</a>,
<a href=#operator.gardener.cloud/v1alpha1.ETCDMain>ETCDMain</a>)</p><p><p>Storage contains storage configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>capacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>Capacity is the storage capacity for the volumes.</p></td></tr><tr><td><code>className</code></br><em>string</em></td><td><em>(Optional)</em><p>ClassName is the name of a storage class.</p></td></tr></tbody></table><h3 id=operator.gardener.cloud/v1alpha1.VirtualCluster>VirtualCluster</h3><p>(<em>Appears on:</em>
<a href=#operator.gardener.cloud/v1alpha1.GardenSpec>GardenSpec</a>)</p><p><p>VirtualCluster contains configuration for the virtual cluster.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>controlPlane</code></br><em><a href=#operator.gardener.cloud/v1alpha1.ControlPlane>ControlPlane</a></em></td><td><em>(Optional)</em><p>ControlPlane holds information about the general settings for the control plane of the virtual cluster.</p></td></tr><tr><td><code>etcd</code></br><em><a href=#operator.gardener.cloud/v1alpha1.ETCD>ETCD</a></em></td><td><em>(Optional)</em><p>ETCD contains configuration for the etcds of the virtual garden cluster.</p></td></tr><tr><td><code>maintenance</code></br><em><a href=#operator.gardener.cloud/v1alpha1.Maintenance>Maintenance</a></em></td><td><p>Maintenance contains information about the time window for maintenance operations.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb894f4ab7d87e13a51f4c08831e9f0a>1.1.6 - Provider Local</h1><p>Packages:</p><ul><li><a href=#local.provider.extensions.gardener.cloud%2fv1alpha1>local.provider.extensions.gardener.cloud/v1alpha1</a></li></ul><h2 id=local.provider.extensions.gardener.cloud/v1alpha1>local.provider.extensions.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 contains the local provider API resources.</p></p>Resource Types:<ul><li><a href=#local.provider.extensions.gardener.cloud/v1alpha1.CloudProfileConfig>CloudProfileConfig</a></li><li><a href=#local.provider.extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</a></li></ul><h3 id=local.provider.extensions.gardener.cloud/v1alpha1.CloudProfileConfig>CloudProfileConfig</h3><p><p>CloudProfileConfig contains provider-specific configuration that is embedded into Gardener&rsquo;s <code>CloudProfile</code>
resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>local.provider.extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>CloudProfileConfig</code></td></tr><tr><td><code>machineImages</code></br><em><a href=#local.provider.extensions.gardener.cloud/v1alpha1.MachineImages>[]MachineImages</a></em></td><td><p>MachineImages is the list of machine images that are understood by the controller. It maps
logical names and versions to provider-specific identifiers.</p></td></tr></tbody></table><h3 id=local.provider.extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</h3><p><p>WorkerStatus contains information about created worker resources.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>local.provider.extensions.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>WorkerStatus</code></td></tr><tr><td><code>machineImages</code></br><em><a href=#local.provider.extensions.gardener.cloud/v1alpha1.MachineImage>[]MachineImage</a></em></td><td><em>(Optional)</em><p>MachineImages is a list of machine images that have been used in this worker. Usually, the extension controller
gets the mapping from name/version to the provider-specific machine image data from the CloudProfile. However, if
a version that is still in use gets removed from this componentconfig it cannot reconcile anymore existing <code>Worker</code>
resources that are still using this version. Hence, it stores the used versions in the provider status to ensure
reconciliation is possible.</p></td></tr></tbody></table><h3 id=local.provider.extensions.gardener.cloud/v1alpha1.MachineImage>MachineImage</h3><p>(<em>Appears on:</em>
<a href=#local.provider.extensions.gardener.cloud/v1alpha1.WorkerStatus>WorkerStatus</a>)</p><p><p>MachineImage is a mapping from logical names and versions to provider-specific machine image data.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the logical name of the machine image.</p></td></tr><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the logical version of the machine image.</p></td></tr><tr><td><code>image</code></br><em>string</em></td><td><p>Image is the image for the machine image.</p></td></tr></tbody></table><h3 id=local.provider.extensions.gardener.cloud/v1alpha1.MachineImageVersion>MachineImageVersion</h3><p>(<em>Appears on:</em>
<a href=#local.provider.extensions.gardener.cloud/v1alpha1.MachineImages>MachineImages</a>)</p><p><p>MachineImageVersion contains a version and a provider-specific identifier.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>version</code></br><em>string</em></td><td><p>Version is the version of the image.</p></td></tr><tr><td><code>image</code></br><em>string</em></td><td><p>Image is the image for the machine image.</p></td></tr></tbody></table><h3 id=local.provider.extensions.gardener.cloud/v1alpha1.MachineImages>MachineImages</h3><p>(<em>Appears on:</em>
<a href=#local.provider.extensions.gardener.cloud/v1alpha1.CloudProfileConfig>CloudProfileConfig</a>)</p><p><p>MachineImages is a mapping from logical names and versions to provider-specific identifiers.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the logical name of the machine image.</p></td></tr><tr><td><code>versions</code></br><em><a href=#local.provider.extensions.gardener.cloud/v1alpha1.MachineImageVersion>[]MachineImageVersion</a></em></td><td><p>Versions contains versions and a provider-specific identifier.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b81a90a686c2492e4425c61e5875a5aa>1.1.7 - Resources</h1><p>Packages:</p><ul><li><a href=#resources.gardener.cloud%2fv1alpha1>resources.gardener.cloud/v1alpha1</a></li></ul><h2 id=resources.gardener.cloud/v1alpha1>resources.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 contains the configuration of the Gardener Resource Manager.</p></p>Resource Types:<ul></ul><h3 id=resources.gardener.cloud/v1alpha1.ManagedResource>ManagedResource</h3><p><p>ManagedResource describes a list of managed resources.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#resources.gardener.cloud/v1alpha1.ManagedResourceSpec>ManagedResourceSpec</a></em></td><td><p>Spec contains the specification of this managed resource.</p><br><br><table><tr><td><code>class</code></br><em>string</em></td><td><em>(Optional)</em><p>Class holds the resource class used to control the responsibility for multiple resource manager instances</p></td></tr><tr><td><code>secretRefs</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#localobjectreference-v1-core>[]Kubernetes core/v1.LocalObjectReference</a></em></td><td><p>SecretRefs is a list of secret references.</p></td></tr><tr><td><code>injectLabels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>InjectLabels injects the provided labels into every resource that is part of the referenced secrets.</p></td></tr><tr><td><code>forceOverwriteLabels</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceOverwriteLabels specifies that all existing labels should be overwritten. Defaults to false.</p></td></tr><tr><td><code>forceOverwriteAnnotations</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceOverwriteAnnotations specifies that all existing annotations should be overwritten. Defaults to false.</p></td></tr><tr><td><code>keepObjects</code></br><em>bool</em></td><td><em>(Optional)</em><p>KeepObjects specifies whether the objects should be kept although the managed resource has already been deleted.
Defaults to false.</p></td></tr><tr><td><code>equivalences</code></br><em>[][]k8s.io/apimachinery/pkg/apis/meta/v1.GroupKind</em></td><td><em>(Optional)</em><p>Equivalences specifies possible group/kind equivalences for objects.</p></td></tr><tr><td><code>deletePersistentVolumeClaims</code></br><em>bool</em></td><td><em>(Optional)</em><p>DeletePersistentVolumeClaims specifies if PersistentVolumeClaims created by StatefulSets, which are managed by this
resource, should also be deleted when the corresponding StatefulSet is deleted (defaults to false).</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#resources.gardener.cloud/v1alpha1.ManagedResourceStatus>ManagedResourceStatus</a></em></td><td><p>Status contains the status of this managed resource.</p></td></tr></tbody></table><h3 id=resources.gardener.cloud/v1alpha1.ManagedResourceSpec>ManagedResourceSpec</h3><p>(<em>Appears on:</em>
<a href=#resources.gardener.cloud/v1alpha1.ManagedResource>ManagedResource</a>)</p><p><p>ManagedResourceSpec contains the specification of this managed resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></br><em>string</em></td><td><em>(Optional)</em><p>Class holds the resource class used to control the responsibility for multiple resource manager instances</p></td></tr><tr><td><code>secretRefs</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#localobjectreference-v1-core>[]Kubernetes core/v1.LocalObjectReference</a></em></td><td><p>SecretRefs is a list of secret references.</p></td></tr><tr><td><code>injectLabels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>InjectLabels injects the provided labels into every resource that is part of the referenced secrets.</p></td></tr><tr><td><code>forceOverwriteLabels</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceOverwriteLabels specifies that all existing labels should be overwritten. Defaults to false.</p></td></tr><tr><td><code>forceOverwriteAnnotations</code></br><em>bool</em></td><td><em>(Optional)</em><p>ForceOverwriteAnnotations specifies that all existing annotations should be overwritten. Defaults to false.</p></td></tr><tr><td><code>keepObjects</code></br><em>bool</em></td><td><em>(Optional)</em><p>KeepObjects specifies whether the objects should be kept although the managed resource has already been deleted.
Defaults to false.</p></td></tr><tr><td><code>equivalences</code></br><em>[][]k8s.io/apimachinery/pkg/apis/meta/v1.GroupKind</em></td><td><em>(Optional)</em><p>Equivalences specifies possible group/kind equivalences for objects.</p></td></tr><tr><td><code>deletePersistentVolumeClaims</code></br><em>bool</em></td><td><em>(Optional)</em><p>DeletePersistentVolumeClaims specifies if PersistentVolumeClaims created by StatefulSets, which are managed by this
resource, should also be deleted when the corresponding StatefulSet is deleted (defaults to false).</p></td></tr></tbody></table><h3 id=resources.gardener.cloud/v1alpha1.ManagedResourceStatus>ManagedResourceStatus</h3><p>(<em>Appears on:</em>
<a href=#resources.gardener.cloud/v1alpha1.ManagedResource>ManagedResource</a>)</p><p><p>ManagedResourceStatus is the status of a managed resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Condition>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition</a></em></td><td></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>resources</code></br><em><a href=#resources.gardener.cloud/v1alpha1.ObjectReference>[]ObjectReference</a></em></td><td><em>(Optional)</em><p>Resources is a list of objects that have been created.</p></td></tr><tr><td><code>secretsDataChecksum</code></br><em>string</em></td><td><em>(Optional)</em><p>SecretsDataChecksum is the checksum of referenced secrets data.</p></td></tr></tbody></table><h3 id=resources.gardener.cloud/v1alpha1.ObjectReference>ObjectReference</h3><p>(<em>Appears on:</em>
<a href=#resources.gardener.cloud/v1alpha1.ManagedResourceStatus>ManagedResourceStatus</a>)</p><p><p>ObjectReference is a reference to another object.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>ObjectReference</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectreference-v1-core>Kubernetes core/v1.ObjectReference</a></em></td><td><p>(Members of <code>ObjectReference</code> are embedded into this type.)</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><p>Labels is a map of labels that were used during last update of the resource.</p></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><p>Annotations is a map of annotations that were used during last update of the resource.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-ba903207997565bcd7b1f010383f1b4f>1.1.8 - Seedmanagement</h1><p>Packages:</p><ul><li><a href=#seedmanagement.gardener.cloud%2fv1alpha1>seedmanagement.gardener.cloud/v1alpha1</a></li></ul><h2 id=seedmanagement.gardener.cloud/v1alpha1>seedmanagement.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is a version of the API.</p></p>Resource Types:<ul><li><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeed>ManagedSeed</a></li><li><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSet>ManagedSeedSet</a></li></ul><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeed>ManagedSeed</h3><p><p>ManagedSeed represents a Shoot that is registered as Seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>seedmanagement.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ManagedSeed</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSpec>ManagedSeedSpec</a></em></td><td><em>(Optional)</em><p>Specification of the ManagedSeed.</p><br><br><table><tr><td><code>shoot</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Shoot>Shoot</a></em></td><td><em>(Optional)</em><p>Shoot references a Shoot that should be registered as Seed.
This field is immutable.</p></td></tr><tr><td><code>gardenlet</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</a></em></td><td><em>(Optional)</em><p>Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster
with the given deployment parameters and GardenletConfiguration.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedStatus>ManagedSeedStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the ManagedSeed.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSet>ManagedSeedSet</h3><p><p>ManagedSeedSet represents a set of identical ManagedSeeds.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>seedmanagement.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ManagedSeedSet</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetSpec>ManagedSeedSetSpec</a></em></td><td><em>(Optional)</em><p>Spec defines the desired identities of ManagedSeeds and Shoots in this set.</p><br><br><table><tr><td><code>replicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>Replicas is the desired number of replicas of the given Template. Defaults to 1.</p></td></tr><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>Selector is a label query over ManagedSeeds and Shoots that should match the replica count.
It must match the ManagedSeeds and Shoots template’s labels. This field is immutable.</p></td></tr><tr><td><code>template</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedTemplate>ManagedSeedTemplate</a></em></td><td><p>Template describes the ManagedSeed that will be created if insufficient replicas are detected.
Each ManagedSeed created / updated by the ManagedSeedSet will fulfill this template.</p></td></tr><tr><td><code>shootTemplate</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.ShootTemplate>github.com/gardener/gardener/pkg/apis/core/v1beta1.ShootTemplate</a></em></td><td><p>ShootTemplate describes the Shoot that will be created if insufficient replicas are detected for hosting the corresponding ManagedSeed.
Each Shoot created / updated by the ManagedSeedSet will fulfill this template.</p></td></tr><tr><td><code>updateStrategy</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.UpdateStrategy>UpdateStrategy</a></em></td><td><em>(Optional)</em><p>UpdateStrategy specifies the UpdateStrategy that will be
employed to update ManagedSeeds / Shoots in the ManagedSeedSet when a revision is made to
Template / ShootTemplate.</p></td></tr><tr><td><code>revisionHistoryLimit</code></br><em>int32</em></td><td><em>(Optional)</em><p>RevisionHistoryLimit is the maximum number of revisions that will be maintained
in the ManagedSeedSet’s revision history. Defaults to 10. This field is immutable.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetStatus>ManagedSeedSetStatus</a></em></td><td><em>(Optional)</em><p>Status is the current status of ManagedSeeds and Shoots in this ManagedSeedSet.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.Bootstrap>Bootstrap
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</a>)</p><p><p>Bootstrap describes a mechanism for bootstrapping gardenlet connection to the Garden cluster.</p></p><h3 id=seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSpec>ManagedSeedSpec</a>)</p><p><p>Gardenlet specifies gardenlet deployment parameters and the GardenletConfiguration used to configure gardenlet.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>deployment</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.GardenletDeployment>GardenletDeployment</a></em></td><td><em>(Optional)</em><p>Deployment specifies certain gardenlet deployment parameters, such as the number of replicas,
the image, etc.</p></td></tr><tr><td><code>config</code></br><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><em>(Optional)</em><p>Config is the GardenletConfiguration used to configure gardenlet.</p></td></tr><tr><td><code>bootstrap</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Bootstrap>Bootstrap</a></em></td><td><em>(Optional)</em><p>Bootstrap is the mechanism that should be used for bootstrapping gardenlet connection to the Garden cluster. One of ServiceAccount, BootstrapToken, None.
If set to ServiceAccount or BootstrapToken, a service account or a bootstrap token will be created in the garden cluster and used to compute the bootstrap kubeconfig.
If set to None, the gardenClientConnection.kubeconfig field will be used to connect to the Garden cluster. Defaults to BootstrapToken.
This field is immutable.</p></td></tr><tr><td><code>mergeWithParent</code></br><em>bool</em></td><td><em>(Optional)</em><p>MergeWithParent specifies whether the GardenletConfiguration of the parent gardenlet
should be merged with the specified GardenletConfiguration. Defaults to true. This field is immutable.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.GardenletDeployment>GardenletDeployment</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</a>)</p><p><p>GardenletDeployment specifies certain gardenlet deployment parameters, such as the number of replicas,
the image, etc.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>replicaCount</code></br><em>int32</em></td><td><em>(Optional)</em><p>ReplicaCount is the number of gardenlet replicas. Defaults to 2.</p></td></tr><tr><td><code>revisionHistoryLimit</code></br><em>int32</em></td><td><em>(Optional)</em><p>RevisionHistoryLimit is the number of old gardenlet ReplicaSets to retain to allow rollback. Defaults to 2.</p></td></tr><tr><td><code>serviceAccountName</code></br><em>string</em></td><td><em>(Optional)</em><p>ServiceAccountName is the name of the ServiceAccount to use to run gardenlet pods.</p></td></tr><tr><td><code>image</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Image>Image</a></em></td><td><em>(Optional)</em><p>Image is the gardenlet container image.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources are the compute resources required by the gardenlet container.</p></td></tr><tr><td><code>podLabels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>PodLabels are the labels on gardenlet pods.</p></td></tr><tr><td><code>podAnnotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>PodAnnotations are the annotations on gardenlet pods.</p></td></tr><tr><td><code>additionalVolumes</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#volume-v1-core>[]Kubernetes core/v1.Volume</a></em></td><td><em>(Optional)</em><p>AdditionalVolumes is the list of additional volumes that should be mounted by gardenlet containers.</p></td></tr><tr><td><code>additionalVolumeMounts</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#volumemount-v1-core>[]Kubernetes core/v1.VolumeMount</a></em></td><td><em>(Optional)</em><p>AdditionalVolumeMounts is the list of additional pod volumes to mount into the gardenlet container’s filesystem.</p></td></tr><tr><td><code>env</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#envvar-v1-core>[]Kubernetes core/v1.EnvVar</a></em></td><td><em>(Optional)</em><p>Env is the list of environment variables to set in the gardenlet container.</p></td></tr><tr><td><code>vpa</code></br><em>bool</em></td><td><em>(Optional)</em><p>VPA specifies whether to enable VPA for gardenlet. Defaults to true.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.Image>Image</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.GardenletDeployment>GardenletDeployment</a>)</p><p><p>Image specifies container image parameters.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>repository</code></br><em>string</em></td><td><em>(Optional)</em><p>Repository is the image repository.</p></td></tr><tr><td><code>tag</code></br><em>string</em></td><td><em>(Optional)</em><p>Tag is the image tag.</p></td></tr><tr><td><code>pullPolicy</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#pullpolicy-v1-core>Kubernetes core/v1.PullPolicy</a></em></td><td><em>(Optional)</em><p>PullPolicy is the image pull policy. One of Always, Never, IfNotPresent.
Defaults to Always if latest tag is specified, or IfNotPresent otherwise.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetSpec>ManagedSeedSetSpec</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSet>ManagedSeedSet</a>)</p><p><p>ManagedSeedSetSpec is the specification of a ManagedSeedSet.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>Replicas is the desired number of replicas of the given Template. Defaults to 1.</p></td></tr><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>Selector is a label query over ManagedSeeds and Shoots that should match the replica count.
It must match the ManagedSeeds and Shoots template’s labels. This field is immutable.</p></td></tr><tr><td><code>template</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedTemplate>ManagedSeedTemplate</a></em></td><td><p>Template describes the ManagedSeed that will be created if insufficient replicas are detected.
Each ManagedSeed created / updated by the ManagedSeedSet will fulfill this template.</p></td></tr><tr><td><code>shootTemplate</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.ShootTemplate>github.com/gardener/gardener/pkg/apis/core/v1beta1.ShootTemplate</a></em></td><td><p>ShootTemplate describes the Shoot that will be created if insufficient replicas are detected for hosting the corresponding ManagedSeed.
Each Shoot created / updated by the ManagedSeedSet will fulfill this template.</p></td></tr><tr><td><code>updateStrategy</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.UpdateStrategy>UpdateStrategy</a></em></td><td><em>(Optional)</em><p>UpdateStrategy specifies the UpdateStrategy that will be
employed to update ManagedSeeds / Shoots in the ManagedSeedSet when a revision is made to
Template / ShootTemplate.</p></td></tr><tr><td><code>revisionHistoryLimit</code></br><em>int32</em></td><td><em>(Optional)</em><p>RevisionHistoryLimit is the maximum number of revisions that will be maintained
in the ManagedSeedSet’s revision history. Defaults to 10. This field is immutable.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetStatus>ManagedSeedSetStatus</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSet>ManagedSeedSet</a>)</p><p><p>ManagedSeedSetStatus represents the current state of a ManagedSeedSet.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><p>ObservedGeneration is the most recent generation observed for this ManagedSeedSet. It corresponds to the
ManagedSeedSet’s generation, which is updated on mutation by the API Server.</p></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td><p>Replicas is the number of replicas (ManagedSeeds and their corresponding Shoots) created by the ManagedSeedSet controller.</p></td></tr><tr><td><code>readyReplicas</code></br><em>int32</em></td><td><p>ReadyReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller that have a Ready Condition.</p></td></tr><tr><td><code>nextReplicaNumber</code></br><em>int32</em></td><td><p>NextReplicaNumber is the ordinal number that will be assigned to the next replica of the ManagedSeedSet.</p></td></tr><tr><td><code>currentReplicas</code></br><em>int32</em></td><td><p>CurrentReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller from the ManagedSeedSet version
indicated by CurrentRevision.</p></td></tr><tr><td><code>updatedReplicas</code></br><em>int32</em></td><td><p>UpdatedReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller from the ManagedSeedSet version
indicated by UpdateRevision.</p></td></tr><tr><td><code>currentRevision</code></br><em>string</em></td><td><p>CurrentRevision, if not empty, indicates the version of the ManagedSeedSet used to generate ManagedSeeds with smaller
ordinal numbers during updates.</p></td></tr><tr><td><code>updateRevision</code></br><em>string</em></td><td><p>UpdateRevision, if not empty, indicates the version of the ManagedSeedSet used to generate ManagedSeeds with larger
ordinal numbers during updates</p></td></tr><tr><td><code>collisionCount</code></br><em>int32</em></td><td><em>(Optional)</em><p>CollisionCount is the count of hash collisions for the ManagedSeedSet. The ManagedSeedSet controller
uses this field as a collision avoidance mechanism when it needs to create the name for the
newest ControllerRevision.</p></td></tr><tr><td><code>conditions</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Condition>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a ManagedSeedSet’s current state.</p></td></tr><tr><td><code>pendingReplica</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.PendingReplica>PendingReplica</a></em></td><td><em>(Optional)</em><p>PendingReplica, if not empty, indicates the replica that is currently pending creation, update, or deletion.
This replica is in a state that requires the controller to wait for it to change before advancing to the next replica.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSpec>ManagedSeedSpec</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeed>ManagedSeed</a>,
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedTemplate>ManagedSeedTemplate</a>)</p><p><p>ManagedSeedSpec is the specification of a ManagedSeed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>shoot</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Shoot>Shoot</a></em></td><td><em>(Optional)</em><p>Shoot references a Shoot that should be registered as Seed.
This field is immutable.</p></td></tr><tr><td><code>gardenlet</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</a></em></td><td><em>(Optional)</em><p>Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster
with the given deployment parameters and GardenletConfiguration.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedStatus>ManagedSeedStatus</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeed>ManagedSeed</a>)</p><p><p>ManagedSeedStatus is the status of a ManagedSeed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Condition>[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of a ManagedSeed’s current state.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><p>ObservedGeneration is the most recent generation observed for this ManagedSeed. It corresponds to the
ManagedSeed’s generation, which is updated on mutation by the API Server.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.ManagedSeedTemplate>ManagedSeedTemplate</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetSpec>ManagedSeedSetSpec</a>)</p><p><p>ManagedSeedTemplate is a template for creating a ManagedSeed object.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSpec>ManagedSeedSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the ManagedSeed.</p><br><br><table><tr><td><code>shoot</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Shoot>Shoot</a></em></td><td><em>(Optional)</em><p>Shoot references a Shoot that should be registered as Seed.
This field is immutable.</p></td></tr><tr><td><code>gardenlet</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.Gardenlet>Gardenlet</a></em></td><td><em>(Optional)</em><p>Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster
with the given deployment parameters and GardenletConfiguration.</p></td></tr></table></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.PendingReplica>PendingReplica</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetStatus>ManagedSeedSetStatus</a>)</p><p><p>PendingReplica contains information about a replica that is currently pending creation, update, or deletion.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the replica name.</p></td></tr><tr><td><code>reason</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.PendingReplicaReason>PendingReplicaReason</a></em></td><td><p>Reason is the reason for the replica to be pending.</p></td></tr><tr><td><code>since</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Since is the moment in time since the replica is pending with the specified reason.</p></td></tr><tr><td><code>retries</code></br><em>int32</em></td><td><em>(Optional)</em><p>Retries is the number of times the shoot operation (reconcile or delete) has been retried after having failed.
Only applicable if Reason is ShootReconciling or ShootDeleting.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.PendingReplicaReason>PendingReplicaReason
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.PendingReplica>PendingReplica</a>)</p><p><p>PendingReplicaReason is a string enumeration type that enumerates all possible reasons for a replica to be pending.</p></p><h3 id=seedmanagement.gardener.cloud/v1alpha1.RollingUpdateStrategy>RollingUpdateStrategy</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.UpdateStrategy>UpdateStrategy</a>)</p><p><p>RollingUpdateStrategy is used to communicate parameters for RollingUpdateStrategyType.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>partition</code></br><em>int32</em></td><td><em>(Optional)</em><p>Partition indicates the ordinal at which the ManagedSeedSet should be partitioned. Defaults to 0.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.Shoot>Shoot</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSpec>ManagedSeedSpec</a>)</p><p><p>Shoot identifies the Shoot that should be registered as Seed.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the Shoot that will be registered as Seed.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.UpdateStrategy>UpdateStrategy</h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.ManagedSeedSetSpec>ManagedSeedSetSpec</a>)</p><p><p>UpdateStrategy specifies the strategy that the ManagedSeedSet
controller will use to perform updates. It includes any additional parameters
necessary to perform the update for the indicated strategy.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.UpdateStrategyType>UpdateStrategyType</a></em></td><td><em>(Optional)</em><p>Type indicates the type of the UpdateStrategy. Defaults to RollingUpdate.</p></td></tr><tr><td><code>rollingUpdate</code></br><em><a href=#seedmanagement.gardener.cloud/v1alpha1.RollingUpdateStrategy>RollingUpdateStrategy</a></em></td><td><em>(Optional)</em><p>RollingUpdate is used to communicate parameters when Type is RollingUpdateStrategyType.</p></td></tr></tbody></table><h3 id=seedmanagement.gardener.cloud/v1alpha1.UpdateStrategyType>UpdateStrategyType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#seedmanagement.gardener.cloud/v1alpha1.UpdateStrategy>UpdateStrategy</a>)</p><p><p>UpdateStrategyType is a string enumeration type that enumerates
all possible update strategies for the ManagedSeedSet controller.</p></p><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-50022620f86c6e0e31acdb7087f2e64c>1.1.9 - Settings</h1><p>Packages:</p><ul><li><a href=#settings.gardener.cloud%2fv1alpha1>settings.gardener.cloud/v1alpha1</a></li></ul><h2 id=settings.gardener.cloud/v1alpha1>settings.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is a version of the API.</p></p>Resource Types:<ul><li><a href=#settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPreset>ClusterOpenIDConnectPreset</a></li><li><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPreset>OpenIDConnectPreset</a></li></ul><h3 id=settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPreset>ClusterOpenIDConnectPreset</h3><p><p>ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied
to a Shoot objects cluster-wide.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>settings.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>ClusterOpenIDConnectPreset</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPresetSpec>ClusterOpenIDConnectPresetSpec</a></em></td><td><p>Spec is the specification of this OpenIDConnect preset.</p><br><br><table><tr><td><code>OpenIDConnectPresetSpec</code></br><em><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</a></em></td><td><p>(Members of <code>OpenIDConnectPresetSpec</code> are embedded into this type.)</p></td></tr><tr><td><code>projectSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Project decides whether to apply the configuration if the
Shoot is in a specific Project matching the label selector.
Use the selector only if the OIDC Preset is opt-in, because end
users may skip the admission by setting the labels.
Defaults to the empty LabelSelector, which matches everything.</p></td></tr></table></td></tr></tbody></table><h3 id=settings.gardener.cloud/v1alpha1.OpenIDConnectPreset>OpenIDConnectPreset</h3><p><p>OpenIDConnectPreset is a OpenID Connect configuration that is applied
to a Shoot in a namespace.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></br>string</td><td><code>settings.gardener.cloud/v1alpha1</code></td></tr><tr><td><code>kind</code></br>string</td><td><code>OpenIDConnectPreset</code></td></tr><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</a></em></td><td><p>Spec is the specification of this OpenIDConnect preset.</p><br><br><table><tr><td><code>server</code></br><em><a href=#settings.gardener.cloud/v1alpha1.KubeAPIServerOpenIDConnect>KubeAPIServerOpenIDConnect</a></em></td><td><p>Server contains the kube-apiserver&rsquo;s OpenID Connect configuration.
This configuration is not overwritting any existing OpenID Connect
configuration already set on the Shoot object.</p></td></tr><tr><td><code>client</code></br><em><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectClientAuthentication>OpenIDConnectClientAuthentication</a></em></td><td><em>(Optional)</em><p>Client contains the configuration used for client OIDC authentication
of Shoot clusters.
This configuration is not overwritting any existing OpenID Connect
client authentication already set on the Shoot object.</p></td></tr><tr><td><code>shootSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>ShootSelector decides whether to apply the configuration if the
Shoot has matching labels.
Use the selector only if the OIDC Preset is opt-in, because end
users may skip the admission by setting the labels.
Default to the empty LabelSelector, which matches everything.</p></td></tr><tr><td><code>weight</code></br><em>int32</em></td><td><p>Weight associated with matching the corresponding preset,
in the range 1-100.
Required.</p></td></tr></table></td></tr></tbody></table><h3 id=settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPresetSpec>ClusterOpenIDConnectPresetSpec</h3><p>(<em>Appears on:</em>
<a href=#settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPreset>ClusterOpenIDConnectPreset</a>)</p><p><p>ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and
project selector matching Shoots in Projects.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>OpenIDConnectPresetSpec</code></br><em><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</a></em></td><td><p>(Members of <code>OpenIDConnectPresetSpec</code> are embedded into this type.)</p></td></tr><tr><td><code>projectSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Project decides whether to apply the configuration if the
Shoot is in a specific Project matching the label selector.
Use the selector only if the OIDC Preset is opt-in, because end
users may skip the admission by setting the labels.
Defaults to the empty LabelSelector, which matches everything.</p></td></tr></tbody></table><h3 id=settings.gardener.cloud/v1alpha1.KubeAPIServerOpenIDConnect>KubeAPIServerOpenIDConnect</h3><p>(<em>Appears on:</em>
<a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</a>)</p><p><p>KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider.
Note: Descriptions were taken from the Kubernetes documentation.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>caBundle</code></br><em>string</em></td><td><em>(Optional)</em><p>If set, the OpenID server&rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host&rsquo;s root CA set will be used.</p></td></tr><tr><td><code>clientID</code></br><em>string</em></td><td><p>The client ID for the OpenID Connect client.
Required.</p></td></tr><tr><td><code>groupsClaim</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.</p></td></tr><tr><td><code>groupsPrefix</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.</p></td></tr><tr><td><code>issuerURL</code></br><em>string</em></td><td><p>The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).
Required.</p></td></tr><tr><td><code>requiredClaims</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.</p></td></tr><tr><td><code>signingAlgs</code></br><em>[]string</em></td><td><em>(Optional)</em><p>List of allowed JOSE asymmetric signing algorithms. JWTs with a &lsquo;alg&rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 <a href=https://tools.ietf.org/html/rfc7518#section-3.1>https://tools.ietf.org/html/rfc7518#section-3.1</a>
Defaults to [RS256]</p></td></tr><tr><td><code>usernameClaim</code></br><em>string</em></td><td><em>(Optional)</em><p>The OpenID claim to use as the user name. Note that claims other than the default (&lsquo;sub&rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details.
Defaults to &ldquo;sub&rdquo;.</p></td></tr><tr><td><code>usernamePrefix</code></br><em>string</em></td><td><em>(Optional)</em><p>If provided, all usernames will be prefixed with this value. If not provided, username claims other than &lsquo;email&rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value &lsquo;-&rsquo;.</p></td></tr></tbody></table><h3 id=settings.gardener.cloud/v1alpha1.OpenIDConnectClientAuthentication>OpenIDConnectClientAuthentication</h3><p>(<em>Appears on:</em>
<a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</a>)</p><p><p>OpenIDConnectClientAuthentication contains configuration for OIDC clients.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>secret</code></br><em>string</em></td><td><em>(Optional)</em><p>The client Secret for the OpenID Connect client.</p></td></tr><tr><td><code>extraConfig</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Extra configuration added to kubeconfig&rsquo;s auth-provider.
Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token</p></td></tr></tbody></table><h3 id=settings.gardener.cloud/v1alpha1.OpenIDConnectPresetSpec>OpenIDConnectPresetSpec</h3><p>(<em>Appears on:</em>
<a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectPreset>OpenIDConnectPreset</a>,
<a href=#settings.gardener.cloud/v1alpha1.ClusterOpenIDConnectPresetSpec>ClusterOpenIDConnectPresetSpec</a>)</p><p><p>OpenIDConnectPresetSpec contains the Shoot selector for which
a specific OpenID Connect configuration is applied.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>server</code></br><em><a href=#settings.gardener.cloud/v1alpha1.KubeAPIServerOpenIDConnect>KubeAPIServerOpenIDConnect</a></em></td><td><p>Server contains the kube-apiserver&rsquo;s OpenID Connect configuration.
This configuration is not overwritting any existing OpenID Connect
configuration already set on the Shoot object.</p></td></tr><tr><td><code>client</code></br><em><a href=#settings.gardener.cloud/v1alpha1.OpenIDConnectClientAuthentication>OpenIDConnectClientAuthentication</a></em></td><td><em>(Optional)</em><p>Client contains the configuration used for client OIDC authentication
of Shoot clusters.
This configuration is not overwritting any existing OpenID Connect
client authentication already set on the Shoot object.</p></td></tr><tr><td><code>shootSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>ShootSelector decides whether to apply the configuration if the
Shoot has matching labels.
Use the selector only if the OIDC Preset is opt-in, because end
users may skip the admission by setting the labels.
Default to the empty LabelSelector, which matches everything.</p></td></tr><tr><td><code>weight</code></br><em>int32</em></td><td><p>Weight associated with matching the corresponding preset,
in the range 1-100.
Required.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8fbd5b9ae8f9df2cc2758feb9fc249f2>1.2 - Concepts</h1></div><div class=td-content><h1 id=pg-ccbcc1039e04b807c6daf64b9352f3d3>1.2.1 - Admission Controller</h1><h1 id=gardener-admission-controller>Gardener Admission Controller</h1><p>While the Gardener API server works with <a href=/docs/gardener/concepts/apiserver_admission_plugins/>admission plugins</a> to validate and mutate resources belonging to Gardener related API groups, e.g. <code>core.gardener.cloud</code>, the same is needed for resources belonging to non-Gardener API groups as well, e.g. secrets in the <code>core</code> API group.
Therefore, the Gardener Admission Controller runs a http(s) server with the following handlers which serve as validating/mutating endpoints for <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>admission webhooks</a>.
It is also used to serve http(s) handlers for authorization webhooks.</p><h2 id=admission-webhook-handlers>Admission Webhook Handlers</h2><p>This section describes the admission webhook handlers that are currently served.</p><h3 id=kubeconfig-secret-validator>Kubeconfig Secret Validator</h3><p><a href=https://github.com/kubernetes/kubectl/issues/697>Malicious Kubeconfigs</a> applied by end users may cause a leakage of sensitive data.
This handler checks if the incoming request contains a Kubernetes secret with a <code>.data.kubeconfig</code> field and denies the request if the Kubeconfig structure violates Gardener&rsquo;s security standards.</p><h3 id=namespace-validator>Namespace Validator</h3><p>Namespaces are the backing entities of Gardener projects in which shoot cluster objects reside.
This validation handler protects active namespaces against premature deletion requests.
Therefore, it denies deletion requests if a namespace still contains shoot clusters or if it belongs to a non-deleting Gardener project (without <code>.metadata.deletionTimestamp</code>).</p><h3 id=resource-size-validator>Resource Size Validator</h3><p>Since users directly apply Kubernetes native objects to the Garden cluster, it also involves the risk of being vulnerable to DoS attacks because these resources are continuously watched and read by controllers.
One example is the creation of shoot resources with large annotation values (up to 256 kB per value), which can cause severe out-of-memory issues for the gardenlet component.
<a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>Vertical autoscaling</a> can help to mitigate such situations, but we cannot expect to scale infinitely, and thus need means to block the attack itself.</p><p>The Resource Size Validator checks arbitrary incoming admission requests against a configured maximum size for the resource&rsquo;s group-version-kind combination. It denies the request if the object exceeds the quota.</p><p>Example for Gardener Admission Controller configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  resourceAdmissionConfiguration:
</span></span><span style=display:flex><span>    limits:
</span></span><span style=display:flex><span>    - apiGroups: [<span style=color:#a31515>&#34;core.gardener.cloud&#34;</span>]
</span></span><span style=display:flex><span>      apiVersions: [<span style=color:#a31515>&#34;*&#34;</span>]
</span></span><span style=display:flex><span>      resources: [<span style=color:#a31515>&#34;shoots&#34;</span>]
</span></span><span style=display:flex><span>      size: 100k
</span></span><span style=display:flex><span>    - apiGroups: [<span style=color:#a31515>&#34;&#34;</span>]
</span></span><span style=display:flex><span>      apiVersions: [<span style=color:#a31515>&#34;v1&#34;</span>]
</span></span><span style=display:flex><span>      resources: [<span style=color:#a31515>&#34;secrets&#34;</span>]
</span></span><span style=display:flex><span>      size: 100k
</span></span><span style=display:flex><span>    unrestrictedSubjects:
</span></span><span style=display:flex><span>    - kind: Group
</span></span><span style=display:flex><span>      name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>      apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span> <span style=color:green>#  - kind: User</span>
</span></span><span style=display:flex><span> <span style=color:green>#    name: admin</span>
</span></span><span style=display:flex><span> <span style=color:green>#    apiGroup: rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span> <span style=color:green>#  - kind: ServiceAccount</span>
</span></span><span style=display:flex><span> <span style=color:green>#    name: &#34;*&#34;</span>
</span></span><span style=display:flex><span> <span style=color:green>#    namespace: garden</span>
</span></span><span style=display:flex><span> <span style=color:green>#    apiGroup: &#34;&#34;</span>
</span></span><span style=display:flex><span>    operationMode: block <span style=color:green>#log</span>
</span></span></code></pre></div><p>With the configuration above, the Resource Size Validator denies requests for shoots with Gardener&rsquo;s core API group which exceed a size of 100 kB. The same is done for Kubernetes secrets.</p><p>As this feature is meant to protect the system from malicious requests sent by users, it is recommended to exclude trusted groups, users or service accounts from the size restriction via <code>resourceAdmissionConfiguration.unrestrictedSubjects</code>.
For example, the backing user for the gardenlet should always be capable of changing the shoot resource instead of being blocked due to size restrictions.
This is because the gardenlet itself occasionally changes the shoot specification, labels or annotations, and might violate the quota if the existing resource is already close to the quota boundary.
Also, operators are supposed to be trusted users and subjecting them to a size limitation can inhibit important operational tasks.
Wildcard ("*") in subject <code>name</code> is supported.</p><p>Size limitations depend on the individual Gardener setup and choosing the wrong values can affect the availability of your Gardener service.
<code>resourceAdmissionConfiguration.operationMode</code> allows to control if a violating request is actually denied (default) or only logged.
It&rsquo;s recommended to start with <code>log</code>, check the logs for exceeding requests, adjust the limits if necessary and finally switch to <code>block</code>.</p><h3 id=seedrestriction>SeedRestriction</h3><p>Please refer to <a href=/docs/gardener/deployment/gardenlet_api_access/>Scoped API Access for Gardenlets</a> for more information.</p><h2 id=authorization-webhook-handlers>Authorization Webhook Handlers</h2><p>This section describes the authorization webhook handlers that are currently served.</p><h3 id=seedauthorization>SeedAuthorization</h3><p>Please refer to <a href=/docs/gardener/deployment/gardenlet_api_access/>Scoped API Access for Gardenlets</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-467f70291b55149caa507ac09cae563f>1.2.2 - API Server</h1><h1 id=gardener-api-server>Gardener API Server</h1><p>The Gardener API server is a Kubernetes-native extension based on its <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>aggregation layer</a>.
It is registered via an <code>APIService</code> object and designed to run inside a Kubernetes cluster whose API it wants to extend.</p><p>After registration, it exposes the following resources:</p><h2 id=cloudprofiles><code>CloudProfile</code>s</h2><p><code>CloudProfile</code>s are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc.
Each shoot has to reference a <code>CloudProfile</code> to declare the environment it should be created in.
In a <code>CloudProfile</code>, the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions they want to offer, etc.
End-users can read <code>CloudProfile</code>s to see these values, but only operators can change the content or create/delete them.
When a shoot is created or updated, then an admission plugin checks that only allowed values are used via the referenced <code>CloudProfile</code>.</p><p>Additionally, a <code>CloudProfile</code> may contain a <code>providerConfig</code>, which is a special configuration dedicated for the infrastructure provider.
Gardener does not evaluate or understand this config, but extension controllers might need it for declaration of provider-specific constraints, or global settings.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml>this</a> example manifest and consult the documentation of your provider extension controller to get information about its <code>providerConfig</code>.</p><h2 id=seeds><code>Seed</code>s</h2><p><code>Seed</code>s are resources that represent seed clusters.
Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.20 and passes the Kubernetes conformance tests.
The Gardener operator has to either deploy the gardenlet into the cluster they want to use as seed (recommended, then the gardenlet will create the <code>Seed</code> object itself after bootstrapping) or provide the kubeconfig to the cluster inside a secret (that is referenced by the <code>Seed</code> resource) and create the <code>Seed</code> resource themselves.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/45-secret-seed-backup.yaml>this</a>, <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>this</a>, and optionally <a href=https://github.com/gardener/gardener/blob/master/example/40-secret-seed.yaml>this</a> example manifests.</p><h2 id=shootquotas>Shoot<code>Quota</code>s</h2><p>To allow end-users not having their dedicated infrastructure account to try out Gardener, the operator can register an account owned by them that they allow to be used for trial clusters.
Trial clusters can be put under quota so that they don&rsquo;t consume too many resources (resulting in costs) and that one user cannot consume all resources on their own.
These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/60-quota.yaml>this</a> example manifest.</p><h2 id=projects><code>Project</code>s</h2><p>The first thing before creating a shoot cluster is to create a <code>Project</code>.
A project is used to group multiple shoot clusters together.
End-users can invite colleagues to the project to enable collaboration, and they can either make them <code>admin</code> or <code>viewer</code>.
After an end-user has created a project, they will get a dedicated namespace in the garden cluster for all their shoots.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml>this</a> example manifest.</p><h2 id=secretbindings><code>SecretBinding</code>s</h2><p>Now that the end-user has a namespace the next step is registering their infrastructure provider account.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml>this</a> example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.</p><p>After the secret has been created, the end-user has to create a special <code>SecretBinding</code> resource that binds this secret.
Later, when creating shoot clusters, they will reference such binding.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/80-secretbinding.yaml>this</a> example manifest.</p><h2 id=shoots><code>Shoot</code>s</h2><p>Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end.
As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well.
Such configurations are not evaluated by Gardener (because it doesn&rsquo;t know/understand them), but they are only transported to the respective extension controller.</p><p>⚠️ This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>this</a> example manifest and consult the documentation of the provider extension controller to get information about its <code>spec.provider.controlPlaneConfig</code>, <code>.spec.provider.infrastructureConfig</code>, and <code>.spec.provider.workers[].providerConfig</code>.</p><h2 id=clusteropenidconnectpresets><code>(Cluster)OpenIDConnectPreset</code>s</h2><p>Please see <a href=/docs/gardener/usage/openidconnect-presets/>this</a> separate documentation file.</p><h2 id=overview-data-model>Overview Data Model</h2><p><img src=/__resources/gardener-data-model-overview_c5b559.png alt="Gardener Overview Data Model"></p></div><div class=td-content style=page-break-before:always><h1 id=pg-330a9a0a66325841218788d3193e6fdf>1.2.3 - APIServer Admission Plugins</h1><h1 id=admission-plugins>Admission Plugins</h1><p>Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins.
If you want to get an overview of the what and why of admission plugins then <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>this document</a> might be a good start.</p><p>This document lists all existing admission plugins with a short explanation of what it is responsible for.</p><h2 id=clusteropenidconnectpreset-openidconnectpreset><code>ClusterOpenIDConnectPreset</code>, <code>OpenIDConnectPreset</code></h2><p><em>(both enabled by default)</em></p><p>These admission controllers react on <code>CREATE</code> operations for <code>Shoot</code>s.
If the <code>Shoot</code> does not specify any OIDC configuration (<code>.spec.kubernetes.kubeAPIServer.oidcConfig=nil</code>), then it tries to find a matching <code>ClusterOpenIDConnectPreset</code> or <code>OpenIDConnectPreset</code>, respectively.
If there are multiple matches, then the one with the highest weight &ldquo;wins&rdquo;.
In this case, the admission controller will default the OIDC configuration in the <code>Shoot</code>.</p><h2 id=controllerregistrationresources><code>ControllerRegistrationResources</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>ControllerRegistration</code>s.
It validates that there exists only one <code>ControllerRegistration</code> in the system that is primarily responsible for a given kind/type resource combination.
This prevents misconfiguration by the Gardener administrator/operator.</p><h2 id=customverbauthorizer><code>CustomVerbAuthorizer</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Project</code>s.
It validates whether the user is bound to a RBAC role with the <code>modify-spec-tolerations-whitelist</code> verb in case the user tries to change the <code>.spec.tolerations.whitelist</code> field of the respective <code>Project</code> resource.
Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on <code>Project</code> basis.</p><h2 id=deletionconfirmation><code>DeletionConfirmation</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>Project</code>s and <code>Shoot</code>s and <code>ShootState</code>s.
It validates that the respective resource is annotated with a deletion confirmation annotation, namely <code>confirmation.gardener.cloud/deletion=true</code>.
Only if this annotation is present it allows the <code>DELETE</code> operation to pass.
This prevents users from accidental/undesired deletions.</p><h2 id=exposureclass><code>ExposureClass</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>Create</code> operations for <code>Shoots</code>s.
It mutates <code>Shoot</code> resources which have an <code>ExposureClass</code> referenced by merging both their <code>shootSelectors</code> and/or <code>tolerations</code> into the <code>Shoot</code> resource.</p><h2 id=extensionvalidator><code>ExtensionValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>BackupEntry</code>s, <code>BackupBucket</code>s, <code>Seed</code>s, and <code>Shoot</code>s.
For all the various extension types in the specifications of these objects, it validates whether there exists a <code>ControllerRegistration</code> in the system that is primarily responsible for the stated extension type(s).
This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don&rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.</p><h2 id=extensionlabels><code>ExtensionLabels</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>BackupBucket</code>s, <code>BackupEntry</code>s, <code>CloudProfile</code>s, <code>Seed</code>s, <code>SecretBinding</code>s and <code>Shoot</code>s. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form <code>&lt;extension-type>.extensions.gardener.cloud/&lt;extension-name> : "true"</code>. For example, an extension label for provider extension type <code>aws</code>, looks like <code>provider.extensions.gardener.cloud/aws : "true"</code>.</p><h2 id=projectvalidator><code>ProjectValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Project</code>s.
It prevents creating <code>Project</code>s with a non-empty <code>.spec.namespace</code> if the value in <code>.spec.namespace</code> does not start with <code>garden-</code>.</p><p>⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the <code>gardener-apiserver</code>.</p><h2 id=resourcequota><code>ResourceQuota</code></h2><p><em>(enabled by default)</em></p><p>This admission controller enables <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota>object count ResourceQuotas</a> for Gardener resources, e.g. <code>Shoots</code>, <code>SecretBindings</code>, <code>Projects</code>, etc.</p><blockquote><p>⚠️ In addition to this admission plugin, the <a href=https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/admission_control_resource_quota.md#resource-quota-controller>ResourceQuota controller</a> must be enabled for the Kube-Controller-Manager of your Garden cluster.</p></blockquote><h2 id=resourcereferencemanager><code>ResourceReferenceManager</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>CloudProfile</code>s, <code>Project</code>s, <code>SecretBinding</code>s, <code>Seed</code>s, and <code>Shoot</code>s.
Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced <code>Secret</code> exists).
However, it also has some special behaviours for certain resources:</p><ul><li><code>CloudProfile</code>s: It rejects removing Kubernetes or machine image versions if there is at least one <code>Shoot</code> that refers to them.</li><li><code>Project</code>s: It sets the <code>.spec.createdBy</code> field for newly created <code>Project</code> resources, and defaults the <code>.spec.owner</code> field in case it is empty (to the same value of <code>.spec.createdBy</code>).</li><li><code>Shoot</code>s: It sets the <code>gardener.cloud/created-by=&lt;username></code> annotation for newly created <code>Shoot</code> resources.</li></ul><h2 id=seedvalidator><code>SeedValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>Seed</code>s.
Rejects the deletion if <code>Shoot</code>(s) reference the seed cluster.</p><h2 id=shootdns><code>ShootDNS</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It tries to assign a default domain to the <code>Shoot</code>.
It also validates the DNS configuration (<code>.spec.dns</code>) for shoots.</p><h2 id=shootnodelocaldnsenabledbydefault><code>ShootNodeLocalDNSEnabledByDefault</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it will enable node local dns within the shoot cluster (for more information, see <a href=/docs/gardener/usage/node-local-dns/>NodeLocalDNS Configuration</a>) by setting <code>spec.systemComponents.nodeLocalDNS.enabled=true</code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable node local dns (<code>spec.systemComponents.nodeLocalDNS.enabled=false</code>)
will not be affected by this admission plugin.</p><h2 id=shootquotavalidator><code>ShootQuotaValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It validates the resource consumption declared in the specification against applicable <code>Quota</code> resources.
Only if the applicable <code>Quota</code> resources admit the configured resources in the <code>Shoot</code> then it allows the request.
Applicable <code>Quota</code>s are referred in the <code>SecretBinding</code> that is used by the <code>Shoot</code>.</p><h2 id=shootvpaenabledbydefault><code>ShootVPAEnabledByDefault</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it will enable the managed <code>VerticalPodAutoscaler</code> components (for more information, see <a href=/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</a>)
by setting <code>spec.kubernetes.verticalPodAutoscaler.enabled=true</code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable VPA (<code>spec.kubernetes.verticalPodAutoscaler.enabled=false</code>)
will not be affected by this admission plugin.</p><h2 id=shoottolerationrestriction><code>ShootTolerationRestriction</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It validates the <code>.spec.tolerations</code> used in <code>Shoot</code>s against the whitelist of its <code>Project</code>, or against the whitelist configured in the admission controller&rsquo;s configuration, respectively.
Additionally, it defaults the <code>.spec.tolerations</code> in <code>Shoot</code>s with those configured in its <code>Project</code>, and those configured in the admission controller&rsquo;s configuration, respectively.</p><h2 id=shootvalidator><code>ShootValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code>, <code>UPDATE</code> and <code>DELETE</code> operations for <code>Shoot</code>s.
It validates certain configurations in the specification against the referred <code>CloudProfile</code> (e.g., machine images, machine types, used Kubernetes version, &mldr;).
Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources).
Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).</p><h2 id=shootmanagedseed><code>ShootManagedSeed</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>UPDATE</code> and <code>DELETE</code> operations for <code>Shoot</code>s.
It validates certain configuration values in the specification that are specific to <code>ManagedSeed</code>s (e.g. the nginx-addon of the Shoot has to be disabled, the Shoot VPA has to be enabled).
It rejects the deletion if the <code>Shoot</code> is referred to by a <code>ManagedSeed</code>.</p><h2 id=managedseedvalidator><code>ManagedSeedValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>ManagedSeeds</code>s.
It validates certain configuration values in the specification against the referred <code>Shoot</code>, for example Seed provider, network ranges, DNS domain, etc.
Similar to <code>ShootValidator</code>, it performs validations that cannot be handled by the static API validation due to their dynamic nature.
Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred <code>Shoot</code>, for example Seed provider, network ranges, DNS domain, etc.</p><h2 id=managedseedshoot><code>ManagedSeedShoot</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>ManagedSeed</code>s.
It rejects the deletion if there are <code>Shoot</code>s that are scheduled onto the <code>Seed</code> that is registered by the <code>ManagedSeed</code>.</p><h2 id=shootdnsrewriting><code>ShootDNSRewriting</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it adds a set of common suffixes configured in its admission plugin configuration to the <code>Shoot</code> (<code>spec.systemComponents.coreDNS.rewriting.commonSuffixes</code>) (for more information, see <a href=/docs/gardener/usage/dns-search-path-optimization/>DNS Search Path Optimization</a>).
Already existing <code>Shoot</code>s will not be affected by this admission plugin.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c898670f6aa58e28fb8510640734a620>1.2.4 - Architecture</h1><h2 id=official-definition---what-is-kubernetes>Official Definition - What is Kubernetes?</h2><blockquote><p>&ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&rdquo;</p></blockquote><h2 id=introduction---basic-principle>Introduction - Basic Principle</h2><p>The foundation of the Gardener (providing <strong>Kubernetes Clusters as a Service</strong>) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it&rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).</p><p>While self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called &ldquo;seed&rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call &ldquo;shoot&rdquo; cluster, as pods into the &ldquo;seed&rdquo; cluster. That means that one &ldquo;seed&rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple &ldquo;shoot&rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the &ldquo;shoot&rdquo; cluster control planes. We simply put the control plane into pods/containers and since the &ldquo;seed&rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual &ldquo;shoot&rdquo; cluster consists only of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.</p><h2 id=setting-the-scene---components-and-procedure>Setting The Scene - Components and Procedure</h2><p>We provide a central operator UI, which we call the &ldquo;Gardener Dashboard&rdquo;. It talks to a dedicated cluster, which we call the &ldquo;Garden&rdquo; cluster and uses custom resources managed by an <a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation>aggregated API server</a>, one of the general extension concepts of Kubernetes) to represent &ldquo;shoot&rdquo; clusters. In this &ldquo;Garden&rdquo; cluster runs the &ldquo;Gardener&rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes &ldquo;shoot&rdquo; clusters. The creation follows basically these steps:</p><ul><li>Create a namespace in the &ldquo;seed&rdquo; cluster for the &ldquo;shoot&rdquo; cluster, which will host the &ldquo;shoot&rdquo; cluster control plane.</li><li>Generate secrets and credentials, which the worker nodes will need to talk to the control plane.</li><li>Create the infrastructure (using <a href=https://www.terraform.io/>Terraform</a>), which basically consists out of the network setup).</li><li>Deploy the &ldquo;shoot&rdquo; cluster control plane into the &ldquo;shoot&rdquo; namespace in the &ldquo;seed&rdquo; cluster, containing the &ldquo;machine-controller-manager&rdquo; pod.</li><li>Create machine CRDs in the &ldquo;seed&rdquo; cluster, describing the configuration and the number of worker machines for the &ldquo;shoot&rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it).</li><li>Wait for the &ldquo;shoot&rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider).</li><li>Finally, we deploy <code>kube-system</code> daemons like <code>kube-proxy</code> and further add-ons like the <code>dashboard</code> into the &ldquo;shoot&rdquo; cluster and the cluster becomes active.</li></ul><h2 id=overview-architecture-diagram>Overview Architecture Diagram</h2><p><img src=/__resources/gardener-architecture-overview_2bd462.png alt="Gardener Overview Architecture Diagram"></p><h2 id=detailed-architecture-diagram>Detailed Architecture Diagram</h2><p><img src=/__resources/gardener-architecture-detailed_945c90.png alt="Gardener Detailed Architecture Diagram"></p><p>Note: The <code>kubelet</code>, as well as the pods inside the &ldquo;shoot&rdquo; cluster, talks through the front-door (load balancer IP; public Internet) to its &ldquo;shoot&rdquo; cluster API server running in the &ldquo;seed&rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into the &ldquo;seed&rdquo; and &ldquo;shoot&rdquo; clusters.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-350220592a34dc9c03bc21617e258ada>1.2.5 - Backup and Restore</h1><h1 id=backup-and-restore>Backup and Restore</h1><p>Kubernetes uses <a href=https://etcd.io/>etcd</a> as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.</p><p>Gardener uses an <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> component to backup the etcd backing the Shoot cluster regularly and restore it in case of disaster. It is deployed as sidecar via <a href=https://github.com/gardener/etcd-druid>etcd-druid</a>. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/06-etcd-druid.md>GEP-06</a> and the documentation on individual repositories.</p><h2 id=bucket-provisioning>Bucket Provisioning</h2><p>Refer to the <a href=/docs/gardener/extensions/backupbucket/>backup bucket extension document</a> to find out details about configuring the backup bucket.</p><h2 id=backup-policy>Backup Policy</h2><p>etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to the following parameters:</p><ul><li>Full Snapshot schedule:<ul><li>Daily, <code>24hr</code> interval.</li><li>For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.</li></ul></li><li>Delta Snapshot schedule:<ul><li>At <code>5min</code> interval.</li><li>If aggregated events size since last snapshot goes beyond <code>100Mib</code>.</li></ul></li><li>Backup History / Garbage backup deletion policy:<ul><li>Gardener configures backup restore to have <code>Exponential</code> garbage collection policy.</li><li>As per policy, the following backups are retained:<ul><li>All full backups and delta backups for the previous hour.</li><li>Latest full snapshot of each previous hour for the day.</li><li>Latest full snapshot of each previous day for 7 days.</li><li>Latest full snapshot of the previous 4 weeks.</li></ul></li><li>Garbage Collection is configured at <code>12hr</code> interval.</li></ul></li><li>Listing:<ul><li>Gardener doesn&rsquo;t have any API to list out the backups.</li><li>To find the backups list, an admin can checkout the <code>BackupEntry</code> resource associated with the Shoot which holds the bucket and prefix details on the object store.</li></ul></li></ul><h2 id=restoration>Restoration</h2><p>The restoration process of etcd is automated through the etcd-backup-restore component from the latest snapshot. Gardener doesn&rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of an etcd disaster, the etcd is recovered from the latest backup automatically. For further details, please refer the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/restoration.md>Restoration</a> topic. Post restoration of etcd, the Shoot reconciliation loop brings the cluster back to its previous state.</p><p>Again, the Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener only takes care of the cluster&rsquo;s etcd.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-deb4b0a712a04f62d9a5892607237729>1.2.6 - Cluster API</h1><h1 id=relation-between-gardener-api-and-cluster-api-sig-cluster-lifecycle>Relation Between Gardener API and Cluster API (SIG Cluster Lifecycle)</h1><p>In essence, the Cluster API harmonizes how to get to clusters, while Gardener goes one step further and also harmonizes the clusters themselves. The Cluster API delegates the specifics to so-called providers for infrastructures or control planes via specific CR(D)s, while Gardener only has one cluster CR(D). Different Cluster API providers, e.g. for AWS, Azure, GCP, etc., give you vastly different Kubernetes clusters. In contrast, Gardener gives you the exact same clusters with the exact same K8s version, operating system, control plane configuration like for API server or kubelet, add-ons like overlay network, HPA/VPA, DNS and certificate controllers, ingress and network policy controllers, control plane monitoring and logging stacks, down to the behavior of update procedures, auto-scaling, self-healing, etc., on all supported infrastructures. These homogeneous clusters are an essential goal for Gardener, as its main purpose is to simplify operations for teams that need to develop and ship software on Kubernetes clusters on a plethora of infrastructures (a.k.a. multi-cloud).</p><p>Incidentally, Gardener influenced the Machine API in the Cluster API with its <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> and was the <a href=https://github.com/kubernetes-sigs/cluster-api/commit/00b1ead264aea6f88585559056c180771cce3815>first to adopt it</a>. You can find more information on that in the <a href="https://www.youtube.com/watch?v=Mtg8jygK3Hs">joint SIG Cluster Lifecycle KubeCon talk</a> where @hardikdr from our Gardener team in India spoke.</p><p>That means that we follow the <a href=https://github.com/kubernetes-sigs/cluster-api#cluster-api>Cluster API</a> with great interest and are active members. It was completely overhauled from <code>v1alpha1</code> to <code>v1alpha2</code>. But because <code>v1alpha2</code> made too many assumptions about the bring-up of masters and was enforcing master machine operations (for more information, see <a href=https://cluster-api.sigs.k8s.io/user/concepts.html#control-plane>The Cluster API Book</a>: “As of <code>v1alpha2</code>, Machine-Based is the only control plane type that Cluster API supports”), services that managed their control planes differently like GKE or Gardener couldn&rsquo;t adopt it (e.g. <a href=https://cloud.google.com/anthos/gke/docs/on-prem/concepts/cluster-api>Google only supports <code>v1alpha1</code></a>). In 2020 <a href=https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/><code>v1alpha3</code></a> was introduced and made it possible (again) to integrate managed services like GKE or Gardener. The mapping from the Gardener API to the Cluster API is mostly syntactic.</p><p>To wrap it up, while the Cluster API knows about clusters, it doesn&rsquo;t know about their make-up. With Gardener, we wanted to go beyond that and harmonize the make-up of the clusters themselves and make them homogeneous across all supported infrastructures. Gardener can therefore deliver homogeneous clusters with exactly the same configuration and behavior on all infrastructures (see also <a href=https://k8s-testgrid.appspot.com/conformance-all>Gardener&rsquo;s coverage in the official conformance test grid</a>).</p><p>With <a href=https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience>Cluster API <code>v1alpha3</code></a> and the support for declarative control plane management, it has became possible (again) to enable Kubernetes managed services like GKE or Gardener. We would be more than happy if the community would be interested to contribute a Gardener control plane provider.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-38b8e4d34b02317602d0c383c3c76cac>1.2.7 - Controller Manager</h1><h1 id=gardener-controller-manager>Gardener Controller Manager</h1><p>The <code>gardener-controller-manager</code> (often refered to as &ldquo;GCM&rdquo;) is a component that runs next to the Gardener API server, similar to the Kubernetes Controller Manager.
It runs several controllers that do not require talking to any seed or shoot cluster.
Also, as of today, it exposes an HTTP server that is serving several health check endpoints and metrics.</p><p>This document explains the various functionalities of the <code>gardener-controller-manager</code> and their purpose.</p><h2 id=controllers>Controllers</h2><h3 id=bastion-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerbastion><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/bastion><code>Bastion</code> Controller</a></h3><p><code>Bastion</code> resources have a limited lifetime which can be extended up to a certain amount by performing a heartbeat on them.
The <code>Bastion</code> controller is responsible for deleting expired or rotten <code>Bastion</code>s.</p><ul><li>&ldquo;expired&rdquo; means a <code>Bastion</code> has exceeded its <code>status.expirationTimestamp</code>.</li><li>&ldquo;rotten&rdquo; means a <code>Bastion</code> is older than the configured <code>maxLifetime</code>.</li></ul><p>The <code>maxLifetime</code> defaults to 24 hours and is an option in the <code>BastionControllerConfiguration</code> which is part of <code>gardener-controller-manager</code>s <code>ControllerManagerControllerConfiguration</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>the example config file</a> for details.</p><p>The controller also deletes <code>Bastion</code>s in case the referenced <code>Shoot</code>:</p><ul><li>no longer exists</li><li>is marked for deletion (i.e., have a non-<code>nil</code> <code>.metadata.deletionTimestamp</code>)</li><li>was migrated to another seed (i.e., <code>Shoot.spec.seedName</code> is different than <code>Bastion.spec.seedName</code>).</li></ul><p>The deletion of <code>Bastion</code>s triggers the <code>gardenlet</code> to perform the necessary cleanups in the Seed cluster, so some time can pass between deletion and the <code>Bastion</code> actually disappearing.
Clients like <code>gardenctl</code> are advised to not re-use <code>Bastion</code>s whose deletion timestamp has been set already.</p><p>Refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/15-manage-bastions-and-ssh-key-pair-rotation.md>GEP-15</a> for more information on the lifecycle of
<code>Bastion</code> resources.</p><h3 id=certificatesigningrequest-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercertificatesigningrequest><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/certificatesigningrequest><code>CertificateSigningRequest</code> Controller</a></h3><p>After the <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a> gets deployed on the Seed cluster, it needs to establish itself as a trusted party to communicate with the Gardener API server. It runs through a bootstrap flow similar to the <a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>kubelet bootstrap</a> process.</p><p>On startup, the gardenlet uses a <code>kubeconfig</code> with a <a href=https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap token</a> which authenticates it as being part of the <code>system:bootstrappers</code> group. This kubeconfig is used to create a <a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/><code>CertificateSigningRequest</code></a> (CSR) against the Gardener API server.</p><p>The controller in <code>gardener-controller-manager</code> checks whether the <code>CertificateSigningRequest</code> has the expected organisation, common name and usages which the gardenlet would request.</p><p>It only auto-approves the CSR if the client making the request is allowed to &ldquo;create&rdquo; the
<code>certificatesigningrequests/seedclient</code> subresource. Clients with the <code>system:bootstrappers</code> group are bound to the <code>gardener.cloud:system:seed-bootstrapper</code> <code>ClusterRole</code>, hence, they have such privileges. As the bootstrap kubeconfig for the gardenlet contains a bootstrap token which is authenticated as being part of the <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/clusterrolebinding-seed-bootstrapper.yaml><code>systems:bootstrappers</code> group</a>, its created CSR gets auto-approved.</p><h3 id=cloudprofile-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercloudprofile><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/cloudprofile><code>CloudProfile</code> Controller</a></h3><p><code>CloudProfile</code>s are essential when it comes to reconciling <code>Shoot</code>s since they contain constraints (like valid machine types, Kubernetes versions, or machine images) and sometimes also some global configuration for the respective environment (typically via provider-specific configuration in <code>.spec.providerConfig</code>).</p><p>Consequently, to ensure that <code>CloudProfile</code>s in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>CloudProfile</code> anymore.</p><h3 id=controllerdeployment-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerdeployment><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerdeployment><code>ControllerDeployment</code> Controller</a></h3><p>Extensions are registered in the garden cluster via <code>ControllerRegistration</code> and deployment of respective extensions are specified via <code>ControllerDeployment</code>. For more info refer to <a href=/docs/gardener/extensions/controllerregistration/>Registering Extension Controllers</a>.</p><p>This controller ensures that <code>ControllerDeployment</code> in-use always exists until the last <code>ControllerRegistration</code> referencing them gets deleted. The controller adds a finalizer which is only released when there is no <code>ControllerRegistration</code> referencing the <code>ControllerDeployment</code> anymore.</p><h3 id=controllerregistration-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistration><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration><code>ControllerRegistration</code> Controller</a></h3><p>The <code>ControllerRegistration</code> controller makes sure that the required <a href=https://github.com/gardener/gardener/blob/master/docs/README.md#extensions>Gardener Extensions</a> specified by the <a href=/docs/gardener/extensions/controllerregistration/><code>ControllerRegistration</code></a> resources are present in the seed clusters.
It also takes care of the creation and deletion of <code>ControllerInstallation</code> objects for a given seed cluster.
The controller has three reconciliation loops.</p><h4 id=main-reconciler>&ldquo;Main&rdquo; Reconciler</h4><p>This reconciliation loop watches the <code>Seed</code> objects and determines which <code>ControllerRegistrations</code> are required for them and reconciles the corresponding <code>ControllerInstallation</code> resources to reach the determined state.
To begin with, it computes the kind/type combinations of extensions required for the seed.
For this, the controller examines a live list of <code>ControllerRegistration</code>s, <code>ControllerInstallation</code>s, <code>BackupBucket</code>s, <code>BackupEntry</code>s, <code>Shoot</code>s, and <code>Secret</code>s from the garden cluster.
For example, it examines the shoots running on the seed and deducts the kind/type, like <code>Infrastructure/gcp</code>.
It also decides whether they should always be deployed based on the <code>.spec.deployment.policy</code>.
For the configuration options, please see this <a href=/docs/gardener/extensions/controllerregistration/#deployment-configuration-options>section</a>.</p><p>Based on these required combinations, each of them are mapped to <code>ControllerRegistration</code> objects and then to their corresponding <code>ControllerInstallation</code> objects (if existing).
The controller then creates or updates the required <code>ControllerInstallation</code> objects for the given seed.
It also deletes every existing <code>ControllerInstallation</code> whose referenced <code>ControllerRegistration</code> is not part of the required list.
For example, if the shoots in the seed are no longer using the DNS provider <code>aws-route53</code>, then the controller proceeds to delete the respective <code>ControllerInstallation</code> object.</p><h4 id=controllerregistration-reconciler>&ldquo;ControllerRegistration&rdquo; Reconciler</h4><p>This reconciliation loop watches the <code>ControllerRegistration</code> resource and adds finalizers to it when they are created.
In case a deletion request comes in for the resource, i.e., if a <code>.metadata.deletionTimestamp</code> is set, it actively scans for a <code>ControllerInstallation</code> resource using this <code>ControllerRegistration</code>, and decides whether the deletion can be allowed.
In case no related <code>ControllerInstallation</code> is present, it removes the finalizer and marks it for deletion.</p><h4 id=seed-reconciler>&ldquo;Seed&rdquo; Reconciler</h4><p>This loop also watches the <code>Seed</code> object and adds finalizers to it at creation.
If a <code>.metadata.deletionTimestamp</code> is set for the seed, then the controller checks for existing <code>ControllerInstallation</code> objects which reference this seed.
If no such objects exist, then it removes the finalizer and allows the deletion.</p><h3 id=event-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerevent><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/event><code>Event</code> Controller</a></h3><p>With the Gardener Event Controller, you can prolong the lifespan of events related to Shoot clusters.
This is an optional controller which will become active once you provide the below mentioned configuration.</p><p>All events in K8s are deleted after a configurable time-to-live (controlled via a <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver argument</a> called <code>--event-ttl</code> (defaulting to 1 hour)).
The need to prolong the time-to-live for Shoot cluster events frequently arises when debugging customer issues on live systems.
This controller leaves events involving Shoots untouched, while deleting all other events after a configured time.
In order to activate it, provide the following configuration:</p><ul><li><code>concurrentSyncs</code>: The amount of goroutines scheduled for reconciling events.</li><li><code>ttlNonShootEvents</code>: When an event reaches this time-to-live it gets deleted unless it is a Shoot-related event (defaults to <code>1h</code>, equivalent to the <code>event-ttl</code> default).</li></ul><blockquote><p>⚠️ In addition, you should also configure the <code>--event-ttl</code> for the kube-apiserver to define an upper-limit of how long Shoot-related events should be stored. The <code>--event-ttl</code> should be larger than the <code>ttlNonShootEvents</code> or this controller will have no effect.</p></blockquote><h3 id=exposureclass-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerexposureclass><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/exposureclass><code>ExposureClass</code> Controller</a></h3><p><code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds. For more information, see <a href=/docs/gardener/usage/exposureclasses/>ExposureClasses</a>.</p><p>Consequently, to ensure that <code>ExposureClass</code>es in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>ExposureClass</code> anymore.</p><h3 id=managedseedset-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollermanagedseedset><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/managedseedset><code>ManagedSeedSet</code> Controller</a></h3><p><code>ManagedSeedSet</code> objects maintain a stable set of replicas of <code>ManagedSeed</code>s, i.e. they guarantee the availability of a specified number of identical <code>ManagedSeed</code>s on an equal number of identical <code>Shoot</code>s.
The <code>ManagedSeedSet</code> controller creates and deletes <code>ManagedSeed</code>s and <code>Shoot</code>s in response to changes to the replicas and selector fields. For more information, refer to the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/13-automated-seed-management.md#managedseedsets><code>ManagedSeedSet</code> proposal document</a>.</p><ol><li>The reconciler first gets all the replicas of the given <code>ManagedSeedSet</code> in the <code>ManagedSeedSet</code>&rsquo;s namespace and with the matching selector. Each replica is a struct that contains a <code>ManagedSeed</code>, its corresponding <code>Seed</code> and <code>Shoot</code> objects.</li><li>Then the pending replica is retrieved, if it exists.</li><li>Next it determines the ready, postponed, and deletable replicas.<ul><li>A replica is considered <code>ready</code> when a <code>Seed</code> owned by a <code>ManagedSeed</code> has been registered either directly or by deploying <code>gardenlet</code> into a <code>Shoot</code>, the <code>Seed</code> is <code>Ready</code> and the <code>Shoot</code>&rsquo;s status is <code>Healthy</code>.</li><li>If a replica is not ready and it is not pending, i.e. it is not specified in the <code>ManagedSeed</code>&rsquo;s <code>status.pendingReplica</code> field, then it is added to the <code>postponed</code> replicas.</li><li>A replica is deletable if it has no scheduled <code>Shoot</code>s and the replica&rsquo;s <code>Shoot</code> and <code>ManagedSeed</code> do not have the <code>seedmanagement.gardener.cloud/protect-from-deletion</code> annotation.</li></ul></li><li>Finally, it checks the actual and target replica counts. If the actual count is less than the target count, the controller scales up the replicas by creating new replicas to match the desired target count. If the actual count is more than the target, the controller deletes replicas to match the desired count. Before scale-out or scale-in, the controller first reconciles the pending replica (there can always only be one) and makes sure the replica is ready before moving on to the next one.<ul><li><code>Scale-out</code>(actual count &lt; target count)<ul><li>During the scale-out phase, the controller first creates the <code>Shoot</code> object from the <code>ManagedSeedSet</code>&rsquo;s <code>spec.shootTemplate</code> field and adds the replica to the <code>status.pendingReplica</code> of the <code>ManagedSeedSet</code>.</li><li>For the subsequent reconciliation steps, the controller makes sure that the pending replica is ready before proceeding to the next replica. Once the <code>Shoot</code> is created successfully, the <code>ManagedSeed</code> object is created from the <code>ManagedSeedSet</code>&rsquo;s <code>spec.template</code>. The <code>ManagedSeed</code> object is reconciled by the <code>ManagedSeed</code> controller and a <code>Seed</code> object is created for the replica. Once the replica&rsquo;s <code>Seed</code> becomes ready and the <code>Shoot</code> becomes healthy, the replica also becomes ready.</li></ul></li><li><code>Scale-in</code>(actual count > target count)<ul><li>During the scale-in phase, the controller first determines the replica that can be deleted. From the deletable replicas, it chooses the one with the lowest priority and deletes it. Priority is determined in the following order:<ul><li>First, compare replica statuses. Replicas with &ldquo;less advanced&rdquo; status are considered lower priority. For example, a replica with <code>StatusShootReconciling</code> status has a lower value than a replica with <code>StatusShootReconciled</code> status. Hence, in this case, a replica with a <code>StatusShootReconciling</code> status will have lower priority and will be considered for deletion.</li><li>Then, the replicas are compared with the readiness of their <code>Seed</code>s. Replicas with non-ready <code>Seed</code>s are considered lower priority.</li><li>Then, the replicas are compared with the health statuses of their <code>Shoot</code>s. Replicas with &ldquo;worse&rdquo; statuses are considered lower priority.</li><li>Finally, the replica ordinals are compared. Replicas with lower ordinals are considered lower priority.</li></ul></li></ul></li></ul></li></ol><h3 id=quota-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerquota><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/quota><code>Quota</code> Controller</a></h3><p><code>Quota</code> object limits the resources consumed by shoot clusters either per provider secret or per project/namespace.</p><p>Consequently, to ensure that <code>Quota</code>s in-use are always present in the system until the last <code>SecretBinding</code> that references them gets deleted, the controller adds a finalizer which is only released when there is no <code>SecretBinding</code> referencing the <code>Quota</code> anymore.</p><h3 id=project-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerproject><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/project><code>Project</code> Controller</a></h3><p>There are multiple controllers responsible for different aspects of <code>Project</code> objects.
Please also refer to the <a href=/docs/gardener/usage/projects/><code>Project</code> documentation</a>.</p><h4 id=main-reconciler-1>&ldquo;Main&rdquo; Reconciler</h4><p>This reconciler manages a dedicated <code>Namespace</code> for each <code>Project</code>.
The namespace name can either be specified explicitly in <code>.spec.namespace</code> (must be prefixed with <code>garden-</code>) or it will be determined by the controller.
If <code>.spec.namespace</code> is set, it tries to create it. If it already exists, it tries to adopt it.
This will only succeed if the <code>Namespace</code> was previously labeled with <code>gardener.cloud/role=project</code> and <code>project.gardener.cloud/name=&lt;project-name></code>.
This is to prevent end-users from being able to adopt arbitrary namespaces and escalate their privileges, e.g. the <code>kube-system</code> namespace.</p><p>After the namespace was created/adopted, the controller creates several <code>ClusterRole</code>s and <code>ClusterRoleBinding</code>s that allow the project members to access related resources based on their roles.
These RBAC resources are prefixed with <code>gardener.cloud:system:project{-member,-viewer}:&lt;project-name></code>.
Gardener administrators and extension developers can define their own roles. For more information, see <a href=/docs/gardener/extensions/project-roles/>Extending Project Roles</a> for more information.</p><p>In addition, operators can configure the Project controller to maintain a default <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>ResourceQuota</a> for project namespaces.
Quotas can especially limit the creation of user facing resources, e.g. <code>Shoots</code>, <code>SecretBindings</code>, <code>Secrets</code> and thus protect the Garden cluster from massive resource exhaustion but also enable operators to align quotas with respective enterprise policies.</p><blockquote><p>⚠️ <strong>Gardener itself is not exempted from configured quotas</strong>. For example, Gardener creates <code>Secrets</code> for every shoot cluster in the project namespace and at the same time increases the available quota count. Please mind this additional resource consumption.</p></blockquote><p>The controller configuration provides a template section <code>controllers.project.quotas</code> where such a ResourceQuota (see the example below) can be deposited.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  project:
</span></span><span style=display:flex><span>    quotas:
</span></span><span style=display:flex><span>    - config:
</span></span><span style=display:flex><span>        apiVersion: v1
</span></span><span style=display:flex><span>        kind: ResourceQuota
</span></span><span style=display:flex><span>        spec:
</span></span><span style=display:flex><span>          hard:
</span></span><span style=display:flex><span>            count/shoots.core.gardener.cloud: <span style=color:#a31515>&#34;100&#34;</span>
</span></span><span style=display:flex><span>            count/secretbindings.core.gardener.cloud: <span style=color:#a31515>&#34;10&#34;</span>
</span></span><span style=display:flex><span>            count/secrets: <span style=color:#a31515>&#34;800&#34;</span>
</span></span><span style=display:flex><span>      projectSelector: {}
</span></span></code></pre></div><p>The Project controller takes the specified <code>config</code> and creates a <code>ResourceQuota</code> with the name <code>gardener</code> in the project namespace.
If a <code>ResourceQuota</code> resource with the name <code>gardener</code> already exists, the controller will only update fields in <code>spec.hard</code> which are <strong>unavailable</strong> at that time.
This is done to configure a default <code>Quota</code> in all projects but to allow manual quota increases as the projects&rsquo; demands increase.
<code>spec.hard</code> fields in the <code>ResourceQuota</code> object that are not present in the configuration are removed from the object.
Labels and annotations on the <code>ResourceQuota</code> <code>config</code> get merged with the respective fields on existing <code>ResourceQuota</code>s.
An optional <code>projectSelector</code> narrows down the amount of projects that are equipped with the given <code>config</code>.
If multiple configs match for a project, then only the first match in the list is applied to the project namespace.</p><p>The <code>.status.phase</code> of the <code>Project</code> resources is set to <code>Ready</code> or <code>Failed</code> by the reconciler to indicate whether the reconciliation loop was performed successfully.
Also, it generates <code>Event</code>s to provide further information about its operations.</p><p>When a <code>Project</code> is marked for deletion, the controller ensures that there are no <code>Shoots</code> left in the project namespace.
Once all <code>Shoots</code> are gone, the <code>Namespace</code> and <code>Project</code> are released.</p><h4 id=stale-projects-reconciler>&ldquo;Stale Projects&rdquo; Reconciler</h4><p>As Gardener is a large-scale Kubernetes as a Service, it is designed for being used by a large amount of end-users.
Over time, it is likely to happen that some of the hundreds or thousands of <code>Project</code> resources are no longer actively used.</p><p>Gardener offers the &ldquo;stale projects&rdquo; reconciler which will take care of identifying such stale projects, marking them with a &ldquo;warning&rdquo;, and eventually deleting them after a certain time period.
This reconciler is enabled by default and works as follows:</p><ol><li>Projects are considered as &ldquo;stale&rdquo;/not actively used when all of the following conditions apply: The namespace associated with the <code>Project</code> does not have any&mldr;<ol><li><code>Shoot</code> resources.</li><li><code>BackupEntry</code> resources.</li><li><code>Secret</code> resources that are referenced by a <code>SecretBinding</code> that is in use by a <code>Shoot</code> (not necessarily in the same namespace).</li><li><code>Quota</code> resources that are referenced by a <code>SecretBinding</code> that is in use by a <code>Shoot</code> (not necessarily in the same namespace).</li><li>The time period when the project was used for the last time (<code>status.lastActivityTimestamp</code>) is longer than the configured <code>minimumLifetimeDays</code></li></ol></li></ol><p>If a project is considered &ldquo;stale&rdquo;, then its <code>.status.staleSinceTimestamp</code> will be set to the time when it was first detected to be stale.
If it gets actively used again, this timestamp will be removed.
After some time, the <code>.status.staleAutoDeleteTimestamp</code> will be set to a timestamp after which Gardener will auto-delete the <code>Project</code> resource if it still is not actively used.</p><p>The component configuration of the <code>gardener-controller-manager</code> offers to configure the following options:</p><ul><li><code>minimumLifetimeDays</code>: Don&rsquo;t consider newly created <code>Project</code>s as &ldquo;stale&rdquo; too early to give people/end-users some time to onboard and get familiar with the system. The &ldquo;stale project&rdquo; reconciler won&rsquo;t set any timestamp for <code>Project</code>s younger than <code>minimumLifetimeDays</code>. When you change this value, then projects marked as &ldquo;stale&rdquo; may be no longer marked as &ldquo;stale&rdquo; in case they are young enough, or vice versa.</li><li><code>staleGracePeriodDays</code>: Don&rsquo;t compute auto-delete timestamps for stale <code>Project</code>s that are unused for less than <code>staleGracePeriodDays</code>. This is to not unnecessarily make people/end-users nervous &ldquo;just because&rdquo; they haven&rsquo;t actively used their <code>Project</code> for a given amount of time. When you change this value, then already assigned auto-delete timestamps may be removed if the new grace period is not yet exceeded.</li><li><code>staleExpirationTimeDays</code>: Expiration time after which stale <code>Project</code>s are finally auto-deleted (after <code>.status.staleSinceTimestamp</code>). If this value is changed and an auto-delete timestamp got already assigned to the projects, then the new value will only take effect if it&rsquo;s increased. Hence, decreasing the <code>staleExpirationTimeDays</code> will not decrease already assigned auto-delete timestamps.</li></ul><blockquote><p>Gardener administrators/operators can exclude specific <code>Project</code>s from the stale check by annotating the related <code>Namespace</code> resource with <code>project.gardener.cloud/skip-stale-check=true</code>.</p></blockquote><h4 id=activity-reconciler>&ldquo;Activity&rdquo; Reconciler</h4><p>Since the other two reconcilers are unable to actively monitor the relevant objects that are used in a <code>Project</code> (<code>Shoot</code>, <code>Secret</code>, etc.), there could be a situation where the user creates and deletes objects in a short period of time. In that case, the <code>Stale Project Reconciler</code> could not see that there was any activity on that project and it will still mark it as a <code>Stale</code>, even though it is actively used.</p><p>The <code>Project Activity Reconciler</code> is implemented to take care of such cases. An event handler will notify the reconciler for any acitivity and then it will update the <code>status.lastActivityTimestamp</code>. This update will also trigger the <code>Stale Project Reconciler</code>.</p><h3 id=secretbinding-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollersecretbinding><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/secretbinding><code>SecretBinding</code> Controller</a></h3><p><code>SecretBinding</code>s reference <code>Secret</code>s and <code>Quota</code>s and are themselves referenced by <code>Shoot</code>s.
The controller adds finalizers to the referenced objects to ensure they don&rsquo;t get deleted while still being referenced.
Similarly, to ensure that <code>SecretBinding</code>s in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>SecretBinding</code> anymore.</p><p>Referenced <code>Secret</code>s will also be labeled with <code>provider.shoot.gardener.cloud/&lt;type>=true</code>, where <code>&lt;type></code> is the value of the <code>.provider.type</code> of the <code>SecretBinding</code>.
Also, all referenced <code>Secret</code>s, as well as <code>Quota</code>s, will be labeled with <code>reference.gardener.cloud/secretbinding=true</code> to allow for easily filtering for objects referenced by <code>SecretBinding</code>s.</p><h3 id=seed-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseed><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed><code>Seed</code> Controller</a></h3><p>The Seed controller in the <code>gardener-controller-manager</code> reconciles <code>Seed</code> objects with the help of the following reconcilers.</p><h4 id=main-reconciler-2>&ldquo;Main&rdquo; Reconciler</h4><p>This reconciliation loop takes care of seed related operations in the Garden cluster. When a new <code>Seed</code> object is created,
the reconciler creates a new <code>Namespace</code> in the garden cluster <code>seed-&lt;seed-name></code>. <code>Namespaces</code> dedicated to single
seed clusters allow us to segregate access permissions i.e., a <code>gardenlet</code> must not have permissions to access objects in
all <code>Namespaces</code> in the Garden cluster.
There are objects in a Garden environment which are created once by the operator e.g., default domain secret,
alerting credentials, and are required for operations happening in the <code>gardenlet</code>. Therefore, we not only need a seed specific
<code>Namespace</code> but also a copy of these &ldquo;shared&rdquo; objects.</p><p>The &ldquo;main&rdquo; reconciler takes care about this replication:</p><table><thead><tr><th style=text-align:center>Kind</th><th style=text-align:center>Namespace</th><th style=text-align:center>Label Selector</th></tr></thead><tbody><tr><td style=text-align:center>Secret</td><td style=text-align:center>garden</td><td style=text-align:center>gardener.cloud/role</td></tr></tbody></table><h4 id=backup-buckets-check-reconciler>&ldquo;Backup Buckets Check&rdquo; Reconciler</h4><p>Every time a <code>BackupBucket</code> object is created or updated, the referenced <code>Seed</code> object is enqueued for reconciliation.
It&rsquo;s the reconciler&rsquo;s task to check the <code>status</code> subresource of all existing <code>BackupBucket</code>s that reference this <code>Seed</code>.
If at least one <code>BackupBucket</code> has <code>.status.lastError != nil</code>, the <code>BackupBucketsReady</code> condition on the <code>Seed</code> will be set to <code>False</code>, and consequently the <code>Seed</code> is considered as <code>NotReady</code>.
If the <code>SeedBackupBucketsCheckControllerConfiguration</code> (which is part of <code>gardener-controller-manager</code>s component configuration) contains a <code>conditionThreshold</code> for the <code>BackupBucketsReady</code>, the condition will instead first be set to <code>Progressing</code> and eventually to <code>False</code> once the <code>conditionThreshold</code> expires. See <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>the example config file</a> for details.
Once the <code>BackupBucket</code> is healthy again, the seed will be re-queued and the condition will turn <code>true</code>.</p><h4 id=extensions-check-reconciler>&ldquo;Extensions Check&rdquo; Reconciler</h4><p>This reconciler reconciles <code>Seed</code> objects and checks whether all <code>ControllerInstallation</code>s referencing them are in a healthy state.
Concretely, all three conditions <code>Valid</code>, <code>Installed</code>, and <code>Healthy</code> must have status <code>True</code> and the <code>Progressing</code> condition must have status <code>False</code>.
Based on this check, it maintains the <code>ExtensionsReady</code> condition in the respective <code>Seed</code>&rsquo;s <code>.status.conditions</code> list.</p><h4 id=lifecycle-reconciler>&ldquo;Lifecycle&rdquo; Reconciler</h4><p>The &ldquo;Lifecycle&rdquo; reconciler processes <code>Seed</code> objects which are enqueued every 10 seconds in order to check if the responsible
<code>gardenlet</code> is still responding and operable. Therefore, it checks renewals via <code>Lease</code> objects of the seed in the garden cluster
which are renewed regularly by the <code>gardenlet</code>.</p><p>In case a <code>Lease</code> is not renewed for the configured amount in <code>config.controllers.seed.monitorPeriod.duration</code>:</p><ol><li>The reconciler assumes that the <code>gardenlet</code> stopped operating and updates the <code>GardenletReady</code> condition to <code>Unknown</code>.</li><li>Additionally, the conditions and constraints of all <code>Shoot</code> resources scheduled on the affected seed are set to <code>Unknown</code> as well,
because a striking <code>gardenlet</code> won&rsquo;t be able to maintain these conditions any more.</li><li>If the gardenlet&rsquo;s client certificate has expired (identified based on the <code>.status.clientCertificateExpirationTimestamp</code> field in the <code>Seed</code> resource) and if it is managed by a <code>ManagedSeed</code>, then this will be triggered for a reconciliation. This will trigger the bootstrapping process again and allows gardenlets to obtain a fresh client certificate.</li></ol><h3 id=shoot-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershoot><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot><code>Shoot</code> Controller</a></h3><h4 id=conditions-reconciler>&ldquo;Conditions&rdquo; Reconciler</h4><p>In case the reconciled <code>Shoot</code> is registered via a <code>ManagedSeed</code> as a seed cluster, this reconciler merges the conditions in the respective <code>Seed</code>&rsquo;s <code>.status.conditions</code> into the <code>.status.conditions</code> of the <code>Shoot</code>.
This is to provide a holistic view on the status of the registered seed cluster by just looking at the <code>Shoot</code> resource.</p><h4 id=hibernation-reconciler>&ldquo;Hibernation&rdquo; Reconciler</h4><p>This reconciler is responsible for hibernating or awakening shoot clusters based on the schedules defined in their <code>.spec.hibernation.schedules</code>.
It ignores <a href=/docs/gardener/usage/shoot_status/#last-operation>failed <code>Shoot</code>s</a> and those marked for deletion.</p><h4 id=maintenance-reconciler>&ldquo;Maintenance&rdquo; Reconciler</h4><p>This reconciler is responsible for maintaining shoot clusters based on the time window defined in their <code>.spec.maintenance.timeWindow</code>.
It might auto-update the Kubernetes version or the operating system versions specified in the worker pools (<code>.spec.provider.workers</code>).
It could also add some operation or task annotations. For more information, see <a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a>.</p><h4 id=quota-reconciler>&ldquo;Quota&rdquo; Reconciler</h4><p>This reconciler might auto-delete shoot clusters in case their referenced <code>SecretBinding</code> is itself referencing a <code>Quota</code> with <code>.spec.clusterLifetimeDays != nil</code>.
If the shoot cluster is older than the configured lifetime, then it gets deleted.
It maintains the expiration time of the <code>Shoot</code> in the value of the <code>shoot.gardener.cloud/expiration-timestamp</code> annotation.
This annotation might be overridden, however only by at most twice the value of the <code>.spec.clusterLifetimeDays</code>.</p><h4 id=reference-reconciler>&ldquo;Reference&rdquo; Reconciler</h4><p>Shoot objects may specify references to other objects in the Garden cluster which are required for certain features.
For example, users can configure various DNS providers via <code>.spec.dns.providers</code> and usually need to refer to a corresponding <code>Secret</code> with valid DNS provider credentials inside.
Such objects need a special protection against deletion requests as long as they are still being referenced by one or multiple shoots.</p><p>Therefore, this reconciler checks <code>Shoot</code>s for referenced objects and adds the finalizer <code>gardener.cloud/reference-protection</code> to their <code>.metadata.finalizers</code> list.
The reconciled <code>Shoot</code> also gets this finalizer to enable a proper garbage collection in case the <code>gardener-controller-manager</code> is offline at the moment of an incoming deletion request.
When an object is not actively referenced anymore because the <code>Shoot</code> specification has changed or all related shoots were deleted (are in deletion), the controller will remove the added finalizer again so that the object can safely be deleted or garbage collected.</p><p>This reconciler inspects the following references:</p><ul><li>DNS provider secrets (<code>.spec.dns.provider</code>)</li><li>Audit policy configmaps (<code>.spec.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef</code>)</li></ul><p>Further checks might be added in the future.</p><h4 id=retry-reconciler>&ldquo;Retry&rdquo; Reconciler</h4><p>This reconciler is responsible for retrying certain failed <code>Shoot</code>s.
Currently, the reconciler retries only failed <code>Shoot</code>s with an error code <code>ERR_INFRA_RATE_LIMITS_EXCEEDED</code>. See <a href=/docs/gardener/usage/shoot_status/#error-codes>Shoot Status</a> for more details.</p><h4 id=status-label-reconciler>&ldquo;Status Label&rdquo; Reconciler</h4><p>This reconciler is responsible for maintaining the <code>shoot.gardener.cloud/status</code> label on <code>Shoot</code>s. See <a href=/docs/gardener/usage/shoot_status/#status-label>Shoot Status</a> for more details.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a7a632a48267b040a5183cbf28440ddf>1.2.8 - etcd</h1><h1 id=etcd---key-value-store-for-kubernetes>etcd - Key-Value Store for Kubernetes</h1><p><a href=https://etcd.io/>etcd</a> is a strongly consistent key-value store and the most prevalent choice for the Kubernetes
persistence layer. All API cluster objects like <code>Pod</code>s, <code>Deployment</code>s, <code>Secret</code>s, etc., are stored in <code>etcd</code>, which
makes it an essential part of a <a href=https://kubernetes.io/docs/concepts/overview/components/#control-plane-components>Kubernetes control plane</a>.</p><h2 id=garden-or-shoot-cluster-persistence>Garden or Shoot Cluster Persistence</h2><p>Each garden or shoot cluster gets its very own persistence for the control plane.
It runs in the shoot namespace on the respective seed cluster (or in the <code>garden</code> namespace in the garden cluster, respectively).
Concretely, there are two etcd instances per shoot cluster, which the <code>kube-apiserver</code> is configured to use in the following way:</p><ul><li><code>etcd-main</code></li></ul><p>A store that contains all &ldquo;cluster critical&rdquo; or &ldquo;long-term&rdquo; objects.
These object kinds are typically considered for a backup to prevent any data loss.</p><ul><li><code>etcd-events</code></li></ul><p>A store that contains all <code>Event</code> objects (<code>events.k8s.io</code>) of a cluster.
<code>Events</code> usually have a short retention period and occur frequently, but are not essential for a disaster recovery.</p><p>The setup above prevents both, the critical <code>etcd-main</code> is not flooded by Kubernetes <code>Events</code>, as well as backup space is not occupied by non-critical data.
This separation saves time and resources.</p><h2 id=etcd-operator>etcd Operator</h2><p>Configuring, maintaining, and health-checking etcd is outsourced to a dedicated operator called <a href=https://github.com/gardener/etcd-druid/>etcd Druid</a>.
When a <a href=/docs/gardener/concepts/gardenlet/><code>gardenlet</code></a> reconciles a <code>Shoot</code> resource or a <a href=/docs/gardener/concepts/operator/><code>gardener-operator</code></a> reconciles a <code>Garden</code> resource, they manage an <a href=https://github.com/gardener/etcd-druid/blob/1d427e9167adac1476d1847c0e265c2c09d6bc62/config/samples/druid_v1alpha1_etcd.yaml><code>Etcd</code></a> resource in the seed or garden cluster, containing necessary information (backup information, defragmentation schedule, resources, etc.).
<code>etcd-druid</code> needs to manage the lifecycle of the desired etcd instance (today <code>main</code> or <code>events</code>).
Likewise, when the <code>Shoot</code> or <code>Garden</code> is deleted, <code>gardenlet</code> or <code>gardener-operator</code> deletes the <code>Etcd</code> resources and <a href=https://github.com/gardener/etcd-druid/>etcd Druid</a> takes care of cleaning up all related objects, e.g. the backing <code>StatefulSet</code>s.</p><h2 id=autoscaling>Autoscaling</h2><p>Gardenlet maintains <a href=https://github.com/gardener/hvpa-controller/blob/master/config/samples/autoscaling_v1alpha1_hvpa.yaml><code>HVPA</code></a> objects for etcd <code>StatefulSet</code>s if the corresponding <a href=/docs/gardener/deployment/feature_gates/>feature gate</a> is enabled.
This enables a vertical scaling for etcd.
Downscaling is handled more pessimistically to prevent many subsequent etcd restarts.
Thus, for <code>production</code> and <code>infrastructure</code> shoot clusters (or all garden clusters), downscaling is deactivated for the main etcd.
For all other shoot clusters, lower advertised requests/limits are only applied during a shoot&rsquo;s maintenance time window.</p><h2 id=backup>Backup</h2><p>If <code>Seed</code>s specify backups for etcd (<a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>example</a>), then Gardener and the respective <a href=/docs/gardener/extensions/overview/>provider extensions</a> are responsible for creating a bucket on the cloud provider&rsquo;s side (modelled through a <a href=/docs/gardener/extensions/backupbucket/>BackupBucket resource</a>).
The bucket stores backups of <code>Shoot</code>s scheduled on that <code>Seed</code>.
Furthermore, Gardener creates a <a href=/docs/gardener/extensions/backupentry/>BackupEntry</a>, which subdivides the bucket and thus makes it possible to store backups of multiple shoot clusters.</p><p>How long backups are stored in the bucket after a shoot has been deleted depends on the configured <em>retention period</em> in the <code>Seed</code> resource.
Please see this <a href=https://github.com/gardener/gardener/blob/849cd857d0d20e5dde26b9740ca2814603a56dfd/example/20-componentconfig-gardenlet.yaml#L20>example configuration</a> for more information.</p><p>For <code>Garden</code>s specifying backups for etcd (<a href=https://github.com/gardener/gardener/blob/master/example/operator/20-garden.yaml>example</a>), the bucket must be pre-created externally and provided via the <code>Garden</code> specification.</p><p>Both etcd instances are configured to run with a special backup-restore <em>sidecar</em>.
It takes care about regularly backing up etcd data and restoring it in case of data loss (in the main etcd only).
The sidecar also performs defragmentation and other house-keeping tasks.
More information can be found in the <a href=https://github.com/gardener/etcd-backup-restore>component&rsquo;s GitHub repository</a>.</p><h2 id=housekeeping>Housekeeping</h2><p><a href=https://etcd.io/docs/v3.3/op-guide/maintenance/>etcd maintenance tasks</a> must be performed from time to time in order to re-gain database storage and to ensure the system&rsquo;s reliability.
The <a href=https://github.com/gardener/etcd-backup-restore>backup-restore</a> <em>sidecar</em> takes care about this job as well.</p><p>For both <code>Shoot</code>s and <code>Garden</code>s, a random time <strong>within the shoot&rsquo;s maintenance time</strong> is chosen for scheduling these tasks.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-899a4a43944208c044aaee6e6023bbba>1.2.9 - gardenelet</h1><h1 id=gardenlet>gardenlet</h1><p>Gardener is implemented using the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/>operator pattern</a>:
It uses custom controllers that act on our own custom resources,
and apply Kubernetes principles to manage clusters instead of containers.
Following this analogy, you can recognize components of the Gardener architecture
as well-known Kubernetes components, for example, shoot clusters can be compared with pods,
and seed clusters can be seen as worker nodes.</p><p>The following Gardener components play a similar role as the corresponding components
in the Kubernetes architecture:</p><table><thead><tr><th style=text-align:left>Gardener Component</th><th style=text-align:left>Kubernetes Component</th></tr></thead><tbody><tr><td style=text-align:left><code>gardener-apiserver</code></td><td style=text-align:left><code>kube-apiserver</code></td></tr><tr><td style=text-align:left><code>gardener-controller-manager</code></td><td style=text-align:left><code>kube-controller-manager</code></td></tr><tr><td style=text-align:left><code>gardener-scheduler</code></td><td style=text-align:left><code>kube-scheduler</code></td></tr><tr><td style=text-align:left><code>gardenlet</code></td><td style=text-align:left><code>kubelet</code></td></tr></tbody></table><p>Similar to how the <code>kube-scheduler</code> of Kubernetes finds an appropriate node
for newly created pods, the <code>gardener-scheduler</code> of Gardener finds an appropriate seed cluster
to host the control plane for newly ordered clusters.
By providing multiple seed clusters for a region or provider, and distributing the workload,
Gardener also reduces the blast radius of potential issues.</p><p>Kubernetes runs a primary &ldquo;agent&rdquo; on every node, the kubelet,
which is responsible for managing pods and containers on its particular node.
Decentralizing the responsibility to the kubelet has the advantage that the overall system
is scalable. Gardener achieves the same for cluster management by using a <strong>gardenlet</strong>
as а primary &ldquo;agent&rdquo; on every seed cluster, and is only responsible for shoot clusters
located in its particular seed cluster:</p><p><img src=/__resources/gardenlet-architecture-similarities_ba8a1c.png alt="Counterparts in the Gardener Architecture and the Kubernetes Architecture"></p><p>The <code>gardener-controller-manager</code> has controllers to manage resources of the Gardener API. However, instead of letting the <code>gardener-controller-manager</code> talk directly to seed clusters or shoot clusters, the responsibility isn’t only delegated to the gardenlet, but also managed using a reversed control flow: It&rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.</p><p>Reversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.</p><p><img src=/__resources/gardenlet-architecture-detailed_6f3172.png alt="Reversed Control Flow Using a gardenlet"></p><h2 id=tls-bootstrapping>TLS Bootstrapping</h2><p>Kubernetes doesn’t manage worker nodes itself, and it’s also not
responsible for the lifecycle of the kubelet running on the workers.
Similarly, Gardener doesn’t manage seed clusters itself,
so it is also not responsible for the lifecycle of the gardenlet running on the seeds.
As a consequence, both the gardenlet and the kubelet need to prepare
a trusted connection to the Gardener API server
and the Kubernetes API server correspondingly.</p><p>To prepare a trusted connection between the gardenlet
and the Gardener API server, the gardenlet initializes
a bootstrapping process after you deployed it into your seed clusters:</p><ol><li><p>The gardenlet starts up with a bootstrap <code>kubeconfig</code>
having a bootstrap token that allows to create <code>CertificateSigningRequest</code> (CSR) resources.</p></li><li><p>After the CSR is signed, the gardenlet downloads
the created client certificate, creates a new <code>kubeconfig</code> with it,
and stores it inside a <code>Secret</code> in the seed cluster.</p></li><li><p>The gardenlet deletes the bootstrap <code>kubeconfig</code> secret,
and starts up with its new <code>kubeconfig</code>.</p></li><li><p>The gardenlet starts normal operation.</p></li></ol><p>The <code>gardener-controller-manager</code> runs a control loop
that automatically signs CSRs created by gardenlets.</p><blockquote><p>The gardenlet bootstrapping process is based on the
kubelet bootstrapping process. More information:
<a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>Kubelet&rsquo;s TLS bootstrapping</a>.</p></blockquote><p>If you don&rsquo;t want to run this bootstrap process, you can create
a <code>kubeconfig</code> pointing to the garden cluster for the gardenlet yourself,
and use the field <code>gardenClientConnection.kubeconfig</code> in the
gardenlet configuration to share it with the gardenlet.</p><h2 id=gardenlet-certificate-rotation>gardenlet Certificate Rotation</h2><p>The certificate used to authenticate the gardenlet against the API server
has a certain validity based on the configuration of the garden cluster
(<code>--cluster-signing-duration</code> flag of the <code>kube-controller-manager</code> (default <code>1y</code>)).</p><blockquote><p>If your garden cluster is of at least Kubernetes v1.22, then you can also configure the validity for the client certificate by specifying <code>.gardenClientConnection.kubeconfigValidity.validity</code> in the gardenlet&rsquo;s component configuration.
Note that changing this value will only take effect when the kubeconfig is rotated again (it is not picked up immediately).
The minimum validity is <code>10m</code> (that&rsquo;s what is enforced by the <code>CertificateSigningRequest</code> API in Kubernetes which is used by the gardenlet).</p></blockquote><p>By default, after about 70-90% of the validity has expired, the gardenlet tries to automatically replace
the current certificate with a new one (certificate rotation).</p><blockquote><p>You can change these boundaries by specifying <code>.gardenClientConnection.kubeconfigValidity.autoRotationJitterPercentage{Min,Max}</code> in the gardenlet&rsquo;s component configuration.</p></blockquote><p>To use a certificate rotation, you need to specify the secret to store
the <code>kubeconfig</code> with the rotated certificate in the field
<code>.gardenClientConnection.kubeconfigSecret</code> of the
gardenlet <a href=#component-configuration>component configuration</a>.</p><h3 id=rotate-certificates-using-bootstrap-kubeconfig>Rotate Certificates Using Bootstrap <code>kubeconfig</code></h3><p>If the gardenlet created the certificate during the initial TLS Bootstrapping
using the Bootstrap <code>kubeconfig</code>, certificates can be rotated automatically.
The same control loop in the <code>gardener-controller-manager</code> that signs
the CSRs during the initial TLS Bootstrapping also automatically signs
the CSR during a certificate rotation.</p><p>ℹ️ You can trigger an immediate renewal by annotating the <code>Secret</code> in the seed
cluster stated in the <code>.gardenClientConnection.kubeconfigSecret</code> field with
<code>gardener.cloud/operation=renew</code> and restarting the gardenlet. After it has booted
up again, gardenlet will issue a new certificate independent of the remaining
validity of the existing one.</p><h3 id=rotate-certificates-using-custom-kubeconfig>Rotate Certificates Using Custom <code>kubeconfig</code></h3><p>When trying to rotate a custom certificate that wasn’t created by gardenlet
as part of the TLS Bootstrap, the x509 certificate&rsquo;s <code>Subject</code> field
needs to conform to the following:</p><ul><li>the Common Name (CN) is prefixed with <code>gardener.cloud:system:seed:</code></li><li>the Organization (O) equals <code>gardener.cloud:system:seeds</code></li></ul><p>Otherwise, the <code>gardener-controller-manager</code> doesn’t automatically
sign the CSR.
In this case, an external component or user needs to approve the CSR manually,
for example, using the command <code>kubectl certificate approve seed-csr-&lt;...></code>).
If that doesn’t happen within 15 minutes,
the gardenlet repeats the process and creates another CSR.</p><h2 id=configuring-the-seed-to-work-with-gardenlet>Configuring the Seed to Work with gardenlet</h2><p>The gardenlet works with a single seed, which must be configured in the
<code>GardenletConfiguration</code> under <code>.seedConfig</code>. This must be a copy of the
<code>Seed</code> resource, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardenlet.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: GardenletConfiguration
</span></span><span style=display:flex><span>seedConfig:
</span></span><span style=display:flex><span>  metadata:
</span></span><span style=display:flex><span>    name: my-seed
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    provider:
</span></span><span style=display:flex><span>      type: aws
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: my-seed-secret
</span></span><span style=display:flex><span>      namespace: garden
</span></span></code></pre></div><p>(see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>this yaml file</a> for a more complete example)</p><p>When using <code>make start-gardenlet</code>, the corresponding script will automatically
fetch the seed cluster&rsquo;s <code>kubeconfig</code> based on the <code>seedConfig.spec.secretRef</code>
and set the environment accordingly.</p><p>On startup, gardenlet registers a <code>Seed</code> resource using the given template
in the <code>seedConfig</code> if it&rsquo;s not present already.</p><h2 id=component-configuration>Component Configuration</h2><p>In the component configuration for the gardenlet, it’s possible to define:</p><ul><li>settings for the Kubernetes clients interacting with the various clusters</li><li>settings for the controllers inside the gardenlet</li><li>settings for leader election and log levels, feature gates, and seed selection or seed configuration.</li></ul><p>More information: <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>Example gardenlet Component Configuration</a>.</p><h2 id=heartbeats>Heartbeats</h2><p>Similar to how Kubernetes uses <code>Lease</code> objects for node heart beats
(see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/589-efficient-node-heartbeats/README.md>KEP</a>),
the gardenlet is using <code>Lease</code> objects for heart beats of the seed cluster.
Every two seconds, the gardenlet checks that the seed cluster&rsquo;s <code>/healthz</code>
endpoint returns HTTP status code 200.
If that is the case, the gardenlet renews the lease in the Garden cluster in the <code>gardener-system-seed-lease</code> namespace and updates
the <code>GardenletReady</code> condition in the <code>status.conditions</code> field of the <code>Seed</code> resource. For more information, see <a href=#lease-reconciler>this section</a>.</p><p>Similar to the <code>node-lifecycle-controller</code> inside the <code>kube-controller-manager</code>,
the <code>gardener-controller-manager</code> features a <code>seed-lifecycle-controller</code> that sets
the <code>GardenletReady</code> condition to <code>Unknown</code> in case the gardenlet fails to renew the lease.
As a consequence, the <code>gardener-scheduler</code> doesn’t consider this seed cluster for newly created shoot clusters anymore.</p><h3 id=healthz-endpoint><code>/healthz</code> Endpoint</h3><p>The gardenlet includes an HTTP server that serves a <code>/healthz</code> endpoint.
It’s used as a liveness probe in the <code>Deployment</code> of the gardenlet.
If the gardenlet fails to renew its lease,
then the endpoint returns <code>500 Internal Server Error</code>, otherwise it returns <code>200 OK</code>.</p><p>Please note that the <code>/healthz</code> only indicates whether the gardenlet
could successfully probe the Seed&rsquo;s API server and renew the lease with
the Garden cluster.
It does <em>not</em> show that the Gardener extension API server (with the Gardener resource groups)
is available.
However, the gardenlet is designed to withstand such connection outages and
retries until the connection is reestablished.</p><h2 id=controllers>Controllers</h2><p>The gardenlet consists out of several controllers which are now described in more detail.</p><h3 id=backupbucket-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbackupbucket><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/backupbucket><code>BackupBucket</code> Controller</a></h3><p>The <code>BackupBucket</code> controller reconciles those <code>core.gardener.cloud/v1beta1.BackupBucket</code> resources whose <code>.spec.seedName</code> value is equal to the name of the <code>Seed</code> the respective <code>gardenlet</code> is responsible for.
A <code>core.gardener.cloud/v1beta1.BackupBucket</code> resource is created by the <code>Seed</code> controller if <code>.spec.backup</code> is defined in the <code>Seed</code>.</p><p>The controller adds finalizers to the <code>BackupBucket</code> and the secret mentioned in the <code>.spec.secretRef</code> of the <code>BackupBucket</code>. The controller also copies this secret to the seed cluster. Additionally, it creates an <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciles it (see <a href=/docs/gardener/extensions/backupbucket/>Contract: BackupBucket Resource</a> for more details).
The status from the reconciliation is reported in the <code>.status.lastOperation</code> field. Once the extension resource is ready and the <code>.status.generatedSecretRef</code> is set by the extension controller, the <code>gardenlet</code> copies the referenced secret to the <code>garden</code> namespace in the garden cluster. An owner reference to the <code>core.gardener.cloud/v1beta1.BackupBucket</code> is added to this secret.</p><p>If the <code>core.gardener.cloud/v1beta1.BackupBucket</code> is deleted, the controller deletes the generated secret in the garden cluster and the <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code> resource in the seed cluster and it waits for the respective extension controller to remove its finalizers from the <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code>. Then it deletes the secret in the seed cluster and finally removes the finalizers from the <code>core.gardener.cloud/v1beta1.BackupBucket</code> and the referred secret.</p><h3 id=backupentry-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbackupentry><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/backupentry><code>BackupEntry</code> Controller</a></h3><p>The <code>BackupEntry</code> controller reconciles those <code>core.gardener.cloud/v1beta1.BackupEntry</code> resources whose <code>.spec.seedName</code> value is equal to the name of a <code>Seed</code> the respective gardenlet is responsible for.
Those resources are created by the <code>Shoot</code> controller (only if backup is enabled for the respective <code>Seed</code>) and there is exactly one <code>BackupEntry</code> per <code>Shoot</code>.</p><p>The controller creates an <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciled it (see <a href=/docs/gardener/extensions/backupentry/>Contract: BackupEntry Resource</a> for more details).
The status is populated in the <code>.status.lastOperation</code> field.</p><p>The <code>core.gardener.cloud/v1beta1.BackupEntry</code> resource has an owner reference pointing to the corresponding <code>Shoot</code>.
Hence, if the <code>Shoot</code> is deleted, the <code>BackupEntry</code> resource also gets deleted.
In this case, the controller deletes the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> resource in the seed cluster and waits until the responsible extension controller has deleted it.
Afterwards, the finalizer of the <code>core.gardener.cloud/v1beta1.BackupEntry</code> resource is released so that it finally disappears from the system.</p><p>If the <code>spec.seedName</code> and <code>.status.seedName</code> of the <code>core.gardener.cloud/v1beta1.BackupEntry</code> are different, the controller will migrate it by annotating the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> in the <code>Source Seed</code> with <code>gardener.cloud/operation: migrate</code>, waiting for it to be migrated successfully and eventually deleting it from the <code>Source Seed</code> cluster. Afterwards, the controller will recreate the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> in the <code>Destination Seed</code>, annotate it with <code>gardener.cloud/operation: restore</code> and wait for the restore operation to finish. For more details about control plane migration, please read <a href=/docs/gardener/usage/control_plane_migration/#shoot-control-plane-migration>Shoot Control Plane Migration</a>.</p><h5 id=keep-backup-for-deleted-shoots>Keep Backup for Deleted Shoots</h5><p>In some scenarios it might be beneficial to not immediately delete the <code>BackupEntry</code>s (and with them, the etcd backup) for deleted <code>Shoot</code>s.</p><p>In this case you can configure the <code>.controllers.backupEntry.deletionGracePeriodHours</code> field in the component configuration of the gardenlet.
For example, if you set it to <code>48</code>, then the <code>BackupEntry</code>s for deleted <code>Shoot</code>s will only be deleted <code>48</code> hours after the <code>Shoot</code> was deleted.</p><p>Additionally, you can limit the <a href=/docs/gardener/usage/shoot_purposes/>shoot purposes</a> for which this applies by setting <code>.controllers.backupEntry.deletionGracePeriodShootPurposes[]</code>.
For example, if you set it to <code>[production]</code> then only the <code>BackupEntry</code>s for <code>Shoot</code>s with <code>.spec.purpose=production</code> will be deleted after the configured grace period. All others will be deleted immediately after the <code>Shoot</code> deletion.</p><h3 id=bastion-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbastion><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/bastion><code>Bastion</code> Controller</a></h3><p>The <code>Bastion</code> controller reconciles those <code>operations.gardener.cloud/v1alpha1.Bastion</code> resources whose <code>.spec.seedName</code> value is equal to the name of a <code>Seed</code> the respective gardenlet is responsible for.</p><p>The controller creates an <code>extensions.gardener.cloud/v1alpha1.Bastion</code> resource in the seed cluster in the shoot namespace with the same name as <code>operations.gardener.cloud/v1alpha1.Bastion</code>. Then it waits until the responsible extension controller has reconciled it (see <a href=/docs/gardener/extensions/bastion/>Contract: Bastion Resource</a> for more details). The status is populated in the <code>.status.conditions</code> and <code>.status.ingress</code> fields.</p><p>During the deletion of <code>operations.gardener.cloud/v1alpha1.Bastion</code> resources, the controller first sets the <code>Ready</code> condition to <code>False</code> and then deletes the <code>extensions.gardener.cloud/v1alpha1.Bastion</code> resource in the seed cluster.
Once this resource is gone, the finalizer of the <code>operations.gardener.cloud/v1alpha1.Bastion</code> resource is released, so it finally disappears from the system.</p><h3 id=controllerinstallation-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollercontrollerinstallation><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/controllerinstallation><code>ControllerInstallation</code> Controller</a></h3><p>The <code>ControllerInstallation</code> controller in the <code>gardenlet</code> reconciles <code>ControllerInstallation</code> objects with the help of the following reconcilers.</p><h4 id=main-reconciler>&ldquo;Main&rdquo; Reconciler</h4><p>This reconciler is responsible for <code>ControllerInstallation</code>s referencing a <code>ControllerDeployment</code> whose <code>type=helm</code>.
It is responsible for unpacking the Helm chart tarball in the <code>ControllerDeployment</code>s <code>.providerConfig.chart</code> field and deploying the rendered resources to the seed cluster.
The Helm chart values in <code>.providerConfig.values</code> will be used and extended with some information about the Gardener environment and the seed cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardener:
</span></span><span style=display:flex><span>  version: &lt;gardenlet-version&gt;
</span></span><span style=display:flex><span>  garden:
</span></span><span style=display:flex><span>    clusterIdentity: &lt;identity-of-garden-cluster&gt;
</span></span><span style=display:flex><span>  seed:
</span></span><span style=display:flex><span>    identity: &lt;seed-name&gt;
</span></span><span style=display:flex><span>    clusterIdentity: &lt;identity-of-seed-cluster&gt;
</span></span><span style=display:flex><span>    annotations: &lt;seed-annotations&gt;
</span></span><span style=display:flex><span>    labels: &lt;seed-labels&gt;
</span></span><span style=display:flex><span>    spec: &lt;seed-specification&gt;
</span></span></code></pre></div><p>As of today, there are a few more fields in <code>.gardener.seed</code>, but it is recommended to use the <code>.gardener.seed.spec</code> if the Helm chart needs more information about the seed configuration.</p><p>The rendered chart will be deployed via a <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.
It is labeled with <code>controllerinstallation-name=&lt;name></code> so that one can easily find the owning <code>ControllerInstallation</code> for an existing <code>ManagedResource</code>.</p><p>The reconciler maintains the <code>Installed</code> condition of the <code>ControllerInstallation</code> and sets it to <code>False</code> if the rendering or deployment fails.</p><h4 id=care-reconciler>&ldquo;Care&rdquo; Reconciler</h4><p>This reconciler reconciles <code>ControllerInstallation</code> objects and checks whether they are in a healthy state.
It checks the <code>.status.conditions</code> of the backing <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.</p><ul><li>If the <code>ResourcesApplied</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Installed</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li><li>If the <code>ResourcesHealthy</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Healthy</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li><li>If the <code>ResourcesProgressing</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Progressing</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li></ul><p>A <code>ControllerInstallation</code> is considered &ldquo;healthy&rdquo; if <code>Applied=Healthy=True</code> and <code>Progressing=False</code>.</p><h4 id=required-reconciler>&ldquo;Required&rdquo; Reconciler</h4><p>This reconciler watches all resources in the <code>extensions.gardener.cloud</code> API group in the seed cluster.
It is responsible for maintaining the <code>Required</code> condition on <code>ControllerInstallation</code>s.
Concretely, when there is at least one extension resource in the seed cluster a <code>ControllerInstallation</code> is responsible for, then the status of the <code>Required</code> condition will be <code>True</code>.
If there are no extension resources anymore, its status will be <code>False</code>.</p><p>This condition is taken into account by the <code>ControllerRegistration</code> controller part of <code>gardener-controller-manager</code> when it computes which extensions have to be deployed to which seed cluster. See <a href=/docs/gardener/concepts/controller-manager/#controllerregistration-controller>Gardener Controller Manager</a> for more details.</p><h3 id=networkpolicy-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollernetworkpolicy><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/networkpolicy><code>NetworkPolicy</code> Controller</a></h3><p>The <code>NetworkPolicy</code> controller reconciles <code>NetworkPolicy</code>s in shoot namespaces in order to ensure access to the Kubernetes API server.</p><p>The controller resolves the IP address of the Kubernetes service in the <code>default</code> namespace and creates an egress <code>NetworkPolicy</code>s for it.</p><p>For more details about <code>NetworkPolicy</code>s in Gardener, please see <a href=/docs/gardener/concepts/network_policies/>Network Policies in Gardener</a>.</p><h3 id=seed-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerseed><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/seed><code>Seed</code> Controller</a></h3><p>The <code>Seed</code> controller in the <code>gardenlet</code> reconciles <code>Seed</code> objects with the help of the following reconcilers.</p><h4 id=main-reconciler-1>&ldquo;Main Reconciler&rdquo;</h4><p>This reconciler is responsible for managing the seed&rsquo;s system components.
Those comprise CA certificates, the various <code>CustomResourceDefinition</code>s, the logging and monitoring stacks, and few central components like <code>gardener-resource-manager</code>, <code>etcd-druid</code>, <code>istio</code>, etc.</p><p>The reconciler also deploys a <code>BackupBucket</code> resource in the garden cluster in case the <code>Seed'</code>s <code>.spec.backup</code> is set.
It also checks whether the seed cluster&rsquo;s Kubernetes version is at least the <a href=/docs/gardener/usage/supported_k8s_versions/#seed-cluster-versions>minimum supported version</a> and errors in case this constraint is not met.</p><p>This reconciler maintains the <code>Bootstrapped</code> condition, i.e. it sets it:</p><ul><li>to <code>Progressing</code> before it executes its reconciliation flow.</li><li>to <code>False</code> in case an error occurs.</li><li>to <code>True</code> in case the reconciliation succeeded.</li></ul><h4 id=care-reconciler-1>&ldquo;Care&rdquo; Reconciler</h4><p>This reconciler checks whether the seed system components (deployed by the &ldquo;main&rdquo; reconciler) are healthy.
It checks the <code>.status.conditions</code> of the backing <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.
A <code>ManagedResource</code> is considered &ldquo;healthy&rdquo; if the conditions <code>ResourcesApplied=ResourcesHealthy=True</code> and <code>ResourcesProgressing=False</code>.</p><p>If all <code>ManagedResource</code>s are healthy, then the <code>SeedSystemComponentsHealthy</code> condition of the <code>Seed</code> will be set to <code>True</code>.
Otherwise, it will be set to <code>False</code>.</p><p>If at least one <code>ManagedResource</code> is unhealthy and there is threshold configuration for the conditions (in <code>.controllers.seedCare.conditionThresholds</code>), then the status of the <code>SeedSystemComponentsHealthy</code> condition will be set:</p><ul><li>to <code>Progressing</code> if it was <code>True</code> before.</li><li>to <code>Progressing</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition does not exceed the configured threshold duration yet.</li><li>to <code>False</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition exceeds the configured threshold duration.</li></ul><p>The condition thresholds can be used to prevent reporting issues too early just because there is a rollout or a short disruption.
Only if the unhealthiness persists for at least the configured threshold duration, then the issues will be reported (by setting the status to <code>False</code>).</p><h4 id=lease-reconciler>&ldquo;Lease&rdquo; Reconciler</h4><p>This reconciler checks whether the connection to the seed cluster&rsquo;s <code>/healthz</code> endpoint works.
If this succeeds, then it renews a <code>Lease</code> resource in the garden cluster&rsquo;s <code>gardener-system-seed-lease</code> namespace.
This indicates a heartbeat to the external world, and internally the <code>gardenlet</code> sets its health status to <code>true</code>.
In addition, the <code>GardenletReady</code> condition in the <code>status</code> of the <code>Seed</code> is set to <code>True</code>.
The whole process is similar to what the <code>kubelet</code> does to report heartbeats for its <code>Node</code> resource and its <code>KubeletReady</code> condition. For more information, see <a href=#heartbeats>this section</a>.</p><p>If the connection to the <code>/healthz</code> endpoint or the update of the <code>Lease</code> fails, then the internal health status of <code>gardenlet</code> is set to <code>false</code>.
Also, this internal health status is set to <code>false</code> automatically after some time, in case the controller gets stuck for whatever reason.
This internal health status is available via the <code>gardenlet</code>&rsquo;s <code>/healthz</code> endpoint and is used for the <code>livenessProbe</code> in the <code>gardenlet</code> pod.</p><h3 id=shoot-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershoot><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shoot><code>Shoot</code> Controller</a></h3><p>The <code>Shoot</code> controller in the <code>gardenlet</code> reconciles <code>Shoot</code> objects with the help of the following reconcilers.</p><h4 id=main-reconciler-2>&ldquo;Main&rdquo; Reconciler</h4><p>This reconciler is responsible for managing all shoot cluster components and implements the core logic for creating, updating, hibernating, deleting, and migrating shoot clusters.
It is also responsible for syncing the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> cluster</a> to the seed cluster before and after each successful shoot reconciliation.</p><p>The main reconciliation logic is performed in 3 different task flows dedicated to specific operation types:</p><ul><li><code>reconcile</code> (operations: create, reconcile, restore): this is the main flow responsible for creation and regular reconciliation of shoots. Hibernating a shoot also triggers this flow. It is also used for restoration of the shoot control plane on the new seed (second half of a <a href=/docs/gardener/usage/control_plane_migration/#shoot-control-plane-migration>Control Plane Migration</a>)</li><li><code>migrate</code>: this flow is triggered when <code>spec.seedName</code> specifies a different seed than <code>status.seedName</code>. It performs the first half of the <a href=/docs/gardener/usage/control_plane_migration/#shoot-control-plane-migration>Control Plane Migration</a>, i.e., a backup (<code>migrate</code> operation) of all control plane components followed by a &ldquo;shallow delete&rdquo;.</li><li><code>delete</code>: this flow is triggered when the shoot&rsquo;s <code>deletionTimestamp</code> is set, i.e., when it is deleted.</li></ul><p>The gardenlet takes special care to prevent unnecessary shoot reconciliations.
This is important for several reasons, e.g., to not overload the seed API servers and to not exhaust infrastructure rate limits too fast.
The gardenlet performs shoot reconciliations according to the following rules:</p><ul><li>If <code>status.observedGeneration</code> is less than <code>metadata.generation</code>: this is the case, e.g., when the spec was changed, a <a href=/docs/gardener/usage/shoot_operations/>manual reconciliation operation</a> was triggered, or the shoot was deleted.</li><li>If the <a href=/docs/gardener/usage/shoot_status/>last operation</a> was not successful.</li><li>If the shoot is in a <a href=/docs/gardener/usage/shoot_status/>failed state</a>, the gardenlet does not perform any reconciliation on the shoot (unless the retry operation was triggered). However, it syncs the <code>Cluster</code> resource to the seed in order to inform the extension controllers about the failed state.</li><li>Regular reconciliations are performed with every <code>GardenletConfiguration.controllers.shoot.syncPeriod</code> (defaults to <code>1h</code>).</li><li>Shoot reconciliations are not performed if the assigned seed cluster is not healthy or has not been reconciled by the current gardenlet version yet (determined by the <code>Seed.status.gardener</code> section). This is done to make sure that shoots are reconciled with fully rolled out seed system components after a Gardener upgrade. Otherwise, the gardenlet might perform operations of the new version that doesn&rsquo;t match the old version of the deployed seed system components, which might lead to unspecified behavior.</li></ul><p>There are a few special cases that overwrite or confine how often and under which circumstances periodic shoot reconciliations are performed:</p><ul><li>In case the gardenlet config allows it (<code>controllers.shoot.respectSyncPeriodOverwrite</code>, disabled by default), the sync period for a shoot can be increased individually by setting the <code>shoot.gardener.cloud/sync-period</code> annotation. This is always allowed for shoots in the <code>garden</code> namespace. Shoots are not reconciled with a higher frequency than specified in <code>GardenletConfiguration.controllers.shoot.syncPeriod</code>.</li><li>In case the gardenlet config allows it (<code>controllers.shoot.respectSyncPeriodOverwrite</code>, disabled by default), shoots can be marked as &ldquo;ignored&rdquo; by setting the <code>shoot.gardener.cloud/ignore</code> annotation. In this case, the gardenlet does not perform any reconciliation for the shoot.</li><li>In case <code>GardenletConfiguration.controllers.shoot.reconcileInMaintenanceOnly</code> is enabled (disabled by default), the gardenlet performs regular shoot reconciliations only once in the respective maintenance time window (<code>GardenletConfiguration.controllers.shoot.syncPeriod</code> is ignored). The gardenlet randomly distributes shoot reconciliations over the maintenance time window to avoid high bursts of reconciliations (see <a href=/docs/gardener/usage/shoot_maintenance/#cluster-reconciliation>Shoot Maintenance</a>).</li><li>In case <code>Shoot.spec.maintenance.confineSpecUpdateRollout</code> is enabled (disabled by default), changes to the shoot specification are not rolled out immediately but only during the respective maintenance time window (see <a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a>).</li></ul><h4 id=care-reconciler-2>&ldquo;Care&rdquo; Reconciler</h4><p>This reconciler performs three &ldquo;care&rdquo; actions related to <code>Shoot</code>s.</p><h5 id=conditions>Conditions</h5><p>It maintains five conditions and performs the following checks:</p><ul><li><code>APIServerAvailable</code>: The <code>/healthz</code> endpoint of the shoot&rsquo;s <code>kube-apiserver</code> is called and considered healthy when it responds with <code>200 OK</code>.</li><li><code>ControlPlaneHealthy</code>: The control plane is considered healthy when the respective <code>Deployment</code>s (for example <code>kube-apiserver</code>,<code>kube-controller-manager</code>), and <code>Etcd</code>s (for example <code>etcd-main</code>) exist and are healthy.</li><li><code>ObservabilityComponentsHealthy</code>: This condition is considered healthy when the respective <code>Deployment</code>s (for example <code>grafana</code>), <code>StatefulSet</code>s (for example <code>prometheus</code>,<code>loki</code>), exist and are healthy.</li><li><code>EveryNodyReady</code>: The conditions of the worker nodes are checked (e.g., <code>Ready</code>, <code>MemoryPressure</code>). Also, it&rsquo;s checked whether the Kubernetes version of the installed <code>kubelet</code> matches the desired version specified in the <code>Shoot</code> resource.</li><li><code>SystemComponentsHealthy</code>: The conditions of the <code>ManagedResource</code>s are checked (e.g., <code>ResourcesApplied</code>). Also, it is verified whether the VPN tunnel connection is established (which is required for the <code>kube-apiserver</code> to communicate with the worker nodes).</li></ul><p>Sometimes, <code>ManagedResource</code>s can have both <code>Healthy</code> and <code>Progressing</code> conditions set to <code>True</code> (e.g., when a <code>DaemonSet</code> rolls out one-by-one on a large cluster with many nodes) while this is not reflected in the <code>Shoot</code> status. In order to catch issues where the rollout gets stuck, one can set <code>.controllers.shootCare.managedResourceProgressingThreshold</code> in the <code>gardenlet</code>&rsquo;s component configuration. If the <code>Progressing</code> condition is still <code>True</code> for more than the configured duration, the <code>SystemComponentsHealthy</code> condition in the <code>Shoot</code> is set to <code>False</code>, eventually.</p><p>Each condition can optionally also have error <code>codes</code> in order to indicate which type of issue was detected (see <a href=/docs/gardener/usage/shoot_status/>Shoot Status</a> for more details).</p><p>Apart from the above, extension controllers can also contribute to the <code>status</code> or error <code>codes</code> of these conditions (see <a href=/docs/gardener/extensions/shoot-health-status-conditions/>Contributing to Shoot Health Status Conditions</a> for more details).</p><p>If all checks for a certain conditions are succeeded, then its <code>status</code> will be set to <code>True</code>.
Otherwise, it will be set to <code>False</code>.</p><p>If at least one check fails and there is threshold configuration for the conditions (in <code>.controllers.seedCare.conditionThresholds</code>), then the status will be set:</p><ul><li>to <code>Progressing</code> if it was <code>True</code> before.</li><li>to <code>Progressing</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition does not exceed the configured threshold duration yet.</li><li>to <code>False</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition exceeds the configured threshold duration.</li></ul><p>The condition thresholds can be used to prevent reporting issues too early just because there is a rollout or a short disruption.
Only if the unhealthiness persists for at least the configured threshold duration, then the issues will be reported (by setting the status to <code>False</code>).</p><h5 id=constraints-and-automatic-webhook-remediation>Constraints And Automatic Webhook Remediation</h5><p>Please see <a href=/docs/gardener/usage/shoot_status/#constraints>Shoot Status</a> for more details.</p><h5 id=garbage-collection>Garbage Collection</h5><p>Stale pods in the shoot namespace in the seed cluster and in the <code>kube-system</code> namespace in the shoot cluster are deleted.
A pod is considered stale when:</p><ul><li>it was terminated with reason <code>Evicted</code>.</li><li>it was terminated with reason starting with <code>OutOf</code> (e.g., <code>OutOfCpu</code>).</li><li>it is stuck in termination (i.e., if its <code>deletionTimestamp</code> is more than <code>5m</code> ago).</li></ul><h3 id=shootstate-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershootstate><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shootstate><code>ShootState</code> Controller</a></h3><p>The <code>ShootState</code> controller in the <code>gardenlet</code> reconciles resources containing information that has to be synced to the <code>ShootState</code>.
This information is used when a <a href=/docs/gardener/usage/control_plane_migration/>control plane migration</a> is performed.</p><h4 id=extensions-reconciler>&ldquo;Extensions&rdquo; Reconciler</h4><p>This reconciler watches resources in the <code>extensions.gardener.cloud</code> API group in the seed cluster which contain a <code>Shoot</code>-specific state or data.
Those are <code>BackupEntry</code>s, <code>ContainerRuntime</code>s, <code>ControlPlane</code>s, <code>DNSRecord</code>s, <code>Extension</code>s, <code>Infrastructure</code>s, <code>Network</code>s, <code>OperatingSystemConfig</code>s, and <code>Worker</code>s.</p><p>When there is a change in the <code>.status.state</code> or <code>.status.resources[]</code> fields of these resources, then this information is synced into the <code>ShootState</code> resource in the garden cluster.</p><h4 id=secret-reconciler>&ldquo;Secret&rdquo; Reconciler</h4><p>This reconciler reconciles <code>Secret</code>s having labels <code>managed-by=secrets-manager</code> and <code>persist=true</code> in the shoot namespaces in the seed cluster.
It syncs them to the <code>ShootState</code> so that the secrets can be restored from there in case a shoot control plane has to be restored to another seed cluster (in case of migration).</p><h2 id=managed-seeds>Managed Seeds</h2><p>Gardener users can use shoot clusters as seed clusters, so-called &ldquo;managed seeds&rdquo; (aka &ldquo;shooted seeds&rdquo;),
by creating <code>ManagedSeed</code> resources.
By default, the gardenlet that manages this shoot cluster then automatically
creates a clone of itself with the same version and the same configuration
that it currently has.
Then it deploys the gardenlet clone into the managed seed cluster.</p><p>For more information, see <a href=/docs/gardener/usage/managed_seed/>Register Shoot as Seed</a>.</p><h2 id=migrating-from-previous-gardener-versions>Migrating from Previous Gardener Versions</h2><p>If your Gardener version doesn’t support gardenlets yet,
no special migration is required, but the following prerequisites must be met:</p><ul><li>Your Gardener version is at least 0.31 before upgrading to v1.</li><li>You have to make sure that your garden cluster is exposed in a way
that it’s reachable from all your seed clusters.</li></ul><p>With previous Gardener versions, you had deployed the Gardener Helm chart
(incorporating the API server, <code>controller-manager</code>, and scheduler).
With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well
into all of your seeds (if they aren’t managed, as mentioned earlier).</p><p>See <a href=/docs/gardener/deployment/deploy_gardenlet/>Deploy a gardenlet</a> for all instructions.</p><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/concepts/architecture/>Gardener Architecture</a></li><li><a href=https://github.com/gardener/gardener/issues/356>#356: Implement Gardener Scheduler</a></li><li><a href=https://github.com/gardener/gardener/pull/2309>#2309: Add /healthz endpoint for gardenlet</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cb07d159d53f4035bc26172627a54c7b>1.2.10 - Network Policies</h1><h1 id=network-policies-in-gardener>Network Policies in Gardener</h1><p>As <code>Seed</code> clusters can host the <a href=https://kubernetes.io/docs/concepts/#kubernetes-control-plane>Kubernetes control planes</a> of many <code>Shoot</code> clusters, it is necessary to isolate the control planes from each other for security reasons.
Besides deploying each control plane in its own namespace, Gardener creates <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>network policies</a> to also isolate the networks.
Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to.
As such, network policies are an important part of Gardener&rsquo;s tenant isolation.</p><p>Gardener deploys network policies into:</p><ul><li>each namespace hosting the Kubernetes control plane of the Shoot cluster.</li><li>the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called <code>garden</code> and contains e.g. the <a href=https://github.com/gardener/gardener/blob/15cae57db802cbe460ff4cb3f80c26b2fc15e26f/docs/concepts/gardenlet.md>Gardenlet</a>.</li><li>the <code>kube-system</code> namespace in the Shoot.</li></ul><p>The aforementioned namespaces in the Seed contain a <code>deny-all</code> network policy that <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic>denies all ingress and egress traffic</a>.
This <a href=https://en.wikipedia.org/wiki/Secure_by_default>secure by default</a> setting requires pods to allow network traffic.
This is done by pods having <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource>labels matching to the selectors of the network policies</a> deployed by Gardener.</p><p>More details on the deployed network policies can be found in the <a href=/docs/gardener/development/seed_network_policies/>development</a> and <a href=/docs/gardener/usage/shoot_network_policies/>usage</a> sections.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b482c2c8a27582776a6072670e48a64d>1.2.11 - Operator</h1><h1 id=gardener-operator>Gardener Operator</h1><p>The <code>gardener-operator</code> is meant to be responsible for the garden cluster environment.
Without this component, users must deploy ETCD, the Gardener control plane, etc., manually and with separate mechanisms (not maintained in this repository).
This is quite unfortunate since this requires separate tooling, processes, etc.
A lot of production- and enterprise-grade features were built into Gardener for managing the seed and shoot clusters, so it makes sense to re-use them as much as possible also for the garden cluster.</p><p><strong>⚠️ Consider this component highly experimental and DO NOT use it in production.</strong></p><h2 id=deployment>Deployment</h2><p>There is a <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/operator>Helm chart</a> which can be used to deploy the <code>gardener-operator</code>.
Once deployed and ready, you can create a <code>Garden</code> resource.
Note that there can only be one <code>Garden</code> resource per system at a time.</p><blockquote><p>ℹ️ Similar to seed clusters, garden runtime clusters require a <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>VPA</a>, see <a href=#vertical-pod-autoscaler>this section</a>.
By default, <code>gardener-operator</code> deploys the VPA components.
However, when there already is a VPA available, then set <code>.spec.runtimeCluster.settings.verticalPodAutoscaler.enabled=false</code> in the <code>Garden</code> resource.</p></blockquote><h2 id=using-garden-runtime-cluster-as-seed-cluster>Using Garden Runtime Cluster As Seed Cluster</h2><p>In production scenarios, you probably wouldn&rsquo;t use the Kubernetes cluster running <code>gardener-operator</code> and the Gardener control plane (called &ldquo;runtime cluster&rdquo;) as seed cluster at the same time.
However, such setup is technically possible and might simplify certain situations (e.g., development, evaluation, &mldr;).</p><p>If the runtime cluster is a seed cluster at the same time, <a href=/docs/gardener/concepts/gardenlet/#seed-controller><code>gardenlet</code>&rsquo;s <code>Seed</code> controller</a> will not manage the components which were already deployed (and reconciled) by <code>gardener-operator</code>.
As of today, this applies to:</p><ul><li><code>gardener-resource-manager</code></li><li><code>vpa-{admission-controller,recommender,updater}</code></li><li><code>hvpa-controller</code> (when <code>HVPA</code> feature gate is enabled)</li><li><code>etcd-druid</code></li></ul><p>Those components are so-called &ldquo;seed system components&rdquo;.
As they were already made available by <code>gardener-operator</code>, the <code>gardenlet</code> just skips them.</p><blockquote><p>ℹ️ There is no need to configure anything - the <code>gardenlet</code> will automatically detect when its seed cluster is the garden runtime cluster at the same time.</p></blockquote><p>⚠️ Note that such setup requires that you upgrade the versions of <code>gardener-operator</code> and <code>gardenlet</code> in lock-step.
Otherwise, you might experience unexpected behaviour or issues with your seed or shoot clusters.</p><h2 id=garden-resources><code>Garden</code> Resources</h2><p>Please find an exemplary <code>Garden</code> resource <a href=https://github.com/gardener/gardener/blob/master/example/operator/20-garden.yaml>here</a>.</p><h3 id=settings-for-runtime-cluster>Settings For Runtime Cluster</h3><p>The <code>Garden</code> resource offers a few settings that are used to control the behaviour of <code>gardener-operator</code> in the runtime cluster.
This section provides an overview over the available settings:</p><h4 id=load-balancer-services>Load Balancer Services</h4><p><code>gardener-operator</code> creates a Kubernetes <code>Service</code> object of type <code>LoadBalancer</code> in the runtime cluster.
It is used for exposing the virtual garden control planes, namely the <code>virtual-garden-kube-apiserver</code>.
In most cases, the <code>cloud-controller-manager</code> (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><p>Note that we might switch to exposing the <code>virtual-garden-kube-apiserver</code> via Istio in the future (similar to how the <code>kube-apiservers</code> of shoot clusters are exposed).
The load balancer service settings might still be relevant, though.</p><h4 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h4><p><code>gardener-operator</code> heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the <code>Garden</code> controller deploys the VPA components into the <code>garden</code> namespace of the respective runtime cluster.
In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of <code>gardener-operator</code>.
Otherwise, you might end up with two VPAs which will cause erratic behaviour.
By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code> you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your runtime cluster.
Using a runtime cluster without VPA is not supported.</p><h2 id=credentials-rotation>Credentials Rotation</h2><p>The credentials rotation works in the same way like it does for <code>Shoot</code> resources, i.e. there are <code>gardener.cloud/operation</code> annotation values for starting or completing the rotation procedures.</p><p>For certificate authorities, <code>gardener-operator</code> generates one which is automatically rotated roughly each month (<code>ca-garden-runtime</code>) and several CAs which are <strong>NOT</strong> automatically rotated but only on demand.</p><p><strong>🚨 Hence, it is the responsibility of the operator to regularly perform the credentials rotation.</strong></p><p>Please refer to <a href=/docs/gardener/usage/shoot_credentials_rotation/#gardener-provided-credentials>this document</a> for more details. As of today, <code>gardener-operator</code> only creates the following types of credentials (i.e., some sections of the document don&rsquo;t apply for <code>Garden</code>s and can be ignored):</p><ul><li>certificate authorities (and related server and client certificates)</li></ul><h2 id=local-development>Local Development</h2><p>The easiest setup is using a local <a href=https://kind.sigs.k8s.io/>KinD</a> cluster and the <a href=https://skaffold.dev/>Skaffold</a> based approach to deploy the <code>gardener-operator</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind-operator-up
</span></span><span style=display:flex><span>make operator-up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># now you can create Garden resources, for example</span>
</span></span><span style=display:flex><span>kubectl create -f example/operator/20-garden.yaml
</span></span><span style=display:flex><span><span style=color:green># alternatively, you can run the e2e test</span>
</span></span><span style=display:flex><span>make test-e2e-local-operator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>make operator-down
</span></span><span style=display:flex><span>make kind-operator-down
</span></span></code></pre></div><p>Generally, any Kubernetes cluster can be used.
An alternative approach is to start the process locally and manually deploy the <code>CustomResourceDefinition</code> for the <code>Garden</code> resources into the targeted cluster (potentially remote):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f example/operator/10-crd-operator.gardener.cloud_gardens.yaml
</span></span><span style=display:flex><span>make KUBECONFIG=... start-operator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># now you can create Garden resources, for example</span>
</span></span><span style=display:flex><span>kubectl create -f example/operator/20-garden.yaml
</span></span><span style=display:flex><span><span style=color:green># alternatively, you can run the e2e test</span>
</span></span><span style=display:flex><span>make KUBECONFIG=... test-e2e-local-operator
</span></span></code></pre></div><h2 id=implementation-details>Implementation Details</h2><h3 id=controllers>Controllers</h3><p>As of today, the <code>gardener-operator</code> only has one controller which is now described in more detail.</p><h4 id=garden-controllerhttpsgithubcomgardenergardenertreemasterpkgoperatorcontrollergarden><a href=https://github.com/gardener/gardener/tree/master/pkg/operator/controller/garden><code>Garden</code> Controller</a></h4><p>The reconciler first generates a general CA certificate which is valid for ~<code>30d</code> and auto-rotated when 80% of its lifetime is reached.
Afterwards, it brings up the so-called &ldquo;garden system components&rdquo;.
The <a href=/docs/gardener/concepts/resource-manager/><code>gardener-resource-manager</code></a> is deployed first since its <code>ManagedResource</code> controller will be used to bring up the remainders.</p><p>Other system components are:</p><ul><li>garden system resources (<a href=/docs/gardener/development/priority-classes/><code>PriorityClass</code>es</a> for the workload resources)</li><li>Vertical Pod Autoscaler (if enabled via <code>.spec.runtimeCluster.settings.verticalPodAutoscaler.enabled=true</code> in the <code>Garden</code>)</li><li>HVPA controller (when <code>HVPA</code> feature gate is enabled)</li><li>ETCD Druid</li></ul><p>As soon as all system components are up, the reconciler deploys the virtual garden cluster.
It comprises out of two ETCDs (one &ldquo;main&rdquo; etcd, one &ldquo;events&rdquo; etcd) which are managed by ETCD Druid via <code>druid.gardener.cloud/v1alpha1.Etcd</code> custom resources.
The whole management works similar to how it works for <code>Shoot</code>s, so you can take a look at <a href=/docs/gardener/concepts/etcd/>this document</a> for more information in general.</p><p>The virtual garden control plane components are:</p><ul><li><code>virtual-garden-etcd-main</code></li><li><code>virtual-garden-etcd-events</code></li></ul><p>If the <code>.spec.virtualCluster.controlPlane.highAvailability={}</code> is set then these components will be deployed in a &ldquo;highly available&rdquo; mode.
For ETCD, this means that there will be 3 replicas each.
This works similar like for <code>Shoot</code>s (see <a href=/docs/gardener/usage/shoot_high_availability/>this document</a>) except for the fact that there is no failure tolerance type configurability.
The <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#high-availability-config>HighAvailabilityConfig webhook</a> makes sure that all pods with multiple replicas are spread on nodes, and if there are at least two zones in <code>.spec.runtimeCluster.provider.zones</code> then they also get spread across availability zones.</p><blockquote><p>If once set, removing <code>.spec.virtualCluster.controlPlane.highAvailability</code> again is not supported.</p></blockquote><p>The <code>virtual-garden-kube-apiserver</code> <code>Deployment</code> (not yet managed by the operator) is exposed via a <code>Service</code> of type <code>LoadBalancer</code> with the same name.
In the future, we might switch to exposing it via Istio, similar to how the <code>kube-apiservers</code> of shoot clusters are exposed.</p><p>The controller maintains the <code>Reconciled</code> condition which indicates the status of an operation.</p><h3 id=webhooks>Webhooks</h3><p>As of today, the <code>gardener-operator</code> only has one webhook handler which is now described in more detail.</p><h4 id=validation>Validation</h4><p>This webhook handler validates <code>CREATE</code>/<code>UPDATE</code>/<code>DELETE</code> operations on <code>Garden</code> resources.
Simple validation is performed via <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation>standard CRD validation</a>.
However, more advanced validation is hard to express via these means and is performed by this webhook handler.</p><p>Furthermore, for deletion requests, it is validated that the <code>Garden</code> is annotated with a deletion confirmation annotation, namely <code>confirmation.gardener.cloud/deletion=true</code>.
Only if this annotation is present it allows the <code>DELETE</code> operation to pass.
This prevents users from accidental/undesired deletions.</p><p>Another validation is to check that there is only one <code>Garden</code> resource at a time.
It prevents creating a second <code>Garden</code> when there is already one in the system.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-41370c41a8d3d04ad03bad8e1b4c465e>1.2.12 - Resource Manager</h1><h1 id=gardener-resource-manager>Gardener Resource Manager</h1><p>Initially, the <code>gardener-resource-manager</code> was a project similar to the <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager>kube-addon-manager</a>.
It manages Kubernetes resources in a target cluster which means that it creates, updates, and deletes them.
Also, it makes sure that manual modifications to these resources are reconciled back to the desired state.</p><p>In the Gardener project we were using the kube-addon-manager since more than two years.
While we have progressed with our <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>extensibility story</a> (moving cloud providers out-of-tree), we had decided that the kube-addon-manager is no longer suitable for this use-case.
The problem with it is that it needs to have its managed resources on its file system.
This requires storing the resources in <code>ConfigMap</code>s or <code>Secret</code>s and mounting them to the kube-addon-manager pod during deployment time.
The <code>gardener-resource-manager</code> uses <code>CustomResourceDefinition</code>s which allows to dynamically add, change, and remove resources with immediate action and without the need to reconfigure the volume mounts/restarting the pod.</p><p>Meanwhile, the <code>gardener-resource-manager</code> has evolved to a more generic component comprising several controllers and webhook handlers.
It is deployed by gardenlet once per seed (in the <code>garden</code> namespace) and once per shoot (in the respective shoot namespaces in the seed).</p><h2 id=component-configuration>Component Configuration</h2><p>Similar to other Gardener components, the <code>gardener-resource-manager</code> uses a so-called component configuration file.
It allows specifying certain central settings like log level and formatting, client connection configuration, server ports and bind addresses, etc.
In addition, controllers and webhooks can be configured and sometimes even disabled.</p><p>Note that the very basic <code>ManagedResource</code>, secret, and health controllers cannot be disabled.</p><p>You can find an example configuration file <a href=https://github.com/gardener/gardener/blob/master/example/resource-manager/10-componentconfig.yaml>here</a>.</p><h2 id=controllers>Controllers</h2><h3 id=managedresource-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollermanagedresource><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/managedresource><code>ManagedResource</code> Controller</a></h3><p>This controller watches custom objects called <code>ManagedResource</code>s in the <code>resources.gardener.cloud/v1alpha1</code> API group.
These objects contain references to secrets, which itself contain the resources to be managed.
The reason why a <code>Secret</code> is used to store the resources is that they could contain confidential information like credentials.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: managedresource-example1
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  objects.yaml: YXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtMTIzNAogIG5hbWVzcGFjZTogZGVmYXVsdAotLS0KYXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtNTY3OAogIG5hbWVzcGFjZTogZGVmYXVsdAo=
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: ConfigMap</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: test-1234</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   namespace: default</span>
</span></span><span style=display:flex><span>    <span style=color:green># ---</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: ConfigMap</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: test-5678</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   namespace: default</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resources.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedResource
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRefs:
</span></span><span style=display:flex><span>  - name: managedresource-example1
</span></span></code></pre></div><p>In the above example, the controller creates two <code>ConfigMap</code>s in the <code>default</code> namespace.
When a user is manually modifying them, they will be reconciled back to the desired state stored in the <code>managedresource-example</code> secret.</p><p>It is also possible to inject labels into all the resources:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: managedresource-example2
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  other-objects.yaml: YXBpVmVyc2lvbjogYXBwcy92MSAjIGZvciB2ZXJzaW9ucyBiZWZvcmUgMS45LjAgdXNlIGFwcHMvdjFiZXRhMgpraW5kOiBEZXBsb3ltZW50Cm1ldGFkYXRhOgogIG5hbWU6IG5naW54LWRlcGxveW1lbnQKc3BlYzoKICBzZWxlY3RvcjoKICAgIG1hdGNoTGFiZWxzOgogICAgICBhcHA6IG5naW54CiAgcmVwbGljYXM6IDIgIyB0ZWxscyBkZXBsb3ltZW50IHRvIHJ1biAyIHBvZHMgbWF0Y2hpbmcgdGhlIHRlbXBsYXRlCiAgdGVtcGxhdGU6CiAgICBtZXRhZGF0YToKICAgICAgbGFiZWxzOgogICAgICAgIGFwcDogbmdpbngKICAgIHNwZWM6CiAgICAgIGNvbnRhaW5lcnM6CiAgICAgIC0gbmFtZTogbmdpbngKICAgICAgICBpbWFnZTogbmdpbng6MS43LjkKICAgICAgICBwb3J0czoKICAgICAgICAtIGNvbnRhaW5lclBvcnQ6IDgwCg==
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: nginx-deployment</span>
</span></span><span style=display:flex><span>    <span style=color:green># spec:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   selector:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     matchLabels:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       app: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   replicas: 2 # tells deployment to run 2 pods matching the template</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   template:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       labels:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         app: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     spec:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       containers:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       - name: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         image: nginx:1.7.9</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         ports:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         - containerPort: 80</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resources.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedResource
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRefs:
</span></span><span style=display:flex><span>  - name: managedresource-example2
</span></span><span style=display:flex><span>  injectLabels:
</span></span><span style=display:flex><span>    foo: bar
</span></span></code></pre></div><p>In this example, the label <code>foo=bar</code> will be injected into the <code>Deployment</code>, as well as into all created <code>ReplicaSet</code>s and <code>Pod</code>s.</p><h4 id=preventing-reconciliations>Preventing Reconciliations</h4><p>If a <code>ManagedResource</code> is annotated with <code>resources.gardener.cloud/ignore=true</code>, then it will be skipped entirely by the controller (no reconciliations or deletions of managed resources at all).
However, when the <code>ManagedResource</code> itself is deleted (for example when a shoot is deleted), then the annotation is not respected and all resources will be deleted as usual.
This feature can be helpful to temporarily patch/change resources managed as part of such <code>ManagedResource</code>.
Condition checks will be skipped for such <code>ManagedResource</code>s.</p><h4 id=modes>Modes</h4><p>The <code>gardener-resource-manager</code> can manage a resource in the following supported modes:</p><ul><li><code>Ignore</code><ul><li>The corresponding resource is removed from the <code>ManagedResource</code> status (<code>.status.resources</code>). No action is performed on the cluster.</li><li>The resource is no longer &ldquo;managed&rdquo; (updated or deleted).</li><li>The primary use case is a migration of a resource from one <code>ManagedResource</code> to another one.</li></ul></li></ul><p>The mode for a resource can be specified with the <code>resources.gardener.cloud/mode</code> annotation. The annotation should be specified in the encoded resource manifest in the Secret that is referenced by the <code>ManagedResource</code>.</p><h4 id=skipping-health-check>Skipping Health Check</h4><p>If a resource in the <code>ManagedResource</code> is annotated with <code>resources.gardener.cloud/skip-health-check=true</code>, then the resource will be skipped during health checks by the health controller. The <code>ManagedResource</code> conditions will not reflect the health condition of this resource anymore. The <code>ResourcesProgressing</code> condition will also be set to <code>False</code>.</p><h4 id=resource-class>Resource Class</h4><p>By default, the <code>gardener-resource-manager</code> controller watches for <code>ManagedResource</code>s in all namespaces.
The <code>.sourceClientConnection.namespace</code> field in the component configuration restricts the watch to <code>ManagedResource</code>s in a single namespace only.
Note that this setting also affects all other controllers and webhooks since it&rsquo;s a central configuration.</p><p>A <code>ManagedResource</code> has an optional <code>.spec.class</code> field that allows it to indicate that it belongs to a given class of resources.
The <code>.controllers.resourceClass</code> field in the component configuration restricts the watch to <code>ManagedResource</code>s with the given <code>.spec.class</code>.
A default class is assumed if no class is specified.</p><h4 id=conditionshttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollerhealth><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/health>Conditions</a></h4><p>A <code>ManagedResource</code> has a <code>ManagedResourceStatus</code>, which has an array of Conditions. Conditions currently include:</p><table><thead><tr><th>Condition</th><th>Description</th></tr></thead><tbody><tr><td><code>ResourcesApplied</code></td><td><code>True</code> if all resources are applied to the target cluster</td></tr><tr><td><code>ResourcesHealthy</code></td><td><code>True</code> if all resources are present and healthy</td></tr><tr><td><code>ResourcesProgressing</code></td><td><code>False</code> if all resources have been fully rolled out</td></tr></tbody></table><p><code>ResourcesApplied</code> may be <code>False</code> when:</p><ul><li>the resource <code>apiVersion</code> is not known to the target cluster</li><li>the resource spec is invalid (for example the label value does not match the required regex for it)</li><li>&mldr;</li></ul><p><code>ResourcesHealthy</code> may be <code>False</code> when:</p><ul><li>the resource is not found</li><li>the resource is a Deployment and the Deployment does not have the minimum availability.</li><li>&mldr;</li></ul><p><code>ResourcesProgressing</code> may be <code>True</code> when:</p><ul><li>a <code>Deployment</code>, <code>StatefulSet</code> or <code>DaemonSet</code> has not been fully rolled out yet, i.e. not all replicas have been updated with the latest changes to <code>spec.template</code>.</li></ul><p>Each Kubernetes resources has different notion for being healthy. For example, a Deployment is considered healthy if the controller observed its current revision and if the number of updated replicas is equal to the number of replicas.</p><p>The following <code>status.conditions</code> section describes a healthy <code>ManagedResource</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>conditions:
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:39Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:39Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources are healthy.
</span></span><span style=display:flex><span>  reason: ResourcesHealthy
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesHealthy
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:36Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:36Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources have been fully rolled out.
</span></span><span style=display:flex><span>  reason: ResourcesRolledOut
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesProgressing
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:18Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:18Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources are applied.
</span></span><span style=display:flex><span>  reason: ApplySucceeded
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesApplied
</span></span></code></pre></div><h4 id=ignoring-updates>Ignoring Updates</h4><p>In some cases, it is not desirable to update or re-apply some of the cluster components (for example, if customization is required or needs to be applied by the end-user).
For these resources, the annotation &ldquo;resources.gardener.cloud/ignore&rdquo; needs to be set to &ldquo;true&rdquo; or a truthy value (Truthy values are &ldquo;1&rdquo;, &ldquo;t&rdquo;, &ldquo;T&rdquo;, &ldquo;true&rdquo;, &ldquo;TRUE&rdquo;, &ldquo;True&rdquo;) in the corresponding managed resource secrets.
This can be done from the components that create the managed resource secrets, for example Gardener extensions or Gardener. Once this is done, the resource will be initially created and later ignored during reconciliation.</p><h4 id=preserving-replicas-or-resources-in-workload-resources>Preserving <code>replicas</code> or <code>resources</code> in Workload Resources</h4><p>The objects which are part of the <code>ManagedResource</code> can be annotated with:</p><ul><li><code>resources.gardener.cloud/preserve-replicas=true</code> in case the <code>.spec.replicas</code> field of workload resources like <code>Deployment</code>s, <code>StatefulSet</code>s, etc., shall be preserved during updates.</li><li><code>resources.gardener.cloud/preserve-resources=true</code> in case the <code>.spec.containers[*].resources</code> fields of all containers of workload resources like <code>Deployment</code>s, <code>StatefulSet</code>s, etc., shall be preserved during updates.</li></ul><blockquote><p>This can be useful if there are non-standard horizontal/vertical auto-scaling mechanisms in place.
Standard mechanisms like <code>HorizontalPodAutoscaler</code> or <code>VerticalPodAutoscaler</code> will be auto-recognized by <code>gardener-resource-manager</code>, i.e., in such cases the annotations are not needed.</p></blockquote><h4 id=origin>Origin</h4><p>All the objects managed by the resource manager get a dedicated annotation
<code>resources.gardener.cloud/origin</code> describing the <code>ManagedResource</code> object that describes
this object. The default format is <code>&lt;namespace>/&lt;objectname></code>.</p><p>In multi-cluster scenarios (the <code>ManagedResource</code> objects are maintained in a
cluster different from the one the described objects are managed), it might
be useful to include the cluster identity, as well.</p><p>This can be enforced by setting the <code>.controllers.clusterID</code> field in the component configuration.
Here, several possibilities are supported:</p><ul><li>given a direct value: use this as id for the source cluster.</li><li><code>&lt;cluster></code>: read the cluster identity from a <code>cluster-identity</code> config map
in the <code>kube-system</code> namespace (attribute <code>cluster-identity</code>). This is
automatically maintained in all clusters managed or involved in a gardener landscape.</li><li><code>&lt;default></code>: try to read the cluster identity from the config map. If not found,
no identity is used.</li><li>empty string: no cluster identity is used (completely cluster local scenarios).</li></ul><p>By default, cluster id is not used. If cluster id is specified, the format is <code>&lt;cluster id>:&lt;namespace>/&lt;objectname></code>.</p><p>In addition to the origin annotation, all objects managed by the resource manager get a dedicated label <code>resources.gardener.cloud/managed-by</code>. This label can be used to describe these objects with a <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>selector</a>. By default it is set to &ldquo;gardener&rdquo;, but this can be overwritten by setting the <code>.conrollers.managedResources.managedByLabelValue</code> field in the component configuration.</p><h3 id=garbage-collector-for-immutable-configmapssecretshttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollergarbagecollector><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/garbagecollector>Garbage Collector For Immutable <code>ConfigMap</code>s/<code>Secret</code>s</a></h3><p>In Kubernetes, workload resources (e.g., <code>Pod</code>s) can mount <code>ConfigMap</code>s or <code>Secret</code>s or reference them via environment variables in containers.
Typically, when the content of such a <code>ConfigMap</code>/<code>Secret</code> gets changed, then the respective workload is usually not dynamically reloading the configuration, i.e., a restart is required.
The most commonly used approach is probably having the so-called <a href=https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments>checksum annotations in the pod template</a>, which makes Kubernetes recreate the pod if the checksum changes.
However, it has the downside that old, still running versions of the workload might not be able to properly work with the already updated content in the <code>ConfigMap</code>/<code>Secret</code>, potentially causing application outages.</p><p>In order to protect users from such outages (and also to improve the performance of the cluster), the Kubernetes community provides the <a href=https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable>&ldquo;immutable <code>ConfigMap</code>s/<code>Secret</code>s feature&rdquo;</a>.
Enabling immutability requires <code>ConfigMap</code>s/<code>Secret</code>s to have unique names.
Having unique names requires the client to delete <code>ConfigMap</code>s/<code>Secret</code>s no longer in use.</p><p>In order to provide a similarly lightweight experience for clients (compared to the well-established checksum annotation approach), the <code>gardener-resource-manager</code> features an optional garbage collector controller (disabled by default).
The purpose of this controller is cleaning up such immutable <code>ConfigMap</code>s/<code>Secret</code>s if they are no longer in use.</p><h4 id=how-does-the-garbage-collector-work>How Does the Garbage Collector Work?</h4><p>The following algorithm is implemented in the GC controller:</p><ol><li>List all <code>ConfigMap</code>s and <code>Secret</code>s labeled with <code>resources.gardener.cloud/garbage-collectable-reference=true</code>.</li><li>List all <code>Deployment</code>s, <code>StatefulSet</code>s, <code>DaemonSet</code>s, <code>Job</code>s, <code>CronJob</code>s, <code>Pod</code>s and for each of them:<ul><li>iterate over the <code>.metadata.annotations</code> and for each of them:<ul><li>If the annotation key follows the <code>reference.resources.gardener.cloud/{configmap,secret}-&lt;hash></code> scheme and the value equals <code>&lt;name></code>, then consider it as &ldquo;in-use&rdquo;.</li></ul></li></ul></li><li>Delete all <code>ConfigMap</code>s and <code>Secret</code>s not considered as &ldquo;in-use&rdquo;.</li></ol><p>Consequently, clients need to:</p><ol><li><p>Create immutable <code>ConfigMap</code>s/<code>Secret</code>s with unique names (e.g., a checksum suffix based on the <code>.data</code>).</p></li><li><p>Label such <code>ConfigMap</code>s/<code>Secret</code>s with <code>resources.gardener.cloud/garbage-collectable-reference=true</code>.</p></li><li><p>Annotate their workload resources with <code>reference.resources.gardener.cloud/{configmap,secret}-&lt;hash>=&lt;name></code> for all <code>ConfigMap</code>s/<code>Secret</code>s used by the containers of the respective <code>Pod</code>s.</p><p>⚠️ Add such annotations to <code>.metadata.annotations</code>, as well as to all templates of other resources (e.g., <code>.spec.template.metadata.annotations</code> in <code>Deployment</code>s or <code>.spec.jobTemplate.metadata.annotations</code> and <code>.spec.jobTemplate.spec.template.metadata.annotations</code> for <code>CronJob</code>s.
This ensures that the GC controller does not unintentionally consider <code>ConfigMap</code>s/<code>Secret</code>s as &ldquo;not in use&rdquo; just because there isn&rsquo;t a <code>Pod</code> referencing them anymore (e.g., they could still be used by a <code>Deployment</code> scaled down to <code>0</code>).</p></li></ol><p>ℹ️ For the last step, there is a helper function <code>InjectAnnotations</code> in the <code>pkg/controller/garbagecollector/references</code>, which you can use for your convenience.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-1234
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    resources.gardener.cloud/garbage-collectable-reference: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-5678
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    resources.gardener.cloud/garbage-collectable-reference: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    reference.resources.gardener.cloud/configmap-82a3537f: test-5678
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: nginx
</span></span><span style=display:flex><span>    image: nginx:1.14.2
</span></span><span style=display:flex><span>    terminationGracePeriodSeconds: 2
</span></span></code></pre></div><p>The GC controller would delete the <code>ConfigMap/test-1234</code> because it is considered as not &ldquo;in-use&rdquo;.</p><p>ℹ️ If the GC controller is activated then the <code>ManagedResource</code> controller will no longer delete <code>ConfigMap</code>s/<code>Secret</code>s having the above label.</p><h4 id=how-to-activate-the-garbage-collector>How to Activate the Garbage Collector?</h4><p>The GC controller can be activated by setting the <code>.controllers.garbageCollector.enabled</code> field to <code>true</code> in the component configuration.</p><h3 id=tokeninvalidator-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollertokeninvalidator><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/tokeninvalidator>TokenInvalidator Controller</a></h3><p>The Kubernetes community is slowly transitioning from static <code>ServiceAccount</code> token <code>Secret</code>s to <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection><code>ServiceAccount</code> Token Volume Projection</a>.
Typically, when you create a <code>ServiceAccount</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ServiceAccount
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: default
</span></span></code></pre></div><p>then the <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/serviceaccount/tokens_controller.go><code>serviceaccount-token</code></a> controller (part of <code>kube-controller-manager</code>) auto-generates a <code>Secret</code> with a static token:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>   annotations:
</span></span><span style=display:flex><span>      kubernetes.io/service-account.name: default
</span></span><span style=display:flex><span>      kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a)
</span></span><span style=display:flex><span>   name: default-token-ntxs9
</span></span><span style=display:flex><span>type: kubernetes.io/service-account-token
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>   ca.crt: base64(cluster-ca-cert)
</span></span><span style=display:flex><span>   namespace: base64(namespace)
</span></span><span style=display:flex><span>   token: base64(static-jwt-token)
</span></span></code></pre></div><p>Unfortunately, when using <code>ServiceAccount</code> Token Volume Projection in a <code>Pod</code>, this static token is actually not used at all:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  serviceAccountName: default
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - image: nginx
</span></span><span style=display:flex><span>    name: nginx
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - mountPath: /var/run/secrets/tokens
</span></span><span style=display:flex><span>      name: token
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: token
</span></span><span style=display:flex><span>    projected:
</span></span><span style=display:flex><span>      sources:
</span></span><span style=display:flex><span>      - serviceAccountToken:
</span></span><span style=display:flex><span>          path: token
</span></span><span style=display:flex><span>          expirationSeconds: 7200
</span></span></code></pre></div><p>While the <code>Pod</code> is now using an expiring and auto-rotated token, the static token is still generated and valid.</p><p>As of Kubernetes v1.22, there is neither a way of preventing <code>kube-controller-manager</code> to generate such static tokens, nor a way to proactively remove or invalidate them:</p><ul><li><a href=https://github.com/kubernetes/kubernetes/issues/77599>https://github.com/kubernetes/kubernetes/issues/77599</a></li><li><a href=https://github.com/kubernetes/kubernetes/issues/77600>https://github.com/kubernetes/kubernetes/issues/77600</a></li></ul><p>Disabling the <code>serviceaccount-token</code> controller is an option, however, especially in the Gardener context it may either break end-users or it may not even be possible to control such settings.
Also, even if a future Kubernetes version supports native configuration of the above behaviour, Gardener still supports older versions which won&rsquo;t get such features but need a solution as well.</p><p>This is where the <em>TokenInvalidator</em> comes into play:
Since it is not possible to prevent <code>kube-controller-manager</code> from generating static <code>ServiceAccount</code> <code>Secret</code>s, the <em>TokenInvalidator</em> is, as its name suggests, just invalidating these tokens.
It considers all such <code>Secret</code>s belonging to <code>ServiceAccount</code>s with <code>.automountServiceAccountToken=false</code>.
By default, all namespaces in the target cluster are watched, however, this can be configured by specifying the <code>.targetClientConnection.namespace</code> field in the component configuration.
Note that this setting also affects all other controllers and webhooks since it&rsquo;s a central configuration.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ServiceAccount
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-serviceaccount
</span></span><span style=display:flex><span>automountServiceAccountToken: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>This will result in a static <code>ServiceAccount</code> token secret whose <code>token</code> value is invalid:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubernetes.io/service-account.name: my-serviceaccount
</span></span><span style=display:flex><span>    kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a
</span></span><span style=display:flex><span>  name: my-serviceaccount-token-ntxs9
</span></span><span style=display:flex><span>type: kubernetes.io/service-account-token
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  ca.crt: base64(cluster-ca-cert)
</span></span><span style=display:flex><span>  namespace: base64(namespace)
</span></span><span style=display:flex><span>  token: AAAA
</span></span></code></pre></div><p>Any attempt to regenerate the token or creating a new such secret will again make the component invalidating it.</p><blockquote><p>You can opt-out of this behaviour for <code>ServiceAccount</code>s setting <code>.automountServiceAccountToken=false</code> by labeling them with <code>token-invalidator.resources.gardener.cloud/skip=true</code>.</p></blockquote><p>In order to enable the <em>TokenInvalidator</em> you have to set both <code>.controllers.tokenValidator.enabled=true</code> and <code>.webhooks.tokenValidator.enabled=true</code> in the component configuration.</p><p>The below graphic shows an overview of the Token Invalidator for Service account secrets in the Shoot cluster.
<img src=/__resources/resource-manager-token-invalidator_b43fa2.jpg alt=image></p><h3 id=tokenrequestor-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollertokenrequestor><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/tokenrequestor>TokenRequestor Controller</a></h3><p>This controller provides the service to create and auto-renew tokens via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/><code>TokenRequest</code> API</a>.</p><p>It provides a functionality similar to the kubelet&rsquo;s <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a>.
It was created to handle the special case of issuing tokens to pods that run in a different cluster than the API server they communicate with (hence, using the native token volume projection feature is not possible).</p><p>The controller differentiates between <code>source cluster</code> and <code>target cluster</code>.
The <code>source cluster</code> hosts the <code>gardener-resource-manager</code> pod. Secrets in this cluster are watched and modified by the controller.
The <code>target cluster</code> <em>can</em> be configured to point to another cluster. The existence of ServiceAccounts are ensured and token requests are issued against the target.
When the <code>gardener-resource-manager</code> is deployed next to the Shoot&rsquo;s controlplane in the Seed, the <code>source cluster</code> is the Seed while the <code>target cluster</code> points to the Shoot.</p><h4 id=reconciliation-loop>Reconciliation Loop</h4><p>This controller reconciles secrets in all namespaces in the source cluster with the label: <code>resources.gardener.cloud/purpose: token-requestor</code>.
See <a href=https://github.com/gardener/gardener/blob/master/example/resource-manager/30-secret-tokenrequestor.yaml>this yaml file</a> for an example of the secret.</p><p>The controller ensures a <code>ServiceAccount</code> exists in the target cluster as specified in the annotations of the <code>Secret</code> in the source cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>serviceaccount.resources.gardener.cloud/name: &lt;sa-name&gt;
</span></span><span style=display:flex><span>serviceaccount.resources.gardener.cloud/namespace: &lt;sa-namespace&gt;
</span></span></code></pre></div><p>The requested tokens will act with the privileges which are assigned to this <code>ServiceAccount</code>.</p><p>The controller will then request a token via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/><code>TokenRequest</code> API</a> and populate it into the <code>.data.token</code> field to the <code>Secret</code> in the source cluster.</p><p>Alternatively, the client can provide a raw kubeconfig (in YAML or JSON format) via the <code>Secret</code>&rsquo;s <code>.data.kubeconfig</code> field.
The controller will then populate the requested token in the kubeconfig for the user used in the <code>.current-context</code>.
For example, if <code>.data.kubeconfig</code> is</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: AAAA
</span></span><span style=display:flex><span>    server: some-server-url
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: shoot--foo--bar
</span></span><span style=display:flex><span>    user: shoot--foo--bar-token
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>current-context: shoot--foo--bar
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>preferences: {}
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: shoot--foo--bar-token
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    token: <span style=color:#a31515>&#34;&#34;</span>
</span></span></code></pre></div><p>then the <code>.users[0].user.token</code> field of the kubeconfig will be updated accordingly.</p><p>The controller also adds an annotation to the <code>Secret</code> to keep track when to renew the token before it expires.
By default, the tokens are issued to expire after 12 hours. The expiration time can be set with the following annotation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>serviceaccount.resources.gardener.cloud/token-expiration-duration: 6h
</span></span></code></pre></div><p>It automatically renews once 80% of the lifetime is reached, or after <code>24h</code>.</p><p>Optionally, the controller can also populate the token into a <code>Secret</code> in the target cluster. This can be requested by annotating the <code>Secret</code> in the source cluster with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>token-requestor.resources.gardener.cloud/target-secret-name: <span style=color:#a31515>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>token-requestor.resources.gardener.cloud/target-secret-namespace: <span style=color:#a31515>&#34;bar&#34;</span>
</span></span></code></pre></div><p>Overall, the TokenRequestor controller provides credentials with limited lifetime (JWT tokens)
used by Shoot control plane components running in the Seed to talk to the Shoot API Server.
Please see the graphic below:</p><p><img src=/__resources/resource-manager-projected-token-controlplane-to-shoot-apiserver_da4cda.jpg alt=image></p><h3 id=kubelet-server-certificatesigningrequest-approverhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollercsrapprover><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/csrapprover>Kubelet Server <code>CertificateSigningRequest</code> Approver</a></h3><p>Gardener configures the kubelets such that they request two certificates via the <code>CertificateSigningRequest</code> API:</p><ol><li>client certificate for communicating with the <code>kube-apiserver</code></li><li>server certificate for serving its HTTPS server</li></ol><p>For client certificates, the <code>kubernetes.io/kube-apiserver-client-kubelet</code> signer is used (see <a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers>Certificate Signing Requests</a> for more details).
The <code>kube-controller-manager</code>&rsquo;s <code>csrapprover</code> controller is responsible for auto-approving such <code>CertificateSigningRequest</code>s so that the respective certificates can be issued.</p><p>For server certificates, the <code>kubernetes.io/kubelet-serving</code> signer is used.
Unfortunately, the <code>kube-controller-manager</code> is not able to auto-approve such <code>CertificateSigningRequest</code>s (see <a href=https://github.com/kubernetes/kubernetes/issues/73356>kubernetes/kubernetes#73356</a> for details).</p><p>That&rsquo;s the motivation for having this controller as part of <code>gardener-resource-manager</code>.
It watches <code>CertificateSigningRequest</code>s with the <code>kubernetes.io/kubelet-serving</code> signer and auto-approves them when all the following conditions are met:</p><ul><li>The <code>.spec.username</code> is prefixed with <code>system:node:</code>.</li><li>There must be at least one DNS name or IP address as part of the certificate SANs.</li><li>The common name in the CSR must match the <code>.spec.username</code>.</li><li>The organization in the CSR must only contain <code>system:nodes</code>.</li><li>There must be a <code>Node</code> object with the same name in the shoot cluster.</li><li>There must be exactly one <code>Machine</code> for the node in the seed cluster.</li><li>The DNS names part of the SANs must be equal to all <code>.status.addresses[]</code> of type <code>Hostname</code> in the <code>Node</code>.</li><li>The IP addresses part of the SANs must be equal to all <code>.status.addresses[]</code> of type <code>InternalIP</code> in the <code>Node</code>.</li></ul><p>If any one of these requirements is violated, the <code>CertificateSigningRequest</code> will be denied.
Otherwise, once approved, the <code>kube-controller-manager</code>&rsquo;s <code>csrsigner</code> controller will issue the requested certificate.</p><h3 id=networkpolicy-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollernetworkpolicy><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/networkpolicy><code>NetworkPolicy</code> Controller</a></h3><p>This controller reconciles <code>Service</code>s with a non-empty <code>.spec.podSelector</code>.
It creates two <code>NetworkPolicy</code>s for each port in the <code>.spec.ports[]</code> list.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-resource-manager
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: gardener-resource-manager
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - name: server
</span></span><span style=display:flex><span>    port: 443
</span></span><span style=display:flex><span>    protocol: TCP
</span></span><span style=display:flex><span>    targetPort: 10250
</span></span></code></pre></div><p>leads to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows ingress TCP traffic to port 10250 for pods
</span></span><span style=display:flex><span>      selected by the a/gardener-resource-manager service selector from pods running
</span></span><span style=display:flex><span>      in namespace a labeled with map[networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250:allowed].
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows egress TCP traffic to port 10250 from pods
</span></span><span style=display:flex><span>      running in namespace a labeled with map[networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250:allowed]
</span></span><span style=display:flex><span>      to pods selected by the a/gardener-resource-manager service selector.
</span></span><span style=display:flex><span>  name: egress-to-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  egress:
</span></span><span style=display:flex><span>  - to:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          app: gardener-resource-manager
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Egress
</span></span></code></pre></div><p>A component that initiates the connection to <code>gardener-resource-manager</code>&rsquo;s <code>tcp/10250</code> port can now be labeled with <code>networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250=allowed</code>.
That&rsquo;s all this component needs to do - it does not need to create any <code>NetworkPolicy</code>s itself.</p><h4 id=cross-namespace-communication>Cross-Namespace Communication</h4><p>Apart from this &ldquo;simple&rdquo; case where both communicating components run in the same namespace <code>a</code>, there is also the cross-namespace communication case.
With above example, let&rsquo;s say there are components running in another namespace <code>b</code>, and they would like to initiate the communication with <code>gardener-resource-manager</code> in <code>a</code>.
To cover this scenario, the <code>Service</code> can be annotated with <code>networking.resources.gardener.cloud/namespace-selectors='[{"matchLabels":{"kubernetes.io/metadata.name":"b"}}]'</code>.</p><blockquote><p>Note that you can specify multiple namespace selectors in this annotation which are OR-ed.</p></blockquote><p>This will make the controller create additional <code>NetworkPolicy</code>s as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows ingress TCP traffic to port 10250 for pods selected
</span></span><span style=display:flex><span>      by the a/gardener-resource-manager service selector from pods running in namespace b
</span></span><span style=display:flex><span>      labeled with map[networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250:allowed].
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-tcp-10250-from-b
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: b
</span></span><span style=display:flex><span>      podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows egress TCP traffic to port 10250 from pods running in
</span></span><span style=display:flex><span>      namespace b labeled with map[networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250:allowed]
</span></span><span style=display:flex><span>      to pods selected by the a/gardener-resource-manager service selector.
</span></span><span style=display:flex><span>  name: egress-to-a-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: b
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  egress:
</span></span><span style=display:flex><span>  - to:
</span></span><span style=display:flex><span>    - namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: a
</span></span><span style=display:flex><span>      podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          app: gardener-resource-manager
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Egress
</span></span></code></pre></div><p>The components in namespace <code>b</code> now need to be labeled with <code>networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250=allowed</code>, but that&rsquo;s already it.</p><blockquote><p>Obviously, this approach also works for namespace selectors different from <code>kubernetes.io/metadata.name</code> to cover scenarios where the namespace name is not known upfront or where multiple namespaces with a similar label are relevant.
The controller creates two dedicated policies for each namespace matching the selectors.</p></blockquote><h4 id=service-targets-in-multiple-namespaces><code>Service</code> Targets In Multiple Namespaces</h4><p>Finally, let&rsquo;s say there is a <code>Service</code> called <code>example</code> which exists in different namespaces whose names are not static (e.g., <code>foo-1</code>, <code>foo-2</code>), and a component in namespace <code>bar</code> wants to initiate connections with all of them.</p><p>The <code>example</code> <code>Service</code>s in these namespaces can now be annotated with <code>networking.resources.gardener.cloud/namespace-selectors='[{"matchLabels":{"kubernetes.io/metadata.name":"bar"}}]'</code>.
As a consequence, the component in namespace <code>bar</code> now needs to be labeled with <code>networking.resources.gardener.cloud/to-foo-1-example-tcp-8080=allowed</code>, <code>networking.resources.gardener.cloud/to-foo-2-example-tcp-8080=allowed</code>, etc.
This approach does not work in practice, however, since the namespace names are neither static nor known upfront.</p><p>To overcome this, it is possible to specify an alias for the concrete namespace in the pod label selector via the <code>networking.resources.gardener.cloud/pod-label-selector-namespace-alias</code> annotation.</p><p>In above case, the <code>example</code> <code>Service</code> in the <code>foo-*</code> namespaces could be annotated with <code>networking.resources.gardener.cloud/pod-label-selector-namespace-alias=all-foos</code>.
This would modify the label selector in all <code>NetworkPolicy</code>s related to cross-namespace communication, i.e. instead of <code>networking.resources.gardener.cloud/to-foo-{1,2,...}-example-tcp-8080=allowed</code>, <code>networking.resources.gardener.cloud/to-all-foos-example-tcp-8080=allowed</code> would be used.
Now the component in namespace <code>bar</code> only needs this single label and is able to talk to all such <code>Service</code>s in the different namespaces.</p><blockquote><p>Real-world examples for this scenario are the <code>kube-apiserver</code> <code>Service</code> (which exists in all shoot namespaces), or the <code>istio-ingressgateway</code> <code>Service</code> (which exists in all <code>istio-ingress*</code> namespaces).
In both cases, the names of the namespaces are not statically known and depend on user input.</p></blockquote><h4 id=overwriting-the-pod-selector-label>Overwriting The Pod Selector Label</h4><p>For a component which initiates the connection to many other components, it&rsquo;s sometimes impractical to specify all the respective labels in its pod template.
For example, let&rsquo;s say a component <code>foo</code> talks to <code>bar{0..9}</code> on ports <code>tcp/808{0..9}</code>.
<code>foo</code> would need to have the ten <code>networking.resources.gardener.cloud/to-bar{0..9}-tcp-808{0..9}=allowed</code> labels.</p><p>As an alternative and to simplify this, it is also possible to annotate the targeted <code>Service</code>s with <code>networking.resources.gardener.cloud/from-policy-pod-label-selector=&lt;some-alias></code>.
For our example, <code>&lt;some-alias></code> could be <code>all-bars</code>.</p><p>As a result, component <code>foo</code> just needs to have the label <code>networking.resources.gardener.cloud/to-all-bars=allowed</code> instead of all the other ten explicit labels.</p><p>⚠️ Note that this also requires to specify the list of allowed container ports since the pod selector label will no longer be specific for a dedicated service/port.
For our example, the <code>Service</code> for <code>barX</code> with <code>X</code> in <code>{0..9}</code> needs to be annotated with <code>networking.resources.gardener.cloud/from-policy-allowed-ports=[{"port":808X,"protocol":"TCP"}]</code> in addition.</p><blockquote><p>Real-world examples for this scenario are the <code>Prometheis</code> in seed clusters which initiate the communication to a lot of components in order to scrape their metrics.</p></blockquote><h4 id=ingress-from-everywhere>Ingress From Everywhere</h4><p>All above scenarios are about components initiating connections to some targets.
However, some components also receive incoming traffic from sources outside the cluster.
This traffic requires adequate ingress policies so that it can be allowed.</p><p>To cover this scenario, the <code>Service</code> can be annotated with <code>networking.resources.gardener.cloud/from-world-to-ports=[{"port":"10250","protocol":"TCP"}]</code>.
As a result, the controller creates the following <code>NetworkPolicy</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-from-world
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - namespaceSelector: {}
</span></span><span style=display:flex><span>      podSelector: {}
</span></span><span style=display:flex><span>    - ipBlock:
</span></span><span style=display:flex><span>        cidr: 0.0.0.0/0
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span></code></pre></div><p>The respective pods don&rsquo;t need any additional labels.
If the annotation&rsquo;s value is empty (<code>[]</code>) then all ports are allowed.</p><h3 id=node-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollernode><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/node><code>Node</code> Controller</a></h3><p>Gardenlet configures kubelet of shoot worker nodes to register the <code>Node</code> object with the <code>node.gardener.cloud/critical-components-not-ready</code> taint (effect <code>NoSchedule</code>).
This controller watches newly created <code>Node</code> objects in the shoot cluster and removes the taint once all node-critical components are scheduled and ready.
If the controller finds node-critical components that are not scheduled or not ready yet, it checks the <code>Node</code> again after the duration configured in <code>ResourceManagerConfiguration.controllers.node.backoff</code>
Please refer to the <a href=/docs/gardener/usage/node-readiness/>feature documentation</a> or <a href=https://github.com/gardener/gardener/issues/7117>proposal issue</a> for more details.</p><h2 id=webhooks>Webhooks</h2><h3 id=mutating-webhooks>Mutating Webhooks</h3><h4 id=high-availability-config>High Availability Config</h4><p>This webhook is used to conveniently apply the configuration to make components deployed to seed or shoot clusters highly available.
The details and scenarios are described in <a href=/docs/gardener/development/high-availability/>High Availability Of Deployed Components</a>.</p><p>The webhook reacts on creation/update of <code>Deployment</code>s, <code>StatefulSet</code>s, <code>HorizontalPodAutoscaler</code>s and <code>HVPA</code>s in namespaces labeled with <code>high-availability-config.resources.gardener.cloud/consider=true</code>.</p><p>The webhook performs the following actions:</p><ol><li><p>The <code>.spec.replicas</code> (or <code>spec.minReplicas</code> respectively) field is mutated based on the <code>high-availability-config.resources.gardener.cloud/type</code> label of the resource and the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation of the namespace:</p><table><thead><tr><th>Failure Tolerance Type ➡️<br>/<br>⬇️ Component Type️ ️</th><th>unset</th><th>empty</th><th>non-empty</th></tr></thead><tbody><tr><td><code>controller</code></td><td><code>2</code></td><td><code>1</code></td><td><code>2</code></td></tr><tr><td><code>server</code></td><td><code>2</code></td><td><code>2</code></td><td><code>2</code></td></tr></tbody></table><ul><li>The replica count values can be overwritten by the <code>high-availability-config.resources.gardener.cloud/replicas</code> annotation.</li><li>It does NOT mutate the replicas when:<ul><li>the replicas are already set to <code>0</code> (hibernation case), or</li><li>when the resource is scaled horizontally by <code>HorizontalPodAutoscaler</code> or <code>Hvpa</code>, and the current replica count is higher than what was computed above.</li></ul></li></ul></li><li><p>When the <code>high-availability-config.resources.gardener.cloud/zones</code> annotation is NOT empty and either the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation is set or the <code>high-availability-config.resources.gardener.cloud/zone-pinning</code> annotation is set to <code>true</code>, then it adds a <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity>node affinity</a> to the pod template spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    nodeAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>        nodeSelectorTerms:
</span></span><span style=display:flex><span>        - matchExpressions:
</span></span><span style=display:flex><span>          - key: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>            - &lt;zone1&gt;
</span></span><span style=display:flex><span>          <span style=color:green># - ...</span>
</span></span></code></pre></div><p>This ensures that all pods are pinned to only nodes in exactly those concrete zones.</p></li><li><p><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/>Topology Spread Constraints</a> are added to the pod template spec when the <code>.spec.replicas</code> are greater than <code>1</code>. When the <code>high-availability-config.resources.gardener.cloud/zones</code> annotation &mldr;</p><ul><li><p>&mldr; contains only one zone, then the following is added:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span></code></pre></div><p>This ensures that the (multiple) pods are scheduled across nodes on best-effort basis.</p></li><li><p>&mldr; contains at least two zones, then the following is added:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span><span style=display:flex><span>  - topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span></code></pre></div><p>This enforces that the (multiple) pods are scheduled across zones.
It circumvents a known limitation in Kubernetes for clusters &lt; 1.26 (ref <a href=https://github.com/kubernetes/kubernetes/issues/109364>kubernetes/kubernetes#109364</a>.
In case the number of replicas is larger than twice the number of zones, then the <code>maxSkew=2</code> for the second spread constraints.</p></li></ul><p>Independent on the number of zones, when the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation is set and NOT empty, then the <code>whenUnsatisfiable</code> is set to <code>DoNotSchedule</code> for the constraint with <code>topologyKey=kubernetes.io/hostname</code> (which enforces the node-spread).</p></li></ol><h4 id=auto-mounting-projected-serviceaccount-tokens>Auto-Mounting Projected <code>ServiceAccount</code> Tokens</h4><p>When this webhook is activated, then it automatically injects projected <code>ServiceAccount</code> token volumes into <code>Pod</code>s and all its containers if all of the following preconditions are fulfilled:</p><ol><li>The <code>Pod</code> is NOT labeled with <code>projected-token-mount.resources.gardener.cloud/skip=true</code>.</li><li>The <code>Pod</code>&rsquo;s <code>.spec.serviceAccountName</code> field is NOT empty and NOT set to <code>default</code>.</li><li>The <code>ServiceAccount</code> specified in the <code>Pod</code>&rsquo;s <code>.spec.serviceAccountName</code> sets <code>.automountServiceAccountToken=false</code>.</li><li>The <code>Pod</code>&rsquo;s <code>.spec.volumes[]</code> DO NOT already contain a volume with a name prefixed with <code>kube-api-access-</code>.</li></ol><p>The projected volume will look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: kube-api-access-gardener
</span></span><span style=display:flex><span>    projected:
</span></span><span style=display:flex><span>      defaultMode: 420
</span></span><span style=display:flex><span>      sources:
</span></span><span style=display:flex><span>      - serviceAccountToken:
</span></span><span style=display:flex><span>          expirationSeconds: 43200
</span></span><span style=display:flex><span>          path: token
</span></span><span style=display:flex><span>      - configMap:
</span></span><span style=display:flex><span>          items:
</span></span><span style=display:flex><span>          - key: ca.crt
</span></span><span style=display:flex><span>            path: ca.crt
</span></span><span style=display:flex><span>          name: kube-root-ca.crt
</span></span><span style=display:flex><span>      - downwardAPI:
</span></span><span style=display:flex><span>          items:
</span></span><span style=display:flex><span>          - fieldRef:
</span></span><span style=display:flex><span>              apiVersion: v1
</span></span><span style=display:flex><span>              fieldPath: metadata.namespace
</span></span><span style=display:flex><span>            path: namespace
</span></span></code></pre></div><blockquote><p>The <code>expirationSeconds</code> are defaulted to <code>12h</code> and can be overwritten with the <code>.webhooks.projectedTokenMount.expirationSeconds</code> field in the component configuration, or with the <code>projected-token-mount.resources.gardener.cloud/expiration-seconds</code> annotation on a <code>Pod</code> resource.</p></blockquote><p>The volume will be mounted into all containers specified in the <code>Pod</code> to the path <code>/var/run/secrets/kubernetes.io/serviceaccount</code>.
This is the default location where client libraries expect to find the tokens and mimics the <a href=https://github.com/kubernetes/kubernetes/tree/v1.22.2/plugin/pkg/admission/serviceaccount>upstream <code>ServiceAccount</code> admission plugin</a>. See <a href=https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller>Managing Service Accounts</a> for more information.</p><p>Overall, this webhook is used to inject projected service account tokens into pods running in the Shoot and the Seed cluster.
Hence, it is served from the Seed GRM and each Shoot GRM.
Please find an overview below for pods deployed in the Shoot cluster:</p><p><img src=/__resources/resource-manager-projected-token-shoot-to-shoot-apiserver_4fdaf3.jpg alt=image></p><h4 id=pod-topology-spread-constraints>Pod Topology Spread Constraints</h4><p>When this webhook is enabled, then it mimics the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#spread-constraint-definition>topologyKey feature</a> for <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints>Topology Spread Constraints (TSC)</a> on the label <code>pod-template-hash</code>.
Concretely, when a pod is labelled with <code>pod-template-hash</code>, the handler of this webhook extends any topology spread constraint in the pod:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    pod-template-hash: 123abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        pod-template-hash: 123abc <span style=color:green># added by webhook</span>
</span></span></code></pre></div><p>The procedure circumvents a <a href=https://github.com/kubernetes/kubernetes/issues/98215>known limitation</a> with TSCs which leads to imbalanced deployments after rolling updates.
Gardener enables this webhook to schedule pods of deployments across nodes and zones.</p><p>Please note that the <code>gardener-resource-manager</code> itself as well as pods labelled with <code>topology-spread-constraints.resources.gardener.cloud/skip</code> are excluded from any mutations.</p><h4 id=system-components-webhook>System Components Webhook</h4><p>If enabled, this webhook handles scheduling concerns for system components <code>Pod</code>s (except those managed by <code>DaemonSet</code>s).
The following tasks are performed by this webhook:</p><ol><li>Add <code>pod.spec.nodeSelector</code> as given in the webhook configuration.</li><li>Add <code>pod.spec.tolerations</code> as given in the webhook configuration.</li><li>Add <code>pod.spec.tolerations</code> for any existing nodes matching the node selector given in the webhook configuration. Known taints and tolerations used for <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>taint based evictions</a> are disregarded.</li></ol><p>Gardener enables this webhook for <code>kube-system</code> and <code>kubernetes-dashboard</code> namespaces in shoot clusters, selecting <code>Pod</code>s being labelled with <code>resources.gardener.cloud/managed-by: gardener</code>.
It adds a configuration, so that <code>Pod</code>s will get the <code>worker.gardener.cloud/system-components: true</code> node selector (step 1) as well as tolerate any custom taint (step 2) that is added to system component worker nodes (<code>shoot.spec.provider.workers[].systemComponents.allow: true</code>).
In addition, the webhook merges these tolerations with the ones required for at that time available system component <code>Node</code>s in the cluster (step 3).
Both is required to ensure system component <code>Pod</code>s can be <em>scheduled</em> or <em>executed</em> during an active shoot reconciliation that is happening due to any modifications to <code>shoot.spec.provider.workers[].taints</code>, e.g. <code>Pod</code>s must be scheduled while there are still <code>Node</code>s not having the updated taint configuration.</p><blockquote><p>You can opt-out of this behaviour for <code>Pod</code>s by labeling them with <code>system-components-config.resources.gardener.cloud/skip=true</code>.</p></blockquote><h3 id=validating-webhooks>Validating Webhooks</h3><h4 id=unconfirmed-deletion-prevention-for-custom-resources-and-definitions>Unconfirmed Deletion Prevention For Custom Resources And Definitions</h4><p>As part of Gardener&rsquo;s <a href=/docs/gardener/extensions/overview/>extensibility concepts</a>, a lot of <code>CustomResourceDefinition</code>s are deployed to the seed clusters that serve as extension points for provider-specific controllers.
For example, the <a href=/docs/gardener/extensions/infrastructure/><code>Infrastructure</code> CRD</a> triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster.
Consequently, these extension CRDs have a lot of power and control large portions of the end-user&rsquo;s shoot cluster.
Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.</p><p>When this webhook is activated, it reacts for <code>CustomResourceDefinition</code>s and most of the custom resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group.
It also reacts for the <code>druid.gardener.cloud/v1alpha1.Etcd</code> resources.</p><p>The webhook prevents <code>DELETE</code> requests for those <code>CustomResourceDefinition</code>s labeled with <code>gardener.cloud/deletion-protected=true</code>, and for all mentioned custom resources if they were not previously annotated with the <code>confirmation.gardener.cloud/deletion=true</code>.
This prevents that undesired <code>kubectl delete &lt;...></code> requests are accepted.</p><h4 id=extension-resource-validation>Extension Resource Validation</h4><p>When this webhook is activated, it reacts for most of the custom resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group.
It also reacts for the <code>druid.gardener.cloud/v1alpha1.Etcd</code> resources.</p><p>The webhook validates the resources specifications for <code>CREATE</code> and <code>UPDATE</code> requests.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d13c2ec04fc608d6d65a3a24ab54fbf9>1.2.13 - Scheduler</h1><h1 id=gardener-scheduler>Gardener Scheduler</h1><p>The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them.
Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.</p><p>Either the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating.
The following sections explain the configuration and flow in greater detail.</p><h2 id=why-is-the-gardener-scheduler-needed>Why Is the Gardener Scheduler Needed?</h2><h3 id=1-decoupling>1. Decoupling</h3><p>Previously, an admission plugin in the Gardener API server conducted the scheduling decisions.
This implies changes to the API server whenever adjustments of the scheduling are needed.
Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.</p><h3 id=2-extensibility>2. Extensibility</h3><p>It should be possible to easily extend and tweak the scheduler in the future.
Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions.
It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.</p><h2 id=algorithm-overview>Algorithm Overview</h2><p>The following <strong>sequence</strong> describes the steps involved to determine a seed candidate:</p><ol><li>Determine usable seeds with &ldquo;usable&rdquo; defined as follows:<ul><li>no <code>.metadata.deletionTimestamp</code></li><li><code>.spec.settings.scheduling.visible</code> is <code>true</code></li><li>conditions <code>Bootstrapped</code>, <code>GardenletReady</code>, <code>BackupBucketsReady</code> (if available) are <code>true</code></li></ul></li><li>Filter seeds:<ul><li>matching <code>.spec.seedSelector</code> in <code>CloudProfile</code> used by the <code>Shoot</code></li><li>matching <code>.spec.seedSelector</code> in <code>Shoot</code></li><li>having no network intersection with the <code>Shoot</code>&rsquo;s networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint)</li><li>whose taints (<code>.spec.taints</code>) are tolerated by the <code>Shoot</code> (<code>.spec.tolerations</code>)</li><li>whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see <a href=#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring seeds capacity for shoots is not exceeded</a></li><li>which have at least three zones in <code>.spec.provider.zones</code> if shoot requests a high available control plane with failure tolerance type <code>zone</code>.</li></ul></li><li>Apply active <a href=#strategies>strategy</a> e.g., <em>Minimal Distance strategy</em></li><li>Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the <code>.spec.seedName</code> field of the <code>Shoot</code>.</li></ol><h2 id=configuration>Configuration</h2><p>The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag.
<a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>This yaml file</a> holds an example scheduler configuration.</p><p>Most of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, &mldr;).
However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.</p><h2 id=strategies>Strategies</h2><p>The scheduling strategy is defined in the <em><strong>candidateDeterminationStrategy</strong></em> of the scheduler&rsquo;s configuration and can have the possible values <code>SameRegion</code> and <code>MinimalDistance</code>.
The <code>SameRegion</code> strategy is the default strategy.</p><ol><li><p><em>Same Region strategy</em></p><p>The Gardener Scheduler reads the <code>spec.provider.type</code> and <code>.spec.region</code> fields from the <code>Shoot</code> resource.
It tries to find a seed that has the identical <code>.spec.provider.type</code> and <code>.spec.provider.region</code> fields set.
If it cannot find a suitable seed, it adds an event to the shoot stating that it is unschedulable.</p></li><li><p><em>Minimal Distance strategy</em></p><p>The Gardener Scheduler tries to find a valid seed with minimal distance to the shoot&rsquo;s intended region.
The distance is calculated based on the Levenshtein distance of the region. Therefore, the region name
is split into a base name and an orientation. Possible orientations are <code>north</code>, <code>south</code>, <code>east</code>, <code>west</code> and <code>central</code>.
The distance then is twice the Levenshtein distance of the region&rsquo;s base name plus a correction value based on the
orientation and the provider.</p><p>If the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if
either the seed&rsquo;s or the shoot&rsquo;s region does not have an orientation it is 1.
If the provider differs, the correction value is additionally incremented by 2.</p><p>Because of this, a matching region with a matching provider is always prefered.</p></li></ol><p>In order to put the scheduling decision into effect, the scheduler sends an update request for the <code>Shoot</code> resource to
the API server. After validation, the Gardener Aggregated API server updates the shoot to have the <code>spec.seedName</code> field set.
Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.</p><ol start=3><li><em>Special handling based on shoot cluster purpose</em></li></ol><p>Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see <a href=/docs/gardener/usage/shoot_purposes/>Shoot Cluster Purpose</a> for more information).</p><p>In case the shoot has the <code>testing</code> purpose, then the scheduler only reads the <code>.spec.provider.type</code> from the <code>Shoot</code> resource and tries to find a <code>Seed</code> that has the identical <code>.spec.provider.type</code>.
The region does not matter, i.e., <code>testing</code> shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.</p><h2 id=shootsbinding-subresource><code>shoots/binding</code> Subresource</h2><p>The <code>shoots/binding</code> subresource is used to bind a <code>Shoot</code> to a <code>Seed</code>. On creation of a shoot cluster/s, the scheduler updates the binding automatically if an appropriate seed cluster is available.
Only an operator with the necessary RBAC can update this binding manually. This can be done by changing the <code>.spec.seedName</code> of the shoot. However, if a different seed is already assigned to the shoot, this will trigger a control-plane migration. For required steps, please see <a href=/docs/gardener/usage/control_plane_migration/#triggering-the-migration>Triggering the Migration</a>.</p><h2 id=specseedname-field-in-the-shoot-specification><code>spec.seedName</code> Field in the <code>Shoot</code> Specification</h2><p>Similar to the <code>.spec.nodeName</code> field in <code>Pod</code>s, the <code>Shoot</code> specification has an optional <code>.spec.seedName</code> field. If this field is set on creation, the shoot will be scheduled to this seed. However, this field can only be set by users having RBAC for the <code>shoots/binding</code> subresource. If this field is not set, the <code>scheduler</code> will assign a suitable seed automatically and populate this field with the seed name.</p><h2 id=seedselector-field-in-the-shoot-specification><code>seedSelector</code> Field in the <code>Shoot</code> Specification</h2><p>Similar to the <code>.spec.nodeSelector</code> field in <code>Pod</code>s, the <code>Shoot</code> specification has an optional <code>.spec.seedSelector</code> field.
It allows the user to provide a label selector that must match the labels of the <code>Seed</code>s in order to be scheduled to one of them.
The labels on the <code>Seed</code>s are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves.
If provided, the Gardener Scheduler will only consider as &ldquo;suitable&rdquo; those seeds whose labels match those provided in the <code>.spec.seedSelector</code> of the <code>Shoot</code>.</p><p>By default, only seeds with the same provider as the shoot are selected. By adding a <code>providerTypes</code> field to the <code>seedSelector</code>,
a dedicated set of possible providers (<code>*</code> means all provider types) can be selected.</p><h2 id=ensuring-a-seeds-capacity-for-shoots-is-not-exceeded>Ensuring a Seed&rsquo;s Capacity for Shoots Is Not Exceeded</h2><p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable, as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed&rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.</p><p>This mechanism works as follows:</p><ul><li>The <code>gardenlet</code> is configured with certain <em>resources</em> and their total <em>capacity</em> (and, for certain resources, the amount <em>reserved</em> for Gardener), see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>/example/20-componentconfig-gardenlet.yaml</a>. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed.</li><li>The <code>gardenlet</code> seed controller updates the <code>capacity</code> and <code>allocatable</code> fields in the Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The <code>allocatable</code> value of a resource is equal to <code>capacity</code> minus <code>reserved</code>.</li><li>When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.</li></ul><h2 id=failure-to-determine-a-suitable-seed>Failure to Determine a Suitable Seed</h2><p>In case the scheduler fails to find a suitable seed, the operation is being retried with exponential backoff.</p><h2 id=current-limitation--future-plans>Current Limitation / Future Plans</h2><ul><li>Azure unfortunately has a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the <code>MinimalDistance</code> strategy with a more suitable one in the future.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-72b9682105f8bd3513ac668369408e8f>1.3 - Deployment</h1></div><div class=td-content><h1 id=pg-b1cfab34422422de1f43457e8491a94f>1.3.1 - Authentication Gardener Control Plane</h1><h1 id=authentication-of-gardener-control-plane-components-against-the-garden-cluster>Authentication of Gardener Control Plane Components Against the Garden Cluster</h1><blockquote><p><strong>Note:</strong> This document refers to Gardener&rsquo;s API server, admission controller, controller manager and scheduler components. Any reference to the term <strong>Gardener control plane component</strong> can be replaced with any of the mentioned above.</p></blockquote><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of Virtual Garden</a> is used.</p><h2 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster>Virtual Garden is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h2><h3 id=automounted-service-account-token>Automounted Service Account Token</h3><p>The easiest way to deploy a <strong>Gardener control plane component</strong> is to not provide a <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><h3 id=service-account-token-volume-projection>Service Account Token Volume Projection</h3><p>Another solution is to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see the example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.&lt;GardenerControlPlaneComponent>.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.&lt;GardenerControlPlaneComponent>.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h2 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster>Virtual Garden is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h2><h3 id=service-account>Service Account</h3><p>The easiest way to setup the authentication is to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code>, which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.deployment.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h3 id=client-certificate>Client Certificate</h3><p>Another solution is to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.deployment.virtualGarden.enabled: true</code> and <code>.Values.global.deployment.virtualGarden.&lt;GardenerControlPlaneComponent>.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h3 id=projected-service-account-token>Projected Service Account Token</h3><p>This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also, the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then, projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.deployment.virtualGarden.enabled: true</code> and <code>.Values.global.deployment.virtualGarden.&lt;GardenerControlPlaneComponent>.user.name</code>.<blockquote><p><strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></p></blockquote></li><li>Set <code>.Values.global.&lt;GardenerControlPlaneComponent>.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.&lt;GardenerControlPlaneComponent>.serviceAccountTokenVolumeProjection.audience</code>.<blockquote><p><strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</p></blockquote></li><li>Craft a kubeconfig (see the example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ac0acc1100465755b2c4985cf68f7409>1.3.2 - Configuring Logging</h1><h1 id=configuring-the-logging-stack-via-gardenlet-configurations>Configuring the Logging Stack via gardenlet Configurations</h1><h2 id=enable-the-logging>Enable the Logging</h2><p>In order to install the Gardener logging stack, the <code>logging.enabled</code> configuration option has to be enabled in the Gardenlet configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>From now on, each Seed is going to have a logging stack which will collect logs from all pods and some systemd services. Logs related to Shoots with <code>testing</code> purpose are dropped in the <code>fluent-bit</code> output plugin. Shoots with a purpose different than <code>testing</code> have the same type of log aggregator (but different instance) as the Seed. The logs can be viewed in the Grafana in the <code>garden</code> namespace for the Seed components and in the respective shoot control plane namespaces.</p><h2 id=enable-logs-from-the-shoots-node-systemd-services>Enable Logs from the Shoot&rsquo;s Node systemd Services</h2><p>The logs from the systemd services on each node can be retrieved by enabling the <code>logging.shootNodeLogging</code> option in the gardenlet configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  shootNodeLogging:
</span></span><span style=display:flex><span>    shootPurposes:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;evaluation&#34;</span>
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;deployment&#34;</span>
</span></span></code></pre></div><p>Under the <code>shootPurpose</code> section, just list all the shoot purposes for which the Shoot node logging feature will be enabled. Specifying the <code>testing</code> purpose has no effect because this purpose prevents the logging stack installation.
Logs can be viewed in the operator Grafana!
The dedicated labels are <code>unit</code>, <code>syslog_identifier</code>, and <code>nodename</code> in the <code>Explore</code> menu.</p><h2 id=configuring-the-log-processor>Configuring the Log Processor</h2><p>Under <code>logging.fluentBit</code> there are three optional sections:</p><ul><li><code>input</code>: This overwrites the input configuration of the fluent-bit log processor.</li><li><code>output</code>: This overwrites the output configuration of the fluent-bit log processor.</li><li><code>service</code>: This overwrites the service configuration of the fluent-bit log processor.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  fluentBit:
</span></span><span style=display:flex><span>    output: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Output]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span><span style=display:flex><span>    input: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Input]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span><span style=display:flex><span>    service: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span></code></pre></div><h2 id=additional-egress-ipblock-for-allow-fluentbit-networkpolicy>Additional egress IPBlock for allow-fluentbit NetworkPolicy</h2><p>The optional setting under <code>logging.fluentBit.networkPolicy.additionalEgressIPBlocks</code> adds an additional egress IPBlock to <code>allow-fluentbit</code> NetworkPolicy to forward logs to a central system.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  fluentBit:
</span></span><span style=display:flex><span>    additionalEgressIpBlock:
</span></span><span style=display:flex><span>      - 123.123.123.123/32
</span></span></code></pre></div><h2 id=configure-central-logging>Configure Central Logging</h2><p>For central logging, the output configuration of the fluent-bit log processor can be overwritten (<code>logging.fluentBit.output</code>) and the Loki instances deployments in the Garden and Shoot namespace can be enabled/disabled (<code>logging.loki.enabled</code>), by default Loki is enabled.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  fluentBit:
</span></span><span style=display:flex><span>    output: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Output]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span><span style=display:flex><span>  loki:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><h2 id=configuring-central-loki-storage-capacity>Configuring Central Loki Storage Capacity</h2><p>By default, the central Loki has <code>100Gi</code> of storage capacity.
To overwrite the current central Loki storage capacity, the <code>logging.loki.garden.storage</code> setting in the gardenlet&rsquo;s component configuration should be altered.
If you need to increase it, you can do so without losing the current data by specifying a higher capacity. By doing so, the Loki&rsquo;s <code>PersistentVolume</code> capacity will be increased instead of deleting the current PV.
However, if you specify less capacity, then the <code>PersistentVolume</code> will be deleted and with it the logs, too.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  fluentBit:
</span></span><span style=display:flex><span>    output: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Output]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span><span style=display:flex><span>  loki:
</span></span><span style=display:flex><span>    garden:
</span></span><span style=display:flex><span>      storage: <span style=color:#a31515>&#34;200Gi&#34;</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e2f3032b3740e0030af0d52262ec390e>1.3.3 - Deploy Gardenlet</h1><h1 id=deploying-gardenlets>Deploying Gardenlets</h1><p>Gardenlets act as decentral &ldquo;agents&rdquo; to manage the shoot clusters of a seed cluster.</p><p>To support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when the shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.</p><h2 id=procedure>Procedure</h2><ol><li><p>First, an initial gardenlet needs to be deployed:</p><ul><li>Deploy it manually if you have special requirements. For more information, see <a href=/docs/gardener/deployment/deploy_gardenlet_manually/>Deploy a Gardenlet Manually</a>.</li><li>Let the Gardener installer deploy it automatically otherwise. For more information, see <a href=/docs/gardener/deployment/deploy_gardenlet_automatically/>Automatic Deployment of Gardenlets</a>.</li></ul></li><li><p>To add additional seed clusters, it is recommended to use regular shoot clusters. You can do this by creating a <code>ManagedSeed</code> resource with a <code>gardenlet</code> section as described in <a href=/docs/gardener/usage/managed_seed/>Register Shoot as Seed</a>.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-0af204f84f4a80fc61988beb16c39e12>1.3.4 - Deploy Gardenlet Automatically</h1><h1 id=automatic-deployment-of-gardenlets>Automatic Deployment of gardenlets</h1><p>The gardenlet can automatically deploy itself into shoot clusters, and register a cluster as a seed cluster.
These clusters are called &ldquo;managed seeds&rdquo; (aka &ldquo;shooted seeds&rdquo;).
This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.</p><h2 id=prerequisites>Prerequisites</h2><p>The only prerequisite is to register an initial cluster as a seed cluster that already has a gardenlet deployed in one of the following ways:</p><ul><li>The gardenlet was deployed as part of a Gardener installation using a setup tool (for example, <code>gardener/garden-setup</code>).</li><li>The gardenlet was deployed manually (for a step-by-step manual installation guide, see <a href=/docs/gardener/deployment/deploy_gardenlet_manually/>Deploy a Gardenlet Manually</a>).</li></ul><blockquote><p>The initial cluster can be the garden cluster itself.</p></blockquote><h2 id=self-deployment-of-gardenlets-in-additional-managed-seed-clusters>Self-Deployment of gardenlets in Additional Managed Seed Clusters</h2><p>For a better scalability, you usually need more seed clusters that you can create, as follows:</p><ol><li>Use the initial cluster as the seed cluster for other managed seed clusters. It hosts the control planes of the other seed clusters.</li><li>The gardenlet deployed in the initial cluster deploys itself automatically into the managed seed clusters.</li></ol><p>The advantage of this approach is that there’s only one initial gardenlet installation required. Every other managed seed cluster has a gardenlet deployed automatically.</p><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/usage/managed_seed/>Register Shoot as Seed</a></li><li><a href=http://github.com/gardener/garden-setup>garden-setup</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ed3b289c8b619ea2a52bab3ae97c199c>1.3.5 - Deploy Gardenlet Manually</h1><h1 id=deploy-a-gardenlet-manually>Deploy a gardenlet Manually</h1><p>Manually deploying a gardenlet is required in the following cases:</p><ul><li><p>The Kubernetes cluster to be registered as a seed cluster has no public endpoint,
because it is behind a firewall.
The gardenlet must then be deployed into the cluster itself.</p></li><li><p>The Kubernetes cluster to be registered as a seed cluster is managed externally
(the Kubernetes cluster is not a shoot cluster, so <a href=/docs/gardener/deployment/deploy_gardenlet_automatically/>Automatic Deployment of Gardenlets</a> cannot be used).</p></li><li><p>The gardenlet runs outside of the Kubernetes cluster
that should be registered as a seed cluster.
(The gardenlet is not restricted to run in the seed cluster or
to be deployed into a Kubernetes cluster at all).</p></li></ul><blockquote><p>Once you’ve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. For more information, see <a href=/docs/gardener/deployment/deploy_gardenlet_automatically/>Automatic Deployment of Gardenlets</a>.</p></blockquote><h2 id=prerequisites>Prerequisites</h2><h3 id=kubernetes-cluster-that-should-be-registered-as-a-seed-cluster>Kubernetes Cluster that Should Be Registered as a Seed Cluster</h3><ul><li><p>Verify that the cluster has a <a href=/docs/gardener/usage/supported_k8s_versions/>supported Kubernetes version</a>.</p></li><li><p>Determine the nodes, pods, and services CIDR of the cluster.
You need to configure this information in the <code>Seed</code> configuration.
Gardener uses this information to check that the shoot cluster isn’t created with overlapping CIDR ranges.</p></li><li><p>Every seed cluster needs an Ingress controller which distributes external requests to internal components like Grafana and Prometheus.
For this, configure the following lines in your <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>Seed resource</a>:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    provider:
</span></span><span style=display:flex><span>      type: aws-route53
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: ingress-secret
</span></span><span style=display:flex><span>        namespace: garden
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    domain: ingress.my-seed.example.com
</span></span><span style=display:flex><span>    controller:
</span></span><span style=display:flex><span>      kind: nginx
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        &lt;some-optional-provider-specific-config-for-the-ingressController&gt;
</span></span></code></pre></div><h3 id=kubeconfig-for-the-seed-cluster><code>kubeconfig</code> for the Seed Cluster</h3><p>The <code>kubeconfig</code> is required to deploy the gardenlet Helm chart to the seed cluster.
The gardenlet requires certain privileges to be able to operate.
These privileges are described in RBAC resources in the gardenlet Helm chart (see <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/gardenlet/templates>charts/gardener/gardenlet/templates</a>).
The Helm chart contains a service account <code>gardenlet</code>
that the gardenlet deployment uses by default to talk to the Seed API server.</p><blockquote><p>If the gardenlet isn’t deployed in the seed cluster,
the gardenlet can be configured to use a <code>kubeconfig</code>,
which also requires the above-mentioned privileges, from a mounted directory.
The <code>kubeconfig</code> is specified in the <code>seedClientConnection.kubeconfig</code> section
of the <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>Gardenlet configuration</a>.
This configuration option isn’t used in the following,
as this procedure only describes the recommended setup option
where the gardenlet is running in the seed cluster itself.</p></blockquote><h2 id=procedure-overview>Procedure Overview</h2><ol><li><p>Prepare the garden cluster:</p><ol><li><a href=#create-a-bootstrap-token-secret-in-the-kube-system-namespace-of-the-garden-cluster>Create a bootstrap token secret in the <code>kube-system</code> namespace of the garden cluster</a></li><li><a href=#create-rbac-roles-for-the-gardenlet-to-allow-bootstrapping-in-the-garden-cluster>Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster</a></li></ol></li><li><p><a href=#prepare-the-gardenlet-helm-chart>Prepare the gardenlet Helm chart</a>.</p></li><li><p><a href=#automatically-register-shoot-cluster-as-a-seed-cluster>Automatically register shoot cluster as a seed cluster</a>.</p></li><li><p><a href=#deploy-the-gardenlet>Deploy the gardenlet</a></p></li><li><p><a href=#check-that-the-gardenlet-is-successfully-deployed>Check that the gardenlet is successfully deployed</a></p></li></ol><h2 id=create-a-bootstrap-token-secret-in-the-kube-system-namespace-of-the-garden-cluster>Create a Bootstrap Token Secret in the <code>kube-system</code> Namespace of the Garden Cluster</h2><p>The gardenlet needs to talk to the <a href=/docs/gardener/concepts/apiserver/>Gardener API server</a> residing in the garden cluster.</p><p>The gardenlet can be configured with an already existing garden cluster <code>kubeconfig</code> in one of the following ways:</p><ul><li>By specifying <code>gardenClientConnection.kubeconfig</code>
in the <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>Gardenlet configuration</a>.</li><li>By supplying the environment variable <code>GARDEN_KUBECONFIG</code> pointing to
a mounted <code>kubeconfig</code> file).</li></ul><p>The preferred way, however, is to use the gardenlet&rsquo;s ability to request
a signed certificate for the garden cluster by leveraging
<a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/>Kubernetes Certificate Signing Requests</a>.
The gardenlet performs a TLS bootstrapping process that is similar to the
<a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>Kubelet TLS Bootstrapping</a>.
Make sure that the API server of the garden cluster has
<a href=https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#enabling-bootstrap-token-authentication>bootstrap token authentication</a>
enabled.</p><p>The client credentials required for the gardenlet&rsquo;s TLS bootstrapping process
need to be either <code>token</code> or <code>certificate</code> (OIDC isn’t supported) and have permissions
to create a Certificate Signing Request (<a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/>CSR</a>).
It’s recommended to use <a href=https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap tokens</a>
due to their desirable security properties (such as a limited token lifetime).</p><p>Therefore, first create a bootstrap token secret for the garden cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  <span style=color:green># Name MUST be of form &#34;bootstrap-token-&lt;token id&gt;&#34;</span>
</span></span><span style=display:flex><span>  name: bootstrap-token-07401b
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Type MUST be &#39;bootstrap.kubernetes.io/token&#39;</span>
</span></span><span style=display:flex><span>type: bootstrap.kubernetes.io/token
</span></span><span style=display:flex><span>stringData:
</span></span><span style=display:flex><span>  <span style=color:green># Human readable description. Optional.</span>
</span></span><span style=display:flex><span>  description: <span style=color:#a31515>&#34;Token to be used by the gardenlet for Seed `sweet-seed`.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Token ID and secret. Required.</span>
</span></span><span style=display:flex><span>  token-id: 07401b <span style=color:green># 6 characters</span>
</span></span><span style=display:flex><span>  token-secret: f395accd246ae52d <span style=color:green># 16 characters</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Expiration. Optional.</span>
</span></span><span style=display:flex><span>  <span style=color:green># expiration: 2017-03-10T03:22:11Z</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Allowed usages.</span>
</span></span><span style=display:flex><span>  usage-bootstrap-authentication: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>  usage-bootstrap-signing: <span style=color:#a31515>&#34;true&#34;</span>
</span></span></code></pre></div><p>When you later prepare the gardenlet Helm chart,
a <code>kubeconfig</code> based on this token is shared with the gardenlet upon deployment.</p><h2 id=create-rbac-roles-for-the-gardenlet-to-allow-bootstrapping-in-the-garden-cluster>Create RBAC Roles for the gardenlet to Allow Bootstrapping in the Garden Cluster</h2><p>This step is only required if the gardenlet you deploy is the first gardenlet
in the Gardener installation.
Additionally, when using the <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane>control plane chart</a>,
the following resources are already contained in the Helm chart,
that is, if you use it you can skip these steps as the needed RBAC roles already exist.</p><p>The gardenlet uses the configured bootstrap <code>kubeconfig</code> in <code>gardenClientConnection.bootstrapKubeconfig</code> to request a signed certificate for the user <code>gardener.cloud:system:seed:&lt;seed-name></code> in the group <code>gardener.cloud:system:seeds</code>.</p><p>Create a <code>ClusterRole</code> and <code>ClusterRoleBinding</code> that grant full admin permissions to authenticated gardenlets.</p><p>Create the following resources in the garden cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRole
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>rules:
</span></span><span style=display:flex><span>  - apiGroups:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>    verbs:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>  - kind: Group
</span></span><span style=display:flex><span>    name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>    apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRole
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seed-bootstrapper
</span></span><span style=display:flex><span>rules:
</span></span><span style=display:flex><span>  - apiGroups:
</span></span><span style=display:flex><span>      - certificates.k8s.io
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      - certificatesigningrequests
</span></span><span style=display:flex><span>    verbs:
</span></span><span style=display:flex><span>      - create
</span></span><span style=display:flex><span>      - get
</span></span><span style=display:flex><span>  - apiGroups:
</span></span><span style=display:flex><span>      - certificates.k8s.io
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      - certificatesigningrequests/seedclient
</span></span><span style=display:flex><span>    verbs:
</span></span><span style=display:flex><span>      - create
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:green># A kubelet/gardenlet authenticating using bootstrap tokens is authenticated as a user in the group system:bootstrappers</span>
</span></span><span style=display:flex><span><span style=color:green># Allows the Gardenlet to create a CSR</span>
</span></span><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seed-bootstrapper
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: gardener.cloud:system:seed-bootstrapper
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>  - kind: Group
</span></span><span style=display:flex><span>    name: system:bootstrappers
</span></span><span style=display:flex><span>    apiGroup: rbac.authorization.k8s.io
</span></span></code></pre></div><p>ℹ️ After bootstrapping, the gardenlet has full administrative access to the garden cluster.
You might be interested to harden this and limit its permissions to only resources related to the seed cluster it is responsible for.
Please take a look at <a href=/docs/gardener/deployment/gardenlet_api_access/>Scoped API Access for Gardenlets</a>.</p><h2 id=prepare-the-gardenlet-helm-chart>Prepare the gardenlet Helm Chart</h2><p>This section only describes the minimal configuration,
using the global configuration values of the gardenlet Helm chart.
For an overview over all values, see the <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/gardenlet/values.yaml>configuration values</a>.
We refer to the global configuration values as <em>gardenlet configuration</em> in the following procedure.</p><ol><li><p>Create a gardenlet configuration <code>gardenlet-values.yaml</code> based on <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/gardenlet/values.yaml>this template</a>.</p></li><li><p>Create a bootstrap <code>kubeconfig</code> based on the bootstrap token created in the garden cluster.</p><p>Replace the <code>&lt;bootstrap-token></code> with <code>token-id.token-secret</code> (from our previous example: <code>07401b.f395accd246ae52d</code>) from the bootstrap token secret.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>current-context: gardenlet-bootstrap@default
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;ca-of-garden-cluster&gt;
</span></span><span style=display:flex><span>    server: https://&lt;endpoint-of-garden-cluster&gt;
</span></span><span style=display:flex><span>  name: default
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: default
</span></span><span style=display:flex><span>    user: gardenlet-bootstrap
</span></span><span style=display:flex><span>  name: gardenlet-bootstrap@default
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: gardenlet-bootstrap
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    token: &lt;bootstrap-token&gt;
</span></span></code></pre></div></li><li><p>In the <code>gardenClientConnection.bootstrapKubeconfig</code> section of your gardenlet configuration, provide the bootstrap <code>kubeconfig</code> together with a name and namespace to the gardenlet Helm chart.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardenClientConnection:
</span></span><span style=display:flex><span>  bootstrapKubeconfig:
</span></span><span style=display:flex><span>    name: gardenlet-kubeconfig-bootstrap
</span></span><span style=display:flex><span>    namespace: garden
</span></span><span style=display:flex><span>    kubeconfig: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      &lt;bootstrap-kubeconfig&gt;  <span style=color:green># will be base64 encoded by helm</span>
</span></span></code></pre></div><p>The bootstrap <code>kubeconfig</code> is stored in the specified secret.</p></li><li><p>In the <code>gardenClientConnection.kubeconfigSecret</code> section of your gardenlet configuration,
define a name and a namespace where the gardenlet stores
the real <code>kubeconfig</code> that it creates during the bootstrap process. If the secret doesn&rsquo;t exist,
the gardenlet creates it for you.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardenClientConnection:
</span></span><span style=display:flex><span>  kubeconfigSecret:
</span></span><span style=display:flex><span>    name: gardenlet-kubeconfig
</span></span><span style=display:flex><span>    namespace: garden
</span></span></code></pre></div></li></ol><h3 id=updating-the-garden-cluster-ca>Updating the Garden Cluster CA</h3><p>The kubeconfig created by the gardenlet in step 4 will not be recreated as long as it exists, even if a new bootstrap kubeconfig is provided. To enable rotation of the garden cluster CA certificate, a new bundle can be provided via the <code>gardenClientConnection.gardenClusterCACert</code> field. If the provided bundle differs from the one currently in the gardenlet&rsquo;s kubeconfig secret then it will be updated. To remove the CA completely (e.g. when switching to a publicly trusted endpoint), this field can be set to either <code>none</code> or <code>null</code>.</p><h2 id=automatically-register-a-shoot-cluster-as-a-seed-cluster>Automatically Register a Shoot Cluster as a Seed Cluster</h2><p>A seed cluster can either be registered by manually creating
the <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>Seed</code> resource</a>
or automatically by the gardenlet.
This functionality is useful for managed seed clusters,
as the gardenlet in the garden cluster deploys a copy of itself
into the cluster with automatic registration of the <code>Seed</code> configured.
However, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.</p><blockquote><p>This procedure doesn’t describe all the possible configurations
for the <code>Seed</code> resource. For more information, see:</p><ul><li><a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>Example Seed resource</a></li><li><a href=/docs/gardener/usage/seed_settings/>Configurable Seed settings</a></li></ul></blockquote><h3 id=adjust-the-gardenlet-component-configuration>Adjust the gardenlet Component Configuration</h3><ol><li><p>Supply the <code>Seed</code> resource in the <code>seedConfig</code> section of your gardenlet configuration <code>gardenlet-values.yaml</code>.</p></li><li><p>Add the <code>seedConfig</code> to your gardenlet configuration <code>gardenlet-values.yaml</code>.
The field <code>seedConfig.spec.provider.type</code> specifies the infrastructure provider type (for example, <code>aws</code>) of the seed cluster.
For all supported infrastructure providers, see <a href=https://github.com/gardener/gardener/blob/master/extensions/README.md#known-extension-implementations>Known Extension Implementations</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>....
</span></span><span style=display:flex><span>seedConfig:
</span></span><span style=display:flex><span>  metadata:
</span></span><span style=display:flex><span>    name: sweet-seed
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    dns:
</span></span><span style=display:flex><span>      provider:
</span></span><span style=display:flex><span>        type: &lt;provider&gt;
</span></span><span style=display:flex><span>        secretRef:
</span></span><span style=display:flex><span>          name: ingress-secret
</span></span><span style=display:flex><span>          namespace: garden
</span></span><span style=display:flex><span>    ingress: <span style=color:green># see prerequisites</span>
</span></span><span style=display:flex><span>      domain: ingress.dev.my-seed.example.com
</span></span><span style=display:flex><span>      controller:
</span></span><span style=display:flex><span>        kind: nginx
</span></span><span style=display:flex><span>    networks: <span style=color:green># see prerequisites</span>
</span></span><span style=display:flex><span>      nodes: 10.240.0.0/16
</span></span><span style=display:flex><span>      pods: 100.244.0.0/16
</span></span><span style=display:flex><span>      services: 100.32.0.0/13
</span></span><span style=display:flex><span>      shootDefaults: # optional: non-overlapping default CIDRs for shoot clusters of that Seed
</span></span><span style=display:flex><span>        pods: 100.96.0.0/11
</span></span><span style=display:flex><span>        services: 100.64.0.0/13
</span></span><span style=display:flex><span>    provider:
</span></span><span style=display:flex><span>      region: eu-west-1
</span></span><span style=display:flex><span>      type: &lt;provider&gt;
</span></span></code></pre></div></li></ol><h3 id=optional-enable-ha-mode>Optional: Enable HA Mode</h3><p>You may consider running <code>gardenlet</code> with multiple replicas, especially if the seed cluster is configured to host <a href=/docs/gardener/usage/shoot_high_availability/>HA shoot control planes</a>.
Therefore, the following Helm chart values define the degree of high availability you want to achieve for the <code>gardenlet</code> deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>replicaCount: 2 <span style=color:green># or more if a higher failure tolerance is required.</span>
</span></span><span style=display:flex><span>failureToleranceType: zone <span style=color:green># One of `zone` or `node` - defines how replicas are spread.</span>
</span></span></code></pre></div><h3 id=optional-enable-backup-and-restore>Optional: Enable Backup and Restore</h3><p>The seed cluster can be set up with backup and restore
for the main <code>etcds</code> of shoot clusters.</p><p>Gardener uses <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a>
that <a href=https://github.com/gardener/etcd-backup-restore/blob/master/doc/usage/getting_started.md#usage>integrates with different storage providers</a>
to store the shoot cluster&rsquo;s main <code>etcd</code> backups.
Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.</p><p>Create a secret in the garden cluster with client credentials for the storage provider.
The format of the secret is cloud provider specific and can be found
in the repository of the respective Gardener extension.
For example, the secret for AWS S3 can be found in the AWS provider extension
(<a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-etcd-backup-secret.yaml>30-etcd-backup-secret.yaml</a>).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: sweet-seed-backup
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># client credentials format is provider specific</span>
</span></span></code></pre></div><p>Configure the <code>Seed</code> resource in the <code>seedConfig</code> section of your gardenlet configuration to use backup and restore:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>seedConfig:
</span></span><span style=display:flex><span>  metadata:
</span></span><span style=display:flex><span>    name: sweet-seed
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    backup:
</span></span><span style=display:flex><span>      provider: &lt;provider&gt;
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: sweet-seed-backup
</span></span><span style=display:flex><span>        namespace: garden
</span></span></code></pre></div><h2 id=deploy-the-gardenlet>Deploy the gardenlet</h2><blockquote><p>The gardenlet doesn’t have to run in the same Kubernetes cluster as the seed cluster
it’s registering and reconciling, but it is in most cases advantageous
to use in-cluster communication to talk to the Seed API server.
Running a gardenlet outside of the cluster is mostly used for local development.</p></blockquote><p>The <code>gardenlet-values.yaml</code> looks something like this
(with automatic Seed registration and backup for shoot clusters enabled):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>&lt;default config&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>config:
</span></span><span style=display:flex><span>  gardenClientConnection:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    bootstrapKubeconfig:
</span></span><span style=display:flex><span>      name: gardenlet-bootstrap-kubeconfig
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>      kubeconfig: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>        apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>        clusters:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - cluster:
</span></span></span><span style=display:flex><span><span style=color:#a31515>            certificate-authority-data: &lt;dummy&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>            server: &lt;my-garden-cluster-endpoint&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>          name: my-kubernetes-cluster
</span></span></span><span style=display:flex><span><span style=color:#a31515>        ....</span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    kubeconfigSecret:
</span></span><span style=display:flex><span>      name: gardenlet-kubeconfig
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  &lt;default config&gt;
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  seedConfig:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      name: sweet-seed
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      dns:
</span></span><span style=display:flex><span>        provider:
</span></span><span style=display:flex><span>          type: &lt;provider&gt;
</span></span><span style=display:flex><span>          secretRef:
</span></span><span style=display:flex><span>            name: ingress-secret
</span></span><span style=display:flex><span>            namespace: garden
</span></span><span style=display:flex><span>      ingress: <span style=color:green># see prerequisites</span>
</span></span><span style=display:flex><span>        domain: ingress.dev.my-seed.example.com
</span></span><span style=display:flex><span>        controller:
</span></span><span style=display:flex><span>          kind: nginx
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        nodes: 10.240.0.0/16
</span></span><span style=display:flex><span>        pods: 100.244.0.0/16
</span></span><span style=display:flex><span>        services: 100.32.0.0/13
</span></span><span style=display:flex><span>        shootDefaults:
</span></span><span style=display:flex><span>          pods: 100.96.0.0/11
</span></span><span style=display:flex><span>          services: 100.64.0.0/13
</span></span><span style=display:flex><span>      provider:
</span></span><span style=display:flex><span>        region: eu-west-1
</span></span><span style=display:flex><span>        type: &lt;provider&gt;
</span></span><span style=display:flex><span>      backup:
</span></span><span style=display:flex><span>        provider: &lt;provider&gt;
</span></span><span style=display:flex><span>        secretRef:
</span></span><span style=display:flex><span>          name: sweet-seed-backup
</span></span><span style=display:flex><span>          namespace: garden
</span></span></code></pre></div><p>Deploy the gardenlet Helm chart to the Kubernetes cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm install gardenlet charts/gardener/gardenlet <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --namespace garden <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  -f gardenlet-values.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --wait
</span></span></code></pre></div><p>This helm chart creates:</p><ul><li>A service account <code>gardenlet</code> that the gardenlet can use to talk to the Seed API server.</li><li>RBAC roles for the service account (full admin rights at the moment).</li><li>The secret (<code>garden</code>/<code>gardenlet-bootstrap-kubeconfig</code>) containing the bootstrap <code>kubeconfig</code>.</li><li>The gardenlet deployment in the <code>garden</code> namespace.</li></ul><h2 id=check-that-the-gardenlet-is-successfully-deployed>Check that the gardenlet Is Successfully Deployed</h2><ol><li><p>Check that the gardenlets certificate bootstrap was successful.</p><p>Check if the secret <code>gardenlet-kubeconfig</code> in the namespace <code>garden</code> in the seed cluster
is created and contains a <code>kubeconfig</code> with a valid certificate.</p><ol><li><p>Get the <code>kubeconfig</code> from the created secret.</p><pre tabindex=0><code>$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d
</code></pre></li><li><p>Test against the garden cluster and verify it’s working.</p></li><li><p>Extract the <code>client-certificate-data</code> from the user <code>gardenlet</code>.</p></li><li><p>View the certificate:</p><pre tabindex=0><code>$ openssl x509 -in ./gardenlet-cert -noout -text
</code></pre><p>Check that the certificate is valid for a year (that is the lifetime of new certificates).</p></li></ol></li><li><p>Check that the bootstrap secret <code>gardenlet-bootstrap-kubeconfig</code> has been deleted from the seed cluster in namespace <code>garden</code>.</p></li><li><p>Check that the seed cluster is registered and <code>READY</code> in the garden cluster.</p><p>Check that the seed cluster <code>sweet-seed</code> exists and all conditions indicate that it’s available.
If so, the <a href=/docs/gardener/concepts/gardenlet/#heartbeats>Gardenlet is sending regular heartbeats</a> and the <a href=/docs/gardener/usage/seed_bootstrapping/>seed bootstrapping</a> was successful.</p><p>Check that the conditions on the <code>Seed</code> resource look similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get seed sweet-seed -o json | jq .status.conditions
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastTransitionTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:17:29Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastUpdateTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:17:29Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;message&#34;</span>: <span style=color:#a31515>&#34;Gardenlet is posting ready status.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;reason&#34;</span>: <span style=color:#a31515>&#34;GardenletReady&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;status&#34;</span>: <span style=color:#a31515>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;type&#34;</span>: <span style=color:#a31515>&#34;GardenletReady&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastTransitionTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:17:49Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastUpdateTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:53:17Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;message&#34;</span>: <span style=color:#a31515>&#34;Seed cluster has been bootstrapped successfully.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;reason&#34;</span>: <span style=color:#a31515>&#34;BootstrappingSucceeded&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;status&#34;</span>: <span style=color:#a31515>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;type&#34;</span>: <span style=color:#a31515>&#34;Bootstrapped&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastTransitionTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:17:49Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;lastUpdateTime&#34;</span>: <span style=color:#a31515>&#34;2020-07-17T09:53:17Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;message&#34;</span>: <span style=color:#a31515>&#34;Backup Buckets are available.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;reason&#34;</span>: <span style=color:#a31515>&#34;BackupBucketsAvailable&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;status&#34;</span>: <span style=color:#a31515>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;type&#34;</span>: <span style=color:#a31515>&#34;BackupBucketsReady&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div></li></ol><h2 id=related-links>Related Links</h2><ul><li><a href=https://github.com/gardener/gardener/issues/1724>Issue #1724: Harden Gardenlet RBAC privileges</a>.</li><li><a href=/docs/gardener/concepts/backup-restore/>Backup and Restore</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3272d62c1687598614ee78517d7640c>1.3.6 - Feature Gates</h1><h1 id=feature-gates-in-gardener>Feature Gates in Gardener</h1><p>This page contains an overview of the various feature gates an administrator can specify on different Gardener components.</p><h2 id=overview>Overview</h2><p>Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the component configuration file for a specific component.</p><p>Each Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example, this is the configuration of the <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>gardenlet</a> component.</p><p>The following tables are a summary of the feature gates that you can set on different Gardener components.</p><ul><li>The “Since” column contains the Gardener release when a feature is introduced or its release stage is changed.</li><li>The “Until” column, if not empty, contains the last Gardener release in which you can still use a feature gate.</li><li>If a feature is in the <em>Alpha</em> or <em>Beta</em> state, you can find the feature listed in the Alpha/Beta feature gate table.</li><li>If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table.</li><li>The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.</li></ul><h2 id=feature-gates-for-alpha-or-beta-features>Feature Gates for Alpha or Beta Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody><tr><td>HVPA</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.31</code></td><td></td></tr><tr><td>HVPAForShootedSeed</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.32</code></td><td></td></tr><tr><td>ManagedIstio</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.5</code></td><td><code>1.18</code></td></tr><tr><td>ManagedIstio</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.19</code></td><td></td></tr><tr><td>ManagedIstio (deprecated)</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.48</code></td><td></td></tr><tr><td>APIServerSNI</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.7</code></td><td><code>1.18</code></td></tr><tr><td>APIServerSNI</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.19</code></td><td></td></tr><tr><td>APIServerSNI (deprecated)</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.48</code></td><td></td></tr><tr><td>SeedChange</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.12</code></td><td><code>1.52</code></td></tr><tr><td>SeedChange</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.53</code></td><td></td></tr><tr><td>CopyEtcdBackupsDuringControlPlaneMigration</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.37</code></td><td><code>1.52</code></td></tr><tr><td>CopyEtcdBackupsDuringControlPlaneMigration</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.53</code></td><td></td></tr><tr><td>HAControlPlanes</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.49</code></td><td></td></tr><tr><td>DefaultSeccompProfile</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.54</code></td><td></td></tr><tr><td>CoreDNSQueryRewriting</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.55</code></td><td></td></tr><tr><td>IPv6SingleStack</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.63</code></td><td></td></tr><tr><td>MutableShootSpecNetworkingNodes</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.64</code></td><td></td></tr></tbody></table><h2 id=feature-gates-for-graduated-or-deprecated-features>Feature Gates for Graduated or Deprecated Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody><tr><td>NodeLocalDNS</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.7</code></td><td></td></tr><tr><td>NodeLocalDNS</td><td></td><td><code>Removed</code></td><td><code>1.26</code></td><td></td></tr><tr><td>KonnectivityTunnel</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.6</code></td><td></td></tr><tr><td>KonnectivityTunnel</td><td></td><td><code>Removed</code></td><td><code>1.27</code></td><td></td></tr><tr><td>MountHostCADirectories</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.11</code></td><td><code>1.25</code></td></tr><tr><td>MountHostCADirectories</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.26</code></td><td><code>1.27</code></td></tr><tr><td>MountHostCADirectories</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.27</code></td><td></td></tr><tr><td>MountHostCADirectories</td><td></td><td><code>Removed</code></td><td><code>1.30</code></td><td></td></tr><tr><td>DisallowKubeconfigRotationForShootInDeletion</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.28</code></td><td><code>1.31</code></td></tr><tr><td>DisallowKubeconfigRotationForShootInDeletion</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.32</code></td><td><code>1.35</code></td></tr><tr><td>DisallowKubeconfigRotationForShootInDeletion</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.36</code></td><td></td></tr><tr><td>DisallowKubeconfigRotationForShootInDeletion</td><td></td><td><code>Removed</code></td><td><code>1.38</code></td><td></td></tr><tr><td>Logging</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.13</code></td><td><code>1.40</code></td></tr><tr><td>Logging</td><td></td><td><code>Removed</code></td><td><code>1.41</code></td><td></td></tr><tr><td>AdminKubeconfigRequest</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.24</code></td><td><code>1.38</code></td></tr><tr><td>AdminKubeconfigRequest</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.39</code></td><td><code>1.41</code></td></tr><tr><td>AdminKubeconfigRequest</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.42</code></td><td><code>1.49</code></td></tr><tr><td>AdminKubeconfigRequest</td><td></td><td><code>Removed</code></td><td><code>1.50</code></td><td></td></tr><tr><td>UseDNSRecords</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.27</code></td><td><code>1.38</code></td></tr><tr><td>UseDNSRecords</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.39</code></td><td><code>1.43</code></td></tr><tr><td>UseDNSRecords</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.44</code></td><td><code>1.49</code></td></tr><tr><td>UseDNSRecords</td><td></td><td><code>Removed</code></td><td><code>1.50</code></td><td></td></tr><tr><td>CachedRuntimeClients</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.7</code></td><td><code>1.33</code></td></tr><tr><td>CachedRuntimeClients</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.34</code></td><td><code>1.44</code></td></tr><tr><td>CachedRuntimeClients</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.45</code></td><td><code>1.49</code></td></tr><tr><td>CachedRuntimeClients</td><td></td><td><code>Removed</code></td><td><code>1.50</code></td><td></td></tr><tr><td>DenyInvalidExtensionResources</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.31</code></td><td><code>1.41</code></td></tr><tr><td>DenyInvalidExtensionResources</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.42</code></td><td><code>1.44</code></td></tr><tr><td>DenyInvalidExtensionResources</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.45</code></td><td><code>1.49</code></td></tr><tr><td>DenyInvalidExtensionResources</td><td></td><td><code>Removed</code></td><td><code>1.50</code></td><td></td></tr><tr><td>RotateSSHKeypairOnMaintenance</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.28</code></td><td><code>1.44</code></td></tr><tr><td>RotateSSHKeypairOnMaintenance</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.45</code></td><td><code>1.47</code></td></tr><tr><td>RotateSSHKeypairOnMaintenance (deprecated)</td><td><code>false</code></td><td><code>Beta</code></td><td><code>1.48</code></td><td><code>1.50</code></td></tr><tr><td>RotateSSHKeypairOnMaintenance (deprecated)</td><td></td><td><code>Removed</code></td><td><code>1.51</code></td><td></td></tr><tr><td>ShootMaxTokenExpirationOverwrite</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.43</code></td><td><code>1.44</code></td></tr><tr><td>ShootMaxTokenExpirationOverwrite</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.45</code></td><td><code>1.47</code></td></tr><tr><td>ShootMaxTokenExpirationOverwrite</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.48</code></td><td><code>1.50</code></td></tr><tr><td>ShootMaxTokenExpirationOverwrite</td><td></td><td><code>Removed</code></td><td><code>1.51</code></td><td></td></tr><tr><td>ShootMaxTokenExpirationValidation</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.43</code></td><td><code>1.45</code></td></tr><tr><td>ShootMaxTokenExpirationValidation</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.46</code></td><td><code>1.47</code></td></tr><tr><td>ShootMaxTokenExpirationValidation</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.48</code></td><td><code>1.50</code></td></tr><tr><td>ShootMaxTokenExpirationValidation</td><td></td><td><code>Removed</code></td><td><code>1.51</code></td><td></td></tr><tr><td>WorkerPoolKubernetesVersion</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.35</code></td><td><code>1.45</code></td></tr><tr><td>WorkerPoolKubernetesVersion</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.46</code></td><td><code>1.49</code></td></tr><tr><td>WorkerPoolKubernetesVersion</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.50</code></td><td><code>1.51</code></td></tr><tr><td>WorkerPoolKubernetesVersion</td><td></td><td><code>Removed</code></td><td><code>1.52</code></td><td></td></tr><tr><td>DisableDNSProviderManagement</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.41</code></td><td><code>1.49</code></td></tr><tr><td>DisableDNSProviderManagement</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.50</code></td><td><code>1.51</code></td></tr><tr><td>DisableDNSProviderManagement</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.52</code></td><td><code>1.59</code></td></tr><tr><td>DisableDNSProviderManagement</td><td></td><td><code>Removed</code></td><td><code>1.60</code></td><td></td></tr><tr><td>SecretBindingProviderValidation</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.38</code></td><td><code>1.50</code></td></tr><tr><td>SecretBindingProviderValidation</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.51</code></td><td><code>1.52</code></td></tr><tr><td>SecretBindingProviderValidation</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.53</code></td><td><code>1.54</code></td></tr><tr><td>SecretBindingProviderValidation</td><td></td><td><code>Removed</code></td><td><code>1.55</code></td><td></td></tr><tr><td>SeedKubeScheduler</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.15</code></td><td><code>1.54</code></td></tr><tr><td>SeedKubeScheduler</td><td><code>false</code></td><td><code>Deprecated</code></td><td><code>1.55</code></td><td><code>1.60</code></td></tr><tr><td>SeedKubeScheduler</td><td></td><td><code>Removed</code></td><td><code>1.61</code></td><td></td></tr><tr><td>ShootCARotation</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.42</code></td><td><code>1.50</code></td></tr><tr><td>ShootCARotation</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.51</code></td><td><code>1.56</code></td></tr><tr><td>ShootCARotation</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.57</code></td><td><code>1.59</code></td></tr><tr><td>ShootCARotation</td><td></td><td><code>Removed</code></td><td><code>1.60</code></td><td></td></tr><tr><td>ShootSARotation</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.48</code></td><td><code>1.50</code></td></tr><tr><td>ShootSARotation</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.51</code></td><td><code>1.56</code></td></tr><tr><td>ShootSARotation</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.57</code></td><td><code>1.59</code></td></tr><tr><td>ShootSARotation</td><td></td><td><code>Removed</code></td><td><code>1.60</code></td><td></td></tr><tr><td>ReversedVPN</td><td><code>false</code></td><td><code>Alpha</code></td><td><code>1.22</code></td><td><code>1.41</code></td></tr><tr><td>ReversedVPN</td><td><code>true</code></td><td><code>Beta</code></td><td><code>1.42</code></td><td><code>1.62</code></td></tr><tr><td>ReversedVPN</td><td><code>true</code></td><td><code>GA</code></td><td><code>1.63</code></td><td></td></tr><tr><td>ForceRestore</td><td><code>false</code></td><td><code>Removed</code></td><td><code>1.66</code></td><td></td></tr></tbody></table><h2 id=using-a-feature>Using a Feature</h2><p>A feature can be in <em>Alpha</em>, <em>Beta</em> or <em>GA</em> stage.
An <em>Alpha</em> feature means:</p><ul><li>Disabled by default.</li><li>Might be buggy. Enabling the feature may expose bugs.</li><li>Support for feature may be dropped at any time without notice.</li><li>The API may change in incompatible ways in a later software release without notice.</li><li>Recommended for use only in short-lived testing clusters, due to increased
risk of bugs and lack of long-term support.</li></ul><p>A <em>Beta</em> feature means:</p><ul><li>Enabled by default.</li><li>The feature is well tested. Enabling the feature is considered safe.</li><li>Support for the overall feature will not be dropped, though details may change.</li><li>The schema and/or semantics of objects may change in incompatible ways in a
subsequent beta or stable release. When this happens, we will provide instructions
for migrating to the next version. This may require deleting, editing, and
re-creating API objects. The editing process may require some thought.
This may require downtime for applications that rely on the feature.</li><li>Recommended for only non-critical uses because of potential for
incompatible changes in subsequent releases.</li></ul><blockquote><p>Please do try <em>Beta</em> features and give feedback on them!
After they exit beta, it may not be practical for us to make more changes.</p></blockquote><p>A <em>General Availability</em> (GA) feature is also referred to as a <em>stable</em> feature. It means:</p><ul><li>The feature is always enabled; you cannot disable it.</li><li>The corresponding feature gate is no longer needed.</li><li>Stable versions of features will appear in released software for many subsequent versions.</li></ul><h2 id=list-of-feature-gates>List of Feature Gates</h2><table><thead><tr><th>Feature</th><th>Relevant Components</th><th>Description</th></tr></thead><tbody><tr><td>HVPA</td><td><code>gardenlet</code>, <code>gardener-operator</code></td><td>Enables simultaneous horizontal and vertical scaling in garden or seed clusters.</td></tr><tr><td>HVPAForShootedSeed</td><td><code>gardenlet</code></td><td>Enables simultaneous horizontal and vertical scaling in managed seed (aka &ldquo;shooted seed&rdquo;) clusters.</td></tr><tr><td>ManagedIstio (deprecated)</td><td><code>gardenlet</code></td><td>Enables a Gardener-tailored <a href=https://istio.io>Istio</a> in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the <a href=/docs/gardener/usage/istio/>detailed documentation</a> for more information.</td></tr><tr><td>APIServerSNI (deprecated)</td><td><code>gardenlet</code></td><td>Enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when <code>ManagedIstio</code> is enabled or Istio is manually deployed in the Seed cluster. See <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>GEP-8</a> for more details.</td></tr><tr><td>SeedChange</td><td><code>gardener-apiserver</code></td><td>Enables updating the <code>spec.seedName</code> field during shoot validation from a non-empty value in order to trigger shoot control plane migration.</td></tr><tr><td>ReversedVPN</td><td><code>gardenlet</code></td><td>Reverses the connection setup of the VPN tunnel between the Seed and the Shoot cluster(s). It allows Seed and Shoot clusters to be in different networks with only direct access in one direction (Shoot -> Seed). In addition to that, it reduces the amount of load balancers required, i.e. no load balancers are required for the VPN tunnel anymore. It requires <code>APIServerSNI</code> and kubernetes version <code>1.18</code> or higher to work. Details can be found in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>GEP-14</a>.</td></tr><tr><td>CopyEtcdBackupsDuringControlPlaneMigration</td><td><code>gardenlet</code></td><td>Enables the copy of etcd backups from the object store of the source seed to the object store of the destination seed during control plane migration.</td></tr><tr><td>SecretBindingProviderValidation</td><td><code>gardener-apiserver</code></td><td>Enables validations on Gardener API server that:<br>- requires the provider type of a SecretBinding to be set (on SecretBinding creation)<br>- requires the SecretBinding provider type to match the Shoot provider type (on Shoot creation)<br>- enforces immutability on the provider type of a SecretBinding</td></tr><tr><td>HAControlPlanes</td><td><code>gardener-apiserver</code></td><td>HAControlPlanes allows shoot control planes to be run in high availability mode.</td></tr><tr><td>DefaultSeccompProfile</td><td><code>gardenlet</code>, <code>gardener-operator</code></td><td>Enables the defaulting of the seccomp profile for Gardener managed workload in the garden or seed to <code>RuntimeDefault</code>.</td></tr><tr><td>CoreDNSQueryRewriting</td><td><code>gardenlet</code></td><td>Enables automatic DNS query rewriting in shoot cluster&rsquo;s CoreDNS to shortcut name resolution of fully qualified in-cluster and out-of-cluster names, which follow a user-defined pattern. Details can be found in <a href=/docs/gardener/usage/dns-search-path-optimization/>DNS Search Path Optimization</a>.</td></tr><tr><td>IPv6SingleStack</td><td><code>gardener-apiserver</code></td><td>Allows creating shoot clusters with <a href=/docs/gardener/usage/ipv6/>IPv6 single-stack networking</a> (<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/21-ipv6-singlestack-local.md>GEP-21</a>).</td></tr><tr><td>MutableShootSpecNetworkingNodes</td><td><code>gardener-apiserver</code></td><td>Allows updating the field <code>spec.networking.nodes</code>. The validity of the values has to be checked in the provider extensions. Only enable this feature gate when your system runs provider extensions which have implemented the validation.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-9cfd104830b4bc8eafa2e90c3d9ff542>1.3.7 - gardenlet API Access</h1><h1 id=scoped-api-access-for-gardenlets>Scoped API Access for gardenlets</h1><p>By default, <code>gardenlet</code>s have administrative access in the garden cluster.
They are able to execute any API request on any object independent of whether the object is related to the seed cluster the <code>gardenlet</code> is responsible for.
As RBAC is not powerful enough for fine-grained checks and for the sake of security, Gardener provides two optional but recommended configurations for your environments that scope the API access for <code>gardenlet</code>s.</p><p>Similar to the <a href=https://kubernetes.io/docs/reference/access-authn-authz/node/><code>Node</code> authorization mode in Kubernetes</a>, Gardener features a <code>SeedAuthorizer</code> plugin.
It is a special-purpose authorization plugin that specifically authorizes API requests made by the <code>gardenlet</code>s.</p><p>Likewise, similar to the <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> admission plugin in Kubernetes</a>, Gardener features a <code>SeedRestriction</code> plugin.
It is a special-purpose admission plugin that specifically limits the Kubernetes objects <code>gardenlet</code>s can modify.</p><p>📚 You might be interested to look into the <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/kubelet-authorizer.md>design proposal for scoped Kubelet API access</a> from the Kubernetes community.
It can be translated to Gardener and Gardenlets with their <code>Seed</code> and <code>Shoot</code> resources.</p><h2 id=flow-diagram>Flow Diagram</h2><p>The following diagram shows how the two plugins are included in the request flow of a <code>gardenlet</code>.
When they are not enabled, then the <code>kube-apiserver</code> is internally authorizing the request via RBAC before forwarding the request directly to the <code>gardener-apiserver</code>, i.e., the <code>gardener-admission-controller</code> would not be consulted (this is not entirely correct because it also serves other admission webhook handlers, but for simplicity reasons this document focuses on the API access scope only).</p><p>When enabling the plugins, there is one additional step for each before the <code>gardener-apiserver</code> responds to the request.</p><p><img src=/__resources/gardenlet_api_access_flow_6dfa06.png alt="Flow Diagram"></p><p>Please note that the example shows a request to an object (<code>Shoot</code>) residing in one of the API groups served by <code>gardener-apiserver</code>.
However, the <code>gardenlet</code> is also interacting with objects in API groups served by the <code>kube-apiserver</code> (e.g., <code>Secret</code>,<code>ConfigMap</code>).
In this case, the consultation of the <code>SeedRestriction</code> admission plugin is performed by the <code>kube-apiserver</code> itself before it forwards the request to the <code>gardener-apiserver</code>.</p><p>Today, the following rules are implemented:</p><table><thead><tr><th>Resource</th><th>Verbs</th><th>Path(s)</th><th>Description</th></tr></thead><tbody><tr><td><code>BackupBucket</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code></td><td><code>BackupBucket</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>BackupBucket</code>s. Allow only <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> requests for <code>BackupBucket</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>BackupEntry</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code></td><td><code>BackupEntry</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>BackupEntry</code>s. Allow only <code>create</code>, <code>update</code>, <code>patch</code> requests for <code>BackupEntry</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code> and referencing <code>BackupBucket</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>Bastion</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code></td><td><code>Bastion</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>Bastion</code>s. Allow only <code>create</code>, <code>update</code>, <code>patch</code> requests for <code>Bastion</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>CertificateSigningRequest</code></td><td><code>get</code>, <code>create</code></td><td><code>CertificateSigningRequest</code> -> <code>Seed</code></td><td>Allow only <code>get</code>, <code>create</code> requests for <code>CertificateSigningRequest</code>s related to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>CloudProfile</code></td><td><code>get</code></td><td><code>CloudProfile</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow only <code>get</code> requests for <code>CloudProfile</code>s referenced by <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>ClusterRoleBinding</code></td><td><code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code>, <code>delete</code></td><td><code>ClusterRoleBinding</code> -> <code>ManagedSeed</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code> requests for <code>ManagedSeed</code>s in the bootstrapping phase assigned to the gardenlet&rsquo;s <code>Seed</code>s. Allow <code>delete</code> requests from gardenlets bootstrapped via <code>ManagedSeed</code>s.</td></tr><tr><td><code>ConfigMap</code></td><td><code>get</code></td><td><code>ConfigMap</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow only <code>get</code> requests for <code>ConfigMap</code>s referenced by <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>. Allows reading the <code>kube-system/cluster-identity</code> <code>ConfigMap</code>.</td></tr><tr><td><code>ControllerRegistration</code></td><td><code>get</code>, <code>list</code>, <code>watch</code></td><td><code>ControllerRegistration</code> -> <code>ControllerInstallation</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>ControllerRegistration</code>s.</td></tr><tr><td><code>ControllerDeployment</code></td><td><code>get</code></td><td><code>ControllerDeployment</code> -> <code>ControllerInstallation</code> -> <code>Seed</code></td><td>Allow <code>get</code> requests for <code>ControllerDeployments</code>s referenced by <code>ControllerInstallation</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>ControllerInstallation</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>update</code>, <code>patch</code></td><td><code>ControllerInstallation</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>ControllerInstallation</code>s. Allow only <code>update</code>, <code>patch</code> requests for <code>ControllerInstallation</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>Event</code></td><td><code>create</code>, <code>patch</code></td><td>none</td><td>Allow to <code>create</code> or <code>patch</code> all kinds of <code>Event</code>s.</td></tr><tr><td><code>ExposureClass</code></td><td><code>get</code></td><td><code>ExposureClass</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>get</code> requests for <code>ExposureClass</code>es referenced by <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>. Deny <code>get</code> requests to other <code>ExposureClass</code>es.</td></tr><tr><td><code>Lease</code></td><td><code>create</code>, <code>get</code>, <code>watch</code>, <code>update</code></td><td><code>Lease</code> -> <code>Seed</code></td><td>Allow <code>create</code>, <code>get</code>, <code>update</code>, and <code>delete</code> requests for <code>Lease</code>s of the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>ManagedSeed</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>update</code>, <code>patch</code></td><td><code>ManagedSeed</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>ManagedSeed</code>s. Allow only <code>update</code>, <code>patch</code> requests for <code>ManagedSeed</code>s referencing a <code>Shoot</code> assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>Namespace</code></td><td><code>get</code></td><td><code>Namespace</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>get</code> requests for <code>Namespace</code>s of <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>. Always allow <code>get</code> requests for the <code>garden</code> <code>Namespace</code>.</td></tr><tr><td><code>Project</code></td><td><code>get</code></td><td><code>Project</code> -> <code>Namespace</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>get</code> requests for <code>Project</code>s referenced by the <code>Namespace</code> of <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>SecretBinding</code></td><td><code>get</code></td><td><code>SecretBinding</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow only <code>get</code> requests for <code>SecretBinding</code>s referenced by <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>Secret</code></td><td><code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code>, <code>delete</code>(, <code>list</code>, <code>watch</code>)</td><td><code>Secret</code> -> <code>Seed</code>, <code>Secret</code> -> <code>Shoot</code> -> <code>Seed</code>, <code>Secret</code> -> <code>SecretBinding</code> -> <code>Shoot</code> -> <code>Seed</code>, <code>BackupBucket</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>Secret</code>s in the <code>seed-&lt;name></code> namespace. Allow only <code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code>, <code>delete</code> requests for the <code>Secret</code>s related to resources assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>s.</td></tr><tr><td><code>Seed</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code></td><td><code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>Seed</code>s. Allow only <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> requests for the <code>gardenlet</code>&rsquo;s <code>Seed</code>s. [1]</td></tr><tr><td><code>ServiceAccount</code></td><td><code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code>, <code>delete</code></td><td><code>ServiceAccount</code> -> <code>ManagedSeed</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>create</code>, <code>get</code>, <code>update</code>, <code>patch</code> requests for <code>ManagedSeed</code>s in the bootstrapping phase assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>s. Allow <code>delete</code> requests from gardenlets bootstrapped via <code>ManagedSeed</code>s.</td></tr><tr><td><code>Shoot</code></td><td><code>get</code>, <code>list</code>, <code>watch</code>, <code>update</code>, <code>patch</code></td><td><code>Shoot</code> -> <code>Seed</code></td><td>Allow <code>get</code>, <code>list</code>, <code>watch</code> requests for all <code>Shoot</code>s. Allow only <code>update</code>, <code>patch</code> requests for <code>Shoot</code>s assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr><tr><td><code>ShootState</code></td><td><code>get</code>, <code>create</code>, <code>update</code>, <code>patch</code></td><td><code>ShootState</code> -> <code>Shoot</code> -> <code>Seed</code></td><td>Allow only <code>get</code>, <code>create</code>, <code>update</code>, <code>patch</code> requests for <code>ShootState</code>s belonging by <code>Shoot</code>s that are assigned to the <code>gardenlet</code>&rsquo;s <code>Seed</code>.</td></tr></tbody></table><blockquote><p>[1] If you use <code>ManagedSeed</code> resources then the <code>gardenlet</code> reconciling them (&ldquo;parent <code>gardenlet</code>&rdquo;) may be allowed to submit certain requests for the <code>Seed</code> resources resulting out of such <code>ManagedSeed</code> reconciliations (even if the &ldquo;parent <code>gardenlet</code>&rdquo; is not responsible for them):</p></blockquote><p>ℹ️ It is allowed to delete the <code>Seed</code> resources if the corresponding <code>ManagedSeed</code> objects already have a <code>deletionTimestamp</code> (this is secure as <code>gardenlet</code>s themselves don&rsquo;t have permissions for deleting <code>ManagedSeed</code>s).</p><h2 id=seedauthorizer-authorization-webhook-enablement><code>SeedAuthorizer</code> Authorization Webhook Enablement</h2><p>The <code>SeedAuthorizer</code> is implemented as a <a href=https://kubernetes.io/docs/reference/access-authn-authz/webhook/>Kubernetes authorization webhook</a> and part of the <a href=/docs/gardener/concepts/admission-controller/><code>gardener-admission-controller</code></a> component running in the garden cluster.</p><p>🎛 In order to activate it, you have to follow these steps:</p><ol><li><p>Set the following flags for the <code>kube-apiserver</code> of the garden cluster (i.e., the <code>kube-apiserver</code> whose API is extended by Gardener):</p><ul><li><code>--authorization-mode=RBAC,Node,Webhook</code> (please note that <code>Webhook</code> should appear after <code>RBAC</code> in the list [1]; <code>Node</code> might not be needed if you use a virtual garden cluster)</li><li><code>--authorization-webhook-config-file=&lt;path-to-the-webhook-config-file></code></li><li><code>--authorization-webhook-cache-authorized-ttl=0</code></li><li><code>--authorization-webhook-cache-unauthorized-ttl=0</code></li></ul></li><li><p>The webhook config file (stored at <code>&lt;path-to-the-webhook-config-file></code>) should look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: base64(CA-CERT-OF-GARDENER-ADMISSION-CONTROLLER)
</span></span><span style=display:flex><span>    server: https://gardener-admission-controller.garden/webhooks/auth/seed
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: kube-apiserver
</span></span><span style=display:flex><span>  user: {}
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- name: auth-webhook
</span></span><span style=display:flex><span>  context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: kube-apiserver
</span></span><span style=display:flex><span>current-context: auth-webhook
</span></span></code></pre></div></li><li><p>When deploying the <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane>Gardener <code>controlplane</code> Helm chart</a>, set <code>.global.rbac.seedAuthorizer.enabled=true</code>. This will ensure that the RBAC resources granting global access for all <code>gardenlet</code>s will be deployed.</p></li><li><p>Delete the existing RBAC resources granting global access for all <code>gardenlet</code>s by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  clusterrole.rbac.authorization.k8s.io/gardener.cloud:system:seeds <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  clusterrolebinding.rbac.authorization.k8s.io/gardener.cloud:system:seeds <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --ignore-not-found
</span></span></code></pre></div></li></ol><p>Please note that you should activate the <a href=#seedrestriction-admission-webhook-enablement><code>SeedRestriction</code></a> admission handler as well.</p><blockquote><p>[1] The reason for the fact that <code>Webhook</code> authorization plugin should appear after <code>RBAC</code> is that the <code>kube-apiserver</code> will be depending on the <code>gardener-admission-controller</code> (serving the webhook). However, the <code>gardener-admission-controller</code> can only start when <code>gardener-apiserver</code> runs, but <code>gardener-apiserver</code> itself can only start when <code>kube-apiserver</code> runs. If <code>Webhook</code> is before <code>RBAC</code>, then <code>gardener-apiserver</code> might not be able to start, leading to a deadlock.</p></blockquote><h3 id=authorizer-decisions>Authorizer Decisions</h3><p>As mentioned earlier, it&rsquo;s the authorizer&rsquo;s job to evaluate API requests and return one of the following decisions:</p><ul><li><code>DecisionAllow</code>: The request is allowed, further configured authorizers won&rsquo;t be consulted.</li><li><code>DecisionDeny</code>: The request is denied, further configured authorizers won&rsquo;t be consulted.</li><li><code>DecisionNoOpinion</code>: A decision cannot be made, further configured authorizers will be consulted.</li></ul><p>For backwards compatibility, no requests are denied at the moment, so that they are still deferred to a subsequent authorizer like RBAC.
Though, this might change in the future.</p><p>First, the <code>SeedAuthorizer</code> extracts the <code>Seed</code> name from the API request. This requires a proper TLS certificate that the <code>gardenlet</code> uses to contact the API server and is automatically given if <a href=/docs/gardener/concepts/gardenlet/#TLS-Bootstrapping>TLS bootstrapping</a> is used.
Concretely, the authorizer checks the certificate for name <code>gardener.cloud:system:seed:&lt;seed-name></code> and group <code>gardener.cloud:system:seeds</code>.
In cases where this information is missing e.g., when a custom Kubeconfig is used, the authorizer cannot make any decision. Thus, RBAC is still a considerable option to restrict the <code>gardenlet</code>&rsquo;s access permission if the above explained preconditions are not given.</p><p>With the <code>Seed</code> name at hand, the authorizer checks for an <strong>existing path</strong> from the resource that a request is being made for to the <code>Seed</code> belonging to the <code>gardenlet</code>. Take a look at the <a href=#implementation-details>Implementation Details</a> section for more information.</p><h3 id=implementation-details>Implementation Details</h3><p>Internally, the <code>SeedAuthorizer</code> uses a directed, acyclic graph data structure in order to efficiently respond to authorization requests for <code>gardenlet</code>s:</p><ul><li>A vertex in this graph represents a Kubernetes resource with its kind, namespace, and name (e.g., <code>Shoot:garden-my-project/my-shoot</code>).</li><li>An edge from vertex <code>u</code> to vertex <code>v</code> in this graph exists when<ul><li>(1) <code>v</code> is referred by <code>u</code> and <code>v</code> is a <code>Seed</code>, or when</li><li>(2) <code>u</code> is referred by <code>v</code>, or when</li><li>(3) <code>u</code> is strictly associated with <code>v</code>.</li></ul></li></ul><p>For example, a <code>Shoot</code> refers to a <code>Seed</code>, a <code>CloudProfile</code>, a <code>SecretBinding</code>, etc., so it has an outgoing edge to the <code>Seed</code> (1) and incoming edges from the <code>CloudProfile</code> and <code>SecretBinding</code> vertices (2).
However, there might also be a <code>ShootState</code> or a <code>BackupEntry</code> resource strictly associated with this <code>Shoot</code>, hence, it has incoming edges from these vertices (3).</p><p><img src=/__resources/gardenlet_api_access_graph_5486ba.png alt="Resource Dependency Graph"></p><p>In the above picture, the resources that are actively watched are shaded.
Gardener resources are green, while Kubernetes resources are blue.
It shows the dependencies between the resources and how the graph is built based on the above rules.</p><p>ℹ️ The above picture shows all resources that may be accessed by <code>gardenlet</code>s, except for the <code>Quota</code> resource which is only included for completeness.</p><p>Now, when a <code>gardenlet</code> wants to access certain resources, then the <code>SeedAuthorizer</code> uses a Depth-First traversal starting from the vertex representing the resource in question, e.g., from a <code>Project</code> vertex.
If there is a path from the <code>Project</code> vertex to the vertex representing the <code>Seed</code> the <code>gardenlet</code> is responsible for. then it allows the request.</p><h4 id=metrics>Metrics</h4><p>The <code>SeedAuthorizer</code> registers the following metrics related to the mentioned graph implementation:</p><table><thead><tr><th>Metric</th><th>Description</th></tr></thead><tbody><tr><td><code>gardener_admission_controller_seed_authorizer_graph_update_duration_seconds</code></td><td>Histogram of duration of resource dependency graph updates in seed authorizer, i.e., how long does it take to update the graph&rsquo;s vertices/edges when a resource is created, changed, or deleted.</td></tr><tr><td><code>gardener_admission_controller_seed_authorizer_graph_path_check_duration_seconds</code></td><td>Histogram of duration of checks whether a path exists in the resource dependency graph in seed authorizer.</td></tr></tbody></table><h4 id=debug-handler>Debug Handler</h4><p>When the <code>.server.enableDebugHandlers</code> field in the <code>gardener-admission-controller</code>&rsquo;s component configuration is set to <code>true</code>, then it serves a handler that can be used for debugging the resource dependency graph under <code>/debug/resource-dependency-graph</code>.</p><p>🚨 Only use this setting for development purposes, as it enables unauthenticated users to view all data if they have access to the <code>gardener-admission-controller</code> component.</p><p>The handler renders an HTML page displaying the current graph with a list of vertices and its associated incoming and outgoing edges to other vertices.
Depending on the size of the Gardener landscape (and consequently, the size of the graph), it might not be possible to render it in its entirety.
If there are more than 2000 vertices, then the default filtering will selected for <code>kind=Seed</code> to prevent overloading the output.</p><p><em>Example output</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>-------------------------------------------------------------------------------
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>| # Seed:my-seed
</span></span><span style=display:flex><span>|   &lt;- (11)
</span></span><span style=display:flex><span>|     BackupBucket:73972fe2-3d7e-4f61-a406-b8f9e670e6b7
</span></span><span style=display:flex><span>|     BackupEntry:garden-my-project/shoot--dev--my-shoot--4656a460-1a69-4f00-9372-7452cbd38ee3
</span></span><span style=display:flex><span>|     ControllerInstallation:dns-external-mxt8m
</span></span><span style=display:flex><span>|     ControllerInstallation:extension-shoot-cert-service-4qw5j
</span></span><span style=display:flex><span>|     ControllerInstallation:networking-calico-bgrb2
</span></span><span style=display:flex><span>|     ControllerInstallation:os-gardenlinux-qvb5z
</span></span><span style=display:flex><span>|     ControllerInstallation:provider-gcp-w4mvf
</span></span><span style=display:flex><span>|     Secret:garden/backup
</span></span><span style=display:flex><span>|     Shoot:garden-my-project/my-shoot
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>-------------------------------------------------------------------------------
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>| # Shoot:garden-my-project/my-shoot
</span></span><span style=display:flex><span>|   &lt;- (5)
</span></span><span style=display:flex><span>|     CloudProfile:gcp
</span></span><span style=display:flex><span>|     Namespace:garden-my-project
</span></span><span style=display:flex><span>|     Secret:garden-my-project/my-dns-secret
</span></span><span style=display:flex><span>|     SecretBinding:garden-my-project/my-credentials
</span></span><span style=display:flex><span>|     ShootState:garden-my-project/my-shoot
</span></span><span style=display:flex><span>|   -&gt; (1)
</span></span><span style=display:flex><span>|     Seed:my-seed
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>-------------------------------------------------------------------------------
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>| # ShootState:garden-my-project/my-shoot
</span></span><span style=display:flex><span>|   -&gt; (1)
</span></span><span style=display:flex><span>|     Shoot:garden-my-project/my-shoot
</span></span><span style=display:flex><span>|
</span></span><span style=display:flex><span>-------------------------------------------------------------------------------
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>... (etc., similarly for the other resources)
</span></span></code></pre></div><p>There are anchor links to easily jump from one resource to another, and the page provides means for filtering the results based on the <code>kind</code>, <code>namespace</code>, and/or <code>name</code>.</p><h4 id=pitfalls>Pitfalls</h4><p>When there is a relevant update to an existing resource, i.e., when a reference to another resource is changed, then the corresponding vertex (along with all associated edges) is first deleted from the graph before it gets added again with the up-to-date edges.
However, this does only work for vertices belonging to resources that are only created in exactly one &ldquo;watch handler&rdquo;.
For example, the vertex for a <code>SecretBinding</code> can either be created in the <code>SecretBinding</code> handler itself or in the <code>Shoot</code> handler.
In such cases, deleting the vertex before (re-)computing the edges might lead to race conditions and potentially renders the graph invalid.
Consequently, instead of deleting the vertex, only the edges the respective handler is responsible for are deleted.
If the vertex ends up with no remaining edges, then it also gets deleted automatically.
Afterwards, the vertex can either be added again or the updated edges can be created.</p><h2 id=seedrestriction-admission-webhook-enablement><code>SeedRestriction</code> Admission Webhook Enablement</h2><p>The <code>SeedRestriction</code> is implemented as <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>Kubernetes admission webhook</a> and part of the <a href=/docs/gardener/concepts/admission-controller/><code>gardener-admission-controller</code></a> component running in the garden cluster.</p><p>🎛 In order to activate it, you have to set <code>.global.admission.seedRestriction.enabled=true</code> when using the <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane>Gardener <code>controlplane</code> Helm chart</a>.
This will add an additional webhook in the existing <code>ValidatingWebhookConfiguration</code> of the <code>gardener-admission-controller</code> which contains the configuration for the <code>SeedRestriction</code> handler.
Please note that it should only be activated when the <code>SeedAuthorizer</code> is active as well.</p><h3 id=admission-decisions>Admission Decisions</h3><p>The admission&rsquo;s purpose is to perform extended validation on requests which require the body of the object in question.
Additionally, it handles <code>CREATE</code> requests of <code>gardenlet</code>s (the above discussed resource dependency graph cannot be used in such cases because there won&rsquo;t be any vertex/edge for non-existing resources).</p><p>Gardenlets are restricted to only create new resources which are somehow related to the seed clusters they are responsible for.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b20736e3fcbc4d202271af720d0806cf>1.3.8 - Getting Started Locally</h1><h1 id=deploying-gardener-locally>Deploying Gardener Locally</h1><p>This document will walk you through deploying Gardener on your local machine.
If you encounter difficulties, please open an issue so that we can make this process easier.</p><h2 id=overview>Overview</h2><p>Gardener runs in any Kubernetes cluster.
In this guide, we will start a <a href=https://kind.sigs.k8s.io/>KinD</a> cluster which is used as both garden and seed cluster (please refer to the <a href=/docs/gardener/concepts/architecture/>architecture overview</a>) for simplicity.</p><p>Based on <a href=https://skaffold.dev/>Skaffold</a>, the container images for all required components will be built and deployed into the cluster (via their <a href=https://helm.sh/>Helm charts</a>).</p><p><img src=/__resources/getting_started_locally_f66391.png alt="Architecture Diagram"></p><h2 id=alternatives>Alternatives</h2><p>When deploying Gardener on your local machine you might face several limitations:</p><ul><li>Your machine doesn&rsquo;t have enough compute resources (see <a href=#prerequisites>prerequisites</a>) for hosting a second seed cluster or multiple shoot clusters.</li><li>Testing Gardener&rsquo;s <a href=/docs/gardener/usage/ipv6/>IPv6 features</a> requires a Linux machine and native IPv6 connectivity to the internet, but you&rsquo;re on macOS or don&rsquo;t have IPv6 connectivity in your office environment or via your home ISP.</li></ul><p>In these cases, you might want to check out one of the following options that run the setup described in this guide elsewhere for circumventing these limitations:</p><ul><li><a href=/docs/gardener/development/getting_started_locally/#remote-local-setup>remote local setup</a>: deploy on a remote pod for more compute resources</li><li><a href=https://github.com/gardener-community/dev-box-gcp>dev box on Google Cloud</a>: deploy on a Google Cloud machine for more compute resource and/or simple IPv4/IPv6 dual-stack networking</li></ul><h2 id=prerequisites>Prerequisites</h2><ul><li>Make sure that you have followed the <a href=/docs/gardener/development/local_setup/>Local Setup guide</a> up until the <a href=/docs/gardener/development/local_setup/#get-the-sources>Get the sources</a> step.</li><li>Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least <code>8</code> CPUs and <code>8Gi</code> memory; see <a href=https://docs.docker.com/desktop/mac/#resources>here</a> how to configure the resources for Docker for Mac).<blockquote><p>Please note that 8 CPU / 8Gi memory might not be enough for more than two <code>Shoot</code> clusters, i.e., you might need to increase these values if you want to run additional <code>Shoot</code>s.
If you plan on following the optional steps to <a href=#optional-setting-up-a-second-seed-cluster>create a second seed cluster</a>, the required resources will be more - at least <code>10</code> CPUs and <code>18Gi</code> memory.
Additionally, please configure at least <code>120Gi</code> of disk size for the Docker daemon.
Tip: You can clean up unused data with <code>docker system df</code> and <code>docker system prune -a</code>.</p></blockquote></li><li>Make sure the <code>kind</code> docker network is using the CIDR <code>172.18.0.0/16</code>.<ul><li>If the network does not exist, it can be created with <code>docker network create kind --subnet 172.18.0.0/16</code></li><li>If the network already exists, the CIDR can be checked with <code>docker network inspect kind | jq '.[].IPAM.Config[].Subnet'</code>. If it is not <code>172.18.0.0/16</code>, delete the network with <code>docker network rm kind</code> and create it with the command above.</li></ul></li></ul><h2 id=setting-up-the-kind-cluster-garden-and-seed>Setting Up the KinD Cluster (Garden and Seed)</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-up
</span></span></code></pre></div><blockquote><p>If you want to setup an IPv6 KinD cluster, use <code>make kind-up IPFAMILY=ipv6</code> instead.</p></blockquote><p>This command sets up a new KinD cluster named <code>gardener-local</code> and stores the kubeconfig in the <code>./example/gardener-local/kind/local/kubeconfig</code> file.</p><blockquote><p>It might be helpful to copy this file to <code>$HOME/.kube/config</code>, since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your <code>KUBECONFIG</code> environment variable to <code>./example/gardener-local/kind/local/kubeconfig</code> for all future steps via <code>export KUBECONFIG=example/gardener-local/kind/local/kubeconfig</code>.</p></blockquote><p>All of the following steps assume that you are using this kubeconfig.</p><p>Additionally, this command also deploys a local container registry to the cluster, as well as a few registry mirrors, that are set up as a pull-through cache for all upstream registries Gardener uses by default.
This is done to speed up image pulls across local clusters.
The local registry can be accessed as <code>localhost:5001</code> for pushing and pulling.
The storage directories of the registries are mounted to the host machine under <code>dev/local-registry</code>.
With this, mirrored images don&rsquo;t have to be pulled again after recreating the cluster.</p><p>The command also deploys a default <a href=https://github.com/projectcalico/calico>calico</a> installation as the cluster&rsquo;s CNI implementation with <code>NetworkPolicy</code> support (the default <code>kindnet</code> CNI doesn&rsquo;t provide <code>NetworkPolicy</code> support).
Furthermore, it deploys the <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a> in order to support HPA and VPA on the seed cluster.</p><h2 id=outgoing-ipv6-single-stack-networking-optional>Outgoing IPv6 Single-Stack Networking (optional)</h2><p>If you want to test IPv6-related features, we need to configure NAT for outgoing traffic from the kind network to the internet.
After <code>make kind-up IPFAMILY=ipv6</code>, check the network created by kind:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ docker network inspect kind | jq <span style=color:#a31515>&#39;.[].IPAM.Config[].Subnet&#39;</span>
</span></span><span style=display:flex><span><span style=color:#a31515>&#34;172.18.0.0/16&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a31515>&#34;fc00:f853:ccd:e793::/64&#34;</span>
</span></span></code></pre></div><p>Determine which device is used for outgoing internet traffic by looking at the default route:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ip route show default
</span></span><span style=display:flex><span>default via 192.168.195.1 dev enp3s0 proto dhcp src 192.168.195.34 metric 100
</span></span></code></pre></div><p>Configure NAT for traffic from the kind cluster to the internet using the IPv6 range and the network device from the previous two steps:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ip6tables -t nat -A POSTROUTING -o enp3s0 -s fc00:f853:ccd:e793::/64 -j MASQUERADE
</span></span></code></pre></div><h2 id=setting-up-gardener>Setting Up Gardener</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make gardener-up
</span></span></code></pre></div><p>This will first build the base image (which might take a bit if you do it for the first time).
Afterwards, the Gardener resources will be deployed into the cluster.</p><h2 id=creating-a-shoot-cluster>Creating a <code>Shoot</code> Cluster</h2><p>You can wait for the <code>Seed</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/usage/wait-for.sh seed local GardenletReady Bootstrapped SeedSystemComponentsHealthy ExtensionsReady
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl get seed local</code> and wait for the <code>STATUS</code> to indicate readiness:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME    STATUS   PROVIDER   REGION   AGE     VERSION       K8S VERSION
</span></span><span style=display:flex><span>local   Ready    local      local    4m42s   vX.Y.Z-dev    v1.21.1
</span></span></code></pre></div><p>In order to create a first shoot cluster, just run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f example/provider-local/shoot.yaml
</span></span></code></pre></div><p>You can wait for the <code>Shoot</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAMESPACE=garden-local ./hack/usage/wait-for.sh shoot APIServerAvailable ControlPlaneHealthy ObservabilityComponentsHealthy EveryNodeReady SystemComponentsHealthy
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl -n garden-local get shoot local</code> and wait for the <code>LAST OPERATION</code> to reach <code>100%</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME    CLOUDPROFILE   PROVIDER   REGION   K8S VERSION   HIBERNATION   LAST OPERATION            STATUS    AGE
</span></span><span style=display:flex><span>local   local          local      local    1.21.0        Awake         Create Processing (43%)   healthy   94s
</span></span></code></pre></div><p>(Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e-local-simple KUBECONFIG=<span style=color:#a31515>&#34;</span>$PWD<span style=color:#a31515>/example/gardener-local/kind/local/kubeconfig&#34;</span>
</span></span></code></pre></div><h3 id=accessing-the-shoot-cluster>Accessing the <code>Shoot</code> Cluster</h3><p>⚠️ Please note that in this setup, shoot clusters are not accessible by default when you download the kubeconfig and try to communicate with them.
The reason is that your host most probably cannot resolve the DNS names of the clusters since <code>provider-local</code> extension runs inside the KinD cluster (for more details, see <a href=/docs/gardener/extensions/provider-local/#dnsrecord>DNSRecord</a>).
Hence, if you want to access the shoot cluster, you have to run the following command which will extend your <code>/etc/hosts</code> file with the required information to make the DNS names resolvable:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt;EOF | sudo tee -a /etc/hosts
</span></span></span><span style=display:flex><span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515># Manually created to access local Gardener shoot clusters.
</span></span></span><span style=display:flex><span><span style=color:#a31515># TODO: Remove this again when the shoot cluster access is no longer required.
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.local.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.local.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-managedseed.garden.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-managedseed.garden.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-hibernated.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-hibernated.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-unpriv.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-unpriv.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-wake-up.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-wake-up.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-migrate.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-migrate.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-rotate.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-rotate.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-default.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-default.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-update-node.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-update-node.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-update-zone.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-update-zone.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-upgrade.local.external.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.e2e-upgrade.local.internal.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>To access the <code>Shoot</code>, you can acquire a <code>kubeconfig</code> by using the <a href=/docs/gardener/usage/shoot_access/#shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> subresource</a>.</p><h2 id=optional-setting-up-a-second-seed-cluster>(Optional): Setting Up a Second Seed Cluster</h2><p>There are cases where you would want to create a second seed cluster in your local setup. For example, if you want to test the <a href=/docs/gardener/usage/control_plane_migration/>control plane migration</a> feature. The following steps describe how to do that.</p><p>If you are on macOS, add a new IP address on your loopback device which will be necessary for the new KinD cluster that you will create. On macOS, the default loopback device is <code>lo0</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ip addr add 127.0.0.2 dev lo0                                     <span style=color:green># adding 127.0.0.2 ip to the loopback interface</span>
</span></span></code></pre></div><p>Next, setup the second KinD cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind2-up
</span></span></code></pre></div><p>This command sets up a new KinD cluster named <code>gardener-local2</code> and stores its kubeconfig in the <code>./example/gardener-local/kind/local2/kubeconfig</code> file.</p><p>In order to deploy required resources in the KinD cluster that you just created, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make gardenlet-kind2-up
</span></span></code></pre></div><p>The following steps assume that you are using the kubeconfig that points to the <code>gardener-local</code> cluster (first KinD cluster): <code>export KUBECONFIG=example/gardener-local/kind/local/kubeconfig</code>.</p><p>You can wait for the <code>local2</code> <code>Seed</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/usage/wait-for.sh seed local2 GardenletReady Bootstrapped ExtensionsReady
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl get seed local2</code> and wait for the <code>STATUS</code> to indicate readiness:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME    STATUS   PROVIDER   REGION   AGE     VERSION       K8S VERSION
</span></span><span style=display:flex><span>local2  Ready    local      local    4m42s   vX.Y.Z-dev    v1.21.1
</span></span></code></pre></div><p>If you want to perform control plane migration, you can follow the steps outlined in <a href=/docs/gardener/usage/control_plane_migration/>Control Plane Migration</a> to migrate the shoot cluster to the second seed you just created.</p><h2 id=deleting-the-shoot-cluster>Deleting the <code>Shoot</code> Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./hack/usage/delete shoot local garden-local
</span></span></code></pre></div><h2 id=optional-tear-down-the-second-seed-cluster>(Optional): Tear Down the Second Seed Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind2-down
</span></span></code></pre></div><h2 id=tear-down-the-gardener-environment>Tear Down the Gardener Environment</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind-down
</span></span></code></pre></div><h2 id=remote-local-setup>Remote Local Setup</h2><p>Just like Prow is executing the KinD based integration tests in a K8s pod, it is
possible to interactively run this KinD based Gardener development environment,
aka &ldquo;local setup&rdquo;, in a &ldquo;remote&rdquo; K8s pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k apply -f docs/deployment/content/remote-local-setup.yaml
</span></span><span style=display:flex><span>k exec -it deployment/remote-local-setup -- sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tmux -u a
</span></span></code></pre></div><h3 id=caveats>Caveats</h3><p>Please refer to the <a href=https://github.com/tmux/tmux/wiki>TMUX documentation</a> for
working effectively inside the remote-local-setup pod.</p><p>To access Grafana, Prometheus or other components in a browser, two port forwards are needed:</p><p>The port forward from the laptop to the pod:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k port-forward deployment/remote-local-setup 3000
</span></span></code></pre></div><p>The port forward in the remote-local-setup pod to the respective component:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k port-forward -n shoot--local--local deployment/grafana 3000
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/extensions/provider-local/>Local Provider Extension</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f278519ade7bb6a4a605318159050e20>1.3.9 - Getting Started Locally With Extensions</h1><h1 id=deploying-gardener-locally-and-enabling-provider-extensions>Deploying Gardener Locally and Enabling Provider-Extensions</h1><p>This document will walk you through deploying Gardener on your local machine and bootstrapping your own seed clusters on an existing Kubernetes cluster.
It is supposed to run your local Gardener developments on a real infrastructure. For running Gardener only entirely local, please check the <a href=/docs/gardener/deployment/getting_started_locally/>getting started locally</a> documentation.
If you encounter difficulties, please open an issue so that we can make this process easier.</p><h2 id=overview>Overview</h2><p>Gardener runs in any Kubernetes cluster.
In this guide, we will start a <a href=https://kind.sigs.k8s.io/>KinD</a> cluster which is used as garden cluster. Any Kubernetes cluster could be used as seed clusters in order to support provider extensions (please refer to the <a href=/docs/gardener/concepts/architecture/>architecture overview</a>). This guide is tested for using Kubernetes clusters provided by Gardener, AWS, Azure, and GCP as seed so far.</p><p>Based on <a href=https://skaffold.dev/>Skaffold</a>, the container images for all required components will be built and deployed into the clusters (via their <a href=https://helm.sh/>Helm charts</a>).</p><p><img src=/__resources/getting_started_locally_with_extensions_41f6e9.png alt="Architecture Diagram"></p><h2 id=prerequisites>Prerequisites</h2><ul><li>Make sure that you have followed the <a href=/docs/gardener/development/local_setup/>Local Setup guide</a> up until the <a href=/docs/gardener/development/local_setup/#get-the-sources>Get the sources</a> step.</li><li>Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least <code>8</code> CPUs and <code>8Gi</code> memory; see the <a href=https://docs.docker.com/desktop/settings/mac/>Docker documentation</a> for how to configure the resources for Docker for Mac).<blockquote><p>Additionally, please configure at least <code>120Gi</code> of disk size for the Docker daemon.
Tip: You can clean up unused data with <code>docker system df</code> and <code>docker system prune -a</code>.</p></blockquote></li><li>Make sure that you have access to a Kubernetes cluster you can use as a seed cluster in this setup.<ul><li>The seed cluster requires at least 16 CPUs in total to run one shoot cluster</li><li>You could use any Kubernetes cluster for your seed cluster. However, using a Gardener shoot cluster for your seed simplifies some configuration steps.</li><li>When bootstrapping <code>gardenlet</code> to the cluster, your new seed will have the same provider type as the shoot cluster you use - an AWS shoot will become an AWS seed, a GCP shoot will become a GCP seed, etc. (only relevant when using a Gardener shoot as seed).</li></ul></li></ul><h2 id=provide-infrastructure-credentials-and-configuration>Provide Infrastructure Credentials and Configuration</h2><p>As this setup is running on a real infrastructure, you have to provide credentials for DNS, the infrastructure, and the kubeconfig for the Gardener cluster you want to use as seed.</p><blockquote><p>There are <code>.gitignore</code> entries for all files and directories which include credentials. Nevertheless, please double check and make sure that credentials are not commited.</p></blockquote><h3 id=dns>DNS</h3><p>Gardener control plane requires DNS for default and internal domains. Thus, you have to configure a valid DNS provider for your setup.</p><p>Please maintain your DNS provider configuration and credentials at <code>./example/provider-extensions/garden/controlplane/values.yaml</code>.</p><p>You can find a template for the file at <code>./example/provider-extensions/garden/controlplane/values.yaml.tmpl</code>.</p><h3 id=infrastructure>Infrastructure</h3><p>Infrastructure secrets and the corresponding secret bindings should be maintained at:</p><ul><li><code>./example/provider-extensions/garden/project/credentials/infrastructure-secrets.yaml</code></li><li><code>./example/provider-extensions/garden/project/credentials/secretbindings.yaml</code></li></ul><p>There are templates with <code>.tmpl</code> suffixes for the files in the same folder.</p><h3 id=seed-cluster-preparation>Seed Cluster Preparation</h3><p>The <code>kubeconfig</code> of your Kubernetes cluster you would like to use as seed should be placed at <code>./example/provider-extensions/seed/kubeconfig</code>.
Additionally, please maintain the configuration of your seed in <code>./example/provider-extensions/gardenlet/values.yaml</code>. It is automatically copied from <code>values.yaml.tmpl</code> in the same directory when you run <code>make gardener-extensions-up</code> for the first time. It also includes explanations of the properties you should set.</p><p>Using a Gardener cluster as seed simplifies the process, because some configuration options can be taken from <code>shoot-info</code> and creating DNS entries and TLS certificates is automated.</p><p>However, you can use different Kubernetes clusters for your seed too and configure these things manually. Please configure the options of <code>./example/provider-extensions/gardenlet/values.yaml</code> upfront. For configuring DNS and TLS certificates, <code>make gardener-extensions-up</code>, which is explained later, will pause and tell you what to do.</p><h3 id=external-controllers>External Controllers</h3><p>You might plan to deploy and register external controllers for networking, operating system, providers, etc. Please put <code>ControllerDeployment</code>s and <code>ControllerRegistration</code>s into the <code>./example/provider-extensions/garden/controllerregistrations</code> directory. The whole content of this folder will be applied to your KinD cluster.</p><h3 id=cloudprofiles><code>CloudProfile</code>s</h3><p>There are no demo <code>CloudProfiles</code> yet. Thus, please copy <code>CloudProfiles</code> from another landscape to the <code>./example/provider-extensions/garden/cloudprofiles</code> directory or create your own <code>CloudProfiles</code> based on the <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml>gardener examples</a>. Please check the GitHub repository of your desired provider-extension. Most of them include example <code>CloudProfile</code>s. All files you place in this folder will be applied to your KinD cluster.</p><h2 id=setting-up-the-kind-cluster>Setting Up the KinD Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-extensions-up
</span></span></code></pre></div><p>This command sets up a new KinD cluster named <code>gardener-local</code> and stores the kubeconfig in the <code>./example/gardener-local/kind/extensions/kubeconfig</code> file.</p><blockquote><p>It might be helpful to copy this file to <code>$HOME/.kube/config</code>, since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your <code>KUBECONFIG</code> environment variable to <code>./example/gardener-local/kind/extensions/kubeconfig</code> for all future steps via <code>export KUBECONFIG=$PWD/example/gardener-local/kind/extensions/kubeconfig</code>.</p></blockquote><p>All of the following steps assume that you are using this kubeconfig.</p><p>Additionally, this command deploys a local container registry to the cluster as well as a few registry mirrors that are set up as a pull-through cache for all upstream registries Gardener uses by default.
This is done to speed up image pulls across local clusters.
The local registry can be accessed as <code>localhost:5001</code> for pushing and pulling.
The storage directories of the registries are mounted to your machine under <code>dev/local-registry</code>.
With this, mirrored images don&rsquo;t have to be pulled again after recreating the cluster.</p><p>The command also deploys a default <a href=https://github.com/projectcalico/calico>calico</a> installation as the cluster&rsquo;s CNI implementation with <code>NetworkPolicy</code> support (the default <code>kindnet</code> CNI doesn&rsquo;t provide <code>NetworkPolicy</code> support).
Furthermore, it deploys the <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a> in order to support HPA and VPA on the seed cluster.</p><h2 id=setting-up-gardener-garden-on-kind-seed-on-gardener-cluster>Setting Up Gardener (Garden on KinD, Seed on Gardener Cluster)</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make gardener-extensions-up
</span></span></code></pre></div><p>This will first prepare the basic configuration of your KinD and Gardener clusters.
Afterwards, the images for the Garden cluster are built and deployed into the KinD cluster.
Finally, the images for the Seed cluster are built, pushed to a container registry on the Seed, and the <code>gardenlet</code> is started.</p><h2 id=pause-and-unpause-the-kind-cluster>Pause and Unpause the KinD Cluster</h2><p>The KinD cluster can be paused by stopping and keeping its docker container. This can be done by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-extensions-down
</span></span></code></pre></div><p>When you run <code>make kind-extensions-up</code> again, you will start the docker container with your previous Gardener configuration again.</p><p>This provides the option to switch off your local KinD cluster fast without leaving orphaned infrastructure elements behind.</p><h2 id=creating-a-shoot-cluster>Creating a <code>Shoot</code> Cluster</h2><p>You can wait for the <code>Seed</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl wait --for=condition=gardenletready seed provider-extensions --timeout=5m
</span></span></code></pre></div><p><code>make kind-extensions-up</code> already includes such a check. However, it might be useful when you wake up your <code>Seed</code> from hibernation or unpause you KinD cluster.</p><p>Alternatively, you can run <code>kubectl get seed provider-extensions</code> and wait for the <code>STATUS</code> to indicate readiness:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME                  STATUS   PROVIDER   REGION         AGE    VERSION      K8S VERSION
</span></span><span style=display:flex><span>provider-extensions   Ready    gcp        europe-west1   111m   v1.61.0-dev   v1.24.7
</span></span></code></pre></div><p>In order to create a first shoot cluster, please create your own <code>Shoot</code> definition and apply it to your KinD cluster. <code>gardener-scheduler</code> includes <code>candidateDeterminationStrategy: MinimalDistance</code> configuration so you are able to run schedule <code>Shoot</code>s of different providers on your <code>Seed</code>.</p><p>You can wait for your <code>Shoot</code>s to be ready by running <code>kubectl -n garden-local get shoots</code> and wait for the <code>LAST OPERATION</code> to reach <code>100%</code>. The output depends on your <code>Shoot</code> definition. This is an example output:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME        CLOUDPROFILE   PROVIDER   REGION         K8S VERSION   HIBERNATION   LAST OPERATION               STATUS    AGE
</span></span><span style=display:flex><span>aws         aws            aws        eu-west-1      1.24.3        Awake         Create Processing (43%)      healthy   84s
</span></span><span style=display:flex><span>aws-arm64   aws            aws        eu-west-1      1.24.3        Awake         Create Processing (43%)      healthy   65s
</span></span><span style=display:flex><span>azure       az             azure      westeurope     1.24.2        Awake         Create Processing (43%)      healthy   57s
</span></span><span style=display:flex><span>gcp         gcp            gcp        europe-west1   1.24.3        Awake         Create Processing (43%)      healthy   94s
</span></span></code></pre></div><h3 id=accessing-the-shoot-cluster>Accessing the <code>Shoot</code> Cluster</h3><p>Your shoot clusters will have a public DNS entries for their API servers, so that they could be reached via the Internet via <code>kubectl</code> after you have created their <code>kubeconfig</code>.</p><p>We encourage you to use the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/16-adminkubeconfig-subresource.md>adminkubeconfig subresource</a> for accessing your shoot cluster. You can find an example how to use it in <a href=/docs/gardener/usage/shoot_access/#shootsadminkubeconfig-subresource>Accessing Shoot Clusters</a>.</p><h2 id=deleting-the-shoot-clusters>Deleting the <code>Shoot</code> Clusters</h2><p>Before tearing down your environment, you have to delete your shoot clusters. This is highly recommended because otherwise you would leave orphaned items on your infrastructure accounts.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/usage/delete shoot &lt;your-shoot&gt; garden-local
</span></span></code></pre></div><h2 id=tear-down-the-gardener-environment>Tear Down the Gardener Environment</h2><p>Before you delete your local KinD cluster, you should shut down your <code>Shoots</code> and <code>Seed</code> in a clean way to avoid orphaned infrastructure elements in your projects.</p><p>Please ensure that your KinD and Seed clusters are online (not paused or hibernated) and run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make gardener-extensions-down
</span></span></code></pre></div><p>This will delete all <code>Shoots</code> first (this could take a couple of minutes), then uninstall <code>gardenlet</code> from the Seed and the gardener components from the KinD. Finally, the additional components like container registry, etc., are deleted from both clusters.</p><p>When this is done, you can securely delete your local KinD cluster by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-extensions-clean
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5a53ffb401b7a9ba7d6243e70af0be09>1.3.10 - Image Vector</h1><h1 id=image-vector>Image Vector</h1><p>The Gardenlet is deploying several different container images into the seed and the shoot clusters.
The image repositories and tags are defined in a <a href=https://github.com/gardener/gardener/blob/master/charts/images.yaml>central image vector file</a>.
Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).</p><h2 id=example>Example</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>images:
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: registry.k8s.io/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.4&#34;</span>
</span></span><span style=display:flex><span>  version: <span style=color:#a31515>&#34;1.20.x&#34;</span>
</span></span><span style=display:flex><span>  architectures:
</span></span><span style=display:flex><span>  - amd64
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: registry.k8s.io/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.5&#34;</span>
</span></span><span style=display:flex><span>  version: <span style=color:#a31515>&#34;&gt;= 1.21&#34;</span>
</span></span><span style=display:flex><span>  architectures:
</span></span><span style=display:flex><span>  - amd64
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>That means that the Gardenlet will use the <code>pause-container</code> with tag <code>3.4</code> for all seed/shoot clusters with Kubernetes version <code>1.20.x</code>, and tag <code>3.5</code> for all clusters with Kubernetes <code>>= 1.21</code>.</p><h2 id=image-vector-architecture>Image Vector Architecture</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>images:
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: registry.k8s.io/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.5&#34;</span>
</span></span><span style=display:flex><span>  architectures:
</span></span><span style=display:flex><span>  - amd64
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: registry.k8s.io/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.5&#34;</span>
</span></span><span style=display:flex><span>  architectures:
</span></span><span style=display:flex><span>  - arm64
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: registry.k8s.io/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.5&#34;</span>
</span></span><span style=display:flex><span>  architectures:
</span></span><span style=display:flex><span>  - amd64
</span></span><span style=display:flex><span>  - arm64
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p><code>architectures</code> is an optional field of image. It is a list of strings specifying CPU architecture of machines on which this image can be used. The valid options for the architectures field are as follows:</p><ul><li><code>amd64</code> : This specifies that the image can run only on machines having CPU architecture <code>amd64</code>.</li><li><code>arm64</code> : This specifies that the image can run only on machines having CPU architecture <code>arm64</code>.</li></ul><p>If an image doesn&rsquo;t specify any architectures, then by default it is considered to support both <code>amd64</code> and <code>arm64</code> architectures.</p><h2 id=overwrite-image-vector>Overwrite Image Vector</h2><p>In some environments it is not possible to use these &ldquo;pre-defined&rdquo; images that come with a Gardener release.
A prominent example for that is Alicloud in China, which does not allow access to Google&rsquo;s GCR.
In these cases, you might want to overwrite certain images, e.g., point the <code>pause-container</code> to a different registry.</p><p>⚠️ If you specify an image that does not fit to the resource manifest, then the seed/shoot reconciliation might fail.</p><p>In order to overwrite the images, you must provide a similar file to gardenlet:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>images:
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: my-custom-image-registry/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.4&#34;</span>
</span></span><span style=display:flex><span>  version: <span style=color:#a31515>&#34;1.20.x&#34;</span>
</span></span><span style=display:flex><span>- name: pause-container
</span></span><span style=display:flex><span>  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
</span></span><span style=display:flex><span>  repository: my-custom-image-registry/pause
</span></span><span style=display:flex><span>  tag: <span style=color:#a31515>&#34;3.5&#34;</span>
</span></span><span style=display:flex><span>  version: <span style=color:#a31515>&#34;&gt;= 1.21&#34;</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>During deployment of the gardenlet, create a <code>ConfigMap</code> containing the above content and mount it as a volume into the gardenlet pod.
Next, specify the environment variable <code>IMAGEVECTOR_OVERWRITE</code>, whose value must be the path to the file you just mounted:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardenlet-images-overwrite
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  images_overwrite.yaml: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    images:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - ...</span>    
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardenlet
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: gardenlet
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: IMAGEVECTOR_OVERWRITE
</span></span><span style=display:flex><span>          value: /charts-overwrite/images_overwrite.yaml
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: gardenlet-images-overwrite
</span></span><span style=display:flex><span>          mountPath: /charts-overwrite
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: gardenlet-images-overwrite
</span></span><span style=display:flex><span>        configMap:
</span></span><span style=display:flex><span>          name: gardenlet-images-overwrite
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=image-vectors-for-dependent-components>Image Vectors for Dependent Components</h2><p>The gardenlet is deploying a lot of different components that might deploy other images themselves.
These components might use an image vector as well.
Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.</p><p>It is possible to specify the <code>IMAGEVECTOR_OVERWRITE_COMPONENTS</code> environment variable to the gardenlet that points to a file with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>components:
</span></span><span style=display:flex><span>- name: etcd-druid
</span></span><span style=display:flex><span>  imageVectorOverwrite: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    images:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: etcd
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tag: v1.2.3
</span></span></span><span style=display:flex><span><span style=color:#a31515>      repository: etcd/etcd</span>    
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>The gardenlet will, if supported by the directly deployed component (<code>etcd-druid</code> in this example), inject the given <code>imageVectorOverwrite</code> into the <code>Deployment</code> manifest.
The respective component is responsible for using the overwritten images instead of its defaults.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d48433bb7b23a8ada3fb3a2f6c8674f1>1.3.11 - Migration V0 To V1</h1><h1 id=migration-from-gardener-v0-to-v1>Migration from Gardener <code>v0</code> to <code>v1</code></h1><p>Please refer to the <a href=https://github.com/gardener/gardener/blob/v1.10.1/docs/deployment/migration_v0_to_v1.md>document for older Gardener versions</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ba94e9b56f741be3998ac8487425f6b3>1.3.12 - Secret Binding Provider Controller</h1><h1 id=secretbinding-provider-controller>SecretBinding Provider Controller</h1><p>This page describes the process on how to enable the SecretBinding provider controller.</p><h2 id=overview>Overview</h2><p>With Gardener v1.38.0, the SecretBinding resource now contains a new optional field <code>.provider.type</code> (details about the motivation can be found in <a href=https://github.com/gardener/gardener/issues/4888>https://github.com/gardener/gardener/issues/4888</a>). To make the process of setting the new field automated and afterwards to enforce validation on the new field in backwards compatible manner, Gardener features the SecretBinding provider controller and a feature gate - <code>SecretBindingProviderValidation</code>.</p><h2 id=process>Process</h2><p>A Gardener landscape operator can follow the following steps:</p><ol><li><p>Enable the SecretBinding provider controller of Gardener Controller Manager.</p><p>The SecretBinding provider controller is responsible for populating the <code>.provider.type</code> field of a SecretBinding based on its current usage by Shoot resources. For example, if a Shoot <code>crazy-botany</code> with <code>.provider.type=aws</code> is using a SecretBinding <code>my-secret-binding</code>, then the SecretBinding provider controller will take care to set the <code>.provider.type</code> field of the SecretBinding to the same provider type (<code>aws</code>).
To enable the SecretBinding provider controller, set the <code>controller.secretBindingProvider.concurentSyncs</code> field in the ControllerManagerConfiguration (e.g set it to <code>5</code>).
Although that it is not recommended, the API allows Shoots from different provider types to reference the same SecretBinding (assuming that the backing Secret contains data for both of the provider types). To preserve the backwards compatibility for such SecretBindings, the provider controller will maintain the multiple provider types in the field (it will join them with the separator <code>,</code> - for example <code>aws,gcp</code>).</p></li><li><p>Disable the SecretBinding provider controller and enable the <code>SecretBindingProviderValidation</code> feature gate of Gardener API server.</p><p>The <code>SecretBindingProviderValidation</code> feature gate of Gardener API server enables a set of validations for the SecretBinding provider field. It forbids creating a Shoot that has a different provider type from the referenced SecretBinding&rsquo;s one. It also enforces immutability on the field.
After making sure that SecretBinding provider controller is enabled and it populated the <code>.provider.type</code> field of a majority of the SecretBindings on a Gardener landscape (the SecretBindings that are unused will have their provider type unset), a Gardener landscape operator has to disable the SecretBinding provider controller and to enable the <code>SecretBindingProviderValidation</code> feature gate of Gardener API server. To disable the SecretBinding provider controller, set the <code>controller.secretBindingProvider.concurentSyncs</code> field in the ControllerManagerConfiguration to <code>0</code>.</p></li></ol><h2 id=implementation-history>Implementation History</h2><ul><li>Gardener v1.38: The SecretBinding resource has a new optional field <code>.provider.type</code>. The SecretBinding provider controller is disabled by default. The <code>SecretBindingProviderValidation</code> feature gate of Gardener API server is disabled by default.</li><li>Gardener v1.42: The SecretBinding provider controller is enabled by default.</li><li>Gardener v1.51: The <code>SecretBindingProviderValidation</code> feature gate of Gardener API server is enabled by default and the SecretBinding provider controller is disabled by default.</li><li>Gardener v1.53: The <code>SecretBindingProviderValidation</code> feature gate of Gardener API server is unconditionally enabled (can no longer be disabled).</li><li>Gardener v1.55: The <code>SecretBindingProviderValidation</code> feature gate of Gardener API server and the SecretBinding provider controller are removed.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-83f23df3bdd8e7e5f8233751ed6600f6>1.3.13 - Setup Gardener</h1><h1 id=deploying-gardener-into-a-kubernetes-cluster>Deploying Gardener into a Kubernetes Cluster</h1><p>Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (gardenlet).
The control plane is deployed in the so-called garden cluster, while the agent is installed into every seed cluster.
Please note that it is possible to use the garden cluster as seed cluster by simply deploying the gardenlet into it.</p><p>We are providing <a href=https://github.com/gardener/gardener/tree/master/charts/gardener>Helm charts</a> in order to manage the various resources of the components.
Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.</p><h2 id=deploying-the-gardener-control-plane-api-server-admission-controller-controller-manager-scheduler>Deploying the Gardener Control Plane (API Server, Admission Controller, Controller Manager, Scheduler)</h2><p>The <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/values.yaml>configuration values</a> depict the various options to configure the different components.
Please consult <a href=/docs/gardener/usage/configuration/>Gardener Configuration and Usage</a> for component specific configurations and <a href=/docs/gardener/deployment/authentication_gardener_control_plane/>Authentication of Gardener Control Plane Components Against the Garden Cluster</a> for authentication related specifics.</p><p>Also, note that all resources and deployments need to be created in the <code>garden</code> namespace (not overrideable).
If you enable the Gardener admission controller as part of you setup, please make sure the <code>garden</code> namespace is labelled with <code>app: gardener</code>.
Otherwise, the backing service account for the admission controller Pod might not be created successfully.
No action is necessary if you deploy the <code>garden</code> namespace with the Gardener control plane Helm chart.</p><p>After preparing your values in a separate <code>controlplane-values.yaml</code> file (<a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/values.yaml>values.yaml</a> can be used as starting point), you can run the following command against your garden cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm install charts/gardener/controlplane <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --namespace garden <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --name gardener-controlplane <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  -f controlplane-values.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --wait
</span></span></code></pre></div><h2 id=deploying-gardener-extensions>Deploying Gardener Extensions</h2><p>Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.</p><p>You have to install extension controllers for these parts.
Please consult <a href=/docs/gardener/extensions/overview/>the documentation regarding extensions</a> to get more information.</p><h2 id=deploying-the-gardener-agent-gardenlet>Deploying the Gardener Agent (gardenlet)</h2><p>Please refer to <a href=/docs/gardener/deployment/deploy_gardenlet/>Deploying Gardenlets</a> on how to deploy a gardenlet.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9dd0ffa442dec2cdede353f4d427f770>1.3.14 - Version Skew Policy</h1><h1 id=version-skew-policy>Version Skew Policy</h1><p>This document describes the maximum version skew supported between various Gardener components.</p><h2 id=supported-gardener-versions>Supported Gardener Versions</h2><p>Gardener versions are expressed as <code>x.y.z</code>, where <code>x</code> is the major version, <code>y</code> is the minor version, and <code>z</code> is the patch version, following Semantic Versioning terminology.</p><p>The Gardener project maintains release branches for the three most recent minor releases.</p><p>Applicable fixes, including security fixes, may be backported to those three release branches, depending on severity and feasibility.
Patch releases are cut from those branches at a regular cadence, plus additional urgent releases when required.</p><p>For more information, see the <a href=/docs/gardener/development/process/#releases>Releases document</a>.</p><h3 id=supported-version-skew>Supported Version Skew</h3><p>Technically, we follow the same <a href=https://kubernetes.io/releases/version-skew-policy/>policy</a> as the Kubernetes project.
However, given that our release cadence is much more frequent compared to Kubernetes (every <code>14d</code> vs. every <code>120d</code>), in many cases it is possible to skip a version.
Still, to be on the safe side, it is highly recommended to follow the described policy.</p><h4 id=gardener-apiserver>gardener-apiserver</h4><p>In multi-instance setups of Gardener, the newest and oldest <code>gardener-apiserver</code> instances must be within one minor version.</p><p>Example:</p><ul><li>newest <code>gardener-apiserver</code> is at <strong>1.37</strong></li><li>other <code>gardener-apiserver</code> instances are supported at <strong>1.37</strong> and <strong>v1.36</strong></li></ul><h4 id=gardener-controller-manager-gardener-scheduler-gardener-admission-controller-gardenlet>gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet</h4><p><code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, and <code>gardenlet</code> must not be newer than the <code>gardener-apiserver</code> instances they communicate with.
They are expected to match the <code>gardener-apiserver</code> minor version, but may be up to one minor version older (to allow live upgrades).</p><p>Example:</p><ul><li><code>gardener-apiserver</code> is at <strong>v1.37</strong></li><li><code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, and <code>gardenlet</code> are supported at <strong>1.37</strong> and <strong>v1.36</strong></li></ul><h3 id=supported-component-upgrade-order>Supported Component Upgrade Order</h3><p>The supported version skew between components has implications on the order in which components must be upgraded.
This section describes the order in which components must be upgraded to transition an existing Gardener installation from version <strong>1.37</strong> to version <strong>1.38</strong>.</p><h4 id=gardener-apiserver-1>gardener-apiserver</h4><p>Prerequisites:</p><ul><li>In a single-instance setup, the existing <code>gardener-apiserver</code> instance is <strong>1.37</strong>.</li><li>In a multi-instance setup, all <code>gardener-apiserver</code> instances are at <strong>1.37</strong> or <strong>1.38</strong> (this ensures maximum skew of 1 minor version between the oldest and newest <code>gardener-apiserver</code> instance).</li><li>The <code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, and <code>gardenlet</code> instances that communicate with this <code>gardener-apiserver</code> are at version <strong>1.37</strong> (this ensures they are not newer than the existing API server version and are within 1 minor version of the new API server version).</li></ul><p>Actions:</p><ul><li>Upgrade <code>gardener-apiserver</code> to <strong>1.38</strong>.</li></ul><h4 id=gardener-controller-manager-gardener-scheduler-gardener-admission-controller-gardenlet-1>gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet</h4><p>Prerequisites:</p><ul><li>The <code>gardener-apiserver</code> instances these components communicate with are at <strong>1.38</strong> (in multi-instance setups in which these components can communicate with any <code>gardener-apiserver</code> instance in the cluster, all <code>gardener-apiserver</code> instances must be upgraded before upgrading these components)</li></ul><p>Actions:</p><ul><li>Upgrade <code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, and <code>gardenlet</code> to <strong>1.38</strong></li></ul><h2 id=supported-kubernetes-versions>Supported Kubernetes Versions</h2><p>Please refer to <a href=/docs/gardener/usage/supported_k8s_versions/>Supported Kubernetes Versions</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d4e003b162dfe49dc28bfcebcbeae620>1.4 - Development</h1></div><div class=td-content><h1 id=pg-d3de31ca8fda3468ee64b931363985a9>1.4.1 - Changing the API</h1><h1 id=changing-the-api>Changing the API</h1><p>This document describes the steps that need to be performed when changing the API.
It provides guidance for API changes to both (Gardener system in general or component configurations).</p><p>Generally, as Gardener is a Kubernetes-native extension, it follows the same API conventions and guidelines like Kubernetes itself. The Kubernetes
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md>API Conventions</a> as well as <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md>Changing the API</a> topics already provide a good overview and general explanation of the basic concepts behind it.
We are following the same approaches.</p><h2 id=gardener-api>Gardener API</h2><p>The Gardener API is defined in the <code>pkg/apis/{core,extensions,settings}</code> directories and is the main point of interaction with the system.
It must be ensured that the API is always backwards-compatible.</p><h3 id=changing-the-api-1>Changing the API</h3><p><strong>Checklist</strong> when changing the API:</p><ol><li>Modify the field(s) in the respective Golang files of all external versions and the internal version.<ol><li>Make sure new fields are being added as &ldquo;optional&rdquo; fields, i.e., they are of pointer types, they have the <code>// +optional</code> comment, and they have the <code>omitempty</code> JSON tag.</li><li>Make sure that the existing field numbers in the protobuf tags are not changed.</li></ol></li><li>If necessary, implement/adapt the conversion logic defined in the versioned APIs (e.g., <code>pkg/apis/core/v1beta1/conversions*.go</code>).</li><li>If necessary, implement/adapt defaulting logic defined in the versioned APIs (e.g., <code>pkg/apis/core/v1beta1/defaults*.go</code>).</li><li>Run the code generation: <code>make generate</code></li><li>If necessary, implement/adapt validation logic defined in the internal API (e.g., <code>pkg/apis/core/validation/validation*.go</code>).</li><li>If necessary, adapt the exemplary YAML manifests of the Gardener resources defined in <code>example/*.yaml</code>.</li><li>In most cases, it makes sense to add/adapt the documentation for administrators/operators and/or end-users in the <code>docs</code> folder to provide information on purpose and usage of the added/changed fields.</li><li>When opening the pull request, always add a release note so that end-users are becoming aware of the changes.</li></ol><h3 id=removing-a-field>Removing a Field</h3><p>If fields shall be removed permanently from the API, then a proper deprecation period must be adhered to so that end-users have enough time to adapt their clients.</p><p>Once the deprecation period is over, the field should be dropped from the API in a two-step process, i.e., in two release cycles. In the first step, all the usages in the code base should be dropped. In the second step, the field should be dropped from API. We need to follow this two-step process cause there can be the case where <code>gardener-apiserver</code> is upgraded to a new version in which the field has been removed but other controllers are still on the old version of Gardener. This can lead to <code>nil</code> pointer exceptions or other unexpected behaviour.</p><p>The steps for removing a field from the code base is:</p><ol><li><p>The field in the external version(s) has to be commented out with appropriate doc string that the protobuf number of the corresponding field is reserved. Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>-	SeedTemplate *gardencorev1beta1.SeedTemplate `json:&#34;seedTemplate,omitempty&#34; protobuf:&#34;bytes,2,opt,name=seedTemplate&#34;`
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>+	// SeedTemplate is tombstoned to show why 2 is reserved protobuf tag.
</span></span><span style=display:flex><span>+	// SeedTemplate *gardencorev1beta1.SeedTemplate `json:&#34;seedTemplate,omitempty&#34; protobuf:&#34;bytes,2,opt,name=seedTemplate&#34;`
</span></span></code></pre></div><p>The reasoning behind this is to prevent the same protobuf number being used by a new field. Introducing a new field with the same protobuf number would be a breaking change for clients still using the old protobuf definitions that have the old field for the given protobuf number.
The field in the internal version can be removed.</p></li><li><p>A unit test has to be added to make sure that a new field does not reuse the already reserved protobuf tag.</p></li></ol><p>Example of field removal can be found in the <a href=https://github.com/gardener/gardener/pull/6972>Remove <code>seedTemplate</code> field from ManagedSeed API</a> PR.</p><h2 id=component-configuration-apis>Component Configuration APIs</h2><p>Most Gardener components have a component configuration that follows similar principles to the Gardener API.
Those component configurations are defined in <code>pkg/{controllermanager,gardenlet,scheduler},pkg/apis/config</code>.
Hence, the above checklist also applies for changes to those APIs.
However, since these APIs are only used internally and only during the deployment of Gardener, the guidelines with respect to changes and backwards-compatibility are slightly relaxed.
If necessary, it is allowed to remove fields without a proper deprecation period if the release note uses the <code>breaking operator</code> keywords.</p><p>In addition to the above checklist:</p><ol><li>If necessary, then adapt the Helm chart of Gardener defined in <code>charts/gardener</code>. Adapt the <code>values.yaml</code> file as well as the manifest templates.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-71a5b626648208e387c23def0d131cde>1.4.2 - Component Checklist</h1><h1 id=checklist-for-adding-new-components>Checklist For Adding New Components</h1><p>Adding new components that run in the garden, seed, or shoot cluster is theoretically quite simple - we just need a <code>Deployment</code> (or other similar workload resource), the respective container image, and maybe a bit of configuration.
In practice, however, there are a couple of things to keep in mind in order to make the deployment production-ready.
This document provides a checklist for them that you can walk through.</p><h2 id=general>General</h2><ol><li><p><strong>Avoid usage of Helm charts</strong> (<a href=https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver>example</a>)</p><p>Nowadays, we use <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/interfaces.go>Golang components</a> instead of Helm charts for deploying components to a cluster.
Please find a typical structure of such components in the provided <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L80-L97>metrics_server.go</a> file (configuration values are typically managed in a <code>Values</code> structure).
There are a few exceptions (e.g., <a href=https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/istio>Istio</a>) still using charts, however the default should be using a Golang-based implementation.
For the exceptional cases, use Golang&rsquo;s <a href=https://pkg.go.dev/embed>embed</a> package to embed the Helm chart directory (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/istio/istiod.go#L51-L52>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/istio/istiod.go#L257-L273>example 2</a>).</p></li><li><p><strong>Choose the proper deployment way</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L210-L225>example 1 (direct application w/ client)</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L442-L484>example 2 (using <code>ManagedResource</code>)</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubestatemetrics/kube_state_metrics.go#L116>example 3 (mixed scenario)</a>)</p><p>For historic reasons, resources related to shoot control plane components are applied directly with the client.
All other resources (seed or shoot system components) are deployed via <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#managedresource-controller>Resource controller</a> (<code>ManagedResource</code>s) since it performs health checks out-of-the-box and has a lot of other features (see its documentation for more information).
Components that can run as both seed system component or shoot control plane component (e.g., VPA or <code>kube-state-metrics</code>) can make use of <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/resourceconfig.go>these utility functions</a>.</p></li><li><p><strong>Do not hard-code container image references</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/charts/images.yaml#L130-L133>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/metricsserver.go#L28-L31>example 2</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L82-L83>example 3</a>)</p><p>We define all image references centrally in the <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/charts/images.yaml><code>charts/images.yaml</code></a> file.
Hence, the image references must not be hard-coded in the pod template spec but read from this so-called <a href=/docs/gardener/deployment/image_vector/>image vector</a> instead.</p></li><li><p><strong>Do not use <code>docker.io</code> container registry</strong></p><p>The <code>docker.io</code> registry doesn&rsquo;t support pulling images over IPv6 (see <a href=https://www.docker.com/blog/beta-ipv6-support-on-docker-hub-registry/>Beta IPv6 Support on Docker Hub Registry</a>).
There is also a strict <a href=https://docs.docker.com/docker-hub/download-rate-limit/>rate-limit</a> that applies to the Docker Hub registry.
If you need an image from <code>docker.io</code>, please use the <a href=https://cloud.google.com/container-registry/docs/pulling-cached-images>Google Mirror</a> (<code>mirror.gcr.io</code>) instead to circumvent these issues.</p></li><li><p><strong>Use unique <code>ConfigMap</code>s/<code>Secret</code>s</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L181-L188>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L347>example 2</a>)</p><p><a href=https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable>Unique <code>ConfigMap</code>s/<code>Secret</code>s</a> are immutable for modification and have a unique name.
This has a couple of benefits, e.g. the <code>kubelet</code> doesn&rsquo;t watch these resources, and it is always clear which resource contains which data since it cannot be changed.
As a consequence, unique/immutable <code>ConfigMap</code>s/<code>Secret</code> are superior to checksum annotations on the pod templates.
Stale/unused <code>ConfigMap</code>s/<code>Secret</code>s are garbage-collected by <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#garbage-collector-for-immutable-configmapssecrets>GarbageCollector</a>.
There are utility functions (see examples above) for using unique <code>ConfigMap</code>s/<code>Secret</code>s in Golang components.
It is essential to inject the annotations into the workload resource to make the garbage-collection work.<br>Note that some <code>ConfigMap</code>s/<code>Secret</code>s should not be unique (e.g., those containing monitoring or logging configuration).
The reason is that the old revision stays in the cluster even if unused until the garbage-collector acts.
During this time, they would be wrongly aggregated to the full configuration.</p></li><li><p><strong>Manage certificates/secrets via <a href=https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/utils/secrets/manager>secrets manager</a></strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L100-L109>example</a>)</p><p>You should use the <a href=/docs/gardener/development/secrets_management/>secrets manager</a> for the management of any kind of credentials.
This makes sure that credentials rotation works out-of-the-box without you requiring to think about it.
Generally, do not use client certificates (see the <a href=#security>Security section</a>).</p></li><li><p><strong>Consider hibernation when calculating replica count</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/kubescheduler.go#L36>example</a>)</p><p>Shoot clusters can be <a href=/docs/gardener/usage/shoot_hibernate/>hibernated</a> meaning that all control plane components in the shoot namespace in the seed cluster are scaled down to zero and all worker nodes are terminated.
If your component runs in the seed cluster then you have to consider this case and provide the proper replica count.
There is a utility function available (see example).</p></li><li><p><strong>Ensure task dependencies are as precise as possible in shoot flows</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go#L508-L512>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot/reconciler_delete.go#L368-L372>example 2</a>)</p><p>Only define the minimum of needed dependency tasks in the <a href=https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot>shoot reconciliation/deletion flows</a>.</p></li><li><p><strong>Handle shoot system components</strong></p><p>Shoot system components deployed by <code>gardener-resource-manager</code> are labelled with <code>resource.gardener.cloud/managed-by: gardener</code>. This makes Gardener adding required label selectors and tolerations so that non-<code>DaemonSet</code> managed <code>Pod</code>s will exclusively run on selected nodes (for more information, see <a href=/docs/gardener/concepts/resource-manager/#system-components-webhook>System Components Webhook</a>).
<code>DaemonSet</code>s on the other hand, should generally tolerate any <code>NoSchedule</code> or <code>NoExecute</code> taints so that they can run on any <code>Node</code>, regardless of user added taints.</p></li></ol><h2 id=security>Security</h2><ol><li><p><strong>Use a <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>dedicated <code>ServiceAccount</code></a> and disable auto-mount</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L145-L151>example</a>)</p><p>Components that need to talk to the API server of their runtime cluster must always use a dedicated <code>ServiceAccount</code> (do not use <code>default</code>), with <code>automountServiceAccountToken</code> set to <code>false</code>.
This makes <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#tokeninvalidator>TokenInvalidator</a> invalidate the static token secret and its <a href=/docs/gardener/concepts/resource-manager/#auto-mounting-projected-serviceaccount-tokens><code>ProjectedTokenMount</code> webhook</a> inject a projected token automatically.</p></li><li><p><strong>Use shoot access tokens instead of a client certificates</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L227-L229>example</a>)</p><p>For components that need to talk to a target cluster different from their runtime cluster (e.g., running in seed cluster but talking to shoot) the <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor>TokenRequestor</a> should be used to manage a so-called &ldquo;shoot access token&rdquo;.</p></li><li><p><strong>Define RBAC roles with minimal privileges</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L153-L223>example</a>)</p><p>The component&rsquo;s <code>ServiceAccount</code> (if it exists) should have as little privileges as possible.
Consequently, please define proper <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>RBAC roles</a> for it.
This might include a combination of <code>ClusterRole</code>s and <code>Role</code>s.
Please do not provide elevated privileges due to laziness (e.g., because there is already a <code>ClusterRole</code> that can be extended vs. creating a <code>Role</code> only when access to a single namespace is needed).</p></li><li><p><strong>Use <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/><code>NetworkPolicy</code>s</a> to restrict network traffic</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/etcd/etcd.go#L293-L339>example</a>)</p><p>You should restrict both ingress and egress traffic to/from your component as much as possible to ensure that it only gets access to/from other components if really needed.
Gardener provides a few default policies for typical usage scenarios. For more information, see <a href=/docs/gardener/development/seed_network_policies/>Seed Network Policies</a> and <a href=/docs/gardener/usage/shoot_network_policies/>Shoot Network Policies</a>.</p></li><li><p><strong>Do not run components in privileged mode</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/nodelocaldns/nodelocaldns.go#L329-L333>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/nodelocaldns/nodelocaldns.go#L507>example 2</a>)</p><p>Avoid running components with <code>privileged=true</code>. Instead, define the needed <a href=https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container>Linux capabilities</a>.</p></li><li><p><strong>Choose the proper Seccomp profile</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/nodelocaldns/nodelocaldns.go#L285-L287>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/nginxingress/nginxingress.go#L427>example 2</a>)</p><p>The <a href=https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-seccomp-profile-for-a-container>Seccomp profile</a> will be defaulted by <code>gardener-resource-manager</code>&rsquo;s SeccompProfile webhook which works well for the majority of components.
However, in some special cases you might need to overwrite it.</p></li><li><p><strong>Define <code>PodSecurityPolicy</code>s</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/vpnshoot/vpnshoot.go#L445-L516>example</a>)</p><p><code>PodSecurityPolicy</code>s are deprecated, however Gardener still supports shoot clusters with older Kubernetes versions (<a href=/docs/gardener/usage/supported_k8s_versions/>ref</a>).
To make sure that such clusters can run with <code>.spec.kubernetes.allowPrivilegedContainers=false</code>, you have to define proper <code>PodSecurityPolicy</code>s.
For more information, see <a href=/docs/gardener/usage/pod-security/>Pod Security</a>.</p></li></ol><h2 id=high-availability--stability>High Availability / Stability</h2><ol><li><p><strong>Specify the component type label for high availability</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L234>example</a>)</p><p>To support high-availability deployments, <code>gardener-resource-manager</code>s <a href=/docs/gardener/concepts/resource-manager/#high-availability-config>HighAvailabilityConfig</a> webhook injects the proper specification like replica or topology spread constraints.
You only need to specify the type label. For more information, see <a href=/docs/gardener/development/high-availability/>High Availability Of Deployed Components</a>.</p></li><li><p><strong>Define a <code>PodDisruptionBudget</code></strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L398-L422>example</a>)</p><p>Closely related to high availability but also to stability in general: The definition of a <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/><code>PodDisruptionBudget</code></a> with <code>maxUnavailable=1</code> should be provided by default.</p></li><li><p><strong>Choose the right <code>PriorityClass</code></strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L301>example</a>)</p><p>Each cluster runs many components with different priorities.
Gardener provides a set of default <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass><code>PriorityClass</code>es</a>. For more information, see <a href=/docs/gardener/development/priority-classes/>Priority Classes</a>.</p></li><li><p><strong>Consider defining liveness and readiness probes</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L335-L358>example</a>)</p><p>To ensure smooth rolling update behaviour, consider the definition of <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>liveness and/or readiness probes</a>.</p></li><li><p><strong>Mark node-critical components</strong> (<a href=https://github.com/gardener/gardener/blob/release-v1.64/pkg/operation/botanist/component/kubeproxy/resources.go#L325>example</a>)</p><p>To ensure user workload pods are only scheduled to <code>Nodes</code> where all node-critical components are ready, these components need to tolerate the <code>node.gardener.cloud/critical-components-not-ready</code> taint (<code>NoSchedule</code> effect).
Also, such <code>DaemonSets</code> and the included <code>PodTemplates</code> need to be labelled with <code>node.gardener.cloud/critical-component=true</code>.
For more information, see <a href=/docs/gardener/usage/node-readiness/>Readiness of Shoot Worker Nodes</a>.</p></li></ol><h2 id=scalability>Scalability</h2><ol><li><p><strong>Provide resource requirements</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L359-L367>example</a>)</p><p>All components should have <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>resource requirements</a>.
Generally, they should always request CPU and memory, while only memory shall be limited (no CPU limits!).</p></li><li><p><strong>Define a <code>VerticalPodAutoscaler</code></strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L424-L460>example</a>)</p><p>We typically perform vertical auto-scaling via the VPA managed by the <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>Kubernetes community</a>.
Each component should have a respective <code>VerticalPodAutoscaler</code> with &ldquo;min allowed&rdquo; resources, &ldquo;auto update mode&rdquo;, and &ldquo;requests only&rdquo;-mode.
VPA is always enabled in garden or seed clusters, while it is optional for shoot clusters.</p></li><li><p><strong>Define a <code>HorizontalPodAutoscaler</code> if needed</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/coredns/coredns.go#L689-L738>example</a>)</p><p>If your component is capable of scaling horizontally, you should consider defining a <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/><code>HorizontalPodAutoscaler</code></a>.</p></li></ol><h2 id=observability--operations-productivity>Observability / Operations Productivity</h2><ol><li><p><strong>Provide monitoring scrape config and alerting rules</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/coredns/monitoring.go>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/monitoring.go#L97>example 2</a>)</p><p>Components should provide scrape configuration and alerting rules for Prometheus/Alertmanager if appropriate.
This should be done inside a dedicated <code>monitoring.go</code> file.
Extensions should follow the guidelines described in <a href=/docs/gardener/extensions/logging-and-monitoring/#extensions-monitoring-integration>Extensions Monitoring Integration</a>.</p></li><li><p><strong>Provide logging parsers and filters</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/coredns/logging.go>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/seed/seed/reconciler_reconcile.go#L563>example 2</a>)</p><p>Components should provide parsers and filters for fluent-bit, if appropriate.
This should be done inside a dedicated <code>logging.go</code> file.
Extensions should follow the guidelines described in <a href=/docs/gardener/extensions/logging-and-monitoring/#fluent-bit-log-parsers-and-filters>Fluent-bit log parsers and filters</a>.</p></li><li><p><strong>Set the <code>revisionHistoryLimit</code> to <code>2</code> for <code>Deployment</code>s</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/metricsserver/metrics_server.go#L273>example</a>)</p><p>In order to allow easy inspection of two <code>ReplicaSet</code>s to quickly find the changes that lead to a rolling update, the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#revision-history-limit>revision history limit</a> should be set to <code>2</code>.</p></li><li><p><strong>Define health checks</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/care/checker.go#L45-L71>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/care/seed_health.go#L46-L54>example 2</a>)</p><p><code>gardenlet</code>&rsquo;s <a href=/docs/gardener/concepts/gardenlet/#controllers>care controllers</a> regularly check the health status of system or control plane components.
You need to enhance the lists of components to check if your component related to the seed system or shoot control plane (shoot system components are automatically checked via their respective <a href=/docs/gardener/concepts/resource-manager/#managedresource-controller><code>ManagedResource</code> conditions</a>), see examples above.</p></li><li><p><strong>Configure automatic restarts in shoot maintenance time window</strong> (<a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/component/kubescheduler/kube_scheduler.go#L243>example 1</a>, <a href=https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/coredns.go#L90-L107>example 2</a>)</p><p>Gardener offers to restart components during the maintenance time window. For more information, see <a href=/docs/gardener/usage/shoot_maintenance/#restart-control-plane-controllers>Restart Control Plane Controllers</a> and <a href=/docs/gardener/usage/shoot_maintenance/#restart-some-core-addons>Restart Some Core Addons</a>.
You can consider adding the needed label to your control plane component to get this automatic restart (probably not needed for most components).</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-58950a0fdac8b00f91fdd5b644642c8f>1.4.3 - Dependencies</h1><h1 id=dependency-management>Dependency Management</h1><p>We are using <a href=https://github.com/golang/go/wiki/Modules>go modules</a> for depedency management.
In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h2 id=updating-dependencies>Updating Dependencies</h2><p>The <code>Makefile</code> contains a rule called <code>revendor</code> which performs <code>go mod tidy</code> and <code>go mod vendor</code>:</p><ul><li><code>go mod tidy</code> makes sure <code>go.mod</code> matches the source code in the module. It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</li><li><code>go mod vendor</code> resets the main module&rsquo;s vendor directory to include all packages needed to build and test all the main module&rsquo;s packages. It does not include test code for vendored packages.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make revendor
</span></span></code></pre></div><p>The dependencies are installed into the <code>vendor</code> folder, which <strong>should be added</strong> to the VCS.</p><p>⚠️ Make sure that you test the code after you have updated the dependencies!</p><h2 id=exported-packages>Exported Packages</h2><p>This repository contains several packages that could be considered &ldquo;exported packages&rdquo;, in a sense that they are supposed to be reused in other Go projects.
For example:</p><ul><li>Gardener&rsquo;s API packages: <code>pkg/apis</code></li><li>Library for building Gardener extensions: <code>extensions</code></li><li>Gardener&rsquo;s Test Framework: <code>test/framework</code></li></ul><p>There are a few more folders in this repository (non-Go sources) that are reused across projects in the Gardener organization:</p><ul><li>GitHub templates: <code>.github</code></li><li>Concourse / cc-utils related helpers: <code>hack/.ci</code></li><li>Development, build and testing helpers: <code>hack</code></li></ul><p>These packages feature a dummy <code>doc.go</code> file to allow other Go projects to pull them in as go mod dependencies.</p><p>These packages are explicitly <em>not</em> supposed to be used in other projects (consider them as &ldquo;non-exported&rdquo;):</p><ul><li>API validation packages: <code>pkg/apis/*/*/validation</code></li><li>Operation package (main Gardener business logic regarding <code>Seed</code> and <code>Shoot</code> clusters): <code>pkg/operation</code></li><li>Third party code: <code>third_party</code></li></ul><p>Currently, we don&rsquo;t have a mechanism yet for selectively syncing out these exported packages into dedicated repositories like kube&rsquo;s <a href=https://github.com/kubernetes/kubernetes/tree/master/staging>staging mechanism</a> (<a href=https://github.com/kubernetes/publishing-bot>publishing-bot</a>).</p><h2 id=import-restrictions>Import Restrictions</h2><p>We want to make sure that other projects can depend on this repository&rsquo;s &ldquo;exported&rdquo; packages without pulling in the entire repository (including &ldquo;non-exported&rdquo; packages) or a high number of other unwanted dependencies.
Hence, we have to be careful when adding new imports or references between our packages.</p><blockquote><p>ℹ️ General rule of thumb: the mentioned &ldquo;exported&rdquo; packages should be as self-contained as possible and depend on as few other packages in the repository and other projects as possible.</p></blockquote><p>In order to support that rule and automatically check compliance with that goal, we leverage <a href=https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss>import-boss</a>.
The tool checks all imports of the given packages (including transitive imports) against rules defined in <code>.import-restrictions</code> files in each directory.
An import is allowed if it matches at least one allowed prefix and does not match any forbidden prefixes.</p><blockquote><p>Note: <code>''</code> (the empty string) is a prefix of everything.
For more details, see the <a href=https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss/README.md>import-boss</a> topic.</p></blockquote><p><code>import-boss</code> is executed on every pull request and blocks the PR if it doesn&rsquo;t comply with the defined import restrictions.
You can also run it locally using <code>make check</code>.</p><p>Import restrictions should be changed in the following situations:</p><ul><li>We spot a new pattern of imports across our packages that was not restricted before but makes it more difficult for other projects to depend on our &ldquo;exported&rdquo; packages.
In that case, the imports should be further restricted to disallow such problematic imports, and the code/package structure should be reworked to comply with the newly given restrictions.</li><li>We want to share code between packages, but existing import restrictions prevent us from doing so.
In that case, please consider what additional dependencies it will pull in, when loosening existing restrictions.
Also consider possible alternatives, like code restructurings or extracting shared code into dedicated packages for minimal impact on dependent projects.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-94e3f754736ad20216bc6c1d11abac6f>1.4.4 - Getting Started Locally</h1><h1 id=developing-gardener-locally>Developing Gardener Locally</h1><p>This document will walk you through running Gardener on your local machine for development purposes.
If you encounter difficulties, please open an issue so that we can make this process easier.</p><p>Gardener runs in any Kubernetes cluster.
In this guide, we will start a <a href=https://kind.sigs.k8s.io/>KinD</a> cluster which is used as both garden and seed cluster (please refer to the <a href=/docs/gardener/concepts/architecture/>architecture overview</a>) for simplicity.</p><p>The Gardener components, however, will be run as regular processes on your machine (hence, no container images are being built).</p><p><img src=/__resources/getting_started_locally_39a5b3.png alt="Architecture Diagram"></p><h2 id=alternatives>Alternatives</h2><p>When developing Gardener on your local machine you might face several limitations:</p><ul><li>Your machine doesn&rsquo;t have enough compute resources (see <a href=#prerequisites>prerequisites</a>) for hosting a second seed cluster or multiple shoot clusters.</li><li>Developing Gardener&rsquo;s <a href=/docs/gardener/usage/ipv6/>IPv6 features</a> requires a Linux machine and native IPv6 connectivity to the internet, but you&rsquo;re on macOS or don&rsquo;t have IPv6 connectivity in your office environment or via your home ISP.</li></ul><p>In these cases, you might want to check out one of the following options that run the setup described in this guide elsewhere for circumventing these limitations:</p><ul><li><a href=#remote-local-setup>remote local setup</a>: develop on a remote pod for more compute resources</li><li><a href=https://github.com/gardener-community/dev-box-gcp>dev box on Google Cloud</a>: develop on a Google Cloud machine for more compute resource and/or simple IPv4/IPv6 dual-stack networking</li></ul><h2 id=prerequisites>Prerequisites</h2><ul><li><p>Make sure that you have followed the <a href=/docs/gardener/development/local_setup/>Local Setup guide</a> up until the <a href=/docs/gardener/development/local_setup/#get-the-sources>Get the sources</a> step.</p></li><li><p>Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least <code>4</code> CPUs and <code>4Gi</code> memory; see <a href=https://docs.docker.com/desktop/mac/#resources>here</a> how to configure the resources for Docker for Mac).</p><blockquote><p>Please note that 4 CPU / 4Gi memory might not be enough for more than one <code>Shoot</code> cluster, i.e., you might need to increase these values if you want to run additional <code>Shoot</code>s.
If you plan on following the optional steps to <a href=#optional-setting-up-a-second-seed-cluster>create a second seed cluster</a>, the required resources will be more - at least <code>10</code> CPUs and <code>16Gi</code> memory.</p></blockquote><p>Additionally, please configure at least <code>120Gi</code> of disk size for the Docker daemon.</p><blockquote><p>Tip: With <code>docker system df</code> and <code>docker system prune -a</code> you can cleanup unused data.</p></blockquote></li><li><p>Make sure the <code>kind</code> docker network is using the CIDR <code>172.18.0.0/16</code>.</p><ul><li>If the network does not exist, it can be created with <code>docker network create kind --subnet 172.18.0.0/16</code></li><li>If the network already exists, the CIDR can be checked with <code>docker network inspect kind | jq '.[].IPAM.Config[].Subnet'</code>. If it is not <code>172.18.0.0/16</code>, delete the network with <code>docker network rm kind</code> and create it with the command above.</li></ul></li><li><p>Make sure that you increase the maximum number of open files on your host:</p><ul><li><p>On Mac, run <code>sudo launchctl limit maxfiles 65536 200000</code></p></li><li><p>On Linux, extend the <code>/etc/security/limits.conf</code> file with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>* hard nofile 97816
</span></span><span style=display:flex><span>* soft nofile 97816
</span></span></code></pre></div><p>and reload the terminal.</p></li></ul></li></ul><h2 id=setting-up-the-kind-cluster-garden-and-seed>Setting Up the KinD Cluster (Garden and Seed)</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-up KIND_ENV=local
</span></span></code></pre></div><blockquote><p>If you want to setup an IPv6 KinD cluster, use <code>make kind-up IPFAMILY=ipv6</code> instead.</p></blockquote><p>This command sets up a new KinD cluster named <code>gardener-local</code> and stores the kubeconfig in the <code>./example/gardener-local/kind/local/kubeconfig</code> file.</p><blockquote><p>It might be helpful to copy this file to <code>$HOME/.kube/config</code> since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your <code>KUBECONFIG</code> environment variable to <code>./example/gardener-local/kind/local/kubeconfig</code> for all future steps via <code>export KUBECONFIG=example/gardener-local/kind/local/kubeconfig</code>.</p></blockquote><p>All following steps assume that you are using this kubeconfig.</p><p>Additionally, this command also deploys a local container registry to the cluster as well as a few registry mirrors, that are set up as a pull-through cache for all upstream registries Gardener uses by default.
This is done to speed up image pulls across local clusters.
The local registry can be accessed as <code>localhost:5001</code> for pushing and pulling.
The storage directories of the registries are mounted to the host machine under <code>dev/local-registry</code>.
With this, mirrored images don&rsquo;t have to be pulled again after recreating the cluster.</p><p>The command also deploys a default <a href=https://github.com/projectcalico/calico>calico</a> installation as the cluster&rsquo;s CNI implementation with <code>NetworkPolicy</code> support (the default <code>kindnet</code> CNI doesn&rsquo;t provide <code>NetworkPolicy</code> support).
Furthermore, it deploys the <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a> in order to support HPA and VPA on the seed cluster.</p><h2 id=outgoing-ipv6-single-stack-networking-optional>Outgoing IPv6 Single-Stack Networking (optional)</h2><p>If you want to test IPv6-related features, we need to configure NAT for outgoing traffic from the kind network to the internet.
After <code>make kind-up IPFAMILY=ipv6</code>, check the network created by kind:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ docker network inspect kind | jq <span style=color:#a31515>&#39;.[].IPAM.Config[].Subnet&#39;</span>
</span></span><span style=display:flex><span><span style=color:#a31515>&#34;172.18.0.0/16&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a31515>&#34;fc00:f853:ccd:e793::/64&#34;</span>
</span></span></code></pre></div><p>Determine which device is used for outgoing internet traffic by looking at the default route:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ip route show default
</span></span><span style=display:flex><span>default via 192.168.195.1 dev enp3s0 proto dhcp src 192.168.195.34 metric 100
</span></span></code></pre></div><p>Configure NAT for traffic from the kind cluster to the internet using the IPv6 range and the network device from the previous two steps:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ip6tables -t nat -A POSTROUTING -o enp3s0 -s fc00:f853:ccd:e793::/64 -j MASQUERADE
</span></span></code></pre></div><h2 id=setting-up-gardener>Setting Up Gardener</h2><p>In a terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make dev-setup                                                                <span style=color:green># preparing the environment (without webhooks for now)</span>
</span></span><span style=display:flex><span>kubectl wait --for=condition=ready pod -l run=etcd -n garden --timeout 2m     <span style=color:green># wait for etcd to be ready</span>
</span></span><span style=display:flex><span>make start-apiserver                                                          <span style=color:green># starting gardener-apiserver</span>
</span></span></code></pre></div><p>In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl wait --for=condition=available apiservice v1beta1.core.gardener.cloud <span style=color:green># wait for gardener-apiserver to be ready</span>
</span></span><span style=display:flex><span>make start-admission-controller                                               <span style=color:green># starting gardener-admission-controller</span>
</span></span></code></pre></div><p>In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make dev-setup DEV_SETUP_WITH_WEBHOOKS=true                                   <span style=color:green># preparing the environment with webhooks</span>
</span></span><span style=display:flex><span>make start-controller-manager                                                 <span style=color:green># starting gardener-controller-manager</span>
</span></span></code></pre></div><p>(Optional): In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-scheduler                                                          <span style=color:green># starting gardener-scheduler</span>
</span></span></code></pre></div><p>In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make register-local-env                                                       <span style=color:green># registering the local environment (CloudProfile, Seed, etc.)</span>
</span></span><span style=display:flex><span>make start-gardenlet SEED_NAME=local                                          <span style=color:green># starting gardenlet</span>
</span></span></code></pre></div><p>In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-extension-provider-local                                           <span style=color:green># starting gardener-extension-provider-local</span>
</span></span></code></pre></div><p>ℹ️ The <a href=/docs/gardener/extensions/provider-local/><code>provider-local</code></a> is started with elevated privileges since it needs to manipulate your <code>/etc/hosts</code> file to enable you accessing the created shoot clusters from your local machine, see <a href=/docs/gardener/extensions/provider-local/#dnsrecord>this</a> for more details.</p><h2 id=creating-a-shoot-cluster>Creating a <code>Shoot</code> Cluster</h2><p>You can wait for the <code>Seed</code> to become ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/usage/wait-for.sh seed local GardenletReady Bootstrapped SeedSystemComponentsHealthy ExtensionsReady
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl get seed local</code> and wait for the <code>STATUS</code> to indicate readiness:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME    STATUS   PROVIDER   REGION   AGE     VERSION       K8S VERSION
</span></span><span style=display:flex><span>local   Ready    local      local    4m42s   vX.Y.Z-dev    v1.21.1
</span></span></code></pre></div><p>In order to create a first shoot cluster, just run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f example/provider-local/shoot.yaml
</span></span></code></pre></div><p>You can wait for the <code>Shoot</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAMESPACE=garden-local ./hack/usage/wait-for.sh shoot local APIServerAvailable ControlPlaneHealthy ObservabilityComponentsHealthy EveryNodeReady SystemComponentsHealthy
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl -n garden-local get shoot local</code> and wait for the <code>LAST OPERATION</code> to reach <code>100%</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME    CLOUDPROFILE   PROVIDER   REGION   K8S VERSION   HIBERNATION   LAST OPERATION            STATUS    AGE
</span></span><span style=display:flex><span>local   local          local      local    1.21.0        Awake         Create Processing (43%)   healthy   94s
</span></span></code></pre></div><p>(Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e-local-simple KUBECONFIG=<span style=color:#a31515>&#34;</span>$PWD<span style=color:#a31515>/example/gardener-local/kind/local/kubeconfig&#34;</span>
</span></span></code></pre></div><p>When the <code>Shoot</code> got created successfully, you can acquire a <code>kubeconfig</code> by using the <a href=/docs/gardener/usage/shoot_access/#shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> subresource</a> to access the cluster.</p><h2 id=optional-setting-up-a-second-seed-cluster>(Optional): Setting Up a Second Seed Cluster</h2><p>There are cases where you would want to create a second seed cluster in your local setup. For example, if you want to test the <a href=/docs/gardener/usage/control_plane_migration/>control plane migration</a> feature. The following steps describe how to do that.</p><p>If you are on macOS, add a new IP address on your loopback device which will be necessary for the new KinD cluster that you will create. On macOS, the default loopback device is <code>lo0</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ip addr add 127.0.0.2 dev lo0                                     <span style=color:green># adding 127.0.0.2 ip to the loopback interface</span>
</span></span></code></pre></div><p>Next, setup the second KinD cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind2-up KIND_ENV=local
</span></span></code></pre></div><p>This command sets up a new KinD cluster named <code>gardener-local2</code> and stores its kubeconfig in the <code>./example/gardener-local/kind/local2/kubeconfig</code> file. You will need this file when starting the <code>provider-local</code> extension controller for the second seed cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make register-kind2-env                                           <span style=color:green># registering the local2 seed</span>
</span></span><span style=display:flex><span>make start-gardenlet SEED_NAME=local2                             <span style=color:green># starting gardenlet for the local2 seed</span>
</span></span></code></pre></div><p>In a new terminal pane, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export KUBECONFIG=./example/gardener-local/kind/local2/kubeconfig       <span style=color:green># setting KUBECONFIG to point to second kind cluster</span>
</span></span><span style=display:flex><span>make start-extension-provider-local <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  WEBHOOK_SERVER_PORT=9444 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  WEBHOOK_CERT_DIR=/tmp/gardener-extension-provider-local2 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  SERVICE_HOST_IP=127.0.0.2 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  METRICS_BIND_ADDRESS=:8082 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  HEALTH_BIND_ADDRESS=:8083                                       <span style=color:green># starting gardener-extension-provider-local</span>
</span></span></code></pre></div><p>If you want to perform a control plane migration you can follow the steps outlined in the <a href=/docs/gardener/usage/control_plane_migration/>Control Plane Migration</a> topic to migrate the shoot cluster to the second seed you just created.</p><h2 id=deleting-the-shoot-cluster>Deleting the <code>Shoot</code> Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./hack/usage/delete shoot local garden-local
</span></span></code></pre></div><h2 id=optional-tear-down-the-second-seed-cluster>(Optional): Tear Down the Second Seed Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make tear-down-kind2-env
</span></span><span style=display:flex><span>make kind2-down
</span></span></code></pre></div><h2 id=tear-down-the-gardener-environment>Tear Down the Gardener Environment</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make tear-down-local-env
</span></span><span style=display:flex><span>make kind-down
</span></span></code></pre></div><h2 id=remote-local-setup>Remote Local Setup</h2><p>Just like Prow is executing the KinD based integration tests in a K8s pod, it is
possible to interactively run this KinD based Gardener development environment
aka &ldquo;local setup&rdquo; in a &ldquo;remote&rdquo; K8s pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k apply -f docs/development/content/remote-local-setup.yaml
</span></span><span style=display:flex><span>k exec -it deployment/remote-local-setup -- sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tmux -u a
</span></span></code></pre></div><h3 id=caveats>Caveats</h3><p>Please refer to the <a href=https://github.com/tmux/tmux/wiki>TMUX documentation</a> for
working effectively inside the remote-local-setup pod.</p><p>To access Grafana, Prometheus, or other components in a browser, two port forwards are needed:</p><p>The port forward from the laptop to the pod:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k port-forward deployment/remote-local-setup 3000
</span></span></code></pre></div><p>The port forward in the remote-local-setup pod to the respective component:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>k port-forward -n shoot--local--local deployment/grafana 3000
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/extensions/provider-local/>Local Provider Extension</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-98608cf34dc96853f47fc158ed5e0886>1.4.5 - High Availability</h1><h1 id=high-availability-of-deployed-components>High Availability of Deployed Components</h1><p><code>gardenlet</code>s and extension controllers are deploying components via <code>Deployment</code>s, <code>StatefulSet</code>s, etc., as part of the shoot control plane, or the seed or shoot system components.</p><p>Some of the above component deployments must be further tuned to improve fault tolerance / resilience of the service. This document outlines what needs to be done to achieve this goal.</p><p>Please be forwarded to the <a href=#convenient-application-of-these-rules>Convenient Application Of These Rules</a> section, if you want to take a shortcut to the list of actions that require developers&rsquo; attention.</p><h2 id=seed-clusters>Seed Clusters</h2><p>The worker nodes of seed clusters can be deployed to one or multiple availability zones.
The <code>Seed</code> specification allows you to provide the information which zones are available:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    region: europe-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - europe-1a
</span></span><span style=display:flex><span>    - europe-1b
</span></span><span style=display:flex><span>    - europe-1c
</span></span></code></pre></div><p>Independent of the number of zones, seed system components like the <code>gardenlet</code> or the extension controllers themselves, or others like <code>etcd-druid</code>, <code>dependency-watchdog</code>, etc., should always be running with multiple replicas.</p><p>Concretely, all seed system components should respect the following conventions:</p><ul><li><p><strong>Replica Counts</strong></p><table><thead><tr><th>Component Type</th><th><code>&lt; 3</code> Zones</th><th><code>>= 3</code> Zones</th><th>Comment</th></tr></thead><tbody><tr><td>Observability (Monitoring, Logging)</td><td>1</td><td>1</td><td>Downtimes accepted due to cost reasons</td></tr><tr><td>Controllers</td><td>2</td><td>2</td><td>/</td></tr><tr><td>(Webhook) Servers</td><td>2</td><td>2</td><td>/</td></tr></tbody></table><p>Apart from the above, there might be special cases where these rules do not apply, for example:</p><ul><li><code>istio-ingressgateway</code> is scaled horizontally, hence the above numbers are the minimum values.</li><li><code>nginx-ingress-controller</code> in the seed cluster is used to advertise all shoot observability endpoints, so due to performance reasons it runs with <code>2</code> replicas at all times. In the future, this component might disappear in favor of the <code>istio-ingressgateway</code> anyways.</li></ul></li><li><p><strong>Topology Spread Constraints</strong></p><p>When the component has <code>>= 2</code> replicas &mldr;</p><ul><li><p>&mldr; then it should also have a <code>topologySpreadConstraint</code>, ensuring the replicas are spread over the nodes:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div><p>Hence, the node spread is done on best-effort basis only.</p></li><li><p>&mldr; and the seed cluster has <code>>= 2</code> zones, then the component should also have a second <code>topologySpreadConstraint</code>, ensuring the replicas are spread over the zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div></li></ul></li></ul><blockquote><p>According to these conventions, even seed clusters with only one availability zone try to be highly available &ldquo;as good as possible&rdquo; by spreading the replicas across multiple nodes.
Hence, while such seed clusters obviously cannot handle zone outages, they can at least handle node failures.</p></blockquote><h2 id=shoot-clusters>Shoot Clusters</h2><p>The <code>Shoot</code> specification allows configuring &ldquo;high availability&rdquo; as well as the failure tolerance type for the control plane components, see <a href=/docs/gardener/usage/shoot_high_availability/>Highly Available Shoot Control Plane</a> for details.</p><p>Regarding the seed cluster selection, the only constraint is that shoot clusters with failure tolerance type <code>zone</code> are only allowed to run on seed clusters with at least three zones.
All other shoot clusters (non-HA or those with failure tolerance type <code>node</code>) can run on seed clusters with any number of zones.</p><h3 id=control-plane-components>Control Plane Components</h3><p>All control plane components should respect the following conventions:</p><ul><li><p><strong>Replica Counts</strong></p><table><thead><tr><th>Component Type</th><th>w/o HA</th><th>w/ HA (<code>node</code>)</th><th>w/ HA (<code>zone</code>)</th><th>Comment</th></tr></thead><tbody><tr><td>Observability (Monitoring, Logging)</td><td>1</td><td>1</td><td>1</td><td>Downtimes accepted due to cost reasons</td></tr><tr><td>Controllers</td><td>1</td><td>2</td><td>2</td><td>/</td></tr><tr><td>(Webhook) Servers</td><td>2</td><td>2</td><td>2</td><td>/</td></tr></tbody></table><p>Apart from the above, there might be special cases where these rules do not apply, for example:</p><ul><li><code>etcd</code> is a server, though the most critical component of a cluster requiring a quorum to survive failures. Hence, it should have <code>3</code> replicas even when the failure tolerance is <code>node</code> only.</li><li><code>kube-apiserver</code> is scaled horizontally, hence the above numbers are the minimum values (even when the shoot cluster is not HA, there might be multiple replicas).</li></ul></li><li><p><strong>Topology Spread Constraints</strong></p><p>When the component has <code>>= 2</code> replicas &mldr;</p><ul><li><p>&mldr; then it should also have a <code>topologySpreadConstraint</code> ensuring the replicas are spread over the nodes:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div><p>Hence, the node spread is done on best-effort basis only.</p><p>However, if the shoot cluster has defined a failure tolerance type, the <code>whenUnsafisfiable</code> field should be set to <code>DoNotSchedule</code>.</p></li><li><p>&mldr; and the failure tolerance type of the shoot cluster is <code>zone</code>, then the component should also have a second <code>topologySpreadConstraint</code> ensuring the replicas are spread over the zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div></li></ul></li><li><p><strong>Node Affinity</strong></p><p>The <code>gardenlet</code> annotates the shoot namespace in the seed cluster with the <code>high-availability-config.resources.gardener.cloud/zones</code> annotation.</p><ul><li>If the shoot cluster is non-HA or has failure tolerance type <code>node</code>, then the value will be always exactly one zone (e.g., <code>high-availability-config.resources.gardener.cloud/zones=europe-1b</code>).</li><li>If the shoot cluster has failure tolerance type <code>zone</code>, then the value will always contain exactly three zones (e.g., <code>high-availability-config.resources.gardener.cloud/zones=europe-1a,europe-1b,europe-1c</code>).</li></ul><p>For backwards-compatibility, this annotation might contain multiple zones for shoot clusters created before <code>gardener/gardener@v1.60</code> and not having failure tolerance type <code>zone</code>.
This is because their volumes might already exist in multiple zones, hence pinning them to only one zone would not work.</p><p>Hence, in case this annotation is present, the components should have the following node affinity:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    nodeAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>        nodeSelectorTerms:
</span></span><span style=display:flex><span>        - matchExpressions:
</span></span><span style=display:flex><span>          - key: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>            - europe-1a
</span></span><span style=display:flex><span>          <span style=color:green># - ...</span>
</span></span></code></pre></div><p>This is to ensure all pods are running in the same (set of) availability zone(s) such that cross-zone network traffic is avoided as much as possible (such traffic is typically charged by the underlying infrastructure provider).</p></li></ul><h3 id=system-components>System Components</h3><p>The availability of system components is independent of the control plane since they run on the shoot worker nodes while the control plane components run on the seed worker nodes (for more information, see the <a href=/docs/gardener/concepts/architecture/>Kubernetes architecture overview</a>).
Hence, it only depends on the number of availability zones configured in the shoot worker pools via <code>.spec.provider.workers[].zones</code>.
Concretely, the highest number of zones of a worker pool with <code>systemComponents.allow=true</code> is considered.</p><p>All system components should respect the following conventions:</p><ul><li><p><strong>Replica Counts</strong></p><table><thead><tr><th>Component Type</th><th><code>1</code> or <code>2</code> Zones</th><th><code>>= 3</code> Zones</th></tr></thead><tbody><tr><td>Controllers</td><td>2</td><td>2</td></tr><tr><td>(Webhook) Servers</td><td>2</td><td>2</td></tr></tbody></table><p>Apart from the above, there might be special cases where these rules do not apply, for example:</p><ul><li><code>coredns</code> is scaled horizontally (today), hence the above numbers are the minimum values (possibly, scaling these components vertically may be more appropriate, but that&rsquo;s unrelated to the HA subject matter).</li><li>Optional addons like <code>nginx-ingress</code> or <code>kubernetes-dashboard</code> are only provided on best-effort basis for evaluation purposes, hence they run with <code>1</code> replica at all times.</li></ul></li><li><p><strong>Topology Spread Constraints</strong></p><p>When the component has <code>>= 2</code> replicas &mldr;</p><ul><li><p>&mldr; then it should also have a <code>topologySpreadConstraint</code> ensuring the replicas are spread over the nodes:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div><p>Hence, the node spread is done on best-effort basis only.</p></li><li><p>&mldr; and the cluster has <code>>= 2</code> zones, then the component should also have a second <code>topologySpreadConstraint</code> ensuring the replicas are spread over the zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div></li></ul></li></ul><h2 id=convenient-application-of-these-rules>Convenient Application of These Rules</h2><p>According to above scenarios and conventions, the <code>replicas</code>, <code>topologySpreadConstraints</code> or <code>affinity</code> settings of the deployed components might need to be adapted.</p><p>In order to apply those conveniently and easily for developers, Gardener installs a mutating webhook into both seed and shoot clusters which reacts on <code>Deployment</code>s and <code>StatefulSet</code>s deployed to namespaces with the <code>high-availability-config.resources.gardener.cloud/consider=true</code> label set.</p><p><strong>The following actions have to be taken by developers:</strong></p><ol><li><p>Check if <code>components</code> are prepared to run concurrently with multiple replicas, e.g. controllers usually use <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/leaderelection>leader election</a> to achieve this.</p></li><li><p>All components should be generally equipped with <code>PodDisruptionBudget</code>s with <code>.spec.maxUnavailable=1</code>:</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maxUnavailable: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels: ...
</span></span></code></pre></div><ol start=3><li>Add the label <code>high-availability-config.resources.gardener.cloud/type</code> to <code>deployment</code>s or <code>statefulset</code>s, as well as optionally involved <code>horizontalpodautoscaler</code>s or <code>HVPA</code>s where the following two values are possible:</li></ol><ul><li><code>controller</code></li><li><code>server</code></li></ul><p>Type <code>server</code> is also preferred if a component is a controller and (webhook) server at the same time.</p><p>You can read more about the webhook&rsquo;s internals in <a href=/docs/gardener/concepts/resource-manager/#high-availability-config>High Availability Config</a>.</p><h2 id=gardenlet-internals><code>gardenlet</code> Internals</h2><p>Make sure you have read the above document about the webhook internals before continuing reading this section.</p><h3 id=seed-controller><code>Seed</code> Controller</h3><p>The <code>gardenlet</code> performs the following changes on all namespaces running seed system components:</p><ul><li>adds the label <code>high-availability-config.resources.gardener.cloud/consider=true</code>.</li><li>adds the annotation <code>high-availability-config.resources.gardener.cloud/zones=&lt;zones></code>, where <code>&lt;zones></code> is the list provided in <code>.spec.provider.zones[]</code> in the <code>Seed</code> specification.</li></ul><p>Note that neither the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code>, nor the <code>high-availability-config.resources.gardener.cloud/zone-pinning</code> annotations are set, hence the node affinity would never be touched by the webhook.</p><p>The only exception to this rule are the istio ingress gateway namespaces. This includes the default istio ingress gateway when SNI is enabled, as well as analogous namespaces for exposure classes and zone-specific istio ingress gateways. Those namespaces
will additionally be annotated with <code>high-availability-config.resources.gardener.cloud/zone-pinning</code> set to <code>true</code>, resulting in the node affinities and the topology spread constraints being set. The replicas are not touched, as the istio ingress gateways
are scaled by a horizontal autoscaler instance.</p><h3 id=shoot-controller><code>Shoot</code> Controller</h3><h4 id=control-plane>Control Plane</h4><p>The <code>gardenlet</code> performs the following changes on the namespace running the shoot control plane components:</p><ul><li>adds the label <code>high-availability-config.resources.gardener.cloud/consider=true</code>. This makes the webhook mutate the replica count and the topology spread constraints.</li><li>adds the annotation <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> with value equal to <code>.spec.controlPlane.highAvailability.failureTolerance.type</code> (or <code>""</code>, if <code>.spec.controlPlane.highAvailability=nil</code>). This makes the webhook mutate the node affinity according to the specified zone(s).</li><li>adds the annotation <code>high-availability-config.resources.gardener.cloud/zones=&lt;zones></code>, where <code>&lt;zones></code> is a &mldr;<ul><li>&mldr; random zone chosen from the <code>.spec.provider.zones[]</code> list in the <code>Seed</code> specification (always only one zone (even if there are multiple available in the seed cluster)) in case the <code>Shoot</code> has no HA setting (i.e., <code>spec.controlPlane.highAvailability=nil</code>) or when the <code>Shoot</code> has HA setting with failure tolerance type <code>node</code>.</li><li>&mldr; list of three randomly chosen zones from the <code>.spec.provider.zones[]</code> list in the <code>Seed</code> specification in case the <code>Shoot</code> has HA setting with failure tolerance type <code>zone</code>.</li></ul></li></ul><h4 id=system-components-1>System Components</h4><p>The <code>gardenlet</code> performs the following changes on all namespaces running shoot system components:</p><ul><li>adds the label <code>high-availability-config.resources.gardener.cloud/consider=true</code>. This makes the webhook mutate the replica count and the topology spread constraints.</li><li>adds the annotation <code>high-availability-config.resources.gardener.cloud/zones=&lt;zones></code> where <code>&lt;zones></code> is the merged list of zones provided in <code>.zones[]</code> with <code>systemComponents.allow=true</code> for all worker pools in <code>.spec.provider.workers[]</code> in the <code>Shoot</code> specification.</li></ul><p>Note that neither the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code>, nor the <code>high-availability-config.resources.gardener.cloud/zone-pinning</code> annotations are set, hence the node affinity would never be touched by the webhook.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c95b2a377d941a326c0914b796d1f8d3>1.4.6 - Kubernetes Clients</h1><h1 id=kubernetes-clients-in-gardener>Kubernetes Clients in Gardener</h1><p>This document aims at providing a general developer guideline on different aspects of using Kubernetes clients in a large-scale distributed system and project like Gardener.
The points included here are not meant to be consulted as absolute rules, but rather as general rules of thumb that allow developers to get a better feeling about certain gotchas and caveats.
It should be updated with lessons learned from maintaining the project and running Gardener in production.</p><h2 id=prerequisites>Prerequisites:</h2><p>Please familiarize yourself with the following basic Kubernetes API concepts first, if you&rsquo;re new to Kubernetes. A good understanding of these basics will help you better comprehend the following document.</p><ul><li><a href=https://kubernetes.io/docs/reference/using-api/api-concepts/>Kubernetes API Concepts</a> (including terminology, watch basics, etc.)</li><li><a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/>Extending the Kubernetes API</a> (including Custom Resources and aggregation layer / extension API servers)</li><li><a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/>Extend the Kubernetes API with CustomResourceDefinitions</a></li><li><a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/>Working with Kubernetes Objects</a></li><li><a href=https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md>Sample Controller</a> (the diagram helps to build an understanding of an controller&rsquo;s basic structure)</li></ul><h2 id=client-types-client-go-generated-controller-runtime>Client Types: Client-Go, Generated, Controller-Runtime</h2><p>For historical reasons, you will find different kinds of Kubernetes clients in Gardener:</p><h3 id=client-go-clients>Client-Go Clients</h3><p><a href=https://github.com/kubernetes/client-go>client-go</a> is the default/official client for talking to the Kubernetes API in Golang.
It features the so called <a href=https://github.com/kubernetes/client-go/blob/release-1.21/kubernetes/clientset.go#L72>&ldquo;client sets&rdquo;</a> for all built-in Kubernetes API groups and versions (e.g. <code>v1</code> (aka <code>core/v1</code>), <code>apps/v1</code>).
client-go clients are generated from the built-in API types using <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/generating-clientset.md>client-gen</a> and are composed of interfaces for every known API GroupVersionKind.
A typical client-go usage looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx        context.Context
</span></span><span style=display:flex><span>  c          kubernetes.Interface <span style=color:green>// &#34;k8s.io/client-go/kubernetes&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  deployment *appsv1.Deployment   <span style=color:green>// &#34;k8s.io/api/apps/v1&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>updatedDeployment, err := c.AppsV1().Deployments(<span style=color:#a31515>&#34;default&#34;</span>).Update(ctx, deployment, metav1.UpdateOptions{})
</span></span></code></pre></div><p><em>Important characteristics of client-go clients:</em></p><ul><li>clients are specific to a given API GroupVersionKind, i.e., clients are hard-coded to corresponding API-paths (don&rsquo;t need to use the discovery API to map GVK to a REST endpoint path).</li><li>client&rsquo;s don&rsquo;t modify the passed in-memory object (e.g. <code>deployment</code> in the above example). Instead, they return a new in-memory object.<br>This means that controllers have to continue working with the new in-memory object or overwrite the shared object to not lose any state updates.</li></ul><h3 id=generated-client-sets-for-gardener-apis>Generated Client Sets for Gardener APIs</h3><p>Gardener&rsquo;s APIs extend the Kubernetes API by registering an extension API server (in the garden cluster) and <code>CustomResourceDefinition</code>s (on Seed clusters), meaning that the Kubernetes API will expose additional REST endpoints to manage Gardener resources in addition to the built-in API resources.
In order to talk to these extended APIs in our controllers and components, client-gen is used to generate client-go-style clients to <a href=https://github.com/gardener/gardener/tree/master/pkg/client><code>pkg/client/{core,extensions,seedmanagement,...}</code></a>.</p><p>Usage of these clients is equivalent to <code>client-go</code> clients, and the same characteristics apply. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx   context.Context
</span></span><span style=display:flex><span>  c     gardencoreclientset.Interface <span style=color:green>// &#34;github.com/gardener/gardener/pkg/client/core/clientset/versioned&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  shoot *gardencorev1beta1.Shoot      <span style=color:green>// &#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>updatedShoot, err := c.CoreV1beta1().Shoots(<span style=color:#a31515>&#34;garden-my-project&#34;</span>).Update(ctx, shoot, metav1.UpdateOptions{})
</span></span></code></pre></div><h3 id=controller-runtime-clients>Controller-Runtime Clients</h3><p><a href=https://github.com/kubernetes-sigs/controller-runtime>controller-runtime</a> is a Kubernetes community project (<a href=https://github.com/kubernetes-sigs/kubebuilder>kubebuilder</a> subproject) for building controllers and operators for custom resources.
Therefore, it features a generic client that follows a different approach and does not rely on generated client sets. Instead, the client can be used for managing any Kubernetes resources (built-in or custom) homogeneously.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx        context.Context
</span></span><span style=display:flex><span>  c          client.Client            <span style=color:green>// &#34;sigs.k8s.io/controller-runtime/pkg/client&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  deployment *appsv1.Deployment       <span style=color:green>// &#34;k8s.io/api/apps/v1&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  shoot      *gardencorev1beta1.Shoot <span style=color:green>// &#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>err := c.Update(ctx, deployment)
</span></span><span style=display:flex><span><span style=color:green>// or
</span></span></span><span style=display:flex><span><span style=color:green></span>err = c.Update(ctx, shoot)
</span></span></code></pre></div><p>A brief introduction to controller-runtime and its basic constructs can be found at the <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime>official Go documentation</a>.</p><p><em>Important characteristics of controller-runtime clients:</em></p><ul><li>The client functions take a generic <code>client.Object</code> or <code>client.ObjectList</code> value. These interfaces are implemented by all Golang types, that represent Kubernetes API objects or lists respectively which can be interacted with via usual API requests. [1]</li><li>The client first consults a <code>runtime.Scheme</code> (configured during client creation) for recognizing the object&rsquo;s <code>GroupVersionKind</code> (this happens on the client-side only).<br>A <code>runtime.Scheme</code> is basically a registry for Golang API types, defaulting and conversion functions. Schemes are usually provided per <code>GroupVersion</code> (see <a href=https://github.com/kubernetes/api/blob/release-1.21/apps/v1/register.go>this example</a> for <code>apps/v1</code>) and can be combined to one single scheme for further usage (<a href=https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L96>example</a>). In controller-runtime clients, schemes are used only for mapping a typed API object to its <code>GroupVersionKind</code>.</li><li>It then consults a <code>meta.RESTMapper</code> (also configured during client creation) for mapping the <code>GroupVersionKind</code> to a <code>RESTMapping</code>, which contains the <code>GroupVersionResource</code> and <code>Scope</code> (namespaced or cluster-scoped). From these values, the client can unambiguously determine the REST endpoint path of the corresponding API resource. For instance: <code>appsv1.DeploymentList</code> is available at <code>/apis/apps/v1/deployments</code> or <code>/apis/apps/v1/namespaces/&lt;namespace>/deployments</code> respectively.<ul><li>There are different <code>RESTMapper</code> implementations, but generally they are talking to the API server&rsquo;s discovery API for retrieving <code>RESTMappings</code> for all API resources known to the API server (either built-in, registered via API extension or <code>CustomResourceDefinition</code>s).</li><li>The default implementation of a controller-runtime (which Gardener uses as well) is the <a href=https://github.com/kubernetes-sigs/controller-runtime/blob/v0.9.0/pkg/client/apiutil/dynamicrestmapper.go#L77>dynamic <code>RESTMapper</code></a>. It caches discovery results (i.e. <code>RESTMappings</code>) in-memory and only re-discovers resources from the API server when a client tries to use an unknown <code>GroupVersionKind</code>, i.e., when it encounters a <code>No{Kind,Resource}MatchError</code>.</li></ul></li><li>The client writes back results from the API server into the passed in-memory object.<ul><li>This means that controllers don&rsquo;t have to worry about copying back the results and should just continue to work on the given in-memory object.</li><li>This is a nice and flexible pattern, and helper functions should try to follow it wherever applicable. Meaning, if possible accept an object param, pass it down to clients and keep working on the same in-memory object instead of creating a new one in your helper function.</li><li>The benefit is that you don&rsquo;t lose updates to the API object and always have the last-known state in memory. Therefore, you don&rsquo;t have to read it again, e.g., for getting the current <code>resourceVersion</code> when working with <a href=#conflicts-concurrency-control-and-optimistic-locking>optimistic locking</a>, and thus minimize the chances for running into conflicts.</li><li>However, controllers <em>must not</em> use the same in-memory object concurrently in multiple goroutines. For example, decoding results from the API server in multiple goroutines into the same maps (e.g., labels, annotations) will cause panics because of &ldquo;concurrent map writes&rdquo;. Also, reading from an in-memory API object in one goroutine while decoding into it in another goroutine will yield non-atomic reads, meaning data might be corrupt and represent a non-valid/non-existing API object.</li><li>Therefore, if you need to use the same in-memory object in multiple goroutines concurrently (e.g., shared state), remember to leverage proper synchronization techniques like channels, mutexes, <code>atomic.Value</code> and/or copy the object prior to use. The average controller however, will not need to share in-memory API objects between goroutines, and it&rsquo;s typically an indicator that the controller&rsquo;s design should be improved.</li></ul></li><li>The client decoder erases the object&rsquo;s <code>TypeMeta</code> (<code>apiVersion</code> and <code>kind</code> fields) after retrieval from the API server, see <a href=https://github.com/kubernetes/kubernetes/issues/80609>kubernetes/kubernetes#80609</a>, <a href=https://github.com/kubernetes-sigs/controller-runtime/issues/1517>kubernetes-sigs/controller-runtime#1517</a>.
Unstructured and metadata-only requests objects are an exception to this because the contained <code>TypeMeta</code> is the only way to identify the object&rsquo;s type.
Because of this behavior, <code>obj.GetObjectKind().GroupVersionKind()</code> is likely to return an empty <code>GroupVersionKind</code>.
I.e., you must not rely on <code>TypeMeta</code> being set or <code>GetObjectKind()</code> to return something usable.<br>If you need to identify an object&rsquo;s <code>GroupVersionKind</code>, use a scheme and its <code>ObjectKinds</code> function instead (or the helper function <code>apiutil.GVKForObject</code>).
This is not specific to controller-runtime clients and applies to client-go clients as well.</li></ul><p>[1] Other lower level, config or internal API types (e.g., such as <a href=https://github.com/kubernetes/api/blob/release-1.21/admission/v1/types.go#L29><code>AdmissionReview</code></a>) don&rsquo;t implement <code>client.Object</code>. However, you also can&rsquo;t interact with such objects via the Kubernetes API and thus also not via a client, so this can be disregarded at this point.</p><h3 id=metadata-only-clients>Metadata-Only Clients</h3><p>Additionally, controller-runtime clients can be used to easily retrieve metadata-only objects or lists.
This is useful for efficiently checking if at least one object of a given kind exists, or retrieving metadata of an object, if one is not interested in the rest (e.g., spec/status).<br>The <code>Accept</code> header sent to the API server then contains <code>application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1</code>, which makes the API server only return metadata of the retrieved object(s).
This saves network traffic and CPU/memory load on the API server and client side.
If the client fully lists all objects of a given kind including their spec/status, the resulting list can be quite large and easily exceed the controllers available memory.
That&rsquo;s why it&rsquo;s important to carefully check if a full list is actually needed, or if metadata-only list can be used instead.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx       context.Context
</span></span><span style=display:flex><span>  c         client.Client                         <span style=color:green>// &#34;sigs.k8s.io/controller-runtime/pkg/client&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  shootList = &amp;metav1.PartialObjectMetadataList{} <span style=color:green>// &#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>)
</span></span><span style=display:flex><span>shootList.SetGroupVersionKind(gardencorev1beta1.SchemeGroupVersion.WithKind(<span style=color:#a31515>&#34;ShootList&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> err := c.List(ctx, shootList, client.InNamespace(<span style=color:#a31515>&#34;garden-my-project&#34;</span>), client.Limit(1)); err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>  <span style=color:#00f>return</span> err
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> len(shootList.Items) &gt; 0 {
</span></span><span style=display:flex><span>  <span style=color:green>// project has at least one shoot
</span></span></span><span style=display:flex><span><span style=color:green></span>} <span style=color:#00f>else</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// project doesn&#39;t have any shoots
</span></span></span><span style=display:flex><span><span style=color:green></span>}
</span></span></code></pre></div><h3 id=gardeners-client-collection-clientmaps>Gardener&rsquo;s Client Collection, ClientMaps</h3><p>The Gardener codebase has a collection of clients (<a href=https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L149><code>kubernetes.Interface</code></a>), which can return all the above mentioned client types.
Additionally, it contains helpers for rendering and applying helm charts (<code>ChartRender</code>, <code>ChartApplier</code>) and retrieving the API server&rsquo;s version (<code>Version</code>).<br>Client sets are managed by so called <code>ClientMap</code>s, which are a form of registry for all client set for a given type of cluster, i.e., Garden, Seed and Shoot.
ClientMaps manage the whole lifecycle of clients: they take care of creating them if they don&rsquo;t exist already, running their caches, refreshing their cached server version and invalidating them when they are no longer needed.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx   context.Context
</span></span><span style=display:flex><span>  cm    clientmap.ClientMap <span style=color:green>// &#34;github.com/gardener/gardener/pkg/client/kubernetes/clientmap&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>  shoot *gardencorev1beta1.Shoot
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cs, err := cm.GetClient(ctx, keys.ForShoot(shoot)) <span style=color:green>// kubernetes.Interface
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>if</span> err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>  <span style=color:#00f>return</span> err
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>c := cs.Client() <span style=color:green>// client.Client
</span></span></span></code></pre></div><p>The client collection mainly exist for historical reasons (there used to be a lot of code using the client-go style clients).
However, Gardener is in the process of moving more towards controller-runtime and only using their clients, as they provide many benefits and are much easier to use.
Also, <a href=https://github.com/gardener/gardener/issues/4251>gardener/gardener#4251</a> aims at refactoring our controller and admission components to native controller-runtime components.</p><blockquote><p>⚠️ Please always prefer controller-runtime clients over other clients when writing new code or refactoring existing code.</p></blockquote><h2 id=cache-types-informers-listers-controller-runtime-caches>Cache Types: Informers, Listers, Controller-Runtime Caches</h2><p>Similar to the different types of client(set)s, there are also different kinds of Kubernetes client caches.
However, all of them are based on the same concept: <code>Informer</code>s.
An <code>Informer</code> is a watch-based cache implementation, meaning it opens <a href=https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes>watch connections</a> to the API server and continuously updates cached objects based on the received watch events (<code>ADDED</code>, <code>MODIFIED</code>, <code>DELETED</code>).
<code>Informer</code>s offer to add indices to the cache for efficient object lookup (e.g., by name or labels) and to add <code>EventHandler</code>s for the watch events.
The latter is used by controllers to fill queues with objects that should be reconciled on watch events.</p><p>Informers are used in and created via several higher-level constructs:</p><h3 id=sharedinformerfactories-listers>SharedInformerFactories, Listers</h3><p>The generated clients (built-in as well as extended) feature a <code>SharedInformerFactory</code> for every API group, which can be used to create and retrieve <code>Informers</code> for all GroupVersionKinds.
Similarly, it can be used to retrieve <code>Listers</code> that allow getting and listing objects from the <code>Informer</code>&rsquo;s cache.
However, both of these constructs are only used for historical reasons, and we are in the process of migrating away from them in favor of cached controller-runtime clients (see <a href=https://github.com/gardener/gardener/issues/2414>gardener/gardener#2414</a>, <a href=https://github.com/gardener/gardener/issues/2822>gardener/gardener#2822</a>). Thus, they are described only briefly here.</p><p><em>Important characteristics of Listers:</em></p><ul><li>Objects read from Informers and Listers can always be slightly out-out-date (i.e., stale) because the client has to first observe changes to API objects via watch events (which can intermittently lag behind by a second or even more).</li><li>Thus, don&rsquo;t make any decisions based on data read from Listers if the consequences of deciding wrongfully based on stale state might be catastrophic (e.g. leaking infrastructure resources). In such cases, read directly from the API server via a client instead.</li><li>Objects retrieved from Informers or Listers are pointers to the cached objects, so they must not be modified without copying them first, otherwise the objects in the cache are also modified.</li></ul><h3 id=controller-runtime-caches>Controller-Runtime Caches</h3><p>controller-runtime features a cache implementation that can be used equivalently as their clients. In fact, it implements a subset of the <code>client.Client</code> interface containing the <code>Get</code> and <code>List</code> functions.
Under the hood, a <code>cache.Cache</code> dynamically creates <code>Informers</code> (i.e., opens watches) for every object GroupVersionKind that is being retrieved from it.</p><p>Note that the underlying Informers of a controller-runtime cache (<code>cache.Cache</code>) and the ones of a <code>SharedInformerFactory</code> (client-go) are not related in any way.
Both create <code>Informers</code> and watch objects on the API server individually.
This means that if you read the same object from different cache implementations, you may receive different versions of the object because the watch connections of the individual Informers are not synced.</p><blockquote><p>⚠️ Because of this, controllers/reconcilers should get the object from the same cache in the reconcile loop, where the <code>EventHandler</code> was also added to set up the controller. For example, if a <code>SharedInformerFactory</code> is used for setting up the controller then read the object in the reconciler from the <code>Lister</code> instead of from a cached controller-runtime client.</p></blockquote><p>By default, the <code>client.Client</code> created by a controller-runtime <code>Manager</code> is a <code>DelegatingClient</code>. It delegates <code>Get</code> and <code>List</code> calls to a <code>Cache</code>, and all other calls to a client that talks directly to the API server. Exceptions are requests with <code>*unstructured.Unstructured</code> objects and object kinds that were configured to be excluded from the cache in the <code>DelegatingClient</code>.</p><blockquote><p>ℹ️
<code>kubernetes.Interface.Client()</code> returns a <code>DelegatingClient</code> that uses the cache returned from <code>kubernetes.Interface.Cache()</code> under the hood. This means that all <code>Client()</code> usages need to be ready for cached clients and should be able to cater with stale cache reads.</p></blockquote><p><em>Important characteristics of cached controller-runtime clients:</em></p><ul><li>Like for Listers, objects read from a controller-runtime cache can always be slightly out of date. Hence, don&rsquo;t base any important decisions on data read from the cache (see above).</li><li>In contrast to Listers, controller-runtime caches fill the passed in-memory object with the state of the object in the cache (i.e., they perform something like a &ldquo;deep copy into&rdquo;). This means that objects read from a controller-runtime cache can safely be modified without unintended side effects.</li><li>Reading from a controller-runtime cache or a cached controller-runtime client implicitly starts a watch for the given object kind under the hood. This has important consequences:<ul><li>Reading a given object kind from the cache for the first time can take up to a few seconds depending on size and amount of objects as well as API server latency. This is because the cache has to do a full list operation and wait for an initial watch sync before returning results.</li><li>⚠️ Controllers need appropriate RBAC permissions for the object kinds they retrieve via cached clients (i.e., <code>list</code> and <code>watch</code>).</li><li>⚠️ By default, watches started by a controller-runtime cache are cluster-scoped, meaning it watches and caches objects across all namespaces. Thus, be careful which objects to read from the cache as it might significantly increase the controller&rsquo;s memory footprint.</li></ul></li><li>There is no interaction with the cache on writing calls (<code>Create</code>, <code>Update</code>, <code>Patch</code> and <code>Delete</code>), see below.</li></ul><p><strong>Uncached objects, filtered caches, <code>APIReader</code>s:</strong></p><p>In order to allow more granular control over which object kinds should be cached and which calls should bypass the cache, controller-runtime offers a few mechanisms to further tweak the client/cache behavior:</p><ul><li>When creating a <code>DelegatingClient</code>, certain object kinds can be configured to always be read directly from the API instead of from the cache. Note that this does not prevent starting a new Informer when retrieving them directly from the cache.</li><li>Watches can be restricted to a given (set of) namespace(s) by using <code>cache.MultiNamespacedCacheBuilder</code> or setting <code>cache.Options.Namespace</code>.</li><li>Watches can be filtered (e.g., by label) per object kind by configuring <code>cache.Options.SelectorsByObject</code> on creation of the cache.</li><li>Retrieving metadata-only objects or lists from a cache results in a metadata-only watch/cache for that object kind.</li><li>The <code>APIReader</code> can be used to always talk directly to the API server for a given <code>Get</code> or <code>List</code> call (use with care and only as a last resort!).</li></ul><h3 id=to-cache-or-not-to-cache>To Cache or Not to Cache</h3><p>Although watch-based caches are an important factor for the immense scalability of Kubernetes, it definitely comes at a price (mainly in terms of memory consumption).
Thus, developers need to be careful when introducing new API calls and caching new object kinds.
Here are some general guidelines on choosing whether to read from a cache or not:</p><ul><li>Always try to use the cache wherever possible and make your controller able to tolerate stale reads.<ul><li>Leverage optimistic locking: use deterministic naming for objects you create (this is what the <code>Deployment</code> controller does [2]).</li><li>Leverage optimistic locking / concurrency control of the API server: send updates/patches with the last-known <code>resourceVersion</code> from the cache (see below). This will make the request fail, if there were concurrent updates to the object (conflict error), which indicates that we have operated on stale data and might have made wrong decisions. In this case, let the controller handle the error with exponential backoff. This will make the controller eventually consistent.</li><li>Track the actions you took, e.g., when creating objects with <code>generateName</code> (this is what the <code>ReplicaSet</code> controller does [3]). The actions can be tracked in memory and repeated if the expected watch events don&rsquo;t occur after a given amount of time.</li><li>Always try to write controllers with the assumption that data will only be eventually correct and can be slightly out of date (even if read directly from the API server!).</li><li>If there is already some other code that needs a cache (e.g., a controller watch), reuse it instead of doing extra direct reads.</li><li>Don&rsquo;t read an object again if you just sent a write request. Write requests (<code>Create</code>, <code>Update</code>, <code>Patch</code> and <code>Delete</code>) don&rsquo;t interact with the cache. Hence, use the current state that the API server returned (filled into the passed in-memory object), which is basically a &ldquo;free direct read&rdquo; instead of reading the object again from a cache, because this will probably set back the object to an older <code>resourceVersion</code>.</li></ul></li><li>If you are concerned about the impact of the resulting cache, try to minimize that by using filtered or metadata-only watches.</li><li>If watching and caching an object type is not feasible, for example because there will be a lot of updates, and you are only interested in the object every ~5m, or because it will blow up the controllers memory footprint, fallback to a direct read. This can either be done by disabling caching the object type generally or doing a single request via an <code>APIReader</code>. In any case, please bear in mind that every direct API call results in a <a href=https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter>quorum read from etcd</a>, which can be costly in a heavily-utilized cluster and impose significant scalability limits. Thus, always try to minimize the impact of direct calls by filtering results by namespace or labels, limiting the number of results and/or using metadata-only calls.</li></ul><p>[2] The <code>Deployment</code> controller uses the pattern <code>&lt;deployment-name>-&lt;podtemplate-hash></code> for naming <code>ReplicaSets</code>. This means, the name of a <code>ReplicaSet</code> it tries to create/update/delete at any given time is deterministically calculated based on the <code>Deployment</code> object. By this, it is insusceptible to stale reads from its <code>ReplicaSets</code> cache.</p><p>[3] In simple terms, the <code>ReplicaSet</code> controller tracks its <code>CREATE pod</code> actions as follows: when creating new <code>Pods</code>, it increases a counter of expected <code>ADDED</code> watch events for the corresponding <code>ReplicaSet</code>. As soon as such events arrive, it decreases the counter accordingly. It only creates new <code>Pods</code> for a given <code>ReplicaSet</code> once all expected events occurred (counter is back to zero) or a timeout has occurred. This way, it prevents creating more <code>Pods</code> than desired because of stale cache reads and makes the controller eventually consistent.</p><h2 id=conflicts-concurrency-control-and-optimistic-locking>Conflicts, Concurrency Control, and Optimistic Locking</h2><p>Every Kubernetes API object contains the <code>metadata.resourceVersion</code> field, which identifies an object&rsquo;s version in the backing data store, i.e., etcd. Every write to an object in etcd results in a newer <code>resourceVersion</code>.
This field is mainly used for concurrency control on the API server in an optimistic locking fashion, but also for efficient resumption of interrupted watch connections.</p><p>Optimistic locking in the Kubernetes API sense means that when a client wants to update an API object, then it includes the object&rsquo;s <code>resourceVersion</code> in the request to indicate the object&rsquo;s version the modifications are based on.
If the <code>resourceVersion</code> in etcd has not changed in the meantime, the update request is accepted by the API server and the updated object is written to etcd.
If the <code>resourceVersion</code> sent by the client does not match the one of the object stored in etcd, there were concurrent modifications to the object. Consequently, the request is rejected with a conflict error (status code <code>409</code>, API reason <code>Conflict</code>), for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;kind&#34;: <span style=color:#a31515>&#34;Status&#34;</span>,
</span></span><span style=display:flex><span>  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  &#34;metadata&#34;: {},
</span></span><span style=display:flex><span>  &#34;status&#34;: <span style=color:#a31515>&#34;Failure&#34;</span>,
</span></span><span style=display:flex><span>  &#34;message&#34;: <span style=color:#a31515>&#34;Operation cannot be fulfilled on configmaps \&#34;foo\&#34;: the object has been modified; please apply your changes to the latest version and try again&#34;</span>,
</span></span><span style=display:flex><span>  &#34;reason&#34;: <span style=color:#a31515>&#34;Conflict&#34;</span>,
</span></span><span style=display:flex><span>  &#34;details&#34;: {
</span></span><span style=display:flex><span>    &#34;name&#34;: <span style=color:#a31515>&#34;foo&#34;</span>,
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;configmaps&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  &#34;code&#34;: 409
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This concurrency control is an important mechanism in Kubernetes as there are typically multiple clients acting on API objects at the same time (humans, different controllers, etc.). If a client receives a conflict error, it should read the object&rsquo;s latest version from the API server, make the modifications based on the newest changes, and retry the update.
The reasoning behind this is that a client might choose to make different decisions based on the concurrent changes made by other actors compared to the outdated version that it operated on.</p><p><em>Important points about concurrency control and conflicts:</em></p><ul><li>The <code>resourceVersion</code> field carries a string value and clients must not assume numeric values (the type and structure of versions depend on the backing data store). This means clients may compare <code>resourceVersion</code> values to detect whether objects were changed. But they must not compare <code>resourceVersion</code>s to figure out which one is newer/older, i.e., no greater/less-than comparisons are allowed.</li><li>By default, update calls (e.g. via client-go and controller-runtime clients) use optimistic locking as the passed in-memory usually object contains the latest <code>resourceVersion</code> known to the controller, which is then also sent to the API server.</li><li>API servers can also choose to accept update calls without optimistic locking (i.e., without a <code>resourceVersion</code> in the object&rsquo;s metadata) for any given resource. However, sending update requests without optimistic locking is strongly discouraged, as doing so overwrites the entire object, discarding any concurrent changes made to it.</li><li>On the other side, patch requests can always be executed either with or without optimistic locking, by (not) including the <code>resourceVersion</code> in the patched object&rsquo;s metadata. Sending patch requests without optimistic locking might be safe and even desirable as a patch typically updates only a specific section of the object. However, there are also situations where patching without optimistic locking is not safe (see below).</li></ul><h3 id=dont-retry-on-conflict>Don’t Retry on Conflict</h3><p>Similar to how a human would typically handle a conflict error, there are helper functions implementing <code>RetryOnConflict</code>-semantics, i.e., try an update call, then re-read the object if a conflict occurs, apply the modification again and retry the update.
However, controllers should generally <em>not</em> use <code>RetryOnConflict</code>-semantics. Instead, controllers should abort their current reconciliation run and let the queue handle the conflict error with exponential backoff.
The reasoning behind this is that a conflict error indicates that the controller has operated on stale data and might have made wrong decisions earlier on in the reconciliation.
When using a helper function that implements <code>RetryOnConflict</code>-semantics, the controller doesn&rsquo;t check which fields were changed and doesn&rsquo;t revise its previous decisions accordingly.
Instead, retrying on conflict basically just ignores any conflict error and blindly applies the modification.</p><p>To properly solve the conflict situation, controllers should immediately return with the error from the update call. This will cause retries with exponential backoff so that the cache has a chance to observe the latest changes to the object.
In a later run, the controller will then make correct decisions based on the newest version of the object, not run into conflict errors, and will then be able to successfully reconcile the object. This way, the controller becomes eventually consistent.</p><p>The other way to solve the situation is to modify objects without optimistic locking in order to avoid running into a conflict in the first place (only if this is safe).
This can be a preferable solution for controllers with long-running reconciliations (which is actually an anti-pattern but quite unavoidable in some of Gardener&rsquo;s controllers).
Aborting the entire reconciliation run is rather undesirable in such cases, as it will add a lot of unnecessary waiting time for end users and overhead in terms of compute and network usage.</p><p>However, in any case, retrying on conflict is probably not the right option to solve the situation (there are some correct use cases for it, though, they are very rare). Hence, don&rsquo;t retry on conflict.</p><h3 id=to-lock-or-not-to-lock>To Lock or Not to Lock</h3><p>As explained before, conflicts are actually important and prevent clients from doing wrongful concurrent updates. This means that conflicts are not something we generally want to avoid or ignore.
However, in many cases controllers are exclusive owners of the fields they want to update and thus it might be safe to run without optimistic locking.</p><p>For example, the gardenlet is the exclusive owner of the <code>spec</code> section of the Extension resources it creates on behalf of a Shoot (e.g., the <code>Infrastructure</code> resource for creating VPC). Meaning, it knows the exact desired state and no other actor is supposed to update the Infrastructure&rsquo;s <code>spec</code> fields.
When the gardenlet now updates the Infrastructures <code>spec</code> section as part of the Shoot reconciliation, it can simply issue a <code>PATCH</code> request that only updates the <code>spec</code> and runs without optimistic locking.
If another controller concurrently updated the object in the meantime (e.g., the <code>status</code> section), the <code>resourceVersion</code> got changed, which would cause a conflict error if running with optimistic locking.
However, concurrent <code>status</code> updates would not change the gardenlet&rsquo;s mind on the desired <code>spec</code> of the Infrastructure resource as it is determined only by looking at the Shoot&rsquo;s specification.
If the <code>spec</code> section was changed concurrently, it&rsquo;s still fine to overwrite it because the gardenlet should reconcile the <code>spec</code> back to its desired state.</p><p>Generally speaking, if a controller is the exclusive owner of a given set of fields and they are independent of concurrent changes to other fields in that object, it can patch these fields without optimistic locking.
This might ignore concurrent changes to other fields or blindly overwrite changes to the same fields, but this is fine if the mentioned conditions apply.
Obviously, this applies only to patch requests that modify only a specific set of fields but not to update requests that replace the entire object.</p><p>In such cases, it&rsquo;s even desirable to run without optimistic locking as it will be more performant and save retries.
If certain requests are made with high frequency and have a good chance of causing conflicts, retries because of optimistic locking can cause a lot of additional network traffic in a large-scale Gardener installation.</p><h2 id=updates-patches-server-side-apply>Updates, Patches, Server-Side Apply</h2><p>There are different ways of modifying Kubernetes API objects.
The following snippet demonstrates how to do a given modification with the most frequently used options using a controller-runtime client:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  ctx   context.Context
</span></span><span style=display:flex><span>  c     client.Client
</span></span><span style=display:flex><span>  shoot *gardencorev1beta1.Shoot
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// update
</span></span></span><span style=display:flex><span><span style=color:green></span>shoot.Spec.Kubernetes.Version = <span style=color:#a31515>&#34;1.22&#34;</span>
</span></span><span style=display:flex><span>err := c.Update(ctx, shoot)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// json merge patch
</span></span></span><span style=display:flex><span><span style=color:green></span>patch := client.MergeFrom(shoot.DeepCopy())
</span></span><span style=display:flex><span>shoot.Spec.Kubernetes.Version = <span style=color:#a31515>&#34;1.22&#34;</span>
</span></span><span style=display:flex><span>err = c.Patch(ctx, shoot, patch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// strategic merge patch
</span></span></span><span style=display:flex><span><span style=color:green></span>patch = client.StrategicMergeFrom(shoot.DeepCopy())
</span></span><span style=display:flex><span>shoot.Spec.Kubernetes.Version = <span style=color:#a31515>&#34;1.22&#34;</span>
</span></span><span style=display:flex><span>err = c.Patch(ctx, shoot, patch)
</span></span></code></pre></div><p><em>Important characteristics of the shown request types:</em></p><ul><li>Update requests always send the entire object to the API server and update all fields accordingly. By default, optimistic locking is used (<code>resourceVersion</code> is included).</li><li>Both patch types run without optimistic locking by default. However, it can be enabled explicitly if needed:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// json merge patch + optimistic locking
</span></span></span><span style=display:flex><span><span style=color:green></span>patch := client.MergeFromWithOptions(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
</span></span><span style=display:flex><span><span style=color:green>// ...
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:green>// strategic merge patch + optimistic locking
</span></span></span><span style=display:flex><span><span style=color:green></span>patch = client.StrategicMergeFrom(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
</span></span><span style=display:flex><span><span style=color:green>// ...
</span></span></span></code></pre></div></li><li>Patch requests only contain the changes made to the in-memory object between the copy passed to <code>client.*MergeFrom</code> and the object passed to <code>Client.Patch()</code>. The diff is calculated on the client-side based on the in-memory objects only. This means that if in the meantime some fields were changed on the API server to a different value than the one on the client-side, the fields will not be changed back as long as they are not changed on the client-side as well (there will be no diff in memory).</li><li>Thus, if you want to ensure a given state using patch requests, always read the object first before patching it, as there will be no diff otherwise, meaning the patch will be empty. For more information, see <a href=https://github.com/gardener/gardener/pull/4057>gardener/gardener#4057</a> and the comments in <a href=https://github.com/gardener/gardener/pull/4027>gardener/gardener#4027</a>.</li><li>Also, always send updates and patch requests even if your controller hasn&rsquo;t made any changes to the current state on the API server. I.e., don&rsquo;t make any optimization for preventing empty patches or no-op updates. There might be mutating webhooks in the system that will modify the object and that rely on update/patch requests being sent (even if they are no-op). Gardener&rsquo;s extension concept makes heavy use of mutating webhooks, so it&rsquo;s important to keep this in mind.</li><li>JSON merge patches always replace lists as a whole and don&rsquo;t merge them. Keep this in mind when operating on lists with merge patch requests. If the controller is the exclusive owner of the entire list, it&rsquo;s safe to run without optimistic locking. Though, if you want to prevent overwriting concurrent changes to the list or its items made by other actors (e.g., additions/removals to the <code>metadata.finalizers</code> list), enable optimistic locking.</li><li>Strategic merge patches are able to make more granular modifications to lists and their elements without replacing the entire list. It uses Golang struct tags of the API types to determine which and how lists should be merged. See <a href=https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>Update API Objects in Place Using kubectl patch</a> or the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md>strategic merge patch documentation</a> for more in-depth explanations and comparison with JSON merge patches.
With this, controllers <em>might</em> be able to issue patch requests for individual list items without optimistic locking, even if they are not exclusive owners of the entire list. Remember to check the <code>patchStrategy</code> and <code>patchMergeKey</code> struct tags of the fields you want to modify before blindly adding patch requests without optimistic locking.</li><li>Strategic merge patches are only supported by built-in Kubernetes resources and custom resources served by Extension API servers. Strategic merge patches are not supported by custom resources defined by <code>CustomResourceDefinition</code>s (see <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#advanced-features-and-flexibility>this comparison</a>). In that case, fallback to JSON merge patches.</li><li><a href=https://kubernetes.io/docs/reference/using-api/server-side-apply/>Server-side Apply</a> is yet another mechanism to modify API objects, which is supported by all API resources (in newer Kubernetes versions). However, it has a few problems and more caveats preventing us from using it in Gardener at the time of writing. See <a href=https://github.com/gardener/gardener/issues/4122>gardener/gardener#4122</a> for more details.</li></ul><blockquote><p>Generally speaking, patches are often the better option compared to update requests because they can save network traffic, encoding/decoding effort, and avoid conflicts under the presented conditions.
If choosing a patch type, consider which type is supported by the resource you&rsquo;re modifying and what will happen in case of a conflict. Consider whether your modification is safe to run without optimistic locking.
However, there is no simple rule of thumb on which patch type to choose.</p></blockquote><h2 id=on-helper-functions>On Helper Functions</h2><p>Here is a note on some helper functions, that should be avoided and why:</p><p><code>controllerutil.CreateOrUpdate</code> does a basic get, mutate and create or update call chain, which is often used in controllers. We should avoid using this helper function in Gardener, because it is likely to cause conflicts for cached clients and doesn&rsquo;t send no-op requests if nothing was changed, which can cause problems because of the heavy use of webhooks in Gardener extensions (see above).
That&rsquo;s why usage of this function was completely replaced in <a href=https://github.com/gardener/gardener/pull/4227>gardener/gardener#4227</a> and similar PRs.</p><p><code>controllerutil.CreateOrPatch</code> is similar to <code>CreateOrUpdate</code> but does a patch request instead of an update request. It has the same drawback as <code>CreateOrUpdate</code> regarding no-op updates.
Also, controllers can&rsquo;t use optimistic locking or strategic merge patches when using <code>CreateOrPatch</code>.
Another reason for avoiding use of this function is that it also implicitly patches the status section if it was changed, which is confusing for others reading the code. To accomplish this, the func does some back and forth conversion, comparison and checks, which are unnecessary in most of our cases and simply wasted CPU cycles and complexity we want to avoid.</p><p>There were some <code>Try{Update,UpdateStatus,Patch,PatchStatus}</code> helper functions in Gardener that were already removed by <a href=https://github.com/gardener/gardener/pull/4378>gardener/gardener#4378</a> but are still used in some extension code at the time of writing.
The reason for eliminating these functions is that they implement <code>RetryOnConflict</code>-semantics. Meaning, they first get the object, mutate it, then try to update and retry if a conflict error occurs.
As explained above, retrying on conflict is a controller anti-pattern and should be avoided in almost every situation.
The other problem with these functions is that they read the object first from the API server (always do a direct call), although in most cases we already have a recent version of the object at hand. So, using this function generally does unnecessary API calls and therefore causes unwanted compute and network load.</p><p>For the reasons explained above, there are similar helper functions that accomplish similar things but address the mentioned drawbacks: <code>controllerutils.{GetAndCreateOrMergePatch,GetAndCreateOrStrategicMergePatch}</code>.
These can be safely used as replacements for the aforementioned helper funcs.
If they are not fitting for your use case, for example because you need to use optimistic locking, just do the appropriate calls in the controller directly.</p><h2 id=related-links>Related Links</h2><ul><li><a href="https://www.youtube.com/watch?v=RPsUo925PUA&t=40s">Kubernetes Client usage in Gardener</a> (Community Meeting talk, 2020-06-26)</li></ul><p>These resources are only partially related to the topics covered in this doc, but might still be interesting for developer seeking a deeper understanding of Kubernetes API machinery, architecture and foundational concepts.</p><ul><li><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md>API Conventions</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/architecture/resource-management.md>The Kubernetes Resource Model</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21c83bbf03336ea56a38d93f1d4fa12f>1.4.7 - Local Setup</h1><h1 id=overview>Overview</h1><p>Conceptually, all Gardener components are designed to run as a Pod inside a Kubernetes cluster.
The Gardener API server extends the Kubernetes API via the user-aggregated API server concepts.
However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time.
That means that the Gardener runs outside a Kubernetes cluster which requires providing a <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> in your local filesystem and point the Gardener to it when starting it (see below).</p><p>Further details can be found in</p><ol><li><a href=https://kubernetes.io/docs/concepts/>Principles of Kubernetes</a>, and its <a href=https://kubernetes.io/docs/concepts/overview/components/>components</a></li><li><a href=https://github.com/kubernetes/community/tree/master/contributors/devel>Kubernetes Development Guide</a></li><li><a href=https://github.com/gardener/documentation/wiki/Architecture>Architecture of Gardener</a></li></ol><p>This guide is split into three main parts:</p><ul><li><a href=#preparing-the-setup>Preparing your setup by installing all dependencies and tools</a></li><li><a href=#start-gardener-locally>Building and starting Gardener components locally</a></li><li><a href=#create-a-shoot>Using your local Gardener setup to create a Shoot</a></li></ul><h1 id=preparing-the-setup>Preparing the Setup</h1><h2 id=macos-only-installing-homebrew>[macOS only] Installing homebrew</h2><p>The copy-paste instructions in this guide are designed for macOS and use the package manager <a href=https://brew.sh/>Homebrew</a>.</p><p>On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/bin/bash -c <span style=color:#a31515>&#34;</span><span style=color:#00f>$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span style=color:#00f>)</span><span style=color:#a31515>&#34;</span>
</span></span></code></pre></div><h2 id=installing-git>Installing git</h2><p>We use <code>git</code> as VCS which you need to install. On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install git
</span></span></code></pre></div><p>For other OS, please check the <a href=https://git-scm.com/book/en/v2/Getting-Started-Installing-Git>Git installation documentation</a>.</p><h2 id=installing-go>Installing Go</h2><p>Install the latest version of Go. On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install go
</span></span></code></pre></div><p>For other OS, please check <a href=https://golang.org/doc/install>Go installation documentation</a>.</p><h2 id=installing-kubectl>Installing kubectl</h2><p>Install <code>kubectl</code>. Please make sure that the version of <code>kubectl</code> is at least <code>v1.20.x</code>. On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install kubernetes-cli
</span></span></code></pre></div><p>For other OS, please check the <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>kubectl installation documentation</a>.</p><h2 id=installing-docker>Installing Docker</h2><p>You need to have docker installed and running. On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install --cask docker
</span></span></code></pre></div><p>For other OS please check the <a href=https://docs.docker.com/get-docker/>docker installation documentation</a>.</p><h2 id=installing-iproute2>Installing iproute2</h2><p><code>iproute2</code> provides a collection of utilities for network administration and configuration. On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install iproute2mac
</span></span></code></pre></div><h2 id=installing-jq>Installing jq</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install jq
</span></span></code></pre></div><h2 id=installing-gnu-parallel>Installing GNU Parallel</h2><p><a href=https://www.gnu.org/software/parallel/>GNU Parallel</a> is a shell tool for executing jobs in parallel, used by the code generation scripts (<code>make generate</code>). On macOS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install parallel
</span></span></code></pre></div><h2 id=macos-only-install-gnu-core-utilities>[macOS only] Install GNU Core Utilities</h2><p>When running on macOS, install the GNU core utilities and friends:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install coreutils gnu-sed gnu-tar grep
</span></span></code></pre></div><p>This will create symbolic links for the GNU utilities with <code>g</code> prefix on your <code>PATH</code>, e.g., <code>gsed</code> or <code>gbase64</code>.
To allow using them without the <code>g</code> prefix, add the <code>gnubin</code> directories to the beginning of your <code>PATH</code> environment variable (<code>brew install</code> and <code>brew info</code> will print out instructions for each formula):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export PATH=<span style=color:#00f>$(</span>brew --prefix<span style=color:#00f>)</span>/opt/coreutils/libexec/gnubin:$PATH
</span></span><span style=display:flex><span>export PATH=<span style=color:#00f>$(</span>brew --prefix<span style=color:#00f>)</span>/opt/gnu-sed/libexec/gnubin:$PATH
</span></span><span style=display:flex><span>export PATH=<span style=color:#00f>$(</span>brew --prefix<span style=color:#00f>)</span>/opt/gnu-tar/libexec/gnubin:$PATH
</span></span><span style=display:flex><span>export PATH=<span style=color:#00f>$(</span>brew --prefix<span style=color:#00f>)</span>/opt/grep/libexec/gnubin:$PATH
</span></span></code></pre></div><h2 id=windows-only-wsl2>[Windows Only] WSL2</h2><p>Apart from Linux distributions and macOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.</p><p>While WSL1, plain docker for Windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:</p><ul><li><a href=https://docs.microsoft.com/en-us/windows/wsl/wsl2-index>WSL2</a></li><li><a href=https://docs.docker.com/docker-for-windows/wsl/>Docker Desktop WSL2 Engine</a></li><li><a href=https://ubuntu.com/blog/ubuntu-on-wsl-2-is-generally-available>Ubuntu 18.04 LTS on WSL2</a></li><li>Nodeless local garden (see below)</li></ul><p>The Gardener repository and all the above-mentioned tools (git, golang, kubectl, &mldr;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.</p><h1 id=start-gardener-locally>Start Gardener Locally</h1><h2 id=get-the-sources>Get the Sources</h2><p>Clone the repository from GitHub into your <code>$GOPATH</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#00f>$(</span>go env GOPATH<span style=color:#00f>)</span>/src/github.com/gardener
</span></span><span style=display:flex><span>cd <span style=color:#00f>$(</span>go env GOPATH<span style=color:#00f>)</span>/src/github.com/gardener
</span></span><span style=display:flex><span>git clone git@github.com:gardener/gardener.git
</span></span><span style=display:flex><span>cd gardener
</span></span></code></pre></div><blockquote><p>Note: Gardener is using Go modules and cloning the repository into <code>$GOPATH</code> is not a hard requirement. However it is still recommended to clone into <code>$GOPATH</code> because <code>k8s.io/code-generator</code> does not work yet outside of <code>$GOPATH</code> - <a href=https://github.com/kubernetes/kubernetes/issues/86753>kubernetes/kubernetes#86753</a>.</p></blockquote><h2 id=start-the-gardener>Start the Gardener</h2><p>ℹ️ In the following guide, you have to define the configuration (<code>CloudProfile</code>s, <code>SecretBinding</code>s, <code>Seed</code>s, etc.) manually for the infrastructure environment you want to develop against.
Additionally, you have to register the respective Gardener extensions manually.
If you are rather looking for a quick start guide to develop entirely locally on your machine (no real cloud provider or infrastructure involved), then you should rather follow <a href=/docs/gardener/development/getting_started_locally/>this guide</a>.</p><h3 id=start-a-local-kubernetes-cluster>Start a Local Kubernetes Cluster</h3><p>For the development of Gardener you need a Kubernetes API server on which you can register Gardener&rsquo;s own Extension API Server as <code>APIService</code>. This cluster doesn&rsquo;t need any worker nodes to run pods, though, therefore, you can use the &ldquo;nodeless Garden cluster setup&rdquo; residing in <code>hack/local-garden</code>. This will start all minimally required components of a Kubernetes cluster (<code>etcd</code>, <code>kube-apiserver</code>, <code>kube-controller-manager</code>)
and an <code>etcd</code> Instance for the <code>gardener-apiserver</code> as Docker containers. This is the easiest way to get your
Gardener development setup up and running.</p><p><strong>Using the nodeless cluster setup</strong></p><p>Use the provided Makefile rules to start your local Garden:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make local-garden-up
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span>Starting gardener-dev kube-etcd cluster..!
</span></span><span style=display:flex><span>Starting gardener-dev kube-apiserver..!
</span></span><span style=display:flex><span>Starting gardener-dev kube-controller-manager..!
</span></span><span style=display:flex><span>Starting gardener-dev gardener-etcd cluster..!
</span></span><span style=display:flex><span>namespace/garden created
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>ℹ️ [Optional] If you want to develop the <code>SeedAuthorization</code> feature then you have to run <code>make ACTIVATE_SEEDAUTHORIZER=true local-garden-up</code>. However, please note that this forces you to start the <code>gardener-admission-controller</code> via <code>make start-admission-controller</code>.</p><p>To tear down the local Garden cluster and remove the Docker containers, simply run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make local-garden-down
</span></span></code></pre></div><details><summary><b>Alternative: Using a local Kubernetes cluster</b></summary><p>Instead of starting a Kubernetes API server and etcd as docker containers, you can also opt for running a local Kubernetes cluster, provided by e.g. <a href=https://minikube.sigs.k8s.io/docs/start/>minikube</a>, <a href=https://kind.sigs.k8s.io/docs/user/quick-start/>kind</a> or docker desktop.</p><blockquote><p>Note: Gardener requires self-contained kubeconfig files because of a <a href=https://banzaicloud.com/blog/kubeconfig-security/>security issue</a>. You can configure your minikube to create self-contained kubeconfig files via:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>minikube config set embed-certs true
</span></span></code></pre></div><p>or when starting the local cluster</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>minikube start --embed-certs
</span></span></code></pre></div></blockquote></details><details><summary><b>Alternative: Using a remote Kubernetes cluster</b></summary><p>For some testing scenarios, you may want to use a remote cluster instead of a local one as your Garden cluster.
To do this, you can use the &ldquo;remote Garden cluster setup&rdquo; residing in <code>hack/remote-garden</code>. This will start an <code>etcd</code> instance for the <code>gardener-apiserver</code> as a Docker container, and open tunnels for accessing local gardener components from the remote cluster.</p><p>To avoid mistakes, the remote cluster must have a <code>garden</code> namespace labeled with <code>gardener.cloud/purpose=remote-garden</code>.
You must create the <code>garden</code> namespace and label it manually before running <code>make remote-garden-up</code> as described below.</p><p>Use the provided <code>Makefile</code> rules to bootstrap your remote Garden:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export KUBECONFIG=&lt;path to kubeconfig&gt;
</span></span><span style=display:flex><span>make remote-garden-up
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span><span style=color:green># Start gardener etcd used to store gardener resources (e.g., seeds, shoots)</span>
</span></span><span style=display:flex><span>Starting gardener-dev-remote gardener-etcd cluster!
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span><span style=color:green># Open tunnels for accessing local gardener components from the remote cluster</span>
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>To close the tunnels and remove the locally-running Docker containers, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make remote-garden-down
</span></span></code></pre></div><p>ℹ️ [Optional] If you want to use the remote Garden cluster setup with the <code>SeedAuthorization</code> feature, you have to adapt the <code>kube-apiserver</code> process of your remote Garden cluster. To do this, perform the following steps after running <code>make remote-garden-up</code>:</p><ul><li><p>Create an <a href=https://kubernetes.io/docs/reference/access-authn-authz/webhook/#configuration-file-format>authorization webhook configuration file</a> using the IP of the <code>garden/quic-server</code> pod running in your remote Garden cluster and port 10444 that tunnels to your locally running <code>gardener-admission-controller</code> process.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>current-context: seedauthorizer
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- name: gardener-admission-controller
</span></span><span style=display:flex><span>  cluster:
</span></span><span style=display:flex><span>    insecure-skip-tls-verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    server: https://&lt;quic-server-pod-ip&gt;:10444/webhooks/auth/seed
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: kube-apiserver
</span></span><span style=display:flex><span>  user: {}
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- name: seedauthorizer
</span></span><span style=display:flex><span>  context:
</span></span><span style=display:flex><span>    cluster: gardener-admission-controller
</span></span><span style=display:flex><span>    user: kube-apiserver
</span></span></code></pre></div></li><li><p>Change or add the following command line parameters to your <code>kube-apiserver</code> process:</p><ul><li><code>--authorization-mode=&lt;...>,Webhook</code></li><li><code>--authorization-webhook-config-file=&lt;path to config file></code></li><li><code>--authorization-webhook-cache-authorized-ttl=0</code></li><li><code>--authorization-webhook-cache-unauthorized-ttl=0</code></li></ul></li><li><p>Delete the cluster role and rolebinding <code>gardener.cloud:system:seeds</code> from your remote Garden cluster.</p></li></ul><p>If your remote Garden cluster is a Gardener shoot, and you can access the seed on which this shoot is scheduled, you can automate the above steps by running the <a href=https://github.com/gardener/gardener/blob/master/hack/local-development/remote-garden/enable-seed-authorizer><code>enable-seed-authorizer</code> script</a> and passing the kubeconfig of the seed cluster and the shoot namespace as parameters:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hack/local-development/remote-garden/enable-seed-authorizer &lt;seed kubeconfig&gt; &lt;namespace&gt;
</span></span></code></pre></div><blockquote><p>Note: This script is not working anymore, as the <code>ReversedVPN</code> feature can&rsquo;t be disabled. The annotation <code>alpha.featuregates.shoot.gardener.cloud/reversed-vpn</code> on <code>Shoot</code>s is no longer respected.</p></blockquote><p>To prevent Gardener from reconciling the shoot and overwriting your changes, add the annotation <code>shoot.gardener.cloud/ignore: 'true'</code> to the remote Garden shoot. Note that this annotation takes effect only if it is enabled via the <code>constollers.shoot.respectSyncPeriodOverwrite: true</code> option in the <code>gardenlet</code> configuration.</p><p>To disable the seed authorizer again, run the same script with <code>-d</code> as a third parameter:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hack/local-development/remote-garden/enable-seed-authorizer &lt;seed kubeconfig&gt; &lt;namespace&gt; -d
</span></span></code></pre></div><p>If the seed authorizer is enabled, you also have to start the <code>gardener-admission-controller</code> via <code>make start-admission-controller</code>.</p><blockquote><p>⚠️ In the remote garden setup all Gardener components run with administrative permissions, i.e., there is no fine-grained access control via RBAC (as opposed to productive installations of Gardener).</p></blockquote></details><h3 id=prepare-the-gardener>Prepare the Gardener</h3><p>Now, that you have started your local cluster, we can go ahead and register the Gardener API Server.
Just point your <code>KUBECONFIG</code> environment variable to the cluster you created in the previous step and run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make dev-setup
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span>namespace/garden created
</span></span><span style=display:flex><span>namespace/garden-dev created
</span></span><span style=display:flex><span>deployment.apps/etcd created
</span></span><span style=display:flex><span>service/etcd created
</span></span><span style=display:flex><span>service/gardener-apiserver created
</span></span><span style=display:flex><span>service/gardener-admission-controller created
</span></span><span style=display:flex><span>endpoints/gardener-apiserver created
</span></span><span style=display:flex><span>endpoints/gardener-admission-controller created
</span></span><span style=display:flex><span>apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created
</span></span><span style=display:flex><span>apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created
</span></span><span style=display:flex><span>apiservice.apiregistration.k8s.io/v1alpha1.seedmanagement.gardener.cloud created
</span></span><span style=display:flex><span>apiservice.apiregistration.k8s.io/v1alpha1.settings.gardener.cloud created
</span></span></code></pre></div><p>ℹ️ [Optional] If you want to enable logging, in the gardenlet configuration add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>logging:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type <code>LoadBalancer</code>.
In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate.
You need to have control over a domain (or subdomain) for which these records will be created.
Please provide an <em>internal domain secret</em> (see <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml>this</a> for an example) which contains credentials with the proper privileges. Further information can be found in <a href=/docs/gardener/usage/configuration/>Gardener Configuration and Usage</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f example/10-secret-internal-domain-unmanaged.yaml
</span></span><span style=display:flex><span>secret/internal-domain-unmanaged created
</span></span></code></pre></div><h3 id=run-the-gardener>Run the Gardener</h3><p>Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the gardenlet in different terminal windows/panes using rules in the <code>Makefile</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-apiserver
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span>I0306 15:23:51.044421   74536 plugins.go:84] Registered admission plugin <span style=color:#a31515>&#34;ResourceReferenceManager&#34;</span>
</span></span><span style=display:flex><span>I0306 15:23:51.044523   74536 plugins.go:84] Registered admission plugin <span style=color:#a31515>&#34;DeletionConfirmation&#34;</span>
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span>I0306 15:23:51.626836   74536 secure_serving.go:116] Serving securely on [::]:8443
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>(Optional) Now you are ready to launch the Gardener Controller Manager.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-controller-manager
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting Gardener controller manager...&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Feature Gates: &#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting HTTP server on 0.0.0.0:2718&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Acquired leadership, starting controllers.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting HTTPS server on 0.0.0.0:2719&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Successfully bootstrapped the Garden cluster.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Gardener controller manager (version 1.0.0-dev) initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;ControllerRegistration controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;SecretBinding controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Project controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Quota controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-03-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;CloudProfile controller initialized.&#34;</span>
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>(Optional) Now you are ready to launch the Gardener Scheduler.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-scheduler
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-05-02T16:31:50+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting Gardener scheduler ...&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-05-02T16:31:50+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting HTTP server on 0.0.0.0:10251&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-05-02T16:31:50+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Acquired leadership, starting scheduler.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-05-02T16:31:50+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Gardener scheduler initialized (with Strategy: SameRegion)&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-05-02T16:31:50+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Scheduler controller initialized.&#34;</span>
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>The Gardener should now be ready to operate on Shoot resources. You can use</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get shoots
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><p>to operate against your local running Gardener API Server.</p><blockquote><p>Note: It may take several seconds until the Gardener API server has been started and is available. <code>No resources found</code> is the expected result of our initial development setup.</p></blockquote><h1 id=create-a-shoot>Create a Shoot</h1><p>The steps below describe the general process of creating a Shoot. Have in mind that the steps do not provide full example manifests. The reader needs to check the provider documentation and adapt the manifests accordingly.</p><h4 id=1-copy-the-example-manifests>1. Copy the Example Manifests</h4><p>The next steps require modifications of the example manifests. These modifications are part of local setup and should not be <code>git push</code>-ed. To do not interfere with git, let&rsquo;s copy the example manifests to <code>dev/</code> which is ignored by git.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp example/*.yaml dev/
</span></span></code></pre></div><h4 id=2-create-a-project>2. Create a Project</h4><p>Every Shoot is associated with a Project. Check the corresponding example manifests <code>dev/00-namespace-garden-dev.yaml</code> and <code>dev/05-project-dev.yaml</code>. Adapt them and create them.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/00-namespace-garden-dev.yaml
</span></span><span style=display:flex><span>kubectl apply -f dev/05-project-dev.yaml
</span></span></code></pre></div><p>Make sure that the Project is successfully reconciled:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get project dev
</span></span><span style=display:flex><span>NAME   NAMESPACE    STATUS   OWNER                  CREATOR            AGE
</span></span><span style=display:flex><span>dev    garden-dev   Ready    john.doe@example.com   kubernetes-admin   6s
</span></span></code></pre></div><h4 id=3-create-a-cloudprofile>3. Create a CloudProfile</h4><p>The <code>CloudProfile</code> resource is provider specific and describes the underlying cloud provider (available machine types, regions, machine images, etc.). Check the corresponding example manifest <code>dev/30-cloudprofile.yaml</code>. Check also the documentation and example manifests of the provider extension. Adapt <code>dev/30-cloudprofile.yaml</code> and apply it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/30-cloudprofile.yaml
</span></span></code></pre></div><h4 id=4-install-necessary-gardener-extensions>4. Install Necessary Gardener Extensions</h4><p>The <a href=https://github.com/gardener/gardener/blob/master/extensions/README.md#known-extension-implementations>Known Extension Implementations</a> section contains a list of available extension implementations. You need to create a ControllerRegistration and ControllerDeployment for:</p><ul><li>at least one infrastructure provider</li><li>a DNS provider (if the DNS for the Seed is not disabled)</li><li>at least one operating system extension</li><li>at least one network plugin extension</li></ul><p>As a convention, the example ControllerRegistration manifest (containing also the necessary ControllerDeployment) for an extension is located under <code>example/controller-registration.yaml</code> in the corresponding repository (for example for AWS the ControllerRegistration can be found <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/controller-registration.yaml>here</a>). An example creation for provider-aws (make sure to replace <code>&lt;version></code> with the newest released version tag):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/gardener/gardener-extension-provider-aws/&lt;version&gt;/example/controller-registration.yaml
</span></span></code></pre></div><p>Instead of updating extensions manually you can use <a href=https://github.com/gardener/gem>Gardener Extensions Manager</a> to install and update extension controllers. This is especially useful if you want to keep and maintain your development setup for a longer time.
Also, please refer to <a href=/docs/gardener/extensions/controllerregistration/>Registering Extension Controllers</a> for further information about how extensions are registered in case you want to use other versions than the latest releases.</p><h4 id=5-register-a-seed>5. Register a Seed</h4><p>Shoot controlplanes run in seed clusters, so we need to create our first Seed now.</p><p>Check the corresponding example manifest <code>dev/40-secret-seed.yaml</code> and <code>dev/50-seed.yaml</code>. Update <code>dev/40-secret-seed.yaml</code> with base64 encoded kubeconfig of the cluster that will be used as Seed (the scope of the permissions should be identical to the kubeconfig that the gardenlet creates during bootstrapping - for now, <code>cluster-admin</code> privileges are recommended).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/40-secret-seed.yaml
</span></span></code></pre></div><p>Adapt <code>dev/50-seed.yaml</code> - adjust <code>.spec.secretRef</code> to refer the newly created Secret, adjust <code>.spec.provider</code> with the Seed cluster provider and revise the other fields.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/50-seed.yaml
</span></span></code></pre></div><h4 id=6-start-the-gardenlet>6. Start the gardenlet</h4><p>Once the Seed is created, start the gardenlet to reconcile it. The <code>make start-gardenlet</code> command will automatically configure the local gardenlet process to use the Seed and its kubeconfig. If you have multiple Seeds, you have to specify which to use by setting the <code>SEED_NAME</code> environment variable like in <code>make start-gardenlet SEED_NAME=my-first-seed</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-gardenlet
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Starting Gardenlet...&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Feature Gates: HVPA=true, Logging=true&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:17+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Acquired leadership, starting controllers.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Gardenlet (version 1.0.0-dev) initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;ControllerInstallation controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Shoot controller initialized.&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2019-11-06T15:24:18+02:00&#34;</span> level=info msg=<span style=color:#a31515>&#34;Seed controller initialized.&#34;</span>
</span></span><span style=display:flex><span>[...]
</span></span></code></pre></div><p>The gardenlet will now reconcile the Seed. Check the progess from time to time until it&rsquo;s <code>Ready</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get seed
</span></span><span style=display:flex><span>NAME       STATUS    PROVIDER    REGION      AGE    VERSION       K8S VERSION
</span></span><span style=display:flex><span>seed-aws   Ready     aws         eu-west-1   4m     v1.61.0-dev   v1.24.8
</span></span></code></pre></div><h4 id=7-create-a-shoot>7. Create a Shoot</h4><p>A Shoot requires a SecretBinding. The SecretBinding refers to a Secret that contains the cloud provider credentials. The Secret data keys are provider specific and you need to check the documentation of the provider to find out which data keys are expected (for example for AWS the related documentation can be found at <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#provider-secret-data>Provider Secret Data</a>). Adapt <code>dev/70-secret-provider.yaml</code> and <code>dev/80-secretbinding.yaml</code> and apply them.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/70-secret-provider.yaml
</span></span><span style=display:flex><span>kubectl apply -f dev/80-secretbinding.yaml
</span></span></code></pre></div><p>After the SecretBinding creation, you are ready to proceed with the Shoot creation. You need to check the documentation of the provider to find out the expected configuration (for example for AWS the related documentation and example Shoot manifest can be found at <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/>Using the AWS provider extension with Gardener as end-user</a>). Adapt <code>dev/90-shoot.yaml</code> and apply it.</p><p>To make sure that a specific Seed cluster will be chosen or to skip the scheduling (the sheduling requires Gardener Scheduler to be running), specify the <code>.spec.seedName</code> field (see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L317-L318>here</a>).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f dev/90-shoot.yaml
</span></span></code></pre></div><p>Watch the progress of the operation and make sure that the Shoot will be successfully created.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>watch kubectl get shoot --all-namespaces
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ba2932673ad3c4565462541e83ca1ede>1.4.8 - Log Parsers</h1><h1 id=how-to-create-log-parser-for-container-into-fluent-bit>How to Create Log Parser for Container into fluent-bit</h1><p>If our log message is parsed correctly, it has to be showed in Grafana like this:</p><pre tabindex=0><code class=language-jsonc data-lang=jsonc>  {&#34;log&#34;:&#34;OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io&#34;,&#34;pid&#34;:&#34;1&#34;,&#34;severity&#34;:&#34;INFO&#34;,&#34;source&#34;:&#34;controller.go:107&#34;}
</code></pre><p>Otherwise it will looks like this:</p><pre tabindex=0><code class=language-jsonc data-lang=jsonc>{
  &#34;log&#34;:&#34;{
  \&#34;level\&#34;:\&#34;info\&#34;,\&#34;ts\&#34;:\&#34;2020-06-01T11:23:26.679Z\&#34;,\&#34;logger\&#34;:\&#34;gardener-resource-manager.health-reconciler\&#34;,\&#34;msg\&#34;:\&#34;Finished ManagedResource health checks\&#34;,\&#34;object\&#34;:\&#34;garden/provider-aws-dsm9r\&#34;
  }\n&#34;
  }
}
</code></pre><h2 id=create-a-custom-parser>Create a Custom Parser</h2><ul><li><p>First of all, we need to know how the log for the specific container looks like (for example, lets take a log from the <code>alertmanager</code> :
<code>level=info ts=2019-01-28T12:33:49.362015626Z caller=main.go:175 build_context="(go=go1.11.2, user=root@4ecc17c53d26, date=20181109-15:40:48)</code>)</p></li><li><p>We can see that this log contains 4 subfields(severity=info, timestamp=2019-01-28T12:33:49.362015626Z, source=main.go:175 and the actual message).
So we have to write a regex which matches this log in 4 groups(We can use <a href=https://regex101.com/>https://regex101.com/</a> like helping tool). So, for this purpose our regex looks like this:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>^level=(?&lt;severity&gt;\w+)\s+ts=(?&lt;time&gt;\d{4}-\d{2}-\d{2}[Tt].*[zZ])\s+caller=(?&lt;source&gt;[^\s]*+)\s+(?&lt;log&gt;.*)
</span></span></code></pre></div><ul><li>Now we have to create correct time format for the timestamp (We can use this site for this purpose: <a href=http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime>http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime</a>).
So our timestamp matches correctly the following format:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>%Y-%m-%dT%H:%M:%S.%L
</span></span></code></pre></div><ul><li>It&rsquo;s time to apply our new regex into fluent-bit configuration. Go to <code>fluent-bit-configmap.yaml</code> and create new filter using the following template:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[FILTER]
</span></span><span style=display:flex><span>        Name                parser
</span></span><span style=display:flex><span>        Match               kubernetes.&lt;&lt; pod-name &gt;&gt;*&lt;&lt; container-name &gt;&gt;*
</span></span><span style=display:flex><span>        Key_Name            log
</span></span><span style=display:flex><span>        Parser              &lt;&lt; parser-name &gt;&gt;
</span></span><span style=display:flex><span>        Reserve_Data        True
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>EXAMPLE
</span></span><span style=display:flex><span>[FILTER]
</span></span><span style=display:flex><span>        Name                parser
</span></span><span style=display:flex><span>        Match               kubernetes.alertmanager*alertmanager*
</span></span><span style=display:flex><span>        Key_Name            log
</span></span><span style=display:flex><span>        Parser              alermanagerParser
</span></span><span style=display:flex><span>        Reserve_Data        True
</span></span></code></pre></div><ul><li>Now lets check if there already exists parser with such a regex and time format that we need. If it doesn&rsquo;t, create one:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[PARSER]
</span></span><span style=display:flex><span>        Name        &lt;&lt; parser-name &gt;&gt;
</span></span><span style=display:flex><span>        Format      regex
</span></span><span style=display:flex><span>        Regex       &lt;&lt; regex &gt;&gt;
</span></span><span style=display:flex><span>        Time_Key    time
</span></span><span style=display:flex><span>        Time_Format &lt;&lt; time-format &gt;&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>EXAMPLE
</span></span><span style=display:flex><span>[PARSER]
</span></span><span style=display:flex><span>        Name        alermanagerParser
</span></span><span style=display:flex><span>        Format      regex
</span></span><span style=display:flex><span>        Regex       ^level=(?&lt;severity&gt;\w+)\s+ts=(?&lt;time&gt;\d{4}-\d{2}-\d{2}[Tt].*[zZ])\s+caller=(?&lt;source&gt;[^\s]*+)\s+(?&lt;log&gt;.*)
</span></span><span style=display:flex><span>        Time_Key    time
</span></span><span style=display:flex><span>        Time_Format %Y-%m-%dT%H:%M:%S.%L
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Follow your development setup to validate that the parsers are working correctly.
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6072f273ee0aa37eba651d226b098168>1.4.9 - Logging</h1><h1 id=logging-in-gardener-components>Logging in Gardener Components</h1><p>This document aims at providing a general developer guideline on different aspects of logging practices and conventions used in the Gardener codebase.
It contains mostly Gardener-specific points, and references other existing and commonly accepted logging guidelines for general advice.
Developers and reviewers should consult this guide when writing, refactoring, and reviewing Gardener code.
If parts are unclear or new learnings arise, this guide should be adapted accordingly.</p><h2 id=logging-libraries--implementations>Logging Libraries / Implementations</h2><p>Historically, Gardener components have been using <a href=https://github.com/sirupsen/logrus>logrus</a>.
There is a global logrus logger (<a href=https://github.com/gardener/gardener/blob/626ba7c10e1150819b3905116d3988512c18c9ee/pkg/logger/logrus.go#L28><code>logger.Logger</code></a>) that is initialized by components on startup and used across the codebase.
In most places, it is used as a <code>printf</code>-style logger and only in some instances we make use of logrus&rsquo; structured logging functionality.</p><p>In the process of migrating our components to native controller-runtime components (see <a href=https://github.com/gardener/gardener/issues/4251>gardener/gardener#4251</a>), we also want to make use of controller-runtime&rsquo;s built-in mechanisms for streamlined logging.
controller-runtime uses <a href=https://github.com/go-logr/logr>logr</a>, a simple structured logging interface, for library-internal logging and logging in controllers.</p><p>logr itself is only an interface and doesn&rsquo;t provide an implementation out of the box.
Instead, it needs to be backed by a logging implementation like <a href=https://github.com/go-logr/zapr>zapr</a>. Code that uses the logr interface is thereby not tied to a specific logging implementation and makes the implementation easily exchangeable.
controller-runtime already provides a <a href=https://github.com/kubernetes-sigs/controller-runtime/tree/v0.11.0/pkg/log/zap>set of helpers</a> for constructing zapr loggers, i.e., logr loggers backed by <a href=https://github.com/uber-go/zap>zap</a>, which is a popular logging library in the go community.
Hence, we are migrating our component logging from logrus to logr (backed by zap) as part of <a href=https://github.com/gardener/gardener/issues/4251>gardener/gardener#4251</a>.</p><blockquote><p>⚠️ <code>logger.Logger</code> (logrus logger) is deprecated in Gardener and shall not be used in new code – use logr loggers when writing new code! (also see <a href=#migration-from-logrus-to-logr>Migration from logrus to logr</a>)</p><p>ℹ️ Don&rsquo;t use zap loggers directly, always use the logr interface in order to avoid tight coupling to a specific logging implementation.</p></blockquote><p>gardener-apiserver differs from the other components as it is based on the <a href=https://github.com/kubernetes/apiserver>apiserver library</a> and therefore uses <a href=https://github.com/kubernetes/klog>klog</a> – just like kube-apiserver.
As gardener-apiserver writes (almost) no logs in our coding (outside the apiserver library), there is currently no plan for switching the logging implementation.
Hence, the following sections focus on logging in the controller and admission components only.</p><h2 id=logcheck-tool><code>logcheck</code> Tool</h2><p>To ensure a smooth migration to logr and make logging in Gardener components more consistent, the <a href=https://github.com/gardener/gardener/tree/master/hack/tools/logcheck><code>logcheck</code> tool</a> was added.
It enforces (parts of) this guideline and detects programmer-level errors early on in order to prevent bugs.
Please check out the <a href=https://github.com/gardener/gardener/tree/master/hack/tools/logcheck>tool&rsquo;s documentation</a> for a detailed description.</p><h2 id=structured-logging>Structured Logging</h2><p>Similar to <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md>efforts in the Kubernetes project</a>, we want to migrate our component logs to structured logging.
As motivated above, we will use the logr interface instead of klog though.</p><p>You can read more about the motivation behind structured logging in <a href=https://github.com/go-logr/logr#background>logr&rsquo;s background and FAQ</a> (also see <a href=http://dave.cheney.net/2015/11/05/lets-talk-about-logging>this blog post by Dave Cheney</a>).
Also, make sure to check out controller-runtime&rsquo;s <a href=https://github.com/kubernetes-sigs/controller-runtime/blob/v0.11.0/TMP-LOGGING.md>logging guideline</a> with specifics for projects using the library.
The following sections will focus on the most important takeaways from those guidelines and give general instructions on how to apply them to Gardener and its controller-runtime components.</p><blockquote><p>Note: Some parts in this guideline differ slightly from controller-runtime&rsquo;s document.</p></blockquote><h3 id=tldr-of-structured-logging>TL;DR of Structured Logging</h3><p>❌ Stop using <code>printf</code>-style logging:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> logger *logrus.Logger
</span></span><span style=display:flex><span>logger.Infof(<span style=color:#a31515>&#34;Scaling deployment %s/%s to %d replicas&#34;</span>, deployment.Namespace, deployment.Name, replicaCount)
</span></span></code></pre></div><p>✅ Instead, write static log messages and enrich them with additional structured information in form of key-value pairs:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> logger logr.Logger
</span></span><span style=display:flex><span>logger.Info(<span style=color:#a31515>&#34;Scaling deployment&#34;</span>, <span style=color:#a31515>&#34;deployment&#34;</span>, client.ObjectKeyFromObject(deployment), <span style=color:#a31515>&#34;replicas&#34;</span>, replicaCount)
</span></span></code></pre></div><h2 id=log-configuration>Log Configuration</h2><p>Gardener components can be configured to either log in <code>json</code> (default) or <code>text</code> format:
<code>json</code> format is supposed to be used in production, while <code>text</code> format might be nicer for development.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span># json
</span></span><span style=display:flex><span>{&#34;level&#34;:&#34;info&#34;,&#34;ts&#34;:&#34;2021-12-16T08:32:21.059+0100&#34;,&#34;msg&#34;:&#34;Hello botanist&#34;,&#34;garden&#34;:&#34;eden&#34;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># text
</span></span><span style=display:flex><span>2021-12-16T08:32:21.059+0100    INFO    Hello botanist  {&#34;garden&#34;: &#34;eden&#34;}
</span></span></code></pre></div><p>Components can be set to one of the following log levels (with increasing verbosity): <code>error</code>, <code>info</code> (default), <code>debug</code>.</p><h2 id=log-levels>Log Levels</h2><p>logr uses <a href=https://github.com/go-logr/logr#why-v-levels>V-levels</a> (numbered log levels), higher V-level means higher verbosity.
V-levels are relative (in contrast to <code>klog</code>&rsquo;s absolute V-levels), i.e., <code>V(1)</code> creates a logger, that is one level more verbose than its parent logger.</p><p>In Gardener components, the mentioned log levels in the component config (<code>error</code>, <code>info</code>, <code>debug</code>) map to the zap levels with the same names (see <a href=https://github.com/gardener/gardener/blob/770fc01a34b70f6cb77b8cfe929d9daef0026d1c/pkg/logger/zap.go#L43-L55>here</a>).
Hence, our loggers follow the same mapping from numerical logr levels to named zap levels like described in <a href=https://github.com/go-logr/zapr/tree/v1.1.0#increasing-verbosity>zapr</a>, i.e.:</p><ul><li>component config specifies <code>debug</code> ➡️ both <code>V(0)</code> and <code>V(1)</code> are enabled</li><li>component config specifies <code>info</code> ➡️ <code>V(0)</code> is enabled, <code>V(1)</code> will not be shown</li><li>component config specifies <code>error</code> ➡️ neither <code>V(0)</code> nor <code>V(1)</code> will be shown</li><li><code>Error()</code> logs will always be shown</li></ul><p>This mapping applies to the components&rsquo; root loggers (the ones that are not &ldquo;derived&rdquo; from any other logger; constructed on component startup).
If you derive a new logger with e.g. <code>V(1)</code>, the mapping will shift by one. For example, <code>V(0)</code> will then log at zap&rsquo;s <code>debug</code> level.</p><p>There is no <code>warning</code> level (see <a href=https://dave.cheney.net/2015/11/05/lets-talk-about-logging>Dave Cheney&rsquo;s post</a>).
If there is an error condition (e.g., unexpected error received from a called function), the error should either be handled or logged at <code>error</code> if it is neither handled nor returned.
If you have an <code>error</code> value at hand that doesn&rsquo;t represent an actual error condition, but you still want to log it as an informational message, log it at <code>info</code> level with key <code>err</code>.</p><p>We might consider to make use of a broader range of log levels in the future when introducing more logs and common command line flags for our components (comparable to <code>--v</code> of Kubernetes components).
For now, we stick to the mentioned two log levels like controller-runtime: info (<code>V(0)</code>) and debug (<code>V(1)</code>).</p><h2 id=logging-in-controllers>Logging in Controllers</h2><h3 id=named-loggers>Named Loggers</h3><p>Controllers should use named loggers that include their name, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>controllerLogger := rootLogger.WithName(<span style=color:#a31515>&#34;controller&#34;</span>).WithName(<span style=color:#a31515>&#34;shoot&#34;</span>)
</span></span><span style=display:flex><span>controllerLogger.Info(<span style=color:#a31515>&#34;Deploying kube-apiserver&#34;</span>)
</span></span></code></pre></div><p>results in</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>2021-12-16T09:27:56.550+0100    INFO    controller.shoot    Deploying kube-apiserver
</span></span></code></pre></div><p>Logger names are hierarchical. You can make use of it, where controllers are composed of multiple &ldquo;subcontrollers&rdquo;, e.g., <code>controller.shoot.hibernation</code> or <code>controller.shoot.maintenance</code>.</p><p>Using the global logger <code>logf.Log</code> directly is discouraged and should be rather exceptional because it makes correlating logs with code harder.
Preferably, all parts of the code should use some named logger.</p><h3 id=reconciler-loggers>Reconciler Loggers</h3><p>In your <code>Reconcile</code> function, retrieve a logger from the given <code>context.Context</code>.
It inherits from the controller&rsquo;s logger (i.e., is already named) and is preconfigured with <code>name</code> and <code>namespace</code> values for the reconciliation request:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>func</span> (r *reconciler) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, <span style=color:#2b91af>error</span>) {
</span></span><span style=display:flex><span>  log := logf.FromContext(ctx)
</span></span><span style=display:flex><span>  log.Info(<span style=color:#a31515>&#34;Reconciling Shoot&#34;</span>)
</span></span><span style=display:flex><span>  <span style=color:green>// ...
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:#00f>return</span> reconcile.Result{}, <span style=color:#00f>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>results in</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>2021-12-16T09:35:59.099+0100    INFO    controller.shoot    Reconciling Shoot        {&#34;name&#34;: &#34;sunflower&#34;, &#34;namespace&#34;: &#34;garden-greenhouse&#34;}
</span></span></code></pre></div><p>The logger is injected by controller-runtime&rsquo;s <code>Controller</code> implementation. The logger returned by <code>logf.FromContext</code> is never <code>nil</code>. If the context doesn&rsquo;t carry a logger, it falls back to the global logger (<code>logf.Log</code>), which might discard logs if not configured, but is also never <code>nil</code>.</p><blockquote><p>⚠️ Make sure that you don&rsquo;t overwrite the <code>name</code> or <code>namespace</code> value keys for such loggers, otherwise you will lose information about the reconciled object.</p></blockquote><p>The controller implementation (controller-runtime) itself takes care of logging the error returned by reconcilers.
Hence, don&rsquo;t log an error that you are returning.
Generally, functions should not return an error, if they already logged it, because that means the error is already handled and not an error anymore.
See <a href=https://dave.cheney.net/2015/11/05/lets-talk-about-logging>Dave Cheney&rsquo;s post</a> for more on this.</p><h3 id=messages>Messages</h3><ul><li>Log messages should be static. Don&rsquo;t put variable content in there, i.e., no <code>fmt.Sprintf</code> or string concatenation (<code>+</code>). Use key-value pairs instead.</li><li>Log messages should be capitalized. Note: This contrasts with error messages, that should not be capitalized. However, both should not end with a punctuation mark.</li></ul><h3 id=keys-and-values>Keys and Values</h3><ul><li><p>Use <code>WithValues</code> instead of repeatedly adding key-value pairs for multiple log statements. <code>WithValues</code> creates a new logger from the parent, that carries the given key-value pairs. E.g., use it when acting on one object in multiple steps and logging something for each step:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>log := parentLog.WithValues(<span style=color:#a31515>&#34;infrastructure&#34;</span>, client.ObjectKeyFromObject(infrastrucutre))
</span></span><span style=display:flex><span><span style=color:green>// ...
</span></span></span><span style=display:flex><span><span style=color:green></span>log.Info(<span style=color:#a31515>&#34;Creating Infrastructure&#34;</span>)
</span></span><span style=display:flex><span><span style=color:green>// ...
</span></span></span><span style=display:flex><span><span style=color:green></span>log.Info(<span style=color:#a31515>&#34;Waiting for Infrastructure to be reconciled&#34;</span>)
</span></span><span style=display:flex><span><span style=color:green>// ...
</span></span></span></code></pre></div></li></ul><blockquote><p>Note: <code>WithValues</code> bypasses controller-runtime&rsquo;s special zap encoder that nicely encodes <code>ObjectKey</code>/<code>NamespacedName</code> and <code>runtime.Object</code> values, see <a href=https://github.com/kubernetes-sigs/controller-runtime/issues/1290>kubernetes-sigs/controller-runtime#1290</a>.
Thus, the end result might look different depending on the value and its <code>Stringer</code> implementation.</p></blockquote><ul><li><p>Use <a href=https://en.wiktionary.org/wiki/lowerCamelCase>lowerCamelCase</a> for keys. Don&rsquo;t put spaces in keys, as it will make log processing with simple tools like <code>jq</code> harder.</p></li><li><p>Keys should be constant, human-readable, consistent across the codebase and naturally match parts of the log message, see <a href=https://github.com/go-logr/logr#how-do-i-choose-my-keys>logr guideline</a>.</p></li><li><p>When logging object keys (name and namespace), use the object&rsquo;s type as the log key and a <code>client.ObjectKey</code>/<code>types.NamespacedName</code> value as value, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> deployment *appsv1.Deployment
</span></span><span style=display:flex><span>log.Info(<span style=color:#a31515>&#34;Creating Deployment&#34;</span>, <span style=color:#a31515>&#34;deployment&#34;</span>, client.ObjectKeyFromObject(deployment))
</span></span></code></pre></div><p>which results in</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#34;level&#34;:&#34;info&#34;,&#34;ts&#34;:&#34;2021-12-16T08:32:21.059+0100&#34;,&#34;msg&#34;:&#34;Creating Deployment&#34;,&#34;deployment&#34;:{&#34;name&#34;: &#34;bar&#34;, &#34;namespace&#34;: &#34;foo&#34;}}
</span></span></code></pre></div><p>Earlier, we often used <code>kutil.ObjectName()</code> for logging object keys, which encodes them into a flat string like <code>foo/bar</code>. However, this flat string cannot be processed so easily by logging stacks (or <code>jq</code>) like a structured log. Hence, the use of <code>kutil.ObjectName()</code> for logging object keys is discouraged. Existing usages should be refactored to use <code>client.ObjectKeyFromObject()</code> instead.</p></li><li><p>There are cases where you don&rsquo;t have the full object key or the object itself at hand, e.g., if an object references another object (in the same namespace) by name (think <code>secretRef</code> or similar).
In such a cases, either construct the full object key including the implied namespace or log the object name under a key ending in <code>Name</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>  <span style=color:green>// object to reconcile
</span></span></span><span style=display:flex><span><span style=color:green></span>  shoot *gardencorev1beta1.Shoot
</span></span><span style=display:flex><span>  <span style=color:green>// retrieved via logf.FromContext, preconfigured by controller with namespace and name of reconciliation request
</span></span></span><span style=display:flex><span><span style=color:green></span>  log logr.Logger
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// option a: full object key, manually constructed
</span></span></span><span style=display:flex><span><span style=color:green></span>log.Info(<span style=color:#a31515>&#34;Shoot uses SecretBinding&#34;</span>, <span style=color:#a31515>&#34;secretBinding&#34;</span>, client.ObjectKey{Namespace: shoot.Namespace, Name: shoot.Spec.SecretBindingName})
</span></span><span style=display:flex><span><span style=color:green>// option b: only name under respective *Name log key
</span></span></span><span style=display:flex><span><span style=color:green></span>log.Info(<span style=color:#a31515>&#34;Shoot uses SecretBinding&#34;</span>, <span style=color:#a31515>&#34;secretBindingName&#34;</span>, shoot.Spec.SecretBindingName)
</span></span></code></pre></div><p>Both options result in well-structured logs, that are easy to interpret and process:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#34;level&#34;:&#34;info&#34;,&#34;ts&#34;:&#34;2022-01-18T18:00:56.672+0100&#34;,&#34;msg&#34;:&#34;Shoot uses SecretBinding&#34;,&#34;name&#34;:&#34;my-shoot&#34;,&#34;namespace&#34;:&#34;garden-project&#34;,&#34;secretBinding&#34;:{&#34;namespace&#34;:&#34;garden-project&#34;,&#34;name&#34;:&#34;aws&#34;}}
</span></span><span style=display:flex><span>{&#34;level&#34;:&#34;info&#34;,&#34;ts&#34;:&#34;2022-01-18T18:00:56.673+0100&#34;,&#34;msg&#34;:&#34;Shoot uses SecretBinding&#34;,&#34;name&#34;:&#34;my-shoot&#34;,&#34;namespace&#34;:&#34;garden-project&#34;,&#34;secretBindingName&#34;:&#34;aws&#34;}
</span></span></code></pre></div></li><li><p>When handling generic <code>client.Object</code> values (e.g. in helper funcs), use <code>object</code> as key.</p></li><li><p>When adding timestamps to key-value pairs, use <code>time.Time</code> values. By this, they will be encoded in the same format as the log entry&rsquo;s timestamp.<br>Don&rsquo;t use <code>metav1.Time</code> values, as they will be encoded in a different format by their <code>Stringer</code> implementation. Pass <code>&lt;someTimestamp>.Time</code> to loggers in case you have a <code>metav1.Time</code> value at hand.</p></li><li><p>Same applies to durations. Use <code>time.Duration</code> values instead of <code>*metav1.Duration</code>. Durations can be handled specially by zap just like timestamps.</p></li><li><p>Event recorders not only create <code>Event</code> objects but also log them.
However, both Gardener&rsquo;s manually instantiated event recorders and the ones that controller-runtime provides log to <code>debug</code> level and use generic formats, that are not very easy to interpret or process (no structured logs).
Hence, don&rsquo;t use event recorders as replacements for well-structured logs.
If a controller records an event for a completed action or important information, it should probably log it as well, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>log.Info(<span style=color:#a31515>&#34;Creating ManagedSeed&#34;</span>, <span style=color:#a31515>&#34;replica&#34;</span>, r.GetObjectKey())
</span></span><span style=display:flex><span>a.recorder.Eventf(managedSeedSet, corev1.EventTypeNormal, EventCreatingManagedSeed, <span style=color:#a31515>&#34;Creating ManagedSeed %s&#34;</span>, r.GetFullName())
</span></span></code></pre></div></li></ul><h2 id=logging-in-test-code>Logging in Test Code</h2><ul><li><p>If the tested production code requires a logger, you can pass <code>logr.Discard()</code> or <code>logf.NullLogger{}</code> in your test, which simply discards all logs.</p></li><li><p><code>logf.Log</code> is safe to use in tests and will not cause a nil pointer deref, even if it&rsquo;s not initialized via <code>logf.SetLogger</code>.
It is initially set to a <code>NullLogger</code> by default, which means all logs are discarded, unless <code>logf.SetLogger</code> is called in the first 30 seconds of execution.</p></li><li><p>Pass <code>zap.WriteTo(GinkgoWriter)</code> in tests where you want to see the logs on test failure but not on success, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>logf.SetLogger(logger.MustNewZapLogger(logger.DebugLevel, logger.FormatJSON, zap.WriteTo(GinkgoWriter)))
</span></span><span style=display:flex><span>log := logf.Log.WithName(<span style=color:#a31515>&#34;test&#34;</span>)
</span></span></code></pre></div></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d42cb3e424a70c7c39b9f2acb996f1c8>1.4.10 - Monitoring Stack</h1><h1 id=extending-the-monitoring-stack>Extending the Monitoring Stack</h1><p>This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.</p><p>Please ensure that you have understood the basic principles of <a href=https://prometheus.io/docs/introduction/overview/>Prometheus</a> and its ecosystem before you continue.</p><p>‼️ <strong>The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.</strong></p><h2 id=overview>Overview</h2><p><img src=/__resources/monitoring-architecture_cd945d.png alt="Monitoring Architecture"></p><p>Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:</p><ul><li>Seed<ul><li><a href=https://github.com/prometheus/prometheus>Prometheus</a></li><li><a href=https://github.com/grafana/grafana>Grafana</a></li><li><a href=https://github.com/prometheus/blackbox_exporter>blackbox-exporter</a></li><li><a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> (Seed metrics)</li><li><a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> (Shoot metrics)</li><li><a href=https://github.com/prometheus/alertmanager>Alertmanager</a> (Optional)</li></ul></li><li>Shoot<ul><li><a href=https://github.com/prometheus/node_exporter>node-exporter(s)</a></li><li><a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a></li><li><a href=https://github.com/prometheus/blackbox_exporter>blackbox-exporter</a></li></ul></li></ul><p>In each Seed cluster there is a Prometheus in the <code>garden</code> namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.</p><p>The alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the <code>garden</code> namespace of the Seed. The purpose of this central Alertmanager is to forward all important alerts to the operators of the Gardener setup.</p><p>The Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described in <a href=/docs/gardener/monitoring/alerting/>Alerting</a>.</p><h2 id=adding-new-monitoring-targets>Adding New Monitoring Targets</h2><p>After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.</p><p>Prometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in <code>charts/seed-monitoring/charts/prometheus/templates/config.yaml</code>.
New scrape jobs can be added in the section <code>scrape_configs</code>. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the <a href=https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config>Prometheus documentation</a>.</p><p>The <code>job_name</code> of a scrape job should be the name of the component e.g. <code>kube-apiserver</code> or <code>vpn</code>. The collection interval should be the default of <code>30s</code>. You do not need to specify this in the configuration.</p><p>Please do not ingest all metrics which are provided by a component. Rather, collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following <code>metric_relabel_configs</code> statement to your scrape jobs (replace <code>exampleComponent</code> with component name).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    - job_name: example-component
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      metric_relabel_configs:
</span></span><span style=display:flex><span>{{ include &#34;prometheus.keep-metrics.metric-relabel-config&#34; .Values.allowedMetrics.exampleComponent | indent 6 }}
</span></span></code></pre></div><p>The whitelist for the metrics of your job can be maintained in <code>charts/seed-monitoring/charts/prometheus/values.yaml</code> in section <code>allowedMetrics.exampleComponent</code> (replace <code>exampleComponent</code> with component name). Check the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>allowedMetrics:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  exampleComponent:
</span></span><span style=display:flex><span>  * metrics_name_1
</span></span><span style=display:flex><span>  * metrics_name_2
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=adding-alerts>Adding Alerts</h2><p>The alert definitons are located in <code>charts/seed-monitoring/charts/prometheus/rules</code>. There are two approaches for adding new alerts.</p><ol><li>Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component.</li><li>Adding alerts for a new component. In this case a new rule file with name scheme <code>example-component.rules.yaml</code> needs to be added.</li><li>Add the new alert to <code>alertInhibitionGraph.dot</code>, add any required inhibition flows and render the new graph. To render the graph, run:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png
</span></span></code></pre></div><ol><li>Create a test for the new alert. See <code>Alert Tests</code>.</li></ol><p>Example alert:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>groups:
</span></span><span style=display:flex><span>* name: example.rules
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  * alert: ExampleAlert
</span></span><span style=display:flex><span>    expr: absent(up{job=&#34;exampleJob&#34;} == 1)
</span></span><span style=display:flex><span>    for: 20m
</span></span><span style=display:flex><span>    labels:
</span></span><span style=display:flex><span>      service: example
</span></span><span style=display:flex><span>      severity: critical <span style=color:green># How severe is the alert? (blocker|critical|info|warning)</span>
</span></span><span style=display:flex><span>      type: shoot <span style=color:green># For which topology is the alert relevant? (seed|shoot)</span>
</span></span><span style=display:flex><span>      visibility: all <span style=color:green># Who should receive the alerts? (all|operator|owner)</span>
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      description: A longer description of the example alert that should also explain the impact of the alert.
</span></span><span style=display:flex><span>      summary: Short summary of an example alert.
</span></span></code></pre></div><p>If the deployment of component is optional then the alert definitions needs to be added to <code>charts/seed-monitoring/charts/prometheus/optional-rules</code> instead. Furthermore the alerts for component need to be activatable in <code>charts/seed-monitoring/charts/prometheus/values.yaml</code> via <code>rules.optional.example-component.enabled</code>. The default should be <code>true</code>.</p><p>Basic instruction how to define alert rules can be found in the <a href=https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules>Prometheus documentation</a>.</p><h3 id=routing-tree>Routing Tree</h3><p>The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency, etc. You can find more information about Alertmanager routing in the <a href=https://prometheus.io/docs/alerting/configuration/#route>Prometheus/Alertmanager documentation</a>. The routing trees for the Alertmanagers deployed by Gardener are depicted below.</p><p>Central Seed Alertmanager</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>∟ main route (all alerts for all shoots on the seed will enter)
</span></span><span style=display:flex><span>  ∟ group by project and shoot name
</span></span><span style=display:flex><span>    ∟ group by visibility &#34;all&#34; and &#34;operator&#34;
</span></span><span style=display:flex><span>      ∟ group by severity &#34;blocker&#34;, &#34;critical&#34;, and &#34;info&#34; → route to Garden operators
</span></span><span style=display:flex><span>      ∟ group by severity &#34;warning&#34; (dropped)
</span></span><span style=display:flex><span>    ∟ group by visibility &#34;owner&#34; (dropped)
</span></span></code></pre></div><p>Shoot Alertmanager</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>∟ main route (only alerts for one Shoot will enter)
</span></span><span style=display:flex><span>  ∟ group by visibility &#34;all&#34; and &#34;owner&#34;
</span></span><span style=display:flex><span>    ∟ group by severity &#34;blocker&#34;, &#34;critical&#34;, and &#34;info&#34; → route to cluster alert receiver
</span></span><span style=display:flex><span>    ∟ group by severity &#34;warning&#34; (dropped, will change soon → route to cluster alert receiver)
</span></span><span style=display:flex><span>  ∟ group by visibility &#34;operator&#34; (dropped)
</span></span></code></pre></div><h3 id=alert-inhibition>Alert Inhibition</h3><p>All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can&rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert, make sure to add it to the diagram.</p><p><img src=/__resources/alertInhibitionGraph_ceaef0.png alt=alertDiagram></p><h3 id=alert-attributes>Alert Attributes</h3><p>Each alert rule definition has to contain the following annotations:</p><ul><li><strong>summary</strong>: A short description of the issue.</li><li><strong>description</strong>: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.</li></ul><p>In addtion, each alert must contain the following labels:</p><ul><li><strong>type</strong><ul><li><code>shoot</code>: Components running on the Shoot worker nodes in the <code>kube-system</code> namespace.</li><li><code>seed</code>: Components running on the Seed in the Shoot namespace as part of/next to the control plane.</li></ul></li><li><strong>service</strong><ul><li>Name of the component (in lowercase) e.g. <code>kube-apiserver</code>, <code>alertmanager</code> or <code>vpn</code>.</li></ul></li><li><strong>severity</strong><ul><li><code>blocker</code>: All issues which make the cluster entirely unusable, e.g. <code>KubeAPIServerDown</code> or <code>KubeSchedulerDown</code></li><li><code>critical</code>: All issues which affect single functionalities/components but do not affect the cluster in its core functionality e.g. <code>VPNDown</code> or <code>KubeletDown</code>.</li><li><code>info</code>: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity)</li><li><code>warning</code>: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. <code>HighLatencyApiServerToWorkers</code> or <code>ApiServerResponseSlow</code>.</li></ul></li></ul><h3 id=alert-tests>Alert Tests</h3><p>To test the Prometheus alerts:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-prometheus
</span></span></code></pre></div><p>If you want to add alert tests:</p><ol><li><p>Create a new file in <code>rules-tests</code> in the form <code>&lt;alert-group-name>.rules.test.yaml</code> or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.</p></li><li><p>Make sure that newly added tests succeed. See above.</p></li></ol><h2 id=adding-grafana-dashboards>Adding Grafana Dashboards</h2><p>The dashboard definition files are located in <code>charts/seed-monitoring/charts/grafana/dashboards</code>. Every dashboard needs its own file.</p><p>If you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.</p><h3 id=dashboard-structure>Dashboard Structure</h3><p>The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.</p><ul><li>Kubernetes control plane components (Tag: <code>control-plane</code>)<ul><li>All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager</li><li>ETCD + Backup/Restore</li><li>Kubernetes Addon Manager</li></ul></li><li>Node/Machine components (Tag: <code>node/machine</code>)<ul><li>All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets</li><li>Machine-Controller-Manager + Cluster Autoscaler</li></ul></li><li>Networking components (Tag: <code>network</code>)<ul><li>CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress</li></ul></li><li>Addon components (Tag: <code>addon</code>)<ul><li>Cert Broker</li></ul></li><li>Monitoring components (Tag: <code>monitoring</code>)</li><li>Logging components (Tag: <code>logging</code>)</li></ul><h4 id=mandatory-charts-for-component-dashboards>Mandatory Charts for Component Dashboards</h4><p>For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.</p><ol><li>Pod up/down status <code>up{job="example-component"}</code></li><li>Pod/containers cpu utilization</li><li>Pod/containers memorty consumption</li><li>Pod/containers network i/o</li></ol><p>That information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.</p><h5 id=chart-requirements>Chart Requirements</h5><p>Each chart needs to contain:</p><ul><li>a meaningful name</li><li>a detailed description (for non trivial charts)</li><li>appropriate x/y axis descriptions</li><li>appropriate scaling levels for the x/y axis</li><li>proper units for the x/y axis</li></ul><h5 id=dashboard-parameters>Dashboard Parameters</h5><p>The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.</p><p>Dashboards have to:</p><ul><li>contain a title which refers to the component name(s)</li><li>contain a timezone statement which should be the browser time</li><li>contain tags which express where the component is running (<code>seed</code> or <code>shoot</code>) and to which category the component belong (see dashboard structure)</li><li>contain a version statement with a value of 1</li><li>be immutable</li></ul><p>Example dashboard configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;title&#34;: <span style=color:#a31515>&#34;example-component&#34;</span>,
</span></span><span style=display:flex><span>  &#34;timezone&#34;: <span style=color:#a31515>&#34;utc&#34;</span>,
</span></span><span style=display:flex><span>  &#34;tags&#34;: [
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;seed&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;control-plane&#34;</span>
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  &#34;version&#34;: 1,
</span></span><span style=display:flex><span>  &#34;editable&#34;: <span style=color:#a31515>&#34;false&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Furthermore, all dashboards should contain the following time options:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;time&#34;: {
</span></span><span style=display:flex><span>    &#34;from&#34;: <span style=color:#a31515>&#34;now-1h&#34;</span>,
</span></span><span style=display:flex><span>    &#34;to&#34;: <span style=color:#a31515>&#34;now&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  &#34;timepicker&#34;: {
</span></span><span style=display:flex><span>    &#34;refresh_intervals&#34;: [
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;30s&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;1m&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;5m&#34;</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;time_options&#34;: [
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;5m&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;15m&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;1h&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;6h&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;12h&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;24h&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;2d&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;10d&#34;</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0323918722a5b41e437d646bc4e3ca40>1.4.11 - New Cloud Provider</h1><h1 id=adding-cloud-providers>Adding Cloud Providers</h1><p>This document provides an overview of how to integrate a new cloud provider into Gardener. Each component that requires integration has a detailed description of how to integrate it and the steps required.</p><h2 id=cloud-components>Cloud Components</h2><p>Gardener is composed of 2 or more Kubernetes clusters:</p><ul><li>Shoot: These are the end-user clusters, the regular Kubernetes clusters you have seen. They provide places for your workloads to run.</li><li>Seed: This is the &ldquo;management&rdquo; cluster. It manages the control planes of shoots by running them as native Kubernetes workloads.</li></ul><p>These two clusters can run in the same cloud provider, but they do not need to. For example, you could run your Seed in AWS, while having one shoot in Azure, two in Google, two in Alicloud, and three in Equinix Metal.</p><p>The Seed cluster deploys and manages the Shoot clusters. Importantly, for this discussion, the <code>etcd</code> data store backing each Shoot runs as workloads inside the Seed. Thus, to use the above example, the clusters in Azure, Google, Alicloud and Equinix Metal will have their worker nodes and master nodes running in those clouds, but the <code>etcd</code> clusters backing them will run as separate <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>deployments</a> in the Seed Kubernetes cluster on AWS.</p><p>This distinction becomes important when preparing the integration to a new cloud provider.</p><h2 id=gardener-cloud-integration>Gardener Cloud Integration</h2><p>Gardener and its related components integrate with cloud providers at the following key lifecycle elements:</p><ul><li>Create/destroy/get/list machines for the Shoot.</li><li>Create/destroy/get/list infrastructure components for the Shoot, e.g. VPCs, subnets, routes, etc.</li><li>Backup/restore etcd for the Seed via writing files to and reading them from object storage.</li></ul><p>Thus, the integrations you need for your cloud provider depend on whether you want to deploy Shoot clusters to the provider, Seed or both.</p><ul><li>Shoot Only: machine lifecycle management, infrastructure</li><li>Seed: etcd backup/restore</li></ul><h2 id=gardener-api>Gardener API</h2><p>In addition to the requirements to integrate with the cloud provider, you also need to enable the core Gardener app to receive, validate, and process requests to use that cloud provider.</p><ul><li>Expose the cloud provider to the consumers of the Gardener API, so it can be told to use that cloud provider as an option.</li><li>Validate that API as requests come in.</li><li>Write cloud provider specific implementation (called &ldquo;provider extension&rdquo;).</li></ul><h2 id=cloud-provider-api-requirements>Cloud Provider API Requirements</h2><p>In order for a cloud provider to integrate with Gardener, the provider must have an API to perform machine lifecycle events, specifically:</p><ul><li>Create a machine</li><li>Destroy a machine</li><li>Get information about a machine and its state</li><li>List machines</li></ul><p>In addition, if the Seed is to run on the given provider, it also must have an API to save files to block storage and retrieve them, for etcd backup/restore.</p><p>The current integration with cloud providers is to add their API calls to Gardener and the Machine Controller Manager. As both Gardener and the Machine Controller Manager are written in <a href=https://golang.org>go</a>, the cloud provider should have a go SDK. However, if it has an API that is wrappable in go, e.g. a REST API, then you can use that to integrate.</p><p>The Gardener team is working on bringing cloud provider integrations out-of-tree, making them pluggable, which should simplify the process and make it possible to use other SDKs.</p><h2 id=summary>Summary</h2><p>To add a new cloud provider, you need some or all of the following. Each repository contains instructions on how to extend it to a new cloud provider.</p><table><thead><tr><th>Type</th><th>Purpose</th><th>Location</th><th>Documentation</th></tr></thead><tbody><tr><td>Seed or Shoot</td><td>Machine Lifecycle</td><td><a href=https://github.com/gardener/machine-controller-manager>machine-controller-manager</a></td><td><a href=/docs/other-components/machine-controller-manager/docs/development/cp_support_new/>MCM new cloud provider</a></td></tr><tr><td>Seed only</td><td>etcd backup/restore</td><td><a href=https://github.com/gardener/etcd-backup-restore/>etcd-backup-restore</a></td><td>In process</td></tr><tr><td>All</td><td>Extension implementation</td><td><a href=https://github.com/gardener/gardener>gardener</a></td><td><a href=/docs/gardener/extensions/overview/>Extension controller</a></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-d926243c7cdf1cf05524b388093c464b>1.4.12 - New Kubernetes Version</h1><h1 id=adding-support-for-a-new-kubernetes-version>Adding Support For a New Kubernetes Version</h1><p>This document describes the steps needed to perform in order to confidently add support for a new Kubernetes <strong>minor</strong> version.</p><blockquote><p>⚠️ Typically, once a minor Kubernetes version <code>vX.Y</code> is supported by Gardener, then all patch versions <code>vX.Y.Z</code> are also automatically supported without any required action.
This is because patch versions do not introduce any new feature or API changes, so there is nothing that needs to be adapted in <code>gardener/gardener</code> code.</p></blockquote><p>The Kubernetes community release a new minor version roughly every 4 months.
Please refer to the <a href=https://kubernetes.io/releases/release/>official documentation</a> about their release cycles for any additional information.</p><p>Shortly before a new release, an &ldquo;umbrella&rdquo; issue should be opened which is used to collect the required adaptations and to track the work items.
For example, <a href=https://github.com/gardener/gardener/issues/5102>#5102</a> can be used as a template for the issue description.
As you can see, the task of supporting a new Kubernetes version also includes the provider extensions maintained in the <code>gardener</code> GitHub organization and is not restricted to <code>gardener/gardener</code> only.</p><p>Generally, the work items can be split into two groups:
The first group contains tasks specific to the changes in the given Kubernetes release, the second group contains Kubernetes release-independent tasks.</p><blockquote><p>ℹ️ Upgrading the <code>k8s.io/*</code> and <code>sigs.k8s.io/controller-runtime</code> Golang dependencies is typically tracked and worked on separately (see e.g. <a href=https://github.com/gardener/gardener/issues/4772>#4772</a> or <a href=https://github.com/gardener/gardener/issues/5282>#5282</a>).</p></blockquote><h2 id=deriving-release-specific-tasks>Deriving Release-Specific Tasks</h2><p>Most new minor Kubernetes releases incorporate API changes, deprecations, or new features.
The community announces them via their <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/>change logs</a>.
In order to derive the release-specific tasks, the respective change log for the new version <code>vX.Y</code> has to be read and understood (for example, <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md>the changelog</a> for <code>v1.24</code>).</p><p>As already mentioned, typical changes to watch out for are:</p><ul><li>API version promotions or deprecations</li><li>Feature gate promotions or deprecations</li><li>CLI flag changes for Kubernetes components</li><li>New default values in resources</li><li>New available fields in resources</li><li>New features potentially relevant for the Gardener system</li><li>Changes of labels or annotations Gardener relies on</li><li>&mldr;</li></ul><p>Obviously, this requires a certain experience and understanding of the Gardener project so that all &ldquo;relevant changes&rdquo; can be identified.
While reading the change log, add the tasks (along with the respective PR in <code>kubernetes/kubernetes</code> to the umbrella issue).</p><blockquote><p>ℹ️ Some of the changes might be specific to certain cloud providers. Pay attention to those as well and add related tasks to the issue.</p></blockquote><h2 id=list-of-release-independent-tasks>List Of Release-Independent Tasks</h2><p>The following paragraphs describe recurring tasks that need to be performed for each new release.</p><h3 id=make-sure-a-new-hyperkube-image-is-released>Make Sure a New <code>hyperkube</code> Image Is Released</h3><p>The <a href=https://github.com/gardener/hyperkube><code>gardener/hyperkube</code></a> repository is used to release container images consisting of the <code>kubectl</code> and <code>kubelet</code> binaries.</p><p>There is a CI/CD job that runs periodically and releases a new <code>hyperkube</code> image when there is a new Kubernetes release. Before proceeding with the next steps, make sure that a new <code>hyperkube</code> image is released for the corresponding new Kubernetes minor version. Make sure that container image is present in GCR.</p><h3 id=adapting-gardener>Adapting Gardener</h3><ul><li>Allow instantiation of a Kubernetes client for the new minor version and update the <code>README.md</code>:<ul><li>See <a href=https://github.com/gardener/gardener/pull/5255/commits/63bdae022f1cb1c9cbd1cd49b557545dca2ec32a>this</a> example commit.</li></ul></li><li>Maintain the Kubernetes feature gates used for validation of <code>Shoot</code> resources:<ul><li>The feature gates are maintained in <a href=https://github.com/gardener/gardener/blob/master/pkg/utils/validation/features/featuregates.go>this</a> file.</li><li>To maintain this list for new Kubernetes versions, run <code>hack/compare-k8s-feature-gates.sh &lt;old-version> &lt;new-version></code> (e.g. <code>hack/compare-k8s-feature-gates.sh v1.22 v1.23</code>).</li><li>It will present 3 lists of feature gates: those added and those removed in <code>&lt;new-version></code> compared to <code>&lt;old-version></code> and feature gates that got locked to default in <code>&lt;new-version></code>.</li><li>Add all added feature gates to the map with <code>&lt;new-version></code> as <code>AddedInVersion</code> and no <code>RemovedInVersion</code>.</li><li>For any removed feature gates, add <code>&lt;new-version></code> as <code>RemovedInVersion</code> to the already existing feature gate in the map.</li><li>For feature gates locked to default, add <code>&lt;new-version></code> as <code>LockedToDefaultInVersion</code> to the already existing feature gate in the map.</li><li>See <a href=https://github.com/gardener/gardener/pull/5255/commits/97923b0604300ff805def8eae981ed388d5e4a83>this</a> example commit.</li></ul></li><li>Maintain the Kubernetes <code>kube-apiserver</code> admission plugins used for validation of <code>Shoot</code> resources:<ul><li>The admission plugins are maintained in <a href=https://github.com/gardener/gardener/blob/master/pkg/utils/validation/admissionplugins/admissionplugins.go>this</a> file.</li><li>To maintain this list for new Kubernetes versions, run <code>hack/compare-k8s-admission-plugins.sh &lt;old-version> &lt;new-version></code> (e.g. <code>hack/compare-k8s-admission-plugins.sh 1.24 1.25</code>).</li><li>It will present 2 lists of admission plugins: those added and those removed in <code>&lt;new-version></code> compared to <code>&lt;old-version></code>.</li><li>Add all added admission plugins to the <code>admissionPluginsVersionRanges</code> map with <code>&lt;new-version></code> as <code>AddedInVersion</code> and no <code>RemovedInVersion</code>.</li><li>For any removed admission plugins, add <code>&lt;new-version></code> as <code>RemovedInVersion</code> to the already existing admission plugin in the map.</li><li>Flag any admission plugins that are required (plugins that must not be disabled in the <code>Shoot</code> spec) by setting the <code>Required</code> boolean variable to true for the admission plugin in the map.</li><li>Flag any admission plugins that are forbidden by setting the <code>Forbidden</code> boolean variable to true for the admission plugin in the map.</li></ul></li><li>Maintain the <code>ServiceAccount</code> names for the controllers part of <code>kube-controller-manager</code>:<ul><li>The names are maintained in <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/shootsystem/shootsystem.go>this</a> file.</li><li>To maintain this list for new Kubernetes versions, run <code>hack/compare-k8s-controllers.sh &lt;old-version> &lt;new-version></code> (e.g. <code>hack/compare-k8s-controllers.sh 1.22 1.23</code>).</li><li>It will present 2 lists of controllers: those added and those removed in <code>&lt;new-version></code> compared to <code>&lt;old-version></code>.</li><li>Double check whether such <code>ServiceAccount</code> indeed appears in the <code>kube-system</code> namespace when creating a cluster with <code>&lt;new-version></code>. Note that it sometimes might be hidden behind a default-off feature gate. You can create a local cluster with the new version using the <a href=/docs/gardener/development/getting_started_locally/>local provider</a>.</li><li>If it appears, add all added controllers to the list based on the Kubernetes version (<a href=https://github.com/gardener/gardener/blob/5f87b18b951e104c2c25a7145548c8a2d08adefc/pkg/operation/botanist/component/shootsystem/shootsystem.go#L170-L174>example</a>).</li><li>For any removed controllers, add them only to the Kubernetes version if it is low enough.</li></ul></li><li>Maintain copies of the <code>DaemonSet</code> controller&rsquo;s scheduling logic:<ul><li><code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#node-controllerpkgresourcemanagercontrollernode><code>Node</code> controller</a> uses a copy of parts of the <code>DaemonSet</code> controller&rsquo;s logic for determining whether a specific <code>Node</code> should run a daemon pod of a given <code>DaemonSet</code>: see <a href=https://github.com/gardener/gardener/v1.65.0/master/pkg/resourcemanager/controller/node/helper/daemon_controller.go>this file</a>.</li><li>Check the referenced upstream files for changes to the <code>DaemonSet</code> controller&rsquo;s logic and adapt our copies accordingly. This might include introducing version-specific checks in our codebase to handle different shoot cluster versions.</li></ul></li><li>Bump the used Kubernetes version for local <code>Shoot</code> and local e2e test.<ul><li>See <a href=https://github.com/gardener/gardener/pull/5255/commits/5707c4c7a4fd265b176387178b755cabeea89ffe>this</a> example commit.</li></ul></li></ul><h4 id=filing-the-pull-request>Filing the Pull Request</h4><p>Work on all the tasks you have collected and validate them using the <a href=/docs/gardener/development/getting_started_locally/>local provider</a>.
Execute the e2e tests and if everything looks good, then go ahead and file the PR (<a href=https://github.com/gardener/gardener/pull/5255>example PR</a>).
Generally, it is great if you add the PRs also to the umbrella issue so that they can be tracked more easily.</p><h3 id=adapting-provider-extensions>Adapting Provider Extensions</h3><p>After the PR in <code>gardener/gardener</code> for the support of the new version has been merged, you can go ahead and work on the provider extensions.</p><blockquote><p>Actually, you can already start even if the PR is not yet merged and use the branch of your fork.</p></blockquote><ul><li>Revendor the <code>github.com/gardener/gardener</code> dependency in the extension and update the <code>README.md</code>.</li><li>Work on release-specific tasks related to this provider.</li></ul><h4 id=maintaining-the-cloud-controller-manager-images>Maintaining the <code>cloud-controller-manager</code> Images</h4><p>Some of the cloud providers are not yet using upstream <code>cloud-controller-manager</code> images.
Instead, we build and maintain them ourselves:</p><ul><li><a href=https://github.com/gardener/cloud-provider-aws>https://github.com/gardener/cloud-provider-aws</a></li><li><a href=https://github.com/gardener/cloud-provider-azure>https://github.com/gardener/cloud-provider-azure</a> (since <code>v1.23</code>, we use the upstream image)</li><li><a href=https://github.com/gardener/cloud-provider-gcp>https://github.com/gardener/cloud-provider-gcp</a></li></ul><p>Until we switch to upstream images, you need to revendor the Kubernetes dependencies and release a new image.
The required steps are as follows:</p><ul><li>Checkout the <code>legacy-cloud-provider</code> branch of the respective repository</li><li>Bump the versions in the <code>Dockerfile</code> (<a href=https://github.com/gardener/cloud-provider-gcp/commit/b7eb3f56b252aaf29adc78406672574b1bc17495>example commit</a>).</li><li>Update the <code>VERSION</code> to <code>vX.Y.Z-dev</code> where <code>Z</code> is the latest available Kubernetes patch version for the <code>vX.Y</code> minor version.</li><li>Update the <code>k8s.io/*</code> dependencies in the <code>go.mod</code> file to <code>vX.Y.Z</code> and run <code>go mod vendor</code> and <code>go mod tidy</code> (<a href=https://github.com/gardener/cloud-provider-gcp/commit/d41cc9f035bcc4893b40d90a4f617c4d436c5d62>example commit</a>).</li><li>Checkout a new <code>release-vX.Y</code> branch and release it (<a href=https://github.com/gardener/cloud-provider-gcp/commits/release-v1.23>example</a>)</li></ul><blockquote><p>As you are already on it, it is great if you also bump the <code>k8s.io/*</code> dependencies for the last three minor releases as well.
In this case, you need to checkout the <code>release-vX.{Y-{1,2,3}}</code> branches and only perform the last three steps (<a href=https://github.com/gardener/cloud-provider-gcp/commits/release-v1.20>example branch</a>, <a href=https://github.com/gardener/cloud-provider-gcp/commit/372aa43fbacdeb76b3da9f6fad6cfd924d916227>example commit</a>).</p></blockquote><p>Now you need to update the new releases in the <code>charts/images.yaml</code> of the respective provider extension so that they are used (see this <a href=https://github.com/gardener/gardener-extension-provider-aws/pull/480/commits/76256de933d5a508aba26a8f589dd1a39026142e>example commit</a> for reference).</p><h4 id=filing-the-pull-request-1>Filing the Pull Request</h4><p>Again, work on all the tasks you have collected.
This time, you cannot use the local provider for validation but should create real clusters on the various infrastructures.
Typically, the following validations should be performed:</p><ul><li>Create new clusters with versions &lt; <code>vX.Y</code></li><li>Create new clusters with version = <code>vX.Y</code></li><li>Upgrade old clusters from version <code>vX.{Y-1}</code> to version <code>vX.Y</code></li><li>Delete clusters with versions &lt; <code>vX.Y</code></li><li>Delete clusters with version = <code>vX.Y</code></li></ul><p>If everything looks good, then go ahead and file the PR (<a href=https://github.com/gardener/gardener-extension-provider-aws/pull/480>example PR</a>).
Generally, it is again great if you add the PRs also to the umbrella issue so that they can be tracked more easily.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-255067da5c8d985de3cf53847d3ecf4b>1.4.13 - Priority Classes</h1><h1 id=priorityclasses-in-gardener-clusters><code>PriorityClass</code>es in Gardener Clusters</h1><p>Gardener makes use of <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/><code>PriorityClass</code>es</a> to improve the overall robustness of the system.
In order to benefit from the full potential of <code>PriorityClass</code>es, the gardenlet manages a set of well-known <code>PriorityClass</code>es with fine-granular priority values.</p><p>All components of the system should use these well-known <code>PriorityClass</code>es instead of creating and using separate ones with arbitrary values, which would compromise the overall goal of using <code>PriorityClass</code>es in the first place.
The gardenlet manages the well-known <code>PriorityClass</code>es listed in this document, so that third parties (e.g., Gardener extensions) can rely on them to be present when deploying components to Seed and Shoot clusters.</p><p>The listed well-known <code>PriorityClass</code>es follow this rough concept:</p><ul><li>Values are close to the maximum that can be declared by the user. This is important to ensure that Shoot system components have higher priority than the workload deployed by end-users.</li><li>Values have a bit of headroom in between to ensure flexibility when the need for intermediate priority values arises.</li><li>Values of <code>PriorityClass</code>es created on Seed clusters are lower than the ones on Shoots to ensure that Shoot system components have higher priority than Seed components, if the Seed is backed by a Shoot (<code>ManagedSeed</code>), e.g. <code>coredns</code> should have higher priority than <code>gardenlet</code>.</li><li>Names simply include the last digits of the value to minimize confusion caused by many (similar) names like <code>critical</code>, <code>importance-high</code>, etc.</li></ul><h2 id=garden-clusters>Garden Clusters</h2><p>When using the <code>gardener-operator</code> for managing the garden runtime and virtual cluster, the following <code>PriorityClass</code>es are available:</p><h3 id=priorityclasses-for-garden-control-plane-components><code>PriorityClass</code>es for Garden Control Plane Components</h3><table><thead><tr><th>Name</th><th>Priority</th><th>Associated Components (Examples)</th></tr></thead><tbody><tr><td><code>gardener-garden-system-critical</code></td><td>999999550</td><td><code>gardener-operator</code>, <code>gardener-resource-manager</code></td></tr><tr><td><code>gardener-garden-system-500</code></td><td>999999500</td><td><code>virtual-garden-etcd-events</code>, <code>virtual-garden-etcd-main</code></td></tr><tr><td><code>gardener-garden-system-400</code></td><td>999999400</td><td></td></tr><tr><td><code>gardener-garden-system-300</code></td><td>999999300</td><td><code>vpa-admission-controller</code>, <code>etcd-druid</code></td></tr><tr><td><code>gardener-garden-system-200</code></td><td>999999200</td><td><code>vpa-recommender</code>, <code>vpa-updater</code>, <code>hvpa-controller</code></td></tr><tr><td><code>gardener-garden-system-100</code></td><td>999999100</td><td></td></tr></tbody></table><h2 id=seed-clusters>Seed Clusters</h2><h3 id=priorityclasses-for-seed-system-components><code>PriorityClass</code>es for Seed System Components</h3><table><thead><tr><th>Name</th><th>Priority</th><th>Associated Components (Examples)</th></tr></thead><tbody><tr><td><code>gardener-system-critical</code></td><td>999998950</td><td><code>gardenlet</code>, <code>gardener-resource-manager</code>, <code>istio-ingressgateway</code>, <code>istiod</code></td></tr><tr><td><code>gardener-system-900</code></td><td>999998900</td><td>Extensions, <code>reversed-vpn-auth-server</code></td></tr><tr><td><code>gardener-system-800</code></td><td>999998800</td><td><code>dependency-watchdog-endpoint</code>, <code>dependency-watchdog-probe</code>, <code>etcd-druid</code>, <code>(auditlog-)mutator</code>, <code>vpa-admission-controller</code></td></tr><tr><td><code>gardener-system-700</code></td><td>999998700</td><td><code>auditlog-seed-controller</code>, <code>hvpa-controller</code>, <code>vpa-recommender</code>, <code>vpa-updater</code></td></tr><tr><td><code>gardener-system-600</code></td><td>999998600</td><td><code>aggregate-alertmanager</code>, <code>alertmanager</code>, <code>fluent-bit</code>, <code>grafana</code>, <code>kube-state-metrics</code>, <code>nginx-ingress-controller</code>, <code>nginx-k8s-backend</code>, <code>prometheus</code>, <code>loki</code>, <code>seed-prometheus</code></td></tr><tr><td><code>gardener-reserve-excess-capacity</code></td><td>-5</td><td><code>reserve-excess-capacity</code> (<a href=https://github.com/gardener/gardener/pull/6135>ref</a>)</td></tr></tbody></table><h3 id=priorityclasses-for-shoot-control-plane-components><code>PriorityClass</code>es for Shoot Control Plane Components</h3><table><thead><tr><th>Name</th><th>Priority</th><th>Associated Components (Examples)</th></tr></thead><tbody><tr><td><code>gardener-system-500</code></td><td>999998500</td><td><code>etcd-events</code>, <code>etcd-main</code>, <code>kube-apiserver</code></td></tr><tr><td><code>gardener-system-400</code></td><td>999998400</td><td><code>gardener-resource-manager</code></td></tr><tr><td><code>gardener-system-300</code></td><td>999998300</td><td><code>cloud-controller-manager</code>, <code>cluster-autoscaler</code>, <code>csi-driver-controller</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>machine-controller-manager</code>, <code>terraformer</code>, <code>vpn-seed-server</code></td></tr><tr><td><code>gardener-system-200</code></td><td>999998200</td><td><code>csi-snapshot-controller</code>, <code>csi-snapshot-validation</code>, <code>cert-controller-manager</code>, <code>shoot-dns-service</code>, <code>vpa-admission-controller</code>, <code>vpa-recommender</code>, <code>vpa-updater</code></td></tr><tr><td><code>gardener-system-100</code></td><td>999998100</td><td><code>alertmanager</code>, <code>grafana</code>, <code>kube-state-metrics</code>, <code>prometheus</code>, <code>loki</code>, <code>event-logger</code></td></tr></tbody></table><h2 id=shoot-clusters>Shoot Clusters</h2><h2 id=priorityclasses-for-shoot-system-components><code>PriorityClass</code>es for Shoot System Components</h2><table><thead><tr><th>Name</th><th>Priority</th><th>Associated Components (Examples)</th></tr></thead><tbody><tr><td><code>system-node-critical</code> (created by Kubernetes)</td><td>2000001000</td><td><code>calico-node</code>, <code>kube-proxy</code>, <code>apiserver-proxy</code>, <code>csi-driver</code>, <code>egress-filter-applier</code></td></tr><tr><td><code>system-cluster-critical</code> (created by Kubernetes)</td><td>2000000000</td><td><code>calico-typha</code>, <code>calico-kube-controllers</code>, <code>coredns</code>, <code>vpn-shoot</code></td></tr><tr><td><code>gardener-shoot-system-900</code></td><td>999999900</td><td><code>node-problem-detector</code></td></tr><tr><td><code>gardener-shoot-system-800</code></td><td>999999800</td><td><code>calico-typha-horizontal-autoscaler</code>, <code>calico-typha-vertical-autoscaler</code></td></tr><tr><td><code>gardener-shoot-system-700</code></td><td>999999700</td><td><code>blackbox-exporter</code>, <code>node-exporter</code></td></tr><tr><td><code>gardener-shoot-system-600</code></td><td>999999600</td><td><code>addons-nginx-ingress-controller</code>, <code>addons-nginx-ingress-k8s-backend</code>, <code>kubernetes-dashboard</code>, <code>kubernetes-metrics-scraper</code></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-00cd3641980cfeb33469951e8feeb244>1.4.14 - Process</h1><h1 id=releases-features-hotfixes>Releases, Features, Hotfixes</h1><p>This document describes how to contribute features or hotfixes, and how new Gardener releases are usually scheduled, validated, etc.</p><ul><li><a href=#releases-features-hotfixes>Releases, Features, Hotfixes</a><ul><li><a href=#releases>Releases</a><ul><li><a href=#release-responsible-plan>Release Responsible Plan</a></li><li><a href=#release-validation>Release Validation</a></li></ul></li><li><a href=#contributing-new-features-or-fixes>Contributing New Features or Fixes</a></li><li><a href=#cherry-picks>Cherry Picks</a><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#initiate-a-cherry-pick>Initiate a Cherry Pick</a></li></ul></li></ul></li></ul><h2 id=releases>Releases</h2><p>The <a href=https://github.com/orgs/gardener/teams/gardener-maintainers>@gardener-maintainers</a> are trying to provide a new release roughly every other week (depending on their capacity and the stability/robustness of the <code>master</code> branch).</p><p>Hotfixes are usually maintained for the latest three minor releases, though, there are no fixed release dates.</p><h3 id=release-responsible-plan>Release Responsible Plan</h3><table><thead><tr><th>Version</th><th>Week No</th><th>Begin Validation Phase</th><th>Due Date</th><th>Release Responsible</th></tr></thead><tbody><tr><td>v1.63</td><td>Week 01-04</td><td>January 2, 2023</td><td>January 29, 2023</td><td><a href=https://github.com/shafeeqes>@shafeeqes</a></td></tr><tr><td>v1.64</td><td>Week 05-06</td><td>January 30, 2023</td><td>February 12, 2023</td><td><a href=https://github.com/ary1992>@ary1992</a></td></tr><tr><td>v1.65</td><td>Week 07-08</td><td>February 13, 2023</td><td>February 26, 2023</td><td><a href=https://github.com/timuthy>@timuthy</a></td></tr><tr><td>v1.66</td><td>Week 09-10</td><td>February 27, 2023</td><td>March 12, 2023</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.67</td><td>Week 11-12</td><td>March 13, 2023</td><td>March 26, 2023</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.68</td><td>Week 13-14</td><td>March 27, 2023</td><td>April 9, 2023</td><td><a href=https://github.com/acumino>@acumino</a></td></tr><tr><td>v1.69</td><td>Week 15-16</td><td>April 10, 2023</td><td>April 23, 2023</td><td><a href=https://github.com/oliver-goetz>@oliver-goetz</a></td></tr><tr><td>v1.70</td><td>Week 17-18</td><td>April 24, 2023</td><td>May 7, 2023</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.71</td><td>Week 19-20</td><td>May 8, 2023</td><td>May 21, 2023</td><td><a href=https://github.com/shafeeqes>@shafeeqes</a></td></tr><tr><td>v1.72</td><td>Week 21-22</td><td>May 22, 2023</td><td>June 4, 2023</td><td><a href=https://github.com/ary1992>@ary1992</a></td></tr><tr><td>v1.73</td><td>Week 23-24</td><td>June 5, 2023</td><td>June 18, 2023</td><td><a href=https://github.com/timuthy>@timuthy</a></td></tr><tr><td>v1.74</td><td>Week 25-26</td><td>June 19, 2023</td><td>July 2, 2023</td><td><a href=https://github.com/oliver-goetz>@oliver-goetz</a></td></tr><tr><td>v1.75</td><td>Week 27-28</td><td>July 3, 2023</td><td>July 16, 2023</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.76</td><td>Week 29-30</td><td>July 17, 2023</td><td>July 30, 2023</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.77</td><td>Week 31-32</td><td>July 31, 2023</td><td>August 13, 2023</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.78</td><td>Week 33-34</td><td>August 14, 2023</td><td>August 27, 2023</td><td><a href=https://github.com/acumino>@acumino</a></td></tr></tbody></table><p>Apart from the release of the next version, the release responsible is also taking care of potential hotfix releases of the last three minor versions.
The release responsible is the main contact person for coordinating new feature PRs for the next minor versions or cherry-pick PRs for the last three minor versions.</p><details><summary>Click to expand the archived release responsible associations!</summary><table><thead><tr><th>Version</th><th>Week No</th><th>Begin Validation Phase</th><th>Due Date</th><th>Release Responsible</th></tr></thead><tbody><tr><td>v1.17</td><td>Week 07-08</td><td>February 15, 2021</td><td>February 28, 2021</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.18</td><td>Week 09-10</td><td>March 1, 2021</td><td>March 14, 2021</td><td><a href=https://github.com/danielfoehrKn>@danielfoehrKn</a></td></tr><tr><td>v1.19</td><td>Week 11-12</td><td>March 15, 2021</td><td>March 28, 2021</td><td><a href=https://github.com/timebertt>@timebertt</a></td></tr><tr><td>v1.20</td><td>Week 13-14</td><td>March 29, 2021</td><td>April 11, 2021</td><td><a href=https://github.com/vpnachev>@vpnachev</a></td></tr><tr><td>v1.21</td><td>Week 15-16</td><td>April 12, 2021</td><td>April 25, 2021</td><td><a href=https://github.com/timuthy>@timuthy</a></td></tr><tr><td>v1.22</td><td>Week 17-18</td><td>April 26, 2021</td><td>May 9, 2021</td><td><a href=https://github.com/BeckerMax>@BeckerMax</a></td></tr><tr><td>v1.23</td><td>Week 19-20</td><td>May 10, 2021</td><td>May 23, 2021</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.24</td><td>Week 21-22</td><td>May 24, 2021</td><td>June 5, 2021</td><td><a href=https://github.com/stoyanr>@stoyanr</a></td></tr><tr><td>v1.25</td><td>Week 23-24</td><td>June 7, 2021</td><td>June 20, 2021</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.26</td><td>Week 25-26</td><td>June 21, 2021</td><td>July 4, 2021</td><td><a href=https://github.com/danielfoehrKn>@danielfoehrKn</a></td></tr><tr><td>v1.27</td><td>Week 27-28</td><td>July 5, 2021</td><td>July 18, 2021</td><td><a href=https://github.com/timebertt>@timebertt</a></td></tr><tr><td>v1.28</td><td>Week 29-30</td><td>July 19, 2021</td><td>August 1, 2021</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.29</td><td>Week 31-32</td><td>August 2, 2021</td><td>August 15, 2021</td><td><a href=https://github.com/timuthy>@timuthy</a></td></tr><tr><td>v1.30</td><td>Week 33-34</td><td>August 16, 2021</td><td>August 29, 2021</td><td><a href=https://github.com/BeckerMax>@BeckerMax</a></td></tr><tr><td>v1.31</td><td>Week 35-36</td><td>August 30, 2021</td><td>September 12, 2021</td><td><a href=https://github.com/stoyanr>@stoyanr</a></td></tr><tr><td>v1.32</td><td>Week 37-38</td><td>September 13, 2021</td><td>September 26, 2021</td><td><a href=https://github.com/vpnachev>@vpnachev</a></td></tr><tr><td>v1.33</td><td>Week 39-40</td><td>September 27, 2021</td><td>October 10, 2021</td><td><a href=https://github.com/voelzmo>@voelzmo</a></td></tr><tr><td>v1.34</td><td>Week 41-42</td><td>October 11, 2021</td><td>October 24, 2021</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.35</td><td>Week 43-44</td><td>October 25, 2021</td><td>November 7, 2021</td><td><a href=https://github.com/kris94>@kris94</a></td></tr><tr><td>v1.36</td><td>Week 45-46</td><td>November 8, 2021</td><td>November 21, 2021</td><td><a href=https://github.com/timebertt>@timebertt</a></td></tr><tr><td>v1.37</td><td>Week 47-48</td><td>November 22, 2021</td><td>December 5, 2021</td><td><a href=https://github.com/danielfoehrKn>@danielfoehrKn</a></td></tr><tr><td>v1.38</td><td>Week 49-50</td><td>December 6, 2021</td><td>December 19, 2021</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.39</td><td>Week 01-04</td><td>January 3, 2022</td><td>January 30, 2022</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a>, <a href=https://github.com/timuthy>@timuthy</a></td></tr><tr><td>v1.40</td><td>Week 05-06</td><td>January 31, 2022</td><td>February 13, 2022</td><td><a href=https://github.com/BeckerMax>@BeckerMax</a></td></tr><tr><td>v1.41</td><td>Week 07-08</td><td>February 14, 2022</td><td>February 27, 2022</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.42</td><td>Week 09-10</td><td>February 28, 2022</td><td>March 13, 2022</td><td><a href=https://github.com/kris94>@kris94</a></td></tr><tr><td>v1.43</td><td>Week 11-12</td><td>March 14, 2022</td><td>March 27, 2022</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.44</td><td>Week 13-14</td><td>March 28, 2022</td><td>April 10, 2022</td><td><a href=https://github.com/timebertt>@timebertt</a></td></tr><tr><td>v1.45</td><td>Week 15-16</td><td>April 11, 2022</td><td>April 24, 2022</td><td><a href=https://github.com/acumino>@acumino</a></td></tr><tr><td>v1.46</td><td>Week 17-18</td><td>April 25, 2022</td><td>May 8, 2022</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.47</td><td>Week 19-20</td><td>May 9, 2022</td><td>May 22, 2022</td><td><a href=https://github.com/shafeeqes>@shafeeqes</a></td></tr><tr><td>v1.48</td><td>Week 21-22</td><td>May 23, 2022</td><td>June 5, 2022</td><td><a href=https://github.com/ary1992>@ary1992</a></td></tr><tr><td>v1.49</td><td>Week 23-24</td><td>June 6, 2022</td><td>June 19, 2022</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.50</td><td>Week 25-26</td><td>June 20, 2022</td><td>July 3, 2022</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.51</td><td>Week 27-28</td><td>July 4, 2022</td><td>July 17, 2022</td><td><a href=https://github.com/timebertt>@timebertt</a></td></tr><tr><td>v1.52</td><td>Week 29-30</td><td>July 18, 2022</td><td>July 31, 2022</td><td><a href=https://github.com/acumino>@acumino</a></td></tr><tr><td>v1.53</td><td>Week 31-32</td><td>August 1, 2022</td><td>August 14, 2022</td><td><a href=https://github.com/kris94>@kris94</a></td></tr><tr><td>v1.54</td><td>Week 33-34</td><td>August 15, 2022</td><td>August 28, 2022</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.55</td><td>Week 35-36</td><td>August 29, 2022</td><td>September 11, 2022</td><td><a href=https://github.com/oliver-goetz>@oliver-goetz</a></td></tr><tr><td>v1.56</td><td>Week 37-38</td><td>September 12, 2022</td><td>September 25, 2022</td><td><a href=https://github.com/shafeeqes>@shafeeqes</a></td></tr><tr><td>v1.57</td><td>Week 39-40</td><td>September 26, 2022</td><td>October 9, 2022</td><td><a href=https://github.com/ary1992>@ary1992</a></td></tr><tr><td>v1.58</td><td>Week 41-42</td><td>October 10, 2022</td><td>October 23, 2022</td><td><a href=https://github.com/plkokanov>@plkokanov</a></td></tr><tr><td>v1.59</td><td>Week 43-44</td><td>October 24, 2022</td><td>November 6, 2022</td><td><a href=https://github.com/rfranzke>@rfranzke</a></td></tr><tr><td>v1.60</td><td>Week 45-46</td><td>November 7, 2022</td><td>November 20, 2022</td><td><a href=https://github.com/acumino>@acumino</a></td></tr><tr><td>v1.61</td><td>Week 47-48</td><td>November 21, 2022</td><td>December 4, 2022</td><td><a href=https://github.com/ialidzhikov>@ialidzhikov</a></td></tr><tr><td>v1.62</td><td>Week 49-50</td><td>December 5, 2022</td><td>December 18, 2022</td><td><a href=https://github.com/oliver-goetz>@oliver-goetz</a></td></tr></tbody></table></details><h3 id=release-validation>Release Validation</h3><p>The release phase for a new minor version lasts two weeks.
Typically, the first week is used for the validation of the release.
This phase includes the following steps:</p><ol><li><code>master</code> (or latest <code>release-*</code> branch) is deployed to a development landscape that already hosts some existing seed and shoot clusters.</li><li>An extended test suite is triggered by the &ldquo;release responsible&rdquo; which:<ol><li>executes the Gardener integration tests for different Kubernetes versions, infrastructures, and <code>Shoot</code> settings.</li><li>executes the Kubernetes conformance tests.</li><li>executes further tests like Kubernetes/OS patch/minor version upgrades.</li></ol></li><li>Additionally, every four hours (or on demand) more tests (e.g., including the Kubernetes e2e test suite) are executed for different infrastructures.</li><li>The &ldquo;release responsible&rdquo; is verifying new features or other notable changes (derived of the draft release notes) in this development system.</li></ol><p>Usually, the new release is triggered in the beginning of the second week if all tests are green, all checks were successful, and if all of the planned verifications were performed by the release responsible.</p><h2 id=contributing-new-features-or-fixes>Contributing New Features or Fixes</h2><p>Please refer to the <a href=https://gardener.cloud/docs/contribute/>Gardener contributor guide</a>.
Besides a lot of a general information, it also provides a checklist for newly created pull requests that may help you to prepare your changes for an efficient review process.
If you are contributing a fix or major improvement, please take care to open cherry-pick PRs to all affected and still supported versions once the change is approved and merged in the <code>master</code> branch.</p><p>⚠️ Please ensure that your modifications pass the verification checks (linting, formatting, static code checks, tests, etc.) by executing</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make verify
</span></span></code></pre></div><p>before filing your pull request.</p><p>The guide applies for both changes to the <code>master</code> and to any <code>release-*</code> branch.
All changes must be submitted via a pull request and be reviewed and approved by at least one code owner.</p><h2 id=cherry-picks>Cherry Picks</h2><p>This section explains how to initiate cherry picks on release branches within the <code>gardener/gardener</code> repository.</p><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#initiate-a-cherry-pick>Initiate a Cherry Pick</a></li></ul><h3 id=prerequisites>Prerequisites</h3><p>Before you initiate a cherry pick, make sure that the following prerequisites are accomplished.</p><ul><li>A pull request merged against the <code>master</code> branch.</li><li>The release branch exists (check in the <a href=https://github.com/gardener/gardener/branches>branches section</a>).</li><li>Have the <code>gardener/gardener</code> repository cloned as follows:<ul><li>the <code>origin</code> remote should point to your fork (alternatively this can be overwritten by passing <code>FORK_REMOTE=&lt;fork-remote></code>).</li><li>the <code>upstream</code> remote should point to the Gardener GitHub org (alternatively this can be overwritten by passing <code>UPSTREAM_REMOTE=&lt;upstream-remote></code>).</li></ul></li><li>Have <code>hub</code> installed, which is most easily installed via
<code>go get github.com/github/hub</code> assuming you have a standard golang
development environment.</li><li>A GitHub token which has permissions to create a PR in an upstream branch.</li></ul><h3 id=initiate-a-cherry-pick>Initiate a Cherry Pick</h3><ul><li><p>Run the [cherry pick script][cherry-pick-script].</p><p>This example applies a master branch PR #3632 to the remote branch
<code>upstream/release-v3.14</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>GITHUB_USER=&lt;your-user&gt; hack/cherry-pick-pull.sh upstream/release-v3.14 3632
</span></span></code></pre></div><ul><li><p>Be aware the cherry pick script assumes you have a git remote called
<code>upstream</code> that points at the Gardener GitHub org.</p></li><li><p>You will need to run the cherry pick script separately for each patch
release you want to cherry pick to. Cherry picks should be applied to all
active release branches where the fix is applicable.</p></li><li><p>When asked for your GitHub password, provide the created GitHub token
rather than your actual GitHub password.
Refer <a href=https://github.com/github/hub/issues/2655#issuecomment-735836048>https://github.com/github/hub/issues/2655#issuecomment-735836048</a></p></li></ul></li><li><p><a href=https://github.com/gardener/gardener/blob/master/hack/cherry-pick-pull.sh>cherry-pick-script</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-165b95fb0235157f8762cac8f525c5ab>1.4.15 - Secrets Management</h1><h1 id=secrets-management-for-seed-and-shoot-cluster>Secrets Management for Seed and Shoot Cluster</h1><p>The gardenlet needs to create quite some amount of credentials (certificates, private keys, passwords) for seed and shoot clusters in order to ensure secure deployments.
Such credentials typically should be renewed automatically when their validity expires, rotated regularly, and they potentially need to be persisted such that they don&rsquo;t get lost in case of a control plane migration or a lost seed cluster.</p><h2 id=secretsmanager-introduction>SecretsManager Introduction</h2><p>These requirements can be covered by using the <code>SecretsManager</code> package maintained in <a href=https://github.com/gardener/gardener/tree/master/pkg/utils/secrets/manager><code>pkg/utils/secrets/manager</code></a>.
It is built on top of the <code>ConfigInterface</code> and <code>DataInterface</code> interfaces part of <a href=https://github.com/gardener/gardener/tree/master/pkg/utils/secrets><code>pkg/utils/secrets</code></a> and provides the following functions:</p><ul><li><p><code>Generate(context.Context, secrets.ConfigInterface, ...GenerateOption) (*corev1.Secret, error)</code></p><p>This method either retrieves the current secret for the given configuration or it (re)generates it in case the configuration changed, the signing CA changed (for certificate secrets), or when proactive rotation was triggered.
If the configuration describes a certificate authority secret then this method automatically generates a bundle secret containing the current and potentially the old certificate.
Available <code>GenerateOption</code>s:</p><ul><li><code>SignedByCA(string, ...SignedByCAOption)</code>: This is only valid for certificate secrets and automatically retrieves the correct certificate authority in order to sign the provided server or client certificate.<ul><li>There are two <code>SignedByCAOption</code>s:<ul><li><code>UseCurrentCA</code>. This option will sign server certificates with the new/current CA in case of a CA rotation. For more information, please refer to the <a href=#certificate-signing>&ldquo;Certificate Signing&rdquo;</a> section below.</li><li><code>UseOldCA</code>. This option will sign client certificates with the old CA in case of a CA rotation. For more information, please refer to the <a href=#certificate-signing>&ldquo;Certificate Signing&rdquo;</a> section below.</li></ul></li></ul></li><li><code>Persist()</code>: This marks the secret such that it gets persisted in the <code>ShootState</code> resource in the garden cluster. Consequently, it should only be used for secrets related to a shoot cluster.</li><li><code>Rotate(rotationStrategy)</code>: This specifies the strategy in case this secret is to be rotated or regenerated (either <code>InPlace</code> which immediately forgets about the old secret, or <code>KeepOld</code> which keeps the old secret in the system).</li><li><code>IgnoreOldSecrets()</code>: This specifies that old secrets should not be considered and loaded (contrary to the default behavior). It should be used when old secrets are no longer important and can be &ldquo;forgotten&rdquo; (e.g. in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md#rotation-sequence-for-cluster-and-client-ca>&ldquo;phase 2&rdquo; (<code>t2</code>) of the CA certificate rotation</a>). Such old secrets will be deleted on <code>Cleanup()</code>.</li><li><code>IgnoreOldSecretsAfter(time.Duration)</code>: This specifies that old secrets should not be considered and loaded once a given duration after rotation has passed. It can be used to clean up old secrets after automatic rotation (e.g. the Seed cluster CA is automatically rotated when its validity will soon end and the old CA will be cleaned up 24 hours after triggering the rotation).</li><li><code>Validity(time.Duration)</code>: This specifies how long the secret should be valid. For certificate secret configurations, the manager will automatically deduce this information from the generated certificate.</li></ul></li><li><p><code>Get(string, ...GetOption) (*corev1.Secret, bool)</code></p><p>This method retrieves the current secret for the given name.
In case the secret in question is a certificate authority secret then it retrieves the bundle secret by default.
It is important that this method only knows about secrets for which there were prior <code>Generate</code> calls.
Available <code>GetOption</code>s:</p><ul><li><code>Bundle</code> (default): This retrieves the bundle secret.</li><li><code>Current</code>: This retrieves the current secret.</li><li><code>Old</code>: This retrieves the old secret.</li></ul></li><li><p><code>Cleanup(context.Context) error</code></p><p>This method deletes secrets which are no longer required.
No longer required secrets are those still existing in the system which weren&rsquo;t detected by prior <code>Generate</code> calls.
Consequently, only call <code>Cleanup</code> after you have executed <code>Generate</code> calls for all desired secrets.</p></li></ul><p>Some exemplary usages would look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>secret, err := k.secretsManager.Generate(
</span></span><span style=display:flex><span>    ctx,
</span></span><span style=display:flex><span>    &amp;secrets.CertificateSecretConfig{
</span></span><span style=display:flex><span>        Name:                        <span style=color:#a31515>&#34;my-server-secret&#34;</span>,
</span></span><span style=display:flex><span>        CommonName:                  <span style=color:#a31515>&#34;server-abc&#34;</span>,
</span></span><span style=display:flex><span>        DNSNames:                    []<span style=color:#2b91af>string</span>{<span style=color:#a31515>&#34;first-name&#34;</span>, <span style=color:#a31515>&#34;second-name&#34;</span>},
</span></span><span style=display:flex><span>        CertType:                    secrets.ServerCert,
</span></span><span style=display:flex><span>        SkipPublishingCACertificate: <span style=color:#00f>true</span>,
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    secretsmanager.SignedByCA(<span style=color:#a31515>&#34;my-ca&#34;</span>),
</span></span><span style=display:flex><span>    secretsmanager.Persist(),
</span></span><span style=display:flex><span>    secretsmanager.Rotate(secretsmanager.InPlace),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#00f>if</span> err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> err
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As explained above, the caller does not need to care about the renewal, rotation or the persistence of this secret - all of these concerns are handled by the secrets manager.
Automatic renewal of secrets happens when their validity approaches 80% or less than <code>10d</code> are left until expiration.</p><p>In case a CA certificate is needed by some component, then it can be retrieved as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>caSecret, found := k.secretsManager.Get(<span style=color:#a31515>&#34;my-ca&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#00f>if</span> !found {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> fmt.Errorf(<span style=color:#a31515>&#34;secret my-ca not found&#34;</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As explained above, this returns the bundle secret for the CA <code>my-ca</code> which might potentially contain both the current and the old CA (in case of rotation/regeneration).</p><h3 id=certificate-signing>Certificate Signing</h3><p>By default, client certificates are always signed by the current CA while server certificate are signed by the old CA (if it exists).
This is to ensure a smooth exchange of certificate during a CA rotation (typically has two phases, ref <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md#rotation-sequence-for-cluster-and-client-ca>GEP-18</a>):</p><ul><li>Client certificates:<ul><li>In phase 1, clients get new certificates as soon as possible to ensure that all clients have been adapted before phase 2.</li><li>In phase 2, the respective server drops accepting certificates signed by the old CA.</li></ul></li><li>Server certificates:<ul><li>In phase 1, servers still use their old/existing certificates to allow clients to update their CA bundle used for verification of the servers&rsquo; certificates.</li><li>In phase 2, the old CA is dropped, hence servers need to get a certificate signed by the new/current CA. At this point in time, clients have already adapted their CA bundles.</li></ul></li></ul><h4 id=always-sign-server-certificates-with-current-ca>Always Sign Server Certificates with Current CA</h4><p>In case you control all clients and update them at the same time as the server, it is possible to make the secrets manager generate even server certificates with the new/current CA.
This can help to prevent certificate mismatches when the CA bundle is already exchanged while the server still serves with a certificate signed by a CA no longer part of the bundle.</p><p>Let&rsquo;s consider the two following examples:</p><ol><li><code>gardenlet</code> deploys a webhook server (<code>gardener-resource-manager</code>) and a corresponding <code>MutatingWebhookConfiguration</code> at the same time. In this case, the server certificate should be generated with the new/current CA to avoid above mentioned certificate mismatches during a CA rotation.</li><li><code>gardenlet</code> deploys a server (<code>etcd</code>) in one step, and a client (<code>kube-apiserver</code>) in a subsequent step. In this case, the default behaviour should apply (server certificate should be signed by old/existing CA).</li></ol><h4 id=always-sign-client-certificate-with-old-ca>Always Sign Client Certificate with Old CA</h4><p>In the unusual case where the client is deployed before the server, it might be useful to always use the old CA for signing the client&rsquo;s certificate.
This can help to prevent certificate mismatches when the client already gets a new certificate while the server still only accepts certificates signed by the old CA.</p><p>Let&rsquo;s consider the following example:</p><ol><li><code>gardenlet</code> deploys the <code>kube-apiserver</code> before the <code>kubelet</code>. However, the <code>kube-apiserver</code> has a client certificate signed by the <code>ca-kubelet</code> in order to communicate with it (e.g., when retrieving logs or forwarding ports). In this case, the client certificate should be generated with the old CA to avoid above mentioned certificate mismatches during a CA rotation.</li></ol><h2 id=reusing-the-secretsmanager-in-other-components>Reusing the SecretsManager in Other Components</h2><p>While the <code>SecretsManager</code> is primarily used by gardenlet, it can be reused by other components (e.g. extensions) as well for managing secrets that are specific to the component or extension. For example, provider extensions might use their own <code>SecretsManager</code> instance for managing the serving certificate of <code>cloud-controller-manager</code>.</p><p>External components that want to reuse the <code>SecretsManager</code> should consider the following aspects:</p><ul><li>On initialization of a <code>SecretsManager</code>, pass an <code>identity</code> specific to the component, controller and purpose. For example, gardenlet&rsquo;s shoot controller uses <code>gardenlet</code> as the <code>SecretsManager</code>&rsquo;s identity, the <code>Worker</code> controller in <code>provider-foo</code> should use <code>provider-foo-worker</code>, and the <code>ControlPlane</code> controller should use <code>provider-foo-controlplane-exposure</code> for <code>ControlPlane</code> objects of purpose <code>exposure</code>.
The given identity is added as a value for the <code>manager-identity</code> label on managed <code>Secret</code>s.
This label is used by the <code>Cleanup</code> function to select only those <code>Secret</code>s that are actually managed by the particular <code>SecretManager</code> instance. This is done to prevent removing still needed <code>Secret</code>s that are managed by other instances.</li><li>Generate dedicated CAs for signing certificates instead of depending on CAs managed by gardenlet.</li><li>Names of <code>Secret</code>s managed by external <code>SecretsManager</code> instances must not conflict with <code>Secret</code> names from other instances (e.g. gardenlet).</li><li>For CAs that should be rotated in lock-step with the Shoot CAs managed by gardenlet, components need to pass information about the last rotation initiation time and the current rotation phase to the <code>SecretsManager</code> upon initialization.
The relevant information can be retrieved from the <code>Cluster</code> resource under <code>.spec.shoot.status.credentials.rotation.certificateAuthorities</code>.</li><li>Independent of the specific identity, secrets marked with the <code>Persist</code> option are automatically saved in the <code>ShootState</code> resource by the gardenlet and are also restored by the gardenlet on Control Plane Migration to the new Seed.</li></ul><h2 id=migrating-existing-secrets-to-secretsmanager>Migrating Existing Secrets To SecretsManager</h2><p>If you already have existing secrets which were not created with <code>SecretsManager</code>, then you can (optionally) migrate them by labeling them with <code>secrets-manager-use-data-for=&lt;config-name></code>.
For example, if your <code>SecretsManager</code> generates a <code>CertificateConfigSecret</code> with name <code>foo</code> like this</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>secret, err := k.secretsManager.Generate(
</span></span><span style=display:flex><span>    ctx,
</span></span><span style=display:flex><span>    &amp;secrets.CertificateSecretConfig{
</span></span><span style=display:flex><span>        Name:                        <span style=color:#a31515>&#34;foo&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green>// ...
</span></span></span><span style=display:flex><span><span style=color:green></span>    },
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>and you already have an existing secret in your system whose data should be kept instead of regenerated, then labeling it with <code>secrets-manager-use-data-for=foo</code> will instruct <code>SecretsManager</code> accordingly.</p><p><strong>⚠️ Caveat: You have to make sure that the existing <code>data</code> keys match with what <code>SecretsManager</code> uses:</strong></p><table><thead><tr><th>Secret Type</th><th>Data Keys</th></tr></thead><tbody><tr><td>Basic Auth</td><td><code>username</code>, <code>password</code>, <code>auth</code></td></tr><tr><td>CA Certificate</td><td><code>ca.crt</code>, <code>ca.key</code></td></tr><tr><td>Non-CA Certificate</td><td><code>tls.crt</code>, <code>tls.key</code></td></tr><tr><td>Control Plane Secret</td><td><code>ca.crt</code>, <code>username</code>, <code>password</code>, <code>token</code>, <code>kubeconfig</code></td></tr><tr><td>ETCD Encryption Key</td><td><code>key</code>, <code>secret</code></td></tr><tr><td>Kubeconfig</td><td><code>kubeconfig</code></td></tr><tr><td>RSA Private Key</td><td><code>id_rsa</code>, <code>id_rsa.pub</code></td></tr><tr><td>Static Token</td><td><code>static_tokens.csv</code></td></tr><tr><td>VPN TLS Auth</td><td><code>vpn.tlsauth</code></td></tr></tbody></table><h2 id=implementation-details>Implementation Details</h2><p>The source of truth for the secrets manager is the list of <code>Secret</code>s in the Kubernetes cluster it acts upon (typically, the seed cluster).
The persisted secrets in the <code>ShootState</code> are only used if and only if the shoot is in the <code>Restore</code> phase - in this case all secrets are just synced to the seed cluster so that they can be picked up by the secrets manager.</p><p>In order to prevent kubelets from unneeded watches (thus, causing some significant traffic against the <code>kube-apiserver</code>), the <code>Secret</code>s are marked as immutable.
Consequently, they have a unique, deterministic name which is computed as follows:</p><ul><li>For CA secrets, the name is just exactly the name specified in the configuration (e.g., <code>ca</code>). This is for backwards-compatibility and will be dropped in a future release once all components depending on the static name have been adapted.</li><li>For all other secrets, the name specified in the configuration is used as prefix followed by an 8-digit hash. This hash is computed out of the checksum of the secret configuration and the checksum of the certificate of the signing CA (only for certificate configurations).</li></ul><p>In all cases, the name of the secrets is suffixed with a 5-digit hash computed out of the time when the rotation for this secret was last started.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-23d6dea7b372c9f66b2ff9c33cb18db8>1.4.16 - Seed Network Policies</h1><h1 id=network-policies-in-the-seed-cluster>Network Policies in the Seed Cluster</h1><p>This document describes the <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes network policies</a> deployed by Gardener into the Seed cluster.
For network policies deployed into the Shoot <code>kube-system</code> namespace, please see the <a href=/docs/gardener/usage/shoot_network_policies/>usage section</a>.</p><p>Network policies deployed by Gardener have names and annotations describing their purpose, so this document only highlights a subset of the policies in detail.</p><h2 id=network-policies-in-the-shoot-namespace-in-the-seed>Network Policies in the Shoot Namespace in the Seed</h2><p>The network policies in the Shoot namespace in the Seed can roughly be grouped into policies required for the control plane components and policies required for logging & monitoring.</p><p>The network policy <code>deny-all</code> plays a special role. This policy <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic>denies all ingress and egress traffic</a> from each pod in the Shoot namespace.
So per default, a pod running in the control plane cannot talk to any other pod in the whole Seed cluster.
This means the pod needs to have labels matching to appropriate network policies allowing it to talk to exactly the components required to execute its desired functionality.
<a href=#implications-for-gardener-extensions>This has also implications for Gardener extensions</a> that need to deploy additional components into the <code>Shoot's</code> control plane.</p><h3 id=network-policies-for-control-plane-components>Network Policies for Control Plane Components</h3><p>This section highlights a selection of network policies that exist in the Shoot namespace in the Seed cluster.
In general, the control plane components serve different purposes and thus need access to different pods and network ranges.</p><p>In contrast to other network policies, the policy <code>allow-to-shoot-networks</code> is tailored to the individual Shoot cluster,
because it is based on the network configuration in the Shoot manifest.
It allows pods with the label <code>networking.gardener.cloud/to-shoot-networks=allowed</code> to access pods in the Shoot pod,
service and node CIDR range. This is used by the Shoot API Server and the Prometheus pods to communicate over VPN/proxy with pods in the Shoot cluster.
This network policy is only useful if reversed vpn is disabled, as otherwise the vpn-seed-server pod in the control plane is the only pod with layer 3 routing to the shoot network.</p><p>The policy <code>allow-to-blocked-cidrs</code> allows pods with the label <code>networking.gardener.cloud/to-blocked-cidrs=allowed</code> to access IPs that are explicitly blocked for all control planes in a Seed cluster (configurable via <code>spec.networks.blockCIDRS</code>).
This is used for instance to block the cloud provider&rsquo;s metadata service.</p><p>Another network policy to be highlighted is <code>allow-to-runtime-apiserver</code>.
Some components need access to the Seed API Server. This can be allowed by labeling the pod with <code>networking.gardener.cloud/to-runtime-apiserver=allowed</code>.
This policy allows exactly the IPs of the <code>kube-apiserver</code> of the Seed.
While all other policies have a static set of permissions (do not change during the lifecycle of the Shoot), the policy <code>allow-to-runtime-apiserver</code> is reconciled to reflect the endpoints in the <code>default</code> namespace.
This is required because endpoint IPs are not necessarily stable (think of scaling the Seed API Server pods or hibernating the Seed cluster (acting as a managed seed) in a local development environment).</p><p>Furthermore, the following network policies exist in the Shoot namespace.
These policies are the same for every Shoot control plane.</p><pre tabindex=0><code>NAME                              POD-SELECTOR      
# Pods that need to access the Shoot API server. Used by all Kubernetes control plane components.
# deprecated. Use networking.resources.gardener.cloud/to-kube-apiserver-tcp-443=allowed instead
allow-to-shoot-apiserver          networking.gardener.cloud/to-shoot-apiserver=allowed

# allows access to kube-dns/core-dns pods for DNS queries                       
allow-to-dns                      networking.gardener.cloud/to-dns=allowed

# allows access to private IP address ranges 
allow-to-private-networks         networking.gardener.cloud/to-private-networks=allowed

# allows access to all but private IP address ranges 
allow-to-public-networks          networking.gardener.cloud/to-public-networks=allowed

# allows Ingress to etcd pods from the Shoot&#39;s Kubernetes API Server
allow-etcd                        app=etcd-statefulset,gardener.cloud/role=controlplane

# used by the Shoot API server to allows ingress from pods labeled
# with&#39;networking.gardener.cloud/to-shoot-apiserver=allowed&#39;, from Prometheus, and allows Egress to etcd pods
allow-kube-apiserver              app=kubernetes,gardener.cloud/role=controlplane,role=apiserver
</code></pre><h3 id=network-policies-for-logging--monitoring>Network Policies for Logging & Monitoring</h3><p>Gardener currently introduces a logging stack based on <a href=https://github.com/grafana/loki>Loki</a>. So this section is subject to change.
For more information, see the <a href="https://www.youtube.com/watch?v=345b8xCcB-U&t=1166s">Loki Gardener Community Meeting</a>.</p><p>These are the logging and monitoring related network policies:</p><pre tabindex=0><code>NAME                                POD-SELECTOR                                                             
allow-from-prometheus (deprecated!) networking.gardener.cloud/from-prometheus=allowed
allow-grafana                       component=grafana,gardener.cloud/role=monitoring
allow-prometheus                    app=prometheus,gardener.cloud/role=monitoring,role=monitoring
allow-to-aggregate-prometheus       networking.gardener.cloud/to-aggregate-prometheus=allowed
allow-to-loki                       networking.gardener.cloud/to-loki=allowed
</code></pre><p>As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus into the shoot namespace.
Each pod that should be scraped for metrics must have a <code>Service</code> which is annotated with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-policy-pod-label-selector: all-scrape-targets
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-policy-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;metrics-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>This automatically allows the needed network traffic from the Prometheus pod.
For more information, take a look at <a href=/docs/gardener/concepts/resource-manager/#networkpolicy-controllerpkgresourcemanagercontrollernetworkpolicy>this document</a>.</p><h3 id=implications-for-gardener-extensions>Implications for Gardener Extensions</h3><p>Gardener extensions sometimes need to deploy additional components into the Shoot namespace in the Seed hosting the control plane.
For example, the Gardener extension <a href=https://github.com/gardener/gardener-extension-provider-aws>provider-aws</a> deploys the <code>MachineControllerManager</code> into the Shoot namespace, that is ultimately responsible to create the VMs with the cloud provider AWS.</p><p>Every Shoot namespace in the Seed contains the network policy <code>deny-all</code>.
This requires a pod deployed by a Gardener extension to have labels from network policies that exist in the Shoot namespace, that allow the required network ranges.</p><p>Additionally, extensions could also deploy their own network policies. This is used e.g by the Gardener extension <a href=https://github.com/gardener/gardener-extension-provider-aws>provider-aws</a>
to serve <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>Admission Webhooks</a> for the Shoot API server that need to be reachable from within the Shoot namespace.</p><p>The pod can use an arbitrary combination of network policies.</p><h2 id=network-policies-in-the-garden-namespace>Network Policies in the <code>garden</code> Namespace</h2><p>The network policies in the <code>garden</code> namespace are, with a few exceptions (e.g Kubernetes control plane specific policies), the same as in the Shoot namespaces.
For your reference, these are all the deployed network policies.</p><pre tabindex=0><code>NAME                              POD-SELECTOR  
allow-fluentbit                   app=fluent-bit,gardener.cloud/role=logging,role=logging              
allow-from-aggregate-prometheus   networking.gardener.cloud/from-aggregate-prometheus=allowed              
allow-to-aggregate-prometheus     networking.gardener.cloud/to-aggregate-prometheus=allowed                
allow-to-all-shoot-apiservers     networking.gardener.cloud/to-all-shoot-apiservers=allowed                
allow-to-blocked-cidrs            networking.gardener.cloud/to-blocked-cidrs=allowed                       
allow-to-dns                      networking.gardener.cloud/to-dns=allowed                                 
allow-to-loki                     networking.gardener.cloud/to-loki=allowed                       
allow-to-private-networks         networking.gardener.cloud/to-private-networks=allowed                    
allow-to-public-networks          networking.gardener.cloud/to-public-networks=allowed                     
allow-to-runtime-apiserver        networking.gardener.cloud/to-runtime-apiserver=allowed                                                    
</code></pre><p>This section describes the network policies that are unique to the <code>garden</code> namespace.</p><p>The network policy <code>allow-to-all-shoot-apiservers</code> allows pods to access every <code>Shoot</code> API server in the <code>Seed</code>.
This is, for instance, used by the <a href=https://github.com/gardener/dependency-watchdog>dependency watchdog</a> to regularly check
the health of all the Shoot API servers.</p><p><a href=/docs/gardener/extensions/logging-and-monitoring/#monitoring>Gardener deploys a central Prometheus instance</a> in the <code>garden</code> namespace that fetches metrics and data from all seed cluster nodes and all seed cluster pods.
The network policies <code>allow-to-aggregate-prometheus</code> and <code>allow-from-aggregate-prometheus</code> allow traffic from and to this Prometheus instance.</p><p>Worth mentioning is, that the network policy <code>allow-to-shoot-networks</code> does not exist in the <code>garden</code> namespace. This is to forbid Gardener system components to talk to workload deployed in the Shoot VPC.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b3aa3f61ef7670f29a90ef82a24cb955>1.4.17 - Testing</h1><h1 id=testing-strategy-and-developer-guideline>Testing Strategy and Developer Guideline</h1><p>This document walks you through:</p><ul><li>What kind of tests we have in Gardener</li><li>How to run each of them</li><li>What purpose each kind of test serves</li><li>How to best write tests that are correct, stable, fast and maintainable</li><li>How to debug tests that are not working as expected</li></ul><p>The document is aimed towards developers that want to contribute code and need to write tests, as well as maintainers and reviewers that review test code.
It serves as a common guide that we commit to follow in our project to ensure consistency in our tests, good coverage for high confidence, and good maintainability.</p><p>The guidelines are not meant to be absolute rules.
Always apply common sense and adapt the guideline if it doesn&rsquo;t make much sense for some cases.
If in doubt, don&rsquo;t hesitate to ask questions during a PR review (as an author, but also as a reviewer).
Add new learnings as soon as we make them!</p><p>Generally speaking, <strong>tests are a strict requirement for contributing new code</strong>.
If you touch code that is currently untested, you need to add tests for the new cases that you introduce as a minimum.
Ideally though, you would add the missing test cases for the current code as well (<strong>boy scout rule</strong> &ndash; &ldquo;always leave the campground cleaner than you found it&rdquo;).</p><h2 id=writing-tests-relevant-for-all-kinds>Writing Tests (Relevant for All Kinds)</h2><ul><li>We follow BDD (behavior-driven development) testing principles and use <a href=https://onsi.github.io/ginkgo/>Ginkgo</a>, along with <a href=http://onsi.github.io/gomega/>Gomega</a>.<ul><li>Make sure to check out their extensive guides for more information and how to best leverage all of their features</li></ul></li><li>Use <code>By</code> to structure test cases with multiple steps, so that steps are easy to follow in the logs: <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/pkg/client/kubernetes/clientmap/internal/generic_clientmap_test.go#L122-L138>example test</a></li><li>Call <code>defer GinkgoRecover()</code> if making assertions in goroutines: <a href=https://pkg.go.dev/github.com/onsi/ginkgo#GinkgoRecover>doc</a>, <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/test/integration/scheduler/scheduler_test.go#L65-L68>example test</a></li><li>Use <code>DeferCleanup</code> instead of cleaning up manually (or use custom coding from the test framework): <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/test/integration/resourcemanager/health/health_suite_test.go#L102-L105>example test</a>, <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/test/integration/resourcemanager/health/health_test.go#L385-L390>example test</a><ul><li><code>DeferCleanup</code> makes sure to run the cleanup code in the right point in time, e.g., a <code>DeferCleanup</code> added in <code>BeforeEach</code> is executed with <code>AfterEach</code>.</li></ul></li><li>Test failures should point to an exact location, so that failures in CI aren&rsquo;t too difficult to debug/fix.<ul><li>Use <code>ExpectWithOffset</code> for making assertions in helper funcs like <code>expectSomethingWasCreated</code>: <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/extensions/pkg/controller/controlplane/genericactuator/actuator_test.go#L732-L736>example test</a></li><li>Make sure to add additional descriptions to Gomega matchers if necessary (e.g. in a loop): <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/test/e2e/shoot/internal/rotation/certificate_authorities.go#L89-L93>example test</a></li></ul></li><li>Introduce helper functions for assertions to make test more readable where applicable: <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/test/integration/gardenlet/shootsecret/controller_test.go#L323-L331>example test</a></li><li>Introduce custom matchers to make tests more readable where applicable: <a href=https://github.com/gardener/gardener/blob/2eb54485231408cbdbabaa49812572a07124364f/pkg/utils/test/matchers/matchers.go#L51-L57>example matcher</a></li><li>Don&rsquo;t rely on accurate timing of <code>time.Sleep</code> and friends.<ul><li>If doing so, CPU throttling in CI will make tests flaky, <a href=https://github.com/gardener/gardener/issues/5410>example flake</a></li><li>Use fake clocks instead, <a href=https://github.com/gardener/gardener/pull/4569>example PR</a></li></ul></li><li>Use the same client schemes that are also used by production code to avoid subtle bugs/regressions: <a href=https://github.com/gardener/gardener/pull/5469>example PR</a>, <a href=https://github.com/gardener/gardener/blob/2de823d0a457beb9d680260243032c95fa47dc72/pkg/resourcemanager/cmd/source.go#L34-L43>production schemes</a>, <a href=https://github.com/gardener/gardener/blob/2de823d0a457beb9d680260243032c95fa47dc72/test/integration/resourcemanager/health/health_suite_test.go#L108-L109>usage in test</a></li><li>Make sure that your test is actually asserting the right thing and it doesn&rsquo;t pass if the exact bug is introduced that you want to prevent.<ul><li>Use specific error matchers instead of asserting any error has happened, make sure that the corresponding branch in the code is tested, e.g., prefer<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>Expect(err).To(MatchError(<span style=color:#a31515>&#34;foo&#34;</span>))
</span></span></code></pre></div>over<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>Expect(err).To(HaveOccurred())
</span></span></code></pre></div></li><li>If you&rsquo;re unsure about your test&rsquo;s behavior, attaching the debugger can sometimes be helpful to make sure your test is correct.</li></ul></li><li>About overwriting global variables:<ul><li>This is a common pattern (or hack?) in go for faking calls to external functions.</li><li>However, this can lead to races, when the global variable is used from a goroutine (e.g., the function is called).</li><li>Alternatively, set fields on structs (passed via parameter or set directly): this is not racy, as struct values are typically (and should be) only used for a single test case.</li><li>An alternative to dealing with function variables and fields:<ul><li>Add an interface which your code depends on</li><li>Write a fake and a real implementation (similar to <code>clock.Clock.Sleep</code>)</li><li>The real implementation calls the actual function (<code>clock.RealClock.Sleep</code> calls <code>time.Sleep</code>)</li><li>The fake implementation does whatever you want it to do for your test (<code>clock.FakeClock.Sleep</code> waits until the test code advanced the time)</li></ul></li></ul></li><li>Use constants in test code with care.<ul><li>Typically, you should not use constants from the same package as the tested code, instead use literals.</li><li>If the constant value is changed, tests using the constant will still pass, although the &ldquo;specification&rdquo; is not fulfilled anymore.</li><li>There are cases where it&rsquo;s fine to use constants, but keep this caveat in mind when doing so.</li></ul></li><li>Creating sample data for tests can be a high effort.<ul><li>If valuable, add a package for generating common sample data, e.g. Shoot/Cluster objects.</li></ul></li><li>Make use of the <code>testdata</code> directory for storing arbitrary sample data needed by tests (helm charts, YAML manifests, etc.), <a href=https://github.com/gardener/gardener/pull/2140>example PR</a><ul><li>From <a href=https://pkg.go.dev/cmd/go/internal/test>https://pkg.go.dev/cmd/go/internal/test</a>:<blockquote><p>The go tool will ignore a directory named &ldquo;testdata&rdquo;, making it available to hold ancillary data needed by the tests.</p></blockquote></li></ul></li></ul><h2 id=unit-tests>Unit Tests</h2><h3 id=running-unit-tests>Running Unit Tests</h3><p>Run all unit tests:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test
</span></span></code></pre></div><p>Run all unit tests with test coverage:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-cov
</span></span><span style=display:flex><span>open test.coverage.html
</span></span><span style=display:flex><span>make test-cov-clean
</span></span></code></pre></div><p>Run unit tests of specific packages:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># run with same settings like in CI (race dector, timeout, ...)</span>
</span></span><span style=display:flex><span>./hack/test.sh ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># freestyle</span>
</span></span><span style=display:flex><span>go test ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/...
</span></span><span style=display:flex><span>ginkgo run ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/...
</span></span></code></pre></div><h3 id=debugging-unit-tests>Debugging Unit Tests</h3><p>Use ginkgo to focus on (a set of) test specs via <a href=https://onsi.github.io/ginkgo/#focused-specs>code</a> or via <a href=https://onsi.github.io/ginkgo/#description-based-filtering>CLI flags</a>.
Remember to unfocus specs before contributing code, otherwise your PR tests will fail.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ginkgo run --focus <span style=color:#a31515>&#34;should delete the unused resources&#34;</span> ./pkg/resourcemanager/controller/garbagecollector
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Will run 1 of 3 specs
</span></span><span style=display:flex><span>SS•
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Ran 1 of 3 Specs in 0.003 seconds
</span></span><span style=display:flex><span>SUCCESS! -- 1 Passed | 0 Failed | 0 Pending | 2 Skipped
</span></span><span style=display:flex><span>PASS
</span></span></code></pre></div><p>Use ginkgo to run tests until they fail:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ginkgo run --until-it-fails ./pkg/resourcemanager/controller/garbagecollector
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Ran 3 of 3 Specs in 0.004 seconds
</span></span><span style=display:flex><span>SUCCESS! -- 3 Passed | 0 Failed | 0 Pending | 0 Skipped
</span></span><span style=display:flex><span>PASS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>All tests passed...
</span></span><span style=display:flex><span>Will keep running them <span style=color:#00f>until</span> they fail.
</span></span><span style=display:flex><span>This was attempt <span style=color:green>#58</span>
</span></span><span style=display:flex><span>No, seriously... you can probably stop now.
</span></span></code></pre></div><p>Use the <a href=https://pkg.go.dev/golang.org/x/tools/cmd/stress><code>stress</code> tool</a> for deflaking tests that fail sporadically in CI, e.g., due resource contention (CPU throttling):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># get the stress tool</span>
</span></span><span style=display:flex><span>go install golang.org/x/tools/cmd/stress@latest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build a test binary</span>
</span></span><span style=display:flex><span>ginkgo build ./pkg/resourcemanager/controller/garbagecollector
</span></span><span style=display:flex><span><span style=color:green># alternatively</span>
</span></span><span style=display:flex><span>go test -c ./pkg/resourcemanager/controller/garbagecollector
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run the test in parallel and report any failures</span>
</span></span><span style=display:flex><span>stress -p 16 ./pkg/resourcemanager/controller/garbagecollector/garbagecollector.test -ginkgo.focus <span style=color:#a31515>&#34;should delete the unused resources&#34;</span>
</span></span><span style=display:flex><span>5s: 1077 runs so far, 0 failures
</span></span><span style=display:flex><span>10s: 2160 runs so far, 0 failures
</span></span></code></pre></div><p><code>stress</code> will output a path to a file containing the full failure message when a test run fails.</p><h3 id=purpose-of-unit-tests>Purpose of Unit Tests</h3><ul><li>Unit tests prove the correctness of a single unit according to the specification of its interface.<ul><li>Think: Is the unit that I introduced doing what it is supposed to do for all cases?</li></ul></li><li>Unit tests protect against regressions caused by adding new functionality to or refactoring of a single unit.<ul><li>Think: Is the unit that was introduced earlier (by someone else) and that I changed still doing what it was supposed to do for all cases?</li></ul></li><li>Example units: functions (conversion, defaulting, validation, helpers), structs (helpers, basic building blocks like the Secrets Manager), predicates, event handlers.</li><li>For these purposes, unit tests need to cover all important cases of input for a single unit and cover edge cases / negative paths as well (e.g., errors).<ul><li>Because of the possible high dimensionality of test input, unit tests need to be fast to execute: individual test cases should not take more than a few seconds, test suites not more than 2 minutes.</li><li>Fuzzing can be used as a technique in addition to usual test cases for covering edge cases.</li></ul></li><li>Test coverage can be used as a tool during test development for covering all cases of a unit.</li><li>However, test coverage data can be a false safety net.<ul><li>Full line coverage doesn&rsquo;t mean you have covered all cases of valid input.</li><li>We don&rsquo;t have strict requirements for test coverage, as it doesn&rsquo;t necessarily yield the desired outcome.</li></ul></li><li>Unit tests should not test too large components, e.g. entire controller <code>Reconcile</code> functions.<ul><li>If a function/component does many steps, it&rsquo;s probably better to split it up into multiple functions/components that can be unit tested individually</li><li>There might be special cases for very small <code>Reconcile</code> functions.</li><li>If there are a lot of edge cases, extract dedicated functions that cover them and use unit tests to test them.</li><li>Usual-sized controllers should rather be tested in integration tests.</li><li>Individual parts (e.g. helper functions) should still be tested in unit test for covering all cases, though.</li></ul></li><li>Unit tests are especially easy to run with a debugger and can help in understanding concrete behavior of components.</li></ul><h3 id=writing-unit-tests>Writing Unit Tests</h3><ul><li>For the sake of execution speed, fake expensive calls/operations, e.g. secret generation: <a href=https://github.com/gardener/gardener/blob/efcc0a9146d3558253b95071f2c652663f916d92/pkg/operation/botanist/component/kubescheduler/kube_scheduler_suite_test.go#L32-L34>example test</a></li><li>Generally, prefer fakes over mocks, e.g., use controller-runtime fake client over mock clients.<ul><li>Mocks decrease maintainability because they expect the tested component to follow a certain way to reach the desired goal (e.g., call specific functions with particular arguments), <a href=https://github.com/gardener/gardener/pull/4027/commits/111aba2c8e306421f2fa6b27e5d8ed8b2fc52be9#diff-8e61507edf985df2625840a690115c43bca6c032f2ff818389633bd4365c3efdR293-R298>example consequence</a></li><li>Generally, fakes should be used in &ldquo;result-oriented&rdquo; test code (e.g., that a certain object was labelled, but the test doesn&rsquo;t care if it was via patch or update as both a valid ways to reach the desired goal).</li><li>Although rare, there are valid use cases for mocks, e.g. if the following aspects are important for correctness:<ul><li>Asserting that an exact function is called</li><li>Asserting that functions are called in a specific order</li><li>Asserting that exact parameters/values/&mldr; are passed</li><li>Asserting that a certain function was not called</li><li>Many of these can also be verified with fakes, although mocks might be simpler</li></ul></li><li>Only use mocks if the tested code directly calls the mock; never if the tested code only calls the mock indirectly (e.g., through a helper package/function).</li><li>Keep in mind the maintenance implications of using mocks:<ul><li>Can you make a valid non-behavioral change in the code without breaking the test or dependent tests?</li></ul></li><li>It&rsquo;s valid to mix fakes and mocks in the same test or between test cases.</li></ul></li><li>Generally, use the go test package, i.e., declare <code>package &lt;production_package>_test</code>:<ul><li>Helps in avoiding cyclic dependencies between production, test and helper packages</li><li>Also forces you to distinguish between the public (exported) API surface of your code and internal state that might not be of interest to tests</li><li>It might be valid to use the same package as the tested code if you want to test unexported functions.<ul><li>Alternatively, an <a href=https://go.dev/doc/go1.4#internalpackages><code>internal</code> package</a> can be used to host &ldquo;internal&rdquo; helpers: <a href=https://github.com/gardener/gardener/tree/2eb54485231408cbdbabaa49812572a07124364f/pkg/client/kubernetes/clientmap>example package</a></li></ul></li><li>Helpers can also be exported if no one is supposed to import the containing package (e.g. controller package).</li></ul></li></ul><h2 id=integration-tests-envtests>Integration Tests (envtests)</h2><p>Integration tests in Gardener use the <code>sigs.k8s.io/controller-runtime/pkg/envtest</code> package.
It sets up a temporary control plane (etcd + kube-apiserver) and runs the test against it.
The test suites start their individual <code>envtest</code> environment before running the tested controller/webhook and executing test cases.
Before exiting, the test suites tear down the temporary test environment.</p><p>Package <code>github.com/gardener/gardener/pkg/envtest</code> augments the controller-runtime&rsquo;s <code>envtest</code> package by starting and registering <code>gardener-apiserver</code>.
This is used to test controllers that act on resources in the Gardener APIs (aggregated APIs).</p><p>Historically, <a href=#test-machinery-tests>test machinery tests</a> have also been called &ldquo;integration tests&rdquo;.
However, test machinery does not perform integration testing but rather executes a form of end-to-end tests against a real landscape.
Hence, we tried to sharpen the terminology that we use to distinguish between &ldquo;real&rdquo; integration tests and test machinery tests but you might still find &ldquo;integration tests&rdquo; referring to test machinery tests in old issues or outdated documents.</p><h3 id=running-integration-tests>Running Integration Tests</h3><p>The <code>test-integration</code> make rule prepares the environment automatically by downloading the respective binaries (if not yet present) and setting the necessary environment variables.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-integration
</span></span></code></pre></div><p>If you want to run a specific set of integration tests, you can also execute them using <code>./hack/test-integration.sh</code> directly instead of using the <code>test-integration</code> rule. Prior to execution, the <code>PATH</code> environment variable needs to be set to also included the tools binary directory. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export PATH=<span style=color:#a31515>&#34;</span>$PWD<span style=color:#a31515>/hack/tools/bin:</span>$PATH<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>source ./hack/test-integration.env
</span></span><span style=display:flex><span>./hack/test-integration.sh ./test/integration/resourcemanager/tokenrequestor
</span></span></code></pre></div><p>The script takes care of preparing the environment for you.
If you want to execute the test suites directly via <code>go test</code> or <code>ginkgo</code>, you have to point the <code>KUBEBUILDER_ASSETS</code> environment variable to the path that contains the etcd and kube-apiserver binaries. Alternatively, you can install the binaries to <code>/usr/local/kubebuilder/bin</code>. Additionally, the environment variables from <code>hack/test-integration.env</code> should be sourced.</p><h3 id=debugging-integration-tests>Debugging Integration Tests</h3><p>You can configure <code>envtest</code> to use an existing cluster or control plane instead of starting a temporary control plane that is torn down immediately after executing the test.
This can be helpful for debugging integration tests because you can easily inspect what is going on in your test environment with <code>kubectl</code>.</p><p>While you can use an existing cluster (e.g., <code>kind</code>), some test suites expect that no controllers and no nodes are running in the test environment (as it is the case in <code>envtest</code> test environments).
Hence, using a full-blown cluster with controllers and nodes might sometimes be impractical, as you would need to stop cluster components for the tests to work.</p><p>You can use <code>make start-envtest</code> to start an <code>envtest</code> test environment that is managed separately from individual test suites.
This allows you to keep the test environment running for as long as you want, and to debug integration tests by executing multiple test runs in parallel or inspecting test runs using <code>kubectl</code>.
When you are finished, just hit <code>CTRL-C</code> for tearing down the test environment.
The kubeconfig for the test environment is placed in <code>dev/envtest-kubeconfig.yaml</code>.</p><p><code>make start-envtest</code> brings up an <code>envtest</code> environment using the default configuration.
If your test suite requires a different control plane configuration (e.g., disabled admission plugins or enabled feature gates), feel free to locally modify the configuration in <a href=https://github.com/gardener/gardener/tree/master/test/start-envtest><code>test/start-envtest</code></a> while debugging.</p><p>Run an <code>envtest</code> suite (not using <code>gardener-apiserver</code>) against an existing test environment:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-envtest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span>export USE_EXISTING_CLUSTER=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run test with verbose output</span>
</span></span><span style=display:flex><span>./hack/test-integration.sh -v ./test/integration/resourcemanager/health -ginkgo.v
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span><span style=color:green># watch test objects</span>
</span></span><span style=display:flex><span>k get managedresource -A -w
</span></span></code></pre></div><p>Run a <code>gardenerenvtest</code> suite (using <code>gardener-apiserver</code>) against an existing test environment:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># modify test/start-envtest to disable admission plugins and enable feature gates like in test suite...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>make start-envtest ENVTEST_TYPE=gardener
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span>export USE_EXISTING_GARDENER=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run test with verbose output</span>
</span></span><span style=display:flex><span>./hack/test-integration.sh -v ./test/integration/controllermanager/bastion -ginkgo.v
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span><span style=color:green># watch test objects</span>
</span></span><span style=display:flex><span>k get bastion -A -w
</span></span></code></pre></div><p>Similar to <a href=#debugging-unit-tests>debugging unit tests</a>, the <code>stress</code> tool can help hunting flakes in integration tests.
Though, you might need to run less tests in parallel though (specified via <code>-p</code>) and have a bit more patience.
Generally, reproducing flakes in integration tests is easier when stress-testing against an existing test environment instead of starting temporary individual control planes per test run.</p><p>Stress-test an <code>envtest</code> suite (not using <code>gardener-apiserver</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># build a test binary</span>
</span></span><span style=display:flex><span>ginkgo build ./test/integration/resourcemanager/health
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># prepare a test environment to run the test against</span>
</span></span><span style=display:flex><span>make start-envtest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span>export USE_EXISTING_CLUSTER=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># use same timeout settings like in CI</span>
</span></span><span style=display:flex><span>source ./hack/test-integration.env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># switch to test package directory like `go test`</span>
</span></span><span style=display:flex><span>cd ./test/integration/resourcemanager/health
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run the test in parallel and report any failures</span>
</span></span><span style=display:flex><span>stress -ignore <span style=color:#a31515>&#34;unable to grab random port&#34;</span> -p 16 ./health.test
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Stress-test a <code>gardenerenvtest</code> suite (using <code>gardener-apiserver</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># modify test/start-envtest to disable admission plugins and enable feature gates like in test suite...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build a test binary</span>
</span></span><span style=display:flex><span>ginkgo build ./test/integration/controllermanager/bastion
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># prepare a test environment including gardener-apiserver to run the test against</span>
</span></span><span style=display:flex><span>make start-envtest ENVTEST_TYPE=gardener
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># in another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/dev/envtest-kubeconfig.yaml
</span></span><span style=display:flex><span>export USE_EXISTING_GARDENER=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># use same timeout settings like in CI</span>
</span></span><span style=display:flex><span>source ./hack/test-integration.env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># switch to test package directory like `go test`</span>
</span></span><span style=display:flex><span>cd ./test/integration/controllermanager/bastion
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run the test in parallel and report any failures</span>
</span></span><span style=display:flex><span>stress -ignore <span style=color:#a31515>&#34;unable to grab random port&#34;</span> -p 16 ./bastion.test
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=purpose-of-integration-tests>Purpose of Integration Tests</h3><ul><li>Integration tests prove that multiple units are correctly integrated into a fully-functional component of the system.</li><li>Example components with multiple units:<ul><li>A controller with its reconciler, watches, predicates, event handlers, queues, etc.</li><li>A webhook with its server, handler, decoder, and webhook configuration.</li></ul></li><li>Integration tests set up a full component (including used libraries) and run it against a test environment close to the actual setup.<ul><li>e.g., start controllers against a real Kubernetes control plane to catch bugs that can only happen when talking to a real API server.</li><li>Integration tests are generally more expensive to run (e.g., in terms of execution time).</li></ul></li><li>Integration tests should not cover each and every detailed case.<ul><li>Rather than that, cover a good portion of the &ldquo;usual&rdquo; cases that components will face during normal operation (positive and negative test cases).</li><li>Also, there is no need to cover all failure cases or all cases of predicates -> they should be covered in unit tests already.</li><li>Generally, not supposed to &ldquo;generate test coverage&rdquo; but to provide confidence that components work well.</li></ul></li><li>As integration tests typically test only one component (or a cohesive set of components) isolated from others, they cannot catch bugs that occur when multiple controllers interact (could be discovered by e2e tests, though).</li><li>Rule of thumb: a new integration tests should be added for each new controller (an integration test doesn&rsquo;t replace unit tests though).</li></ul><h3 id=writing-integration-tests>Writing Integration Tests</h3><ul><li>Make sure to have a clean test environment on both test suite and test case level:<ul><li>Set up dedicated test environments (envtest instances) per test suite.</li><li>Use dedicated namespaces per test suite:<ul><li>Use <code>GenerateName</code> with a test-specific prefix: <a href=https://github.com/gardener/gardener/blob/ee3e50387fc7e6298908242f59894a7ea6f91fa7/test/integration/resourcemanager/secret/secret_suite_test.go#L94-L105>example test</a></li><li>Restrict the controller-runtime manager to the test namespace by setting <code>manager.Options.Namespace</code>: <a href=https://github.com/gardener/gardener/blob/d9b00b574182094c5d03ab16dfc2d20515e9b6ed/test/integration/controllermanager/cloudprofile/cloudprofile_suite_test.go#L104>example test</a></li><li>Alternatively, use a test-specific prefix with a random suffix determined upfront: <a href=https://github.com/gardener/gardener/blob/ce3973a605886ab6a496cf7f9fb6a5de54a5e7c2/test/integration/resourcemanager/tokeninvalidator/tokeninvalidator_suite_test.go#L68>example test</a><ul><li>This can be used to restrict webhooks to a dedicated test namespace: <a href=https://github.com/gardener/gardener/blob/ce3973a605886ab6a496cf7f9fb6a5de54a5e7c2/test/integration/resourcemanager/tokeninvalidator/tokeninvalidator_suite_test.go#L73>example test</a></li></ul></li><li>This allows running a test in parallel against the same existing cluster for deflaking and stress testing: <a href=https://github.com/gardener/gardener/pull/5953>example PR</a></li></ul></li><li>If the controller works on cluster-scoped resources:<ul><li>Label the resources with a label specific to the test run, e.g. the test namespace&rsquo;s name: <a href=https://github.com/gardener/gardener/blob/b01239edfd594b09ecd44dd77fba7a05a74820e8/test/integration/controllermanager/cloudprofile/cloudprofile_test.go#L38>example test</a></li><li>Restrict the manager&rsquo;s cache for these objects with a corresponding label selector: <a href=https://github.com/gardener/gardener/blob/b01239edfd594b09ecd44dd77fba7a05a74820e8/test/integration/controllermanager/cloudprofile/cloudprofile_suite_test.go#L110-L116>example test</a></li><li>Alternatively, use a checksum of a random UUID using <code>uuid.NewUUID()</code> function: <a href=https://github.com/gardener/gardener/blob/3840acaaf57955fd65330c83b2b2d5bdaad56179/test/integration/resourcemanager/tokeninvalidator/tokeninvalidator_suite_test.go#L71-L72>example test</a></li><li>This allows running a test in parallel against the same existing cluster for deflaking and stress testing, even if it works with cluster-scoped resources that are visible to all parallel test runs: <a href=https://github.com/gardener/gardener/pull/6527>example PR</a></li></ul></li><li>Use dedicated test resources for each test case:<ul><li>Use <code>GenerateName</code>: <a href=https://github.com/gardener/gardener/blob/ee3e50387fc7e6298908242f59894a7ea6f91fa7/test/integration/resourcemanager/health/health_test.go#L38-L48>example test</a></li><li>Alternatively, use a checksum of a random UUID using <code>uuid.NewUUID()</code> function: <a href=https://github.com/gardener/gardener/blob/3840acaaf57955fd65330c83b2b2d5bdaad56179/test/integration/resourcemanager/tokeninvalidator/tokeninvalidator_suite_test.go#L71-L72>example test</a></li><li>Logging the created object names is generally a good idea to support debugging failing or flaky tests: <a href=https://github.com/gardener/gardener/blob/50f92c5dc35160fe05da9002a79e7ce4a9cf3509/test/integration/controllermanager/cloudprofile/cloudprofile_test.go#L94-L96>example test</a></li><li>Always delete all resources after the test case (e.g., via <code>DeferCleanup</code>) that were created for the test case</li><li>This avoids conflicts between test cases and cascading failures which distract from the actual root failures</li></ul></li><li>Don&rsquo;t tolerate already existing resources (~dirty test environment), code smell: ignoring already exist errors</li></ul></li><li>Don&rsquo;t use a cached client in test code (e.g., the one from a controller-runtime manager), always construct a dedicated test client (uncached): <a href=https://github.com/gardener/gardener/blob/ee3e50387fc7e6298908242f59894a7ea6f91fa7/test/integration/resourcemanager/managedresource/resource_suite_test.go#L96-L97>example test</a></li><li>Use <a href=https://onsi.github.io/gomega/#making-asynchronous-assertions>asynchronous assertions</a>: <code>Eventually</code> and <code>Consistently</code>.<ul><li>Never <code>Expect</code> anything to happen synchronously (immediately).</li><li>Don&rsquo;t use retry or wait until functions -> use <code>Eventually</code>, <code>Consistently</code> instead: <a href=https://github.com/gardener/gardener/blob/ee3e50387fc7e6298908242f59894a7ea6f91fa7/test/integration/controllermanager/shootmaintenance/utils_test.go#L36-L48>example test</a></li><li>This allows to override the interval/timeout values from outside instead of hard-coding this in the test (see <code>hack/test-integration.sh</code>): <a href=https://github.com/gardener/gardener/pull/5938#discussion_r869155906>example PR</a></li><li>Beware of the default <code>Eventually</code> / <code>Consistently</code> timeouts / poll intervals: <a href=https://onsi.github.io/gomega/#eventually>docs</a></li><li>Don&rsquo;t set custom (high) timeouts and intervals in test code: <a href=https://github.com/gardener/gardener/pull/4983>example PR</a><ul><li>iInstead, shorten sync period of controllers, overwrite intervals of the tested code, or use fake clocks: <a href=https://github.com/gardener/gardener/blob/7c4031a57836de20758f32e1015c8a0f6c754d0f/test/integration/resourcemanager/managedresource/resource_suite_test.go#L137-L139>example test</a></li></ul></li><li>Pass <code>g Gomega</code> to <code>Eventually</code>/<code>Consistently</code> and use <code>g.Expect</code> in it: <a href=https://onsi.github.io/gomega/#category-3-making-assertions-eminem-the-function-passed-into-codeeventuallycode>docs</a>, <a href=https://github.com/gardener/gardener/blob/708f65c279276abd3a770c2f84a89e02876b3c38/test/e2e/shoot/internal/rotation/certificate_authorities.go#L111-L122>example test</a>, <a href=https://github.com/gardener/gardener/pull/4936>example PR</a></li><li>Don&rsquo;t forget to call <code>{Eventually,Consistently}.Should()</code>, otherwise the assertions always silently succeeds without errors: <a href=https://github.com/onsi/gomega/issues/561>onsi/gomega#561</a></li></ul></li><li>When using Gardener&rsquo;s envtest (<code>envtest.GardenerTestEnvironment</code>):<ul><li>Disable gardener-apiserver&rsquo;s admission plugins that are not relevant to the integration test itself by passing <code>--disable-admission-plugins</code>: <a href=https://github.com/gardener/gardener/blob/50f92c5dc35160fe05da9002a79e7ce4a9cf3509/test/integration/controllermanager/shoot/maintenance/maintenance_suite_test.go#L61-L67>example test</a></li><li>This makes setup / teardown code simpler and ensures to only test code relevant to the tested component itself (but not the entire set of admission plugins)</li><li>e.g., you can disable the <code>ShootValidator</code> plugin to create <code>Shoots</code> that reference non-existing <code>SecretBindings</code> or disable the <code>DeletionConfirmation</code> plugin to delete Gardener resources without adding a deletion confirmation first.</li></ul></li><li>Use a custom rate limiter for controllers in integration tests: <a href=https://github.com/gardener/gardener/blob/3dd6b111d677eb4bceadaa8fc469877097660577/test/integration/controllermanager/exposureclass/exposureclass_suite_test.go#L130-L131>example test</a><ul><li>This can be used for limiting exponential backoff to shorten wait times.</li><li>Otherwise, if using the default rate limiter, exponential backoff might exceed the timeout of <code>Eventually</code> calls and cause flakes.</li></ul></li></ul><h2 id=end-to-end-e2e-tests-using-provider-local>End-to-End (e2e) Tests (Using provider-local)</h2><p>We run a suite of e2e tests on every pull request and periodically on the <code>master</code> branch.
It uses a <a href=https://kind.sigs.k8s.io/>KinD cluster</a> and <a href=https://skaffold.dev/>skaffold</a> to boostrap a full installation of Gardener based on the current revision, including <a href=/docs/gardener/extensions/provider-local/>provider-local</a>.
This allows us to run e2e tests in an isolated test environment and fully locally without any infrastructure interaction.
The tests perform a set of operations on Shoot clusters, e.g. creating, deleting, hibernating and waking up.</p><p>These tests are executed in our prow instance at <a href=https://prow.gardener.cloud/>prow.gardener.cloud</a>, see <a href=https://github.com/gardener/ci-infra/blob/e324cb79c39c013d7f253c33690b7fcc92c001d8/config/jobs/gardener/gardener-e2e-kind.yaml>job definition</a> and <a href="https://prow.gardener.cloud/?repo=gardener%2Fgardener&job=*gardener-e2e-kind">job history</a>.</p><h3 id=running-e2e-tests>Running e2e Tests</h3><p>You can also run these tests on your development machine, using the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-up
</span></span><span style=display:flex><span>export KUBECONFIG=$PWD/example/gardener-local/kind/local/kubeconfig
</span></span><span style=display:flex><span>make gardener-up
</span></span><span style=display:flex><span>make test-e2e-local  <span style=color:green># alternatively: make test-e2e-local-simple</span>
</span></span></code></pre></div><p>If you want to run a specific set of e2e test cases, you can also execute them using <code>./hack/test-e2e-local.sh</code> directly in combination with <a href=https://onsi.github.io/ginkgo/#spec-labels>ginkgo label filters</a>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/test-e2e-local.sh --label-filter <span style=color:#a31515>&#34;Shoot &amp;&amp; credentials-rotation&#34;</span> ./test/e2e/gardener/...
</span></span></code></pre></div><p>If you want to use an existing shoot instead of creating a new one for the test case and deleting it afterwards, you can specify the existing shoot via the following flags.
This can be useful to speed up the development of e2e tests.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/test-e2e-local.sh --label-filter <span style=color:#a31515>&#34;Shoot &amp;&amp; credentials-rotation&#34;</span> ./test/e2e/gardener/... -- --project-namespace=garden-local --existing-shoot-name=local
</span></span></code></pre></div><p>For more information, see <a href=/docs/gardener/development/getting_started_locally/>Developing Gardener Locally</a> and <a href=/docs/gardener/deployment/getting_started_locally/>Deploying Gardener Locally</a>.</p><h3 id=debugging-e2e-tests>Debugging e2e Tests</h3><p>When debugging e2e test failures in CI, logs of the cluster components can be very helpful.
Our e2e test jobs export logs of all containers running in the kind cluster to prow&rsquo;s artifacts storage.
You can find them by clicking the <code>Artifacts</code> link in the top bar in prow&rsquo;s job view and navigating to <code>artifacts</code>.
This directory will contain all cluster component logs grouped by node.</p><p>Pull all artifacts using <a href=https://cloud.google.com/storage/docs/gsutil><code>gsutil</code></a> for searching and filtering the logs locally (use the path displayed in the artifacts view):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gsutil cp -r gs://gardener-prow/pr-logs/pull/gardener_gardener/6136/pull-gardener-e2e-kind/1542030416616099840/artifacts/gardener-local-control-plane /tmp
</span></span></code></pre></div><h3 id=purpose-of-e2e-tests>Purpose of e2e Tests</h3><ul><li>e2e tests provide a high level of confidence that our code runs as expected by users when deployed to production.</li><li>They are supposed to catch bugs resulting from interaction between multiple components.</li><li>Test cases should be as close as possible to real usage by end users:<ul><li>You should test &ldquo;from the perspective of the user&rdquo; (or operator).</li><li>Example: I create a Shoot and expect to be able to connect to it via the provided kubeconfig.</li><li>Accordingly, don&rsquo;t assert details of the system.<ul><li>e.g., the user also wouldn&rsquo;t expect that there is a kube-apiserver deployment in the seed, they rather expect that they can talk to it no matter how it is deployed</li><li>Only assert details of the system if the tested feature is not fully visible to the end-user and there is no other way of ensuring that the feature works reliably</li><li>e.g., the Shoot CA rotation is not fully visible to the user but is assertable by looking at the secrets in the Seed.</li></ul></li></ul></li><li>Pro: can be executed by developers and users without any real infrastructure (provider-local).</li><li>Con: they currently cannot be executed with real infrastructure (e.g., provider-aws), we will work on this as part of <a href=https://github.com/gardener/gardener/issues/6016>#6016</a>.</li><li>Keep in mind that the tested scenario is still artificial in a sense of using default configuration, only a few objects, only a few config/settings combinations are covered.<ul><li>We will never be able to cover the full &ldquo;test matrix&rdquo; and this should not be our goal.</li><li>Bugs will still be released and will still happen in production; we can&rsquo;t avoid it.</li><li>Instead, we should add test cases for preventing bugs in features or settings that were frequently regressed: <a href=https://github.com/gardener/gardener/pull/5725>example PR</a></li></ul></li><li>Usually e2e tests cover the &ldquo;straight-forward cases&rdquo;.<ul><li>However, negative test cases can also be included, especially if they are important from the user&rsquo;s perspective.</li></ul></li></ul><h3 id=writing-e2e-tests>Writing e2e Tests</h3><ul><li>Always wrap API calls and similar things in <code>Eventually</code> blocks: <a href=https://github.com/gardener/gardener/blob/a66b8ec47995561393bf1ad9a817463089a0255e/test/e2e/shoot/internal/rotation/observability.go#L46-L55>example test</a><ul><li>At this point, we are pretty much working with a distributed system and failures can happen anytime.</li><li>Wrapping calls in <code>Eventually</code> makes tests more stable and more realistic (usually, you wouldn&rsquo;t call the system broken if a single API call fails because of a short connectivity issue).</li></ul></li><li>Most of the points from <a href=#writing-integration-tests>writing integration tests</a> are relevant for e2e tests as well (especially the points about asynchronous assertions).</li><li>In contrast to integration tests, in e2e tests, it might make sense to specify higher timeouts for <code>Eventually</code> calls, e.g., when waiting for a <code>Shoot</code> to be reconciled.<ul><li>Generally, try to use the default settings for <code>Eventually</code> specified via the environment variables.</li><li>Only set higher timeouts if waiting for long-running reconciliations to be finished.</li></ul></li></ul><h2 id=gardener-upgrade-tests-using-provider-local>Gardener Upgrade Tests (Using provider-local)</h2><p>Gardener upgrade tests setup a kind cluster and deploy Gardener version <code>vX.X.X</code> before upgrading it to a given version <code>vY.Y.Y</code>.</p><p>This allows verifying whether the current (unreleased) revision/branch (or a specific release) is compatible with the latest (or a specific other) release. The <code>GARDENER_PREVIOUS_RELEASE</code> and <code>GARDENER_NEXT_RELEASE</code> environment variables are used to specify the respective versions.</p><p>This helps understanding what happens or how the system reacts when Gardener upgrades from versions <code>vX.X.X</code> to <code>vY.Y.Y</code> for existing shoots in different states (<code>creation</code>/<code>hibernation</code>/<code>wakeup</code>/<code>deletion</code>). Gardener upgrade tests also help qualifying releases for all flavors (<strong>non-HA</strong> or <strong>HA</strong> with failure tolerance <code>node</code>/<code>zone</code>).</p><p>Just like E2E tests, upgrade tests also use a <a href=https://kind.sigs.k8s.io/>KinD cluster</a> and <a href=https://skaffold.dev/>skaffold</a> for bootstrapping a full Gardener installation based on the current revision/branch, including <a href=/docs/gardener/extensions/provider-local/>provider-local</a>.
This allows running e2e tests in an isolated test environment, fully locally without any infrastructure interaction.
The tests perform a set of operations on Shoot clusters, e.g. create, delete, hibernate and wake up.</p><p>Below is a sequence describing how the tests are performed.</p><ul><li>Create a <code>kind</code> cluster.</li><li>Install Gardener version <code>vX.X.X</code>.</li><li>Run gardener pre-upgrade tests which are labeled with <code>pre-upgrade</code>.</li><li>Upgrade Gardener version from <code>vX.X.X</code> to <code>vY.Y.Y</code>.</li><li>Run gardener post-upgrade tests which are labeled with <code>post-upgrade</code></li><li>Tear down seed and kind cluster.</li></ul><h3 id=how-to-run-upgrade-tests-between-two-gardener-releases>How to Run Upgrade Tests Between Two Gardener Releases</h3><p>Sometimes, we need to verify/qualify two Gardener releases when we upgrade from one version to another.<br>This can performed by fetching the two Gardener versions from the <strong><a href=https://github.com/gardener/gardener/releases/latest>GitHub Gardener release page</a></strong> and setting appropriate env variables <code>GARDENER_PREVIOUS_RELEASE</code>, <code>GARDENER_NEXT_RELEASE</code>.</p><blockquote><p><strong><code>GARDENER_PREVIOUS_RELEASE</code></strong> &ndash; This env variable refers to a source revision/branch (or a specific release) which has to be installed first and then upgraded to version <strong><code>GARDENER_NEXT_RELEASE</code></strong>. By default, it fetches the latest release version from <strong><a href=https://github.com/gardener/gardener/releases/latest>GitHub Gardener release page</a></strong>.</p></blockquote><blockquote><p><strong><code>GARDENER_NEXT_RELEASE</code></strong> &ndash; This env variable refers to the target revision/branch (or a specific release) to be upgraded to after successful installation of <strong><code>GARDENER_PREVIOUS_RELEASE</code></strong>. By default, it considers the local HEAD revision, builds code, and installs Gardener from the current revision where the Gardener upgrade tests triggered.</p></blockquote><ul><li><code>make ci-e2e-kind-upgrade GARDENER_PREVIOUS_RELEASE=v1.60.0 GARDENER_NEXT_RELEASE=v1.61.0</code></li><li><code>make ci-e2e-kind-ha-single-zone-upgrade GARDENER_PREVIOUS_RELEASE=v1.60.0 GARDENER_NEXT_RELEASE=v1.61.0</code></li><li><code>make ci-e2e-kind-ha-multi-zone-upgrade GARDENER_PREVIOUS_RELEASE=v1.60.0 GARDENER_NEXT_RELEASE=v1.61.0</code></li></ul><h3 id=purpose-of-upgrade-tests>Purpose of Upgrade Tests</h3><ul><li>Tests will ensure that shoot clusters reconciled with the previous version of Gardener work as expected even with the next Gardener version.</li><li>This will reproduce or catch actual issues faced by end users.</li><li>One of the test cases ensures no downtime is faced by the end-users for shoots while upgrading Gardener if the shoot&rsquo;s control-plane is configured as HA.</li></ul><h3 id=writing-upgrade-tests>Writing Upgrade Tests</h3><ul><li>Tests are divided into two parts and labeled with <code>pre-upgrade</code> and <code>post-upgrade</code> labels.</li><li>An example test case which ensures a shoot which was <code>hibernated</code> in a previous Gardener release should <code>wakeup</code> as expected in next release:<ul><li>Creating a shoot and hibernating a shoot is pre-upgrade test case which should be labeled <code>pre-upgrade</code> label.</li><li>Then wakeup a shoot and delete a shoot is post-upgrade test case which should be labeled <code>post-upgrade</code> label.</li></ul></li></ul><h2 id=test-machinery-tests>Test Machinery Tests</h2><p>Please see <a href=/docs/gardener/development/testmachinery_tests/>Test Machinery Tests</a>.</p><h3 id=purpose-of-test-machinery-tests>Purpose of Test Machinery Tests</h3><ul><li>Test machinery tests have to be executed against full-blown Gardener installations.</li><li>They can provide a very high level of confidence that an installation is functional in its current state, this includes: all Gardener components, Extensions, the used Cloud Infrastructure, all relevant settings/configuration.</li><li>This brings the following benefits:<ul><li>They test more realistic scenarios than e2e tests (real configuration, real infrastructure, etc.).</li><li>Tests run &ldquo;where the users are&rdquo;.</li></ul></li><li>However, this also brings significant drawbacks:<ul><li>Tests are difficult to develop and maintain.</li><li>Tests require a full Gardener installation and cannot be executed in CI (on PR-level or against master).</li><li>Tests require real infrastructure (think cloud provider credentials, cost).</li><li>Using <code>TestDefinitions</code> under <code>.test-defs</code> requires a full test machinery installation.</li><li>Accordingly, tests are heavyweight and expensive to run.</li><li>Testing against real infrastructure can cause flakes sometimes (e.g., in outage situations).</li><li>Failures are hard to debug, because clusters are deleted after the test (for obvious cost reasons).</li><li>Bugs can only be caught, once it&rsquo;s &ldquo;too late&rdquo;, i.e., when code is merged and deployed.</li></ul></li><li>Today, test machinery tests cover a bigger &ldquo;test matrix&rdquo; (e.g., Shoot creation across infrastructures, kubernetes versions, machine image versions).</li><li>Test machinery also runs Kubernetes conformance tests.</li><li>However, because of the listed drawbacks, we should rather focus on augmenting our e2e tests, as we can run them locally and in CI in order to catch bugs before they get merged.</li><li>It&rsquo;s still a good idea to add test machinery tests if a feature that is depending on some installation-specific configuration needs to be tested.</li></ul><h3 id=writing-test-machinery-tests>Writing Test Machinery Tests</h3><ul><li>Generally speaking, most points from <a href=#writing-integration-tests>writing integration tests</a> and <a href=#writing-e2e-tests>writing e2e tests</a> apply here as well.</li><li>However, test machinery tests contain a lot of technical debt and existing code doesn&rsquo;t follow these best practices.</li><li>As test machinery tests are out of our general focus, we don&rsquo;t intend on reworking the tests soon or providing more guidance on how to write new ones.</li></ul><h2 id=manual-tests>Manual Tests</h2><ul><li>Manual tests can be useful when the cost of trying to automatically test certain functionality are too high.</li><li>Useful for PR verification, if a reviewer wants to verify that all cases are properly tested by automated tests.</li><li>Currently, it&rsquo;s the simplest option for testing upgrade scenarios.<ul><li>e.g. migration coding is probably best tested manually, as it&rsquo;s a high effort to write an automated test for little benefit</li></ul></li><li>Obviously, the need for manual tests should be kept at a bare minimum.<ul><li>Instead, we should add e2e tests wherever sensible/valuable.</li><li>We want to implement some form of general upgrade tests as part of <a href=https://github.com/gardener/gardener/issues/6016>#6016</a>.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e4015db9c3c500c41833f0a4177f9f0e>1.4.18 - Testmachinery Tests</h1><h1 id=test-machinery-tests>Test Machinery Tests</h1><p>In order to automatically qualify Gardener releases, we execute a set of end-to-end tests using <a href=https://github.com/gardener/test-infra>Test Machinery</a>.
This requires a full Gardener installation including infrastructure extensions, as well as a setup of Test Machinery itself.
These tests operate on Shoot clusters across different Cloud Providers, using different supported Kubernetes versions and various configuration options (huge test matrix).</p><p>This manual gives an overview about test machinery tests in Gardener.</p><ul><li><a href=#structure>Structure</a></li><li><a href=#add-a-new-test>Add a new test</a></li><li><a href=#test-labels>Test Labels</a></li><li><a href=#framework>Framework</a></li></ul><h2 id=structure>Structure</h2><p>Gardener test machinery tests are split into two test suites that can be found under <a href=https://github.com/gardener/gardener/tree/master/test/testmachinery/suites><code>test/testmachinery/suites</code></a>:</p><ul><li>The <strong>Gardener Test Suite</strong> contains all tests that only require a running gardener instance.</li><li>The <strong>Shoot Test Suite</strong> contains all tests that require a predefined running shoot cluster.</li></ul><p>The corresponding tests of a test suite are defined in the import statement of the suite definition (see <a href=https://github.com/gardener/gardener/blob/master/test/testmachinery/suites/shoot/run_suite_test.go><code>shoot/run_suite_test.go</code></a>)
and their source code can be found under <a href=https://github.com/gardener/gardener/tree/master/test/testmachinery><code>test/testmachinery</code></a>.</p><p>The <code>test</code> directory is structured as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>test
</span></span><span style=display:flex><span>├── e2e           # end-to-end tests (using provider-local)
</span></span><span style=display:flex><span>│  └── shoot
</span></span><span style=display:flex><span>├── framework     # helper code shared across integration, e2e and testmachinery tests
</span></span><span style=display:flex><span>├── integration   # integration tests (envtests)
</span></span><span style=display:flex><span>│  ├── controllermanager
</span></span><span style=display:flex><span>│  ├── envtest
</span></span><span style=display:flex><span>│  ├── resourcemanager
</span></span><span style=display:flex><span>│  ├── scheduler
</span></span><span style=display:flex><span>│  ├── shootmaintenance
</span></span><span style=display:flex><span>│  └── ...
</span></span><span style=display:flex><span>└── testmachinery # test machinery tests
</span></span><span style=display:flex><span>   ├── gardener   # actual test cases imported by suites/gardener
</span></span><span style=display:flex><span>   │  └── security
</span></span><span style=display:flex><span>   ├── shoots     # actual test cases imported by suites/shoot
</span></span><span style=display:flex><span>   │  ├── applications
</span></span><span style=display:flex><span>   │  ├── care
</span></span><span style=display:flex><span>   │  ├── logging
</span></span><span style=display:flex><span>   │  ├── operatingsystem
</span></span><span style=display:flex><span>   │  ├── operations
</span></span><span style=display:flex><span>   │  └── vpntunnel
</span></span><span style=display:flex><span>   ├── suites     # suites that run agains a running garden or shoot cluster
</span></span><span style=display:flex><span>   │  ├── gardener
</span></span><span style=display:flex><span>   │  └── shoot
</span></span><span style=display:flex><span>   └── system     # suites that are used for building a full test flow
</span></span><span style=display:flex><span>      ├── complete_reconcile
</span></span><span style=display:flex><span>      ├── managed_seed_creation
</span></span><span style=display:flex><span>      ├── managed_seed_deletion
</span></span><span style=display:flex><span>      ├── shoot_cp_migration
</span></span><span style=display:flex><span>      ├── shoot_creation
</span></span><span style=display:flex><span>      ├── shoot_deletion
</span></span><span style=display:flex><span>      ├── shoot_hibernation
</span></span><span style=display:flex><span>      ├── shoot_hibernation_wakeup
</span></span><span style=display:flex><span>      └── shoot_update
</span></span></code></pre></div><p>A suite can be executed by running the suite definition with ginkgo&rsquo;s <code>focus</code> and <code>skip</code> flags
to control the execution of specific labeled test. See the example below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \
</span></span><span style=display:flex><span>      --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
</span></span><span style=display:flex><span>      --report-file=/tmp/report.json \                     # write elasticsearch formatted output to a file
</span></span><span style=display:flex><span>      --disable-dump=false \                               # disables dumping of teh current state if a test fails
</span></span><span style=display:flex><span>      -kubecfg=/path/to/gardener/kubeconfig \
</span></span><span style=display:flex><span>      -shoot-name=&lt;shoot-name&gt; \                           # Name of the shoot to test
</span></span><span style=display:flex><span>      -project-namespace=&lt;gardener project namespace&gt; \    # Name of the gardener project the test shoot resides
</span></span><span style=display:flex><span>      -ginkgo.focus=&#34;\[RELEASE\]&#34; \                        # Run all tests that are tagged as release
</span></span><span style=display:flex><span>      -ginkgo.skip=&#34;\[SERIAL\]|\[DISRUPTIVE\]&#34;             # Exclude all tests that are tagged SERIAL or DISRUPTIVE
</span></span></code></pre></div><h2 id=add-a-new-test>Add a New Test</h2><p>To add a new test the framework requires the following steps (step 1. and 2. can be skipped if the test is added to an existing package):</p><ol><li>Create a new test file e.g. <code>test/testmachinery/shoot/security/my-sec-test.go</code></li><li>Import the test into the appropriate test suite (gardener or shoot): <code>import _ "github.com/gardener/gardener/test/testmachinery/shoot/security"</code></li><li>Define your test with the testframework. The framework will automatically add its initialization, cleanup and dump functions.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=display:flex><span><span style=color:#00f>var</span> _ = ginkgo.Describe(<span style=color:#a31515>&#34;my suite&#34;</span>, <span style=color:#00f>func</span>(){
</span></span><span style=display:flex><span>  f := framework.NewShootFramework(<span style=color:#00f>nil</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  f.Beta().CIt(<span style=color:#a31515>&#34;my first test&#34;</span>, <span style=color:#00f>func</span>(ctx context.Context) {
</span></span><span style=display:flex><span>    f.ShootClient.Get(xx)
</span></span><span style=display:flex><span>    <span style=color:green>// testing ...
</span></span></span><span style=display:flex><span><span style=color:green></span>  })
</span></span><span style=display:flex><span>})
</span></span></code></pre></div><p>The newly created test can be tested by focusing the test with the default ginkgo focus <code>f.Beta().FCIt("my first test", func(ctx context.Context)</code>
and running the shoot test suite with:</p><pre tabindex=0><code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \
      --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
      --report-file=/tmp/report.json \                     # write elasticsearch formatted output to a file
      --disable-dump=false \                               # disables dumping of the current state if a test fails
      -kubecfg=/path/to/gardener/kubeconfig \
      -shoot-name=&lt;shoot-name&gt; \                           # Name of the shoot to test
      -project-namespace=&lt;gardener project namespace&gt; \
      -fenced=&lt;true|false&gt;                                 # Tested shoot is running in a fenced environment and cannot be reached by gardener
</code></pre><p>or for the gardener suite with:</p><pre tabindex=0><code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \
      --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
      --report-file=/tmp/report.json \                     # write elasticsearch formatted output to a file
      --disable-dump=false \                               # disables dumping of the current state if a test fails
      -kubecfg=/path/to/gardener/kubeconfig \
      -project-namespace=&lt;gardener project namespace&gt;
</code></pre><p>⚠️ Make sure that you do not commit any focused specs as this feature is only intended for local development! Ginkgo will fail the test suite if there are any focused specs.</p><p>Alternatively, a test can be triggered by specifying a ginkgo focus regex with the name of the test e.g.</p><pre tabindex=0><code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \
      --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
      --report-file=/tmp/report.json \                     # write elasticsearch formatted output to a file
      -kubecfg=/path/to/gardener/kubeconfig \
      -project-namespace=&lt;gardener project namespace&gt; \
      -ginkgo.focus=&#34;my first test&#34;                        # regex to match test cases
</code></pre><h2 id=test-labels>Test Labels</h2><p>Every test should be labeled by using the predefined labels available with every framework to have consistent labeling across
all test machinery tests.</p><p>The labels are applied to every new <code>It()/CIt()</code> definition by:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=display:flex><span>f := framework.NewCommonFramework()
</span></span><span style=display:flex><span>f.Default().Serial().It(<span style=color:#a31515>&#34;my test&#34;</span>) =&gt; <span style=color:#a31515>&#34;[DEFAULT] [SERIAL] my test&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>f := framework.NewShootFramework()
</span></span><span style=display:flex><span>f.Default().Serial().It(<span style=color:#a31515>&#34;my test&#34;</span>) =&gt; <span style=color:#a31515>&#34;[DEFAULT] [SERIAL] [SHOOT] my test&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>f := framework.NewGardenerFramework()
</span></span><span style=display:flex><span>f.Default().Serial().It(<span style=color:#a31515>&#34;my test&#34;</span>) =&gt; <span style=color:#a31515>&#34;[DEFAULT] [GARDENER] [SERIAL] my test&#34;</span>
</span></span></code></pre></div><p>Labels:</p><ul><li><em>Beta</em>: Newly created tests with no experience on stableness should be first labeled as beta tests.
They should be watched (and probably improved) until stable enough to be promoted to <em>Default</em>.</li><li><em>Default</em>: Tests that were <em>Beta</em> before and proved to be stable are promoted to <em>Default</em> eventually.
<em>Default</em> tests run more often, produce alerts and are <em>considered</em> during the release decision although they don&rsquo;t necessarily block a release.</li><li><em>Release</em>: Test are release relevant. A failing <em>Release</em> test blocks the release pipeline.
Therefore, these tests need to be stable. Only tests proven to be stable will eventually be promoted to <em>Release</em>.</li></ul><p>Behavior Labels:</p><ul><li><em>Serial</em>: The test should always be executed in serial with no other tests running, as it may impact other tests.</li><li><em>Destructive</em>: The test is destructive. Which means that is runs with no other tests and may break Gardener or the shoot.
Only create such tests if really necessary, as the execution will be expensive (neither Gardener nor the shoot can be reused in this case for other tests).</li></ul><h2 id=framework>Framework</h2><p>The framework directory contains all the necessary functions / utilities for running test machinery tests.
For example, there are methods for creation/deletion of shoots, waiting for shoot deletion/creation, downloading/installing/deploying helm charts, logging, etc.</p><p>The framework itself consists of 3 different frameworks that expect different prerequisites and offer context specific functionality.</p><ul><li><strong>CommonFramework</strong>: The common framework is the base framework that handles logging and setup of commonly needed resources like helm.
It also contains common functions for interacting with Kubernetes clusters like <code>Waiting for resources to be ready</code> or <code>Exec into a running pod</code>.</li><li><strong>GardenerFramework</strong> contains all functions of the common framework and expects a running Gardener instance with the provided Gardener kubeconfig and a project namespace.
It also contains functions to interact with gardener like <code>Waiting for a shoot to be reconciled</code> or <code>Patch a shoot</code> or <code>Get a seed</code>.</li><li><strong>ShootFramework</strong>: contains all functions of the common and the gardener framework.
It expects a running shoot cluster defined by the shoot&rsquo;s name and namespace (project namespace).
This framework contains functions to directly interact with the specific shoot.</li></ul><p>The whole framework also includes commonly used checks, ginkgo wrapper, etc., as well as commonly used tests.
Theses common application tests (like the guestbook test) can be used within multiple tests to have a default application (with ingress, deployment, stateful backend) to test external factors.</p><p><strong>Config</strong></p><p>Every framework commandline flag can also be defined by a configuration file (the value of the configuration file is only used if a flag is not specified by commandline).
The test suite searches for a configuration file (yaml is preferred) if the command line flag <code>--config=/path/to/config/file</code> is provided.
A framework can be defined in the configuration file by just using the flag name as root key e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>verbose: debug
</span></span><span style=display:flex><span>kubecfg: /kubeconfig/path
</span></span><span style=display:flex><span>project-namespace: garden-it
</span></span></code></pre></div><p><strong>Report</strong></p><p>The framework automatically writes the ginkgo default report to stdout and a specifically structured elastichsearch bulk report file to a specified location.
The elastichsearch bulk report will write one json document per testcase and injects the metadata of the whole testsuite.
An example document for one test case would look like the following document:</p><pre tabindex=0><code>{
    &#34;suite&#34;: {
        &#34;name&#34;: &#34;Shoot Test Suite&#34;,
        &#34;phase&#34;: &#34;Succeeded&#34;,
        &#34;tests&#34;: 3,
        &#34;failures&#34;: 1,
        &#34;errors&#34;: 0,
        &#34;time&#34;: 87.427
    },
    &#34;name&#34;: &#34;Shoot application testing  [DEFAULT] [RELEASE] [SHOOT] should download shoot kubeconfig successfully&#34;,
    &#34;shortName&#34;: &#34;should download shoot kubeconfig successfully&#34;,
    &#34;labels&#34;: [
        &#34;DEFAULT&#34;,
        &#34;RELEASE&#34;,
        &#34;SHOOT&#34;
    ],
    &#34;phase&#34;: &#34;Succeeded&#34;,
    &#34;time&#34;: 0.724512057
}
</code></pre><p><strong>Resources</strong></p><p>The resources directory contains all the templates, helm config files (e.g., repositories.yaml, charts, and cache index which are downloaded upon the start of the test), shoot configs, etc.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>resources
</span></span><span style=display:flex><span>├── charts
</span></span><span style=display:flex><span>├── repository
</span></span><span style=display:flex><span>│   └── repositories.yaml
</span></span><span style=display:flex><span>└── templates
</span></span><span style=display:flex><span>    ├── guestbook-app.yaml.tpl
</span></span><span style=display:flex><span>    └── logger-app.yaml.tpl
</span></span></code></pre></div><p>There are two special directories that are dynamically filled with the correct test files:</p><ul><li><strong>charts</strong> - the charts will be downloaded and saved in this directory</li><li><strong>repository</strong> - contains the <code>repository.yaml</code> file that the target helm repos will be read from and the cache where the <code>stable-index.yaml</code> file will be created</li></ul><h3 id=system-tests>System Tests</h3><p>This directory contains the system tests that have a special meaning for the testmachinery with their own Test Definition.
Currently, these system tests consist of:</p><ul><li>Shoot creation</li><li>Shoot deletion</li><li>Shoot Kubernetes update</li><li>Gardener Full reconcile check</li></ul><h4 id=shoot-creation-test>Shoot Creation Test</h4><p>Create Shoot test is meant to test shoot creation.</p><p><strong>Example Run</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_creation \
</span></span><span style=display:flex><span>  --v -ginkgo.v -ginkgo.progress \
</span></span><span style=display:flex><span>  -kubecfg=$HOME/.kube/config \
</span></span><span style=display:flex><span>  -shoot-name=$SHOOT_NAME \
</span></span><span style=display:flex><span>  -cloud-profile=$CLOUDPROFILE \
</span></span><span style=display:flex><span>  -seed=$SEED \
</span></span><span style=display:flex><span>  -secret-binding=$SECRET_BINDING \
</span></span><span style=display:flex><span>  -provider-type=$PROVIDER_TYPE \
</span></span><span style=display:flex><span>  -region=$REGION \
</span></span><span style=display:flex><span>  -k8s-version=$K8S_VERSION \
</span></span><span style=display:flex><span>  -project-namespace=$PROJECT_NAMESPACE \
</span></span><span style=display:flex><span>  -annotations=$SHOOT_ANNOTATIONS \
</span></span><span style=display:flex><span>  -infrastructure-provider-config-filepath=$INFRASTRUCTURE_PROVIDER_CONFIG_FILEPATH \
</span></span><span style=display:flex><span>  -controlplane-provider-config-filepath=$CONTROLPLANE_PROVIDER_CONFIG_FILEPATH \
</span></span><span style=display:flex><span>  -workers-config-filepath=$$WORKERS_CONFIG_FILEPATH \
</span></span><span style=display:flex><span>  -worker-zone=$ZONE \
</span></span><span style=display:flex><span>  -networking-pods=$NETWORKING_PODS \
</span></span><span style=display:flex><span>  -networking-services=$NETWORKING_SERVICES \
</span></span><span style=display:flex><span>  -networking-nodes=$NETWORKING_NODES \
</span></span><span style=display:flex><span>  -start-hibernated=$START_HIBERNATED
</span></span></code></pre></div><h4 id=shoot-deletion-test>Shoot Deletion Test</h4><p>Delete Shoot test is meant to test the deletion of a shoot.</p><p><strong>Example Run</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>go test -mod=vendor -timeout=0 -ginkgo.v -ginkgo.progress \
</span></span><span style=display:flex><span>  ./test/testmachinery/system/shoot_deletion \
</span></span><span style=display:flex><span>  -kubecfg=$HOME/.kube/config \
</span></span><span style=display:flex><span>  -shoot-name=$SHOOT_NAME \
</span></span><span style=display:flex><span>  -project-namespace=$PROJECT_NAMESPACE
</span></span></code></pre></div><h4 id=shoot-update-test>Shoot Update Test</h4><p>The Update Shoot test is meant to test the Kubernetes version update of a existing shoot.
If no specific version is provided, the next patch version is automatically selected.
If there is no available newer version, this test is a noop.</p><p><strong>Example Run</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_update \
</span></span><span style=display:flex><span>  --v -ginkgo.v -ginkgo.progress \
</span></span><span style=display:flex><span>  -kubecfg=$HOME/.kube/config \
</span></span><span style=display:flex><span>  -shoot-name=$SHOOT_NAME \
</span></span><span style=display:flex><span>  -project-namespace=$PROJECT_NAMESPACE \
</span></span><span style=display:flex><span>  -version=$K8S_VERSION
</span></span></code></pre></div><h4 id=gardener-full-reconcile-test>Gardener Full Reconcile Test</h4><p>The Gardener Full Reconcile test is meant to test if all shoots of a Gardener instance are successfully reconciled.</p><p><strong>Example Run</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/complete_reconcile \
</span></span><span style=display:flex><span>  --v -ginkgo.v -ginkgo.progress \
</span></span><span style=display:flex><span>  -kubecfg=$HOME/.kube/config \
</span></span><span style=display:flex><span>  -project-namespace=$PROJECT_NAMESPACE \
</span></span><span style=display:flex><span>  -gardenerVersion=$GARDENER_VERSION # needed to validate the last acted gardener version of a shoot
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-06612ee007f8c645961e7f6c98417686>1.5 - Extensions</h1></div><div class=td-content><h1 id=pg-976400864159d95d6036cf8762b0583b>1.5.1 - Admission</h1><h1 id=extension-admission>Extension Admission</h1><p>The extensions are expected to validate their respective resources for their extension specific configurations, when the resources are newly created or updated. For example, <a href=https://github.com/gardener/gardener/blob/master/extensions/README.md#infrastructure-provider>provider extensions</a> would validate <code>spec.provider.infrastructureConfig</code> and <code>spec.provider.controlPlaneConfig</code> in the <code>Shoot</code> resource and <code>spec.providerConfig</code> in the <code>CloudProfile</code> resource, <a href=https://github.com/gardener/gardener/blob/master/extensions/README.md#network-plugin>networking extensions</a> would validate <code>spec.networking.providerConfig</code> in the <code>Shoot</code> resource. As best practice, the validation should be performed only if there is a change in the <code>spec</code> of the resource. Please find an exemplary implementation in the <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/admission/validator>gardener/gardener-extension-provider-aws</a> repository.</p><p>When a resource is newly created or updated, Gardener adds an extension label for all the extension types referenced in the <code>spec</code> of the resource. This label is of the form <code>&lt;extension-type>.extensions.gardener.cloud/&lt;extension-name> : "true"</code>. For example, an extension label for a provider extension type <code>aws</code> looks like <code>provider.extensions.gardener.cloud/aws : "true"</code>. The extensions should add object selectors in their admission webhooks for these labels, to filter out the objects they are responsible for. At present, these labels are added to <code>BackupEntry</code>s, <code>BackupBucket</code>s, <code>CloudProfile</code>s, <code>Seed</code>s, <code>SecretBinding</code>s and <code>Shoot</code>s. Please see the <a href=https://github.com/gardener/gardener/blob/master/pkg/apis/core/v1beta1/constants/types_constants.go>types_constants.go</a> file for the full list of extension labels.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-14974039d50fb15810aafaf28cbdcf7d>1.5.2 - BackupBucket</h1><h1 id=contract-backupbucket-resource>Contract: <code>BackupBucket</code> Resource</h1><p>The Gardener project features a sub-project called <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The <code>BackupBucket</code> resource takes this responsibility in Gardener.</p><p>Before introducing the <code>BackupBucket</code> extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see <a href=https://github.com/gardener/gardener/tree/0.27.0/charts/seed-terraformer/charts/aws-backup>AWS Backup</a>).
Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md>backupInfra proposal documentation</a> to get an idea about how the transition was done and understand the resource in a broader scope.</p><h2 id=what-is-the-scope-of-a-bucket>What Is the Scope of a Bucket?</h2><p>A bucket will be provisioned per <code>Seed</code>. So, a backup of every <code>Shoot</code> created on that <code>Seed</code> will be stored under a different shoot specific prefix under the bucket.
For the backup of the <code>Shoot</code> rescheduled on different <code>Seed</code>, it will continue to use the same bucket.</p><h2 id=what-is-the-lifespan-of-a-backupbucket>What Is the Lifespan of a <code>BackupBucket</code>?</h2><p>The bucket associated with <code>BackupBucket</code> will be created at the creation of the <code>Seed</code>. And as per current implementation, it will also be deleted on deletion of the <code>Seed</code>, if there isn&rsquo;t any <code>BackupEntry</code> resource associated with it.</p><p>In the future, we plan to introduce a schedule for <code>BackupBucket</code> - the deletion logic for the <code>BackupBucket</code> resource, which will reschedule it on different available <code>Seed</code>s on deletion or failure of a health check for the currently associated <code>seed</code>. In that case, the <code>BackupBucket</code> will be deleted only if there isn&rsquo;t any schedulable <code>Seed</code> available and there isn&rsquo;t any associated <code>BackupEntry</code> resource.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What Needs to Be Implemented to Support a New Infrastructure Provider?</h2><p>As part of the seed flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupBucket
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    &lt;some-optional-provider-specific-backupbucket-configuration&gt;
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: backupprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by the Gardener operator in the <code>Seed</code> resource and propagated over there by the seed controller.</p><p>After your controller has created the required bucket, if required, it generates the secret to access the objects in the bucket and put a reference to it in <code>status</code>. This secret is supposed to be used by Gardener, or eventually a <code>BackupEntry</code> resource and etcd-backup-restore component, to backup the etcd.</p><p>In order to support a new infrastructure provider, you need to write a controller that watches all <code>BackupBucket</code>s with <code>.spec.type=&lt;my-provider-name></code>. You can take a look at the below referenced example implementation for the Azure provider.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=/docs/gardener/api-reference/extensions/#backupbucket><code>BackupBucket</code> API Reference</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/backupbucket>Exemplary Implementation for the Azure Provider</a></li><li><a href=/docs/gardener/extensions/backupentry/><code>BackupEntry</code> Resource Documentation</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md>Shared Bucket Proposal</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b1fda6ecdcfcb6e58a7f82c09b95f838>1.5.3 - BackupEntry</h1><h1 id=contract-backupentry-resource>Contract: <code>BackupEntry</code> Resource</h1><p>The Gardener project features a sub-project called <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The <code>BackupEntry</code> resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component.</p><p>That being said, the core motivation for introducing this resource was to support retention of backups post deletion of <code>Shoot</code>. The etcd-backup-restore components take responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the <code>BackupEntry</code> resource for this housekeeping work post deletion of a <code>Shoot</code>. The <code>BackupEntry</code> resource is responsible for shoot specific prefix under referred bucket.</p><p>Before introducing the <code>BackupEntry</code> extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see <a href=https://github.com/gardener/gardener/tree/0.27.0/charts/seed-terraformer/charts/aws-backup>AWS Backup</a>).
Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md>backupInfra proposal documentation</a> to get idea about how the transition was done and understand the resource in broader scope.</p><h2 id=what-is-the-lifespan-of-a-backupentry>What Is the Lifespan of a <code>BackupEntry</code>?</h2><p>The bucket associated with <code>BackupEntry</code> will be created by using a <code>BackupBucket</code> resource. The <code>BackupEntry</code> resource will be created as a part of the <code>Shoot</code> creation. But resources might continue to exist post deletion of a <code>Shoot</code> (see <a href=/docs/gardener/concepts/gardenlet/#backupentry-controller>gardenlet</a> for more details).</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What Needs to be Implemented to Support a New Infrastructure Provider?</h2><p>As part of the shoot flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    &lt;some-optional-provider-specific-backup-bucket-configuration&gt;
</span></span><span style=display:flex><span>  backupBucketProviderStatus:
</span></span><span style=display:flex><span>    &lt;some-optional-provider-specific-backup-bucket-status&gt;
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  bucketName: foo
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: backupprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from the <code>BackupBucket</code> resource by the shoot controller.</p><p>Your controller is supposed to create the <code>etcd-backup</code> secret in the control plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually by the etcd-backup-restore component to backup the etcd. The controller implementation should clean up the objects created under the shoot specific prefix in the bucket equivalent to the name of the <code>BackupEntry</code> resource.</p><p>In order to support a new infrastructure provider, you need to write a controller that watches all the <code>BackupBucket</code>s with <code>.spec.type=&lt;my-provider-name></code>. You can take a look at the below referenced example implementation for the Azure provider.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=/docs/gardener/api-reference/extensions/#backupbucket><code>BackupEntry</code> API Reference</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/backupentry>Exemplary Implementation for the Azure Provider</a></li><li><a href=/docs/gardener/extensions/backupbucket/><code>BackupBucket</code> Resource Documentation</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md>Shared Bucket Proposal</a></li><li><a href=https://github.com/gardener/gardener/blob/master/pkg/controllermanager/apis/config/types.go#L101-%23L107>Gardener-controller-manager-component-config API Specification</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-852c52090550872df673db8e9fb654ba>1.5.4 - Bastion</h1><h1 id=contract-bastion-resource>Contract: <code>Bastion</code> Resource</h1><p>The Gardener project allows users to connect to Shoot worker nodes via SSH. As nodes are usually firewalled and not directly accessible from the public internet, <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/15-manage-bastions-and-ssh-key-pair-rotation.md>GEP-15</a> introduced the concept of &ldquo;Bastions&rdquo;. A bastion is a dedicated server that only serves to allow SSH ingress to the worker nodes.</p><p><code>Bastion</code> resources contain the user&rsquo;s public SSH key and IP address, in order to provision the server accordingly: The public key is put onto the Bastion and SSH ingress is only authorized for the given IP address (in fact, it&rsquo;s not a single IP address, but a set of IP ranges, however for most purposes a single IP is be used).</p><h2 id=what-is-the-lifespan-of-a-bastion>What Is the Lifespan of a <code>Bastion</code>?</h2><p>Once a <code>Bastion</code> has been created in the garden, it will be replicated to the appropriate seed cluster, where a controller then reconciles a server and firewall rules etc., on the cloud provider used by the target Shoot. When the Bastion is ready (i.e. has a public IP), that IP is stored in the <code>Bastion</code>&rsquo;s status and from there it is picked up by the garden cluster and <code>gardenctl</code> eventually.</p><p>To make multiple SSH sessions possible, the existence of the <code>Bastion</code> is not directly tied to the execution of <code>gardenctl</code>: users can exit out of <code>gardenctl</code> and use <code>ssh</code> manually to connect to the bastion and worker nodes.</p><p>However, <code>Bastion</code>s have an expiry date, after which they will be garbage collected.</p><p>When SSH access is set to <code>false</code> for the <code>Shoot</code> in the workers settings (see <a href=/docs/gardener/usage/shoot_workers_settings/>Shoot Worker Nodes Settings</a>), <code>Bastion</code> resources are deleted during <code>Shoot</code> reconciliation and new <code>Bastion</code>s are prevented from being created.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What Needs to Be Implemented to Support a New Infrastructure Provider?</h2><p>As part of the shoot flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Bastion
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: mybastion
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  <span style=color:green># userData is base64-encoded cloud provider user data; this contains the</span>
</span></span><span style=display:flex><span>  <span style=color:green># user&#39;s SSH key</span>
</span></span><span style=display:flex><span>  userData: IyEvYmluL2Jhc2ggL....Nlcgo=
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    - ipBlock:
</span></span><span style=display:flex><span>        cidr: 192.88.99.0/32 <span style=color:green># this is most likely the user&#39;s IP address</span>
</span></span></code></pre></div><p>Your controller is supposed to create a new instance at the given cloud provider, firewall it to only allow SSH (TCP port 22) from the given IP blocks, and then configure the firewall for the worker nodes to allow SSH from the bastion instance. When a <code>Bastion</code> is deleted, all these changes need to be reverted.</p><h2 id=implementation-details>Implementation Details</h2><h3 id=configvalidator-interface><code>ConfigValidator</code> Interface</h3><p>For bastion controllers, the generic <code>Reconciler</code> also delegates to a <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/bastion/configvalidator.go><code>ConfigValidator</code> interface</a> that contains a single <code>Validate</code> method. This method is called by the generic <code>Reconciler</code> at the beginning of every reconciliation, and can be implemented by the extension to validate the <code>.spec.providerConfig</code> part of the <code>Bastion</code> resource with the respective cloud provider, typically the existence and validity of cloud provider resources such as VPCs, images, etc.</p><p>The <code>Validate</code> method returns a list of errors. If this list is non-empty, the generic <code>Reconciler</code> will fail with an error. This error will have the error code <code>ERR_CONFIGURATION_PROBLEM</code>, unless there is at least one error in the list that has its <code>ErrorType</code> field set to <code>field.ErrorTypeInternal</code>.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=/docs/gardener/api-reference/extensions/#bastion><code>Bastion</code> API Reference</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/bastion>Exemplary Implementation for the AWS Provider</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/15-manage-bastions-and-ssh-key-pair-rotation.md>GEP-15</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-dfa4a7f5eddd3e92a6a94007ba1f4648>1.5.5 - CA Rotation</h1><h1 id=ca-rotation-in-extensions>CA Rotation in Extensions</h1><p><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> proposes adding support for automated rotation of Shoot cluster certificate authorities (CAs).
This document outlines all the requirements that Gardener extensions need to fulfill in order to support the CA rotation feature.</p><h2 id=requirements-for-shoot-cluster-ca-rotation>Requirements for Shoot Cluster CA Rotation</h2><ul><li>Extensions must not rely on static CA <code>Secret</code> names managed by the gardenlet, because their names are changing during CA rotation.</li><li>Extensions cannot issue or use client certificates for authenticating against shoot API servers. Instead, they should use short-lived auto-rotated <code>ServiceAccount</code> tokens via gardener-resource-manager&rsquo;s <code>TokenRequestor</code>. Also see <a href=/docs/gardener/extensions/conventions/>Conventions</a> and <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor><code>TokenRequestor</code></a> documents.</li><li>Extensions need to generate dedicated CAs for signing server certificates (e.g. <code>cloud-controller-manager</code>). There should be one CA per controller and purpose in order to bind the lifecycle to the reconciliation cycle of the respective object for which it is created.</li><li>CAs managed by extensions should be rotated in lock-step with the shoot cluster CA.
When the user triggers a rotation, the gardenlet writes phase and initiation time to <code>Shoot.status.credentials.rotation.certificateAuthorities.{phase,lastInitiationTime}</code>. See <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md#rotation-sequence-for-cluster-and-client-ca>GEP-18</a> for a detailed description on what needs to happen in each phase.
Extensions can retrieve this information from <a href=/docs/gardener/extensions/cluster/><code>Cluster.shoot.status</code></a>.</li></ul><h2 id=utilities-for-secrets-management>Utilities for Secrets Management</h2><p>In order to fulfill the requirements listed above, extension controllers can reuse the <a href=/docs/gardener/development/secrets_management/><code>SecretsManager</code></a> that the gardenlet uses to manage all shoot cluster CAs, certificates, and other secrets as well.
It implements the core logic for managing secrets that need to be rotated, auto-renewed, etc.</p><p>Additionally, there are utilities for reusing <code>SecretsManager</code> in extension controllers.
They already implement the above requirements based on the <code>Cluster</code> resource and allow focusing on the extension controllers&rsquo; business logic.</p><p>For example, a simple <code>SecretsManager</code> usage in an extension controller could look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  <span style=color:green>// identity for SecretsManager instance in ControlPlane controller
</span></span></span><span style=display:flex><span><span style=color:green></span>  identity = <span style=color:#a31515>&#34;provider-foo-controlplane&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:green>// secret config name of the dedicated CA
</span></span></span><span style=display:flex><span><span style=color:green></span>  caControlPlaneName = <span style=color:#a31515>&#34;ca-provider-foo-controlplane&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>func</span> Reconcile() {
</span></span><span style=display:flex><span>  <span style=color:#00f>var</span> (
</span></span><span style=display:flex><span>    cluster *extensionscontroller.Cluster
</span></span><span style=display:flex><span>    client  client.Client
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>// define wanted secrets with options
</span></span></span><span style=display:flex><span><span style=color:green></span>    secretConfigs = []extensionssecretsmanager.SecretConfigWithOptions{
</span></span><span style=display:flex><span>      {
</span></span><span style=display:flex><span>        <span style=color:green>// dedicated CA for ControlPlane controller
</span></span></span><span style=display:flex><span><span style=color:green></span>        Config: &amp;secretutils.CertificateSecretConfig{
</span></span><span style=display:flex><span>          Name:       caControlPlaneName,
</span></span><span style=display:flex><span>          CommonName: <span style=color:#a31515>&#34;ca-provider-foo-controlplane&#34;</span>,
</span></span><span style=display:flex><span>          CertType:   secretutils.CACert,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:green>// persist CA so that it gets restored on control plane migration
</span></span></span><span style=display:flex><span><span style=color:green></span>        Options: []secretsmanager.GenerateOption{secretsmanager.Persist()},
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      {
</span></span><span style=display:flex><span>        <span style=color:green>// server cert for control plane component
</span></span></span><span style=display:flex><span><span style=color:green></span>        Config: &amp;secretutils.CertificateSecretConfig{
</span></span><span style=display:flex><span>          Name:       <span style=color:#a31515>&#34;cloud-controller-manager&#34;</span>,
</span></span><span style=display:flex><span>          CommonName: <span style=color:#a31515>&#34;cloud-controller-manager&#34;</span>,
</span></span><span style=display:flex><span>          DNSNames:   kutil.DNSNamesForService(<span style=color:#a31515>&#34;cloud-controller-manager&#34;</span>, namespace),
</span></span><span style=display:flex><span>          CertType:   secretutils.ServerCert,
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:green>// sign with our dedicated CA
</span></span></span><span style=display:flex><span><span style=color:green></span>        Options: []secretsmanager.GenerateOption{secretsmanager.SignedByCA(caControlPlaneName)},
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// initialize SecretsManager based on Cluster object
</span></span></span><span style=display:flex><span><span style=color:green></span>  sm, err := extensionssecretsmanager.SecretsManagerForCluster(ctx, logger.WithName(<span style=color:#a31515>&#34;secretsmanager&#34;</span>), clock.RealClock{}, client, cluster, identity, secretConfigs)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:green>// generate all wanted secrets (first CAs, then the rest)
</span></span></span><span style=display:flex><span><span style=color:green></span>  secrets, err := extensionssecretsmanager.GenerateAllSecrets(ctx, sm, secretConfigs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// cleanup any secrets that are not needed any more (e.g. after rotation)
</span></span></span><span style=display:flex><span><span style=color:green></span>  err = sm.Cleanup(ctx)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Please pay attention to the following points:</p><ul><li>There should be one <code>SecretsManager</code> identity per controller (and purpose if applicable) in order to prevent conflicts between different instances.
E.g., there should be different identities for <code>Infrastructrue</code>, <code>Worker</code> controller, etc., and the <code>ControlPlane</code> controller should use dedicated <code>SecretsManager</code> identities per purpose (e.g. <code>provider-foo-controlplane</code> and <code>provider-foo-controlplane-exposure</code>).</li><li>All other points in <a href=/docs/gardener/development/secrets_management/#reusing-the-secretsmanager-in-other-components>Reusing the SecretsManager in Other Components</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1dcf839b2e87160673811784e7578d4a>1.5.6 - Cluster</h1><h1 id=cluster-resource><code>Cluster</code> Resource</h1><p>As part of the extensibility epic, a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters.
These extensions often serve a well-defined purpose, e.g. the management of <a href=/docs/gardener/extensions/dnsrecord/>DNS records</a>, <a href=/docs/gardener/extensions/infrastructure/>infrastructure</a>, etc.
We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.</p><p>However, the extensions sometimes require more information that is not directly part of the specification.
One example of that is the GCP infrastructure controller which needs to know the shoot&rsquo;s pod and service network.
Another example is the Azure infrastructure controller which requires some information out of the <code>CloudProfile</code> resource.
The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.</p><p>In order to deal with this problem we have introduced the <code>Cluster</code> extension resource.
This CRD is written into the seeds, however, it does not contain a <code>status</code>, so it is not expected that something acts upon it.
Instead, you can treat it like a <code>ConfigMap</code> which contains data that might be interesting for you.
In the context of Gardener, seeds and shoots, and extensibility the <code>Cluster</code> resource contains the <code>CloudProfile</code>, <code>Seed</code>, and <code>Shoot</code> manifest.
Extension controllers can take whatever information they want out of it that might help completing their individual tasks.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Cluster
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfile:
</span></span><span style=display:flex><span>    apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    kind: CloudProfile
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  seed:
</span></span><span style=display:flex><span>    apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    kind: Seed
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    kind: Shoot
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>The resource is written by Gardener before it starts the reconciliation flow of the shoot.</p><p>⚠️ All Gardener components use the <code>core.gardener.cloud/v1beta1</code> version, i.e., the <code>Cluster</code> resource will contain the objects in this version.</p><h2 id=important-information-that-should-be-taken-into-account>Important Information that Should Be Taken into Account</h2><p>There are some fields in the <code>Shoot</code> specification that might be interesting to take into account.</p><ul><li><code>.spec.hibernation.enabled={true,false}</code>: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example).</li><li><code>.status.lastOperation.state=Failed</code>: If Gardener sets the shoot&rsquo;s last operation state to <code>Failed</code>, it means that Gardener won&rsquo;t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last <code>24h</code> (default). In this case, end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the <code>Cluster</code> resource.</li></ul><h2 id=extension-resources-not-associated-with-a-shoot>Extension Resources Not Associated with a Shoot</h2><p>In some cases, Gardener may create extension resources that are not associated with a shoot, but are needed to support some functionality internal to Gardener. Such resources will be created in the <code>garden</code> namespace of a seed cluster.</p><p>For example, if the <a href=/docs/gardener/deployment/deploy_gardenlet_manually/>managed ingress controller</a> is active on the seed, Gardener will create a <a href=/docs/gardener/extensions/dnsrecord/>DNSRecord</a> resource(s) in the <code>garden</code> namespace of the seed cluster for the ingress DNS record.</p><p>Extension controllers that may be expected to reconcile extension resources in the <code>garden</code> namespace should make sure that they can tolerate the absence of a cluster resource. This means that they should not attempt to read the cluster resource in such cases, or if they do they should ignore the &ldquo;not found&rdquo; error.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_cluster.go><code>Cluster</code> API (Golang Specification)</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd0f3158741613ba8829c3093bc71acd>1.5.7 - ContainerRuntime</h1><h1 id=gardener-container-runtime-extension>Gardener Container Runtime Extension</h1><p>At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called “Container Runtime”.
The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.</p><p>Kubernetes supports different container runtimes using Container Runtime Interface (CRI) – a plugin interface which enables kubelet to use a wide variety of container runtimes.</p><p>Gardener supports creation of Worker machines using CRI. For more information, see <a href=/docs/gardener/extensions/operatingsystemconfig/#cri-support>CRI Support</a>.</p><h2 id=motivation>Motivation</h2><p>Prior to the <code>Container Runtime Extensibility</code> concept, Gardener used Docker as the only
container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes
offering multiple important features (for example, enhanced security concepts), it is important to enable end users to use other container runtimes as well.</p><h2 id=the-containerruntime-extension-resource>The <code>ContainerRuntime</code> Extension Resource</h2><p>Here is what a typical <code>ContainerRuntime</code> resource would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ContainerRuntime
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-container-runtime
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  binaryPath: /var/bin/containerruntimes
</span></span><span style=display:flex><span>  type: gvisor
</span></span><span style=display:flex><span>  workerPool:
</span></span><span style=display:flex><span>    name: worker-ubuntu
</span></span><span style=display:flex><span>    selector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        worker.gardener.cloud/pool: worker-ubuntu
</span></span></code></pre></div><p>Gardener deploys one <code>ContainerRuntime</code> resource per worker pool per CRI.
To exemplify this, consider a Shoot having two worker pools (<code>worker-one</code>, <code>worker-two</code>) using <code>containerd</code> as the CRI as well as <code>gvisor</code> and <code>kata</code> as enabled container runtimes.
Gardener would deploy four <code>ContainerRuntime</code> resources. For <code>worker-one</code>: one <code>ContainerRuntime</code> for type <code>gvisor</code> and one for type <code>kata</code>. The same resource are being deployed for <code>worker-two</code>.</p><h2 id=supporting-a-new-container-runtime-provider>Supporting a New Container Runtime Provider</h2><p>To add support for another container runtime (e.g., gvisor, kata-containers), a container runtime extension controller needs to be implemented. It should support Gardener&rsquo;s supported CRI plugins.</p><p>The container runtime extension should install the necessary resources into the shoot cluster (e.g., <code>RuntimeClass</code>es), and it should copy the runtime binaries to the relevant worker machines in path: <code>spec.binaryPath</code>.
Gardener labels the shoot nodes according to the CRI configured: <code>worker.gardener.cloud/cri-name=&lt;value></code> (e.g <code>worker.gardener.cloud/cri-name=containerd</code>) and multiple labels for each of the container runtimes configured for the shoot Worker machine:
<code>containerruntime.worker.gardener.cloud/&lt;container-runtime-type-value>=true</code> (e.g <code>containerruntime.worker.gardener.cloud/gvisor=true</code>).
The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker&rsquo;s nodes (avoid downloading binaries from the internet to also cater with isolated environments).</p><p>For additional reference, please have a look at the <a href=https://github.com/gardener/gardener-extension-runtime-gvisor>runtime-gvsior</a> provider extension, which provides more information on how to configure the necessary charts, as well as the actuators required to reconcile container runtime inside the <code>Shoot</code> cluster to the desired state.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6a93f55712b73e1c9926ae9a1827ffc1>1.5.8 - ControllerRegistration</h1><h1 id=registering-extension-controllers>Registering Extension Controllers</h1><p>Extensions are registered in the garden cluster via <a href=https://github.com/gardener/gardener/blob/master/example/25-controllerregistration.yaml><code>ControllerRegistration</code></a> resources.
Deployment for respective extensions are specified via <a href=https://github.com/gardener/gardener/blob/master/example/25-controllerdeployment.yaml><code>ControllerDeployment</code></a> resources.
Gardener evaluates the registrations and deployments and creates <a href=https://github.com/gardener/gardener/blob/master/example/25-controllerinstallation.yaml><code>ControllerInstallation</code></a> resources which describe the request &ldquo;please install this controller <code>X</code> to this seed <code>Y</code>&rdquo;.</p><p>Similar to how <code>CloudProfile</code> or <code>Seed</code> resources get into the system, the Gardener administrator must deploy the <code>ControllerRegistration</code> and <code>ControllerDeployment</code> resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).</p><p>The specification mainly describes which of Gardener&rsquo;s extension CRDs are managed, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: os-gardenlinux
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: H4sIFAAAAAAA/yk... <span style=color:green># &lt;base64-gzip-chart&gt;</span>
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    foo: bar
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: os-gardenlinux
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  deployment:
</span></span><span style=display:flex><span>    deploymentRefs:
</span></span><span style=display:flex><span>    - name: os-gardenlinux
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - kind: OperatingSystemConfig
</span></span><span style=display:flex><span>    type: gardenlinux
</span></span><span style=display:flex><span>    primary: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>This information tells Gardener that there is an extension controller that can handle <code>OperatingSystemConfig</code> resources of type <code>gardenlinux</code>.
A reference to the shown <code>ControllerDeployment</code> specifies how the deployment of the extension controller is accomplished.</p><p>Also, it specifies that this controller is the primary one responsible for the lifecycle of the <code>OperatingSystemConfig</code> resource.
Setting <code>primary</code> to <code>false</code> would allow to register additional, secondary controllers that may also watch/react on the <code>OperatingSystemConfig/coreos</code> resources, however, only the primary controller may change/update the main <code>status</code> of the extension object (that are used to &ldquo;communicate&rdquo; with the gardenlet).
Particularly, only the primary controller may set <code>.status.lastOperation</code>, <code>.status.lastError</code>, <code>.status.observedGeneration</code>, and <code>.status.state</code>.
Secondary controllers may contribute to the <code>.status.conditions[]</code> if they like, of course.</p><p>Secondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.</p><p>⚠️ There must be exactly one primary controller for every registered kind/type combination.
Also, please note that the <code>primary</code> field cannot be changed after creation of the <code>ControllerRegistration</code>.</p><h2 id=deploying-extension-controllers>Deploying Extension Controllers</h2><p>Submitting the above <code>ControllerDeployment</code> and <code>ControllerRegistration</code> will create a <code>ControllerInstallation</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerInstallation
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: os-gardenlinux
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  deploymentRef:
</span></span><span style=display:flex><span>    name: os-gardenlinux
</span></span><span style=display:flex><span>  registrationRef:
</span></span><span style=display:flex><span>    name: os-gardenlinux
</span></span><span style=display:flex><span>  seedRef:
</span></span><span style=display:flex><span>    name: aws-eu1
</span></span></code></pre></div><p>This resource expresses that Gardener requires the <code>os-gardenlinux</code> extension controller to run on the <code>aws-eu1</code> seed cluster.</p><p>The Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create <code>ControllerInstallation</code> objects for those.
Also, it will automatically delete <code>ControllerInstallation</code>s referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted).
There are additional configuration options, please see the <a href=#deployment-configuration-options>Deployment Configuration Options section</a>.</p><h2 id=how-do-extension-controllers-get-deployed-to-seeds>How do extension controllers get deployed to seeds?</h2><p>After Gardener has written the <code>ControllerInstallation</code> resource, some component must satisfy this request and start deploying the extension controller to the seed.
Depending on the complexity of the controller&rsquo;s lifecycle management, configuration, etc., there are two possible scenarios:</p><h3 id=scenario-1-deployed-by-gardener>Scenario 1: Deployed by Gardener</h3><p>In many cases, the extension controllers are easy to deploy and configure.
It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values.
Gardener supports this scenario and allows to provide arbitrary deployment information in the <code>ControllerDeployment</code> resource&rsquo;s <code>.providerConfig</code> section:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: H4sIFAAAAAAA/yk...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    foo: bar
</span></span></code></pre></div><p>If <code>.type=helm</code>, then Gardener itself will take over the responsibility the deployment.
It base64-decodes the provided Helm chart (<code>.providerConfig.chart</code>) and deploys it with the provided static configuration (<code>.providerConfig.values</code>).
The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.</p><p>In order to allow extensions to get information about the garden and the seed cluster, Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardener:
</span></span><span style=display:flex><span>  garden:
</span></span><span style=display:flex><span>    identifier: &lt;uuid-of-gardener-installation&gt;
</span></span><span style=display:flex><span>  seed:
</span></span><span style=display:flex><span>    identifier: &lt;seed-name&gt;
</span></span><span style=display:flex><span>    region: europe
</span></span><span style=display:flex><span>    spec: &lt;complete-seed-spec&gt;
</span></span></code></pre></div><p>Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment.
The list might be extended in the future.</p><p>ℹ️ Gardener uses the UUID of the <code>garden</code> <code>Namespace</code> object in the <code>.gardener.garden.identifier</code> property.</p><h3 id=scenario-2-deployed-by-a-non-human-kubernetes-operator>Scenario 2: Deployed by a (Non-Human) Kubernetes Operator</h3><p>Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration.
In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster.
The <code>ControllerDeployments</code>&rsquo;s <code>.type</code> field would then not be <code>helm</code>, and no Helm chart or values need to be provided there.
Instead, the operator itself knows how to deploy the extension into the seed.
It must watch <code>ControllerInstallation</code> resources and act one those referencing a <code>ControllerRegistration</code> the operator is responsible for.</p><p>In order to let Gardener know that the extension controller is ready and running in the seed, the <code>ControllerInstallation</code>&rsquo;s <code>.status</code> field supports two conditions: <code>RegistrationValid</code> and <code>InstallationSuccessful</code> - both must be provided by the responsible operator:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: <span style=color:#a31515>&#34;2019-01-22T11:51:11Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2019-01-22T11:51:11Z&#34;</span>
</span></span><span style=display:flex><span>    message: Chart could be rendered successfully.
</span></span><span style=display:flex><span>    reason: RegistrationValid
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Valid
</span></span><span style=display:flex><span>  - lastTransitionTime: <span style=color:#a31515>&#34;2019-01-22T11:51:12Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2019-01-22T11:51:12Z&#34;</span>
</span></span><span style=display:flex><span>    message: Installation of new resources succeeded.
</span></span><span style=display:flex><span>    reason: InstallationSuccessful
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Installed
</span></span></code></pre></div><p>Additionally, the <code>.status</code> field has a <code>providerStatus</code> section into which the operator can (optionally) put any arbitrary data associated with this installation.</p><h2 id=extensions-in-the-garden-cluster-itself>Extensions in the Garden Cluster Itself</h2><p>The <code>Shoot</code> resource itself will contain some provider-specific data blobs.
As a result, some extensions might also want to run in the garden cluster, e.g., to provide <code>ValidatingWebhookConfiguration</code>s for validating the correctness of their provider-specific blobs:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloud:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.cloud.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>        <span style=color:green># id: vpc-123456</span>
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        internal:
</span></span><span style=display:flex><span>        - 10.250.112.0/22
</span></span><span style=display:flex><span>        public:
</span></span><span style=display:flex><span>        - 10.250.96.0/22
</span></span><span style=display:flex><span>        workers:
</span></span><span style=display:flex><span>        - 10.250.0.0/19
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-west-1a
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure.
However, if this part of the <code>Shoot</code> resource should be validated, then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a <code>MutatingWebhookConfiguration</code>).</p><p>Again, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.</p><h3 id=extension-resource-configurations><code>Extension</code> Resource Configurations</h3><p>The <code>Extension</code> resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener.
Hence, it is slightly special and allows further configuration when registering it:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - kind: Extension
</span></span><span style=display:flex><span>    type: foo
</span></span><span style=display:flex><span>    primary: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    globallyEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    reconcileTimeout: 30s
</span></span><span style=display:flex><span>    lifecycle:
</span></span><span style=display:flex><span>      reconcile: AfterKubeAPIServer
</span></span><span style=display:flex><span>      delete: BeforeKubeAPIServer
</span></span><span style=display:flex><span>      migrate: BeforeKubeAPIServer
</span></span></code></pre></div><p>The <code>globallyEnabled=true</code> option specifies that the <code>Extension/foo</code> object shall be created by default for all shoots (unless they opted out by setting <code>.spec.extensions[].enabled=false</code> in the <code>Shoot</code> spec).</p><p>The <code>reconcileTimeout</code> tells Gardener how long it should wait during its shoot reconciliation flow for the <code>Extension/foo</code>&rsquo;s reconciliation to finish.</p><h4 id=extension-lifecycle><code>Extension</code> Lifecycle</h4><p>The <code>lifecycle</code> field tells Gardener when to perform a certain action on the <code>Extension</code> resource during the reconciliation flows. If omitted, then the default behaviour will be applied. Please find more information on the defaults in the explanation below. Possible values for each control flow are <code>AfterKubeAPIServer</code> and <code>BeforeKubeAPIServer</code>. Let&rsquo;s take the following configuration and explain it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    lifecycle:
</span></span><span style=display:flex><span>      reconcile: AfterKubeAPIServer
</span></span><span style=display:flex><span>      delete: BeforeKubeAPIServer
</span></span><span style=display:flex><span>      migrate: BeforeKubeAPIServer
</span></span></code></pre></div><ul><li><code>reconcile: AfterKubeAPIServer</code> means that the extension resource will be reconciled after the successful reconciliation of the <code>kube-apiserver</code> during shoot reconciliation. This is also the default behaviour if this value is not specified. During shoot hibernation, the opposite rule is applied, meaning that in this case the reconciliation of the extension will happen before the <code>kube-apiserver</code> is scaled to 0 replicas. On the other hand, if the extension needs to be reconciled before the <code>kube-apiserver</code> and scaled down after it, then the value <code>BeforeKubeAPIServer</code> should be used.</li><li><code>delete: BeforeKubeAPIServer</code> means that the extension resource will be deleted before the <code>kube-apiserver</code> is destroyed during shoot deletion. This is the default behaviour if this value is not specified.</li><li><code>migrate: BeforeKubeAPIServer</code> means that the extension resource will be migrated before the <code>kube-apiserver</code> is destroyed in the source cluster during <a href=/docs/gardener/usage/control_plane_migration/>control plane migration</a>. This is the default behaviour if this value is not specified. The restoration of the control plane follows the reconciliation control flow.</li></ul><h3 id=deployment-configuration-options>Deployment Configuration Options</h3><p>The <code>.spec.deployment</code> resource allows to configure a deployment <code>policy</code>.
There are the following policies:</p><ul><li><code>OnDemand</code> (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like <code>Shoot</code>s) whether it is required and decide accordingly.</li><li><code>Always</code>: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters.</li><li><code>AlwaysExceptNoShoots</code>: Similar to <code>Always</code>, but if the seed does not have any shoots, then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.</li></ul><p>Also, the <code>.spec.deployment.seedSelector</code> allows to specify a label selector for seed clusters.
Only if it matches the labels of a seed, then it will be deployed to it.
Please note that a seed selector can only be specified for secondary controllers (<code>primary=false</code> for all <code>.spec.resources[]</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-863811f5b98898fe3fba05a3241b77ae>1.5.9 - ControlPlane</h1><h1 id=contract-controlplane-resource>Contract: <code>ControlPlane</code> Resource</h1><p>Most Kubernetes clusters require a <code>cloud-controller-manager</code> or CSI drivers in order to work properly.
Before introducing the <code>ControlPlane</code> extension resource Gardener was having several different Helm charts for the <code>cloud-controller-manager</code> deployments for the various providers.
Now, Gardener commissions an external, provider-specific controller to take over this task.</p><h2 id=which-control-plane-resources-are-required>Which control plane resources are required?</h2><p>As mentioned in the <a href=/docs/gardener/extensions/controlplane-webhooks/>controlplane customization webhooks</a> document, Gardener shall not deploy any <code>cloud-controller-manager</code> or any other provider-specific component.
Instead, it creates a <code>ControlPlane</code> CRD that should be picked up by provider extensions.
Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What needs to be implemented to support a new infrastructure provider?</h2><p>As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlane
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: control-plane
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: openstack
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: cloudprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    loadBalancerProvider: provider
</span></span><span style=display:flex><span>    zone: eu-1a
</span></span><span style=display:flex><span>    cloudControllerManager:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        CustomResourceValidation: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  infrastructureProviderStatus:
</span></span><span style=display:flex><span>    apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      floatingPool:
</span></span><span style=display:flex><span>        id: vpc-1234
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - purpose: nodes
</span></span><span style=display:flex><span>        id: subnetid
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster.
However, the most important section is the <code>.spec.providerConfig</code> and the <code>.spec.infrastructureProviderStatus</code>.
The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself).
You are responsible for designing how this configuration looks like.
Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the <code>Shoot</code> resource.
The second one contains the output of the <a href=/docs/gardener/extensions/infrastructure/><code>Infrastructure</code> resource</a> (that might be relevant for the CCM config).</p><p>In order to support a new control plane provider, you need to write a controller that watches all <code>ControlPlane</code>s with <code>.spec.type=&lt;my-provider-name></code>.
You can take a look at the below referenced example implementation for the Alicloud provider.</p><p>The control plane controller as part of the <code>ControlPlane</code> reconciliation often deploys resources (e.g. pods/deployments) into the Shoot namespace in the <code>Seed</code> as part of its <code>ControlPlane</code> reconciliation loop.
Because the namespace contains <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>network policies</a> that per default <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic>deny all ingress and egress traffic</a>,
the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic.
Otherwise, they won&rsquo;t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot).
For more information, see <a href=/docs/gardener/development/seed_network_policies/>Network Policies in the Seed Cluster</a>.</p><h2 id=non-provider-specific-information-required-for-infrastructure-creation>Non-Provider Specific Information Required for Infrastructure Creation</h2><p>Most providers might require further information that is not provider specific but already part of the shoot resource.
One example for this is the <a href=https://github.com/gardener/gardener-extension-provider-gcp/tree/master/pkg/controller/controlplane>GCP control plane controller</a>, which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager).
As Gardener cannot know which information is required by providers, it simply mirrors the <code>Shoot</code>, <code>Seed</code>, and <code>CloudProfile</code> resources into the seed.
They are part of the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> extension resource</a> and can be used to extract information that is not part of the <code>Infrastructure</code> resource itself.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_controlplane.go><code>ControlPlane</code> API (Golang Specification)</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-alicloud/tree/master/pkg/controller/controlplane>Exemplary Implementation for the Alicloud Provider</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3a3deb3218151c4ac388e35f40184d2a>1.5.10 - ControlPlane Exposure</h1><h1 id=contract-controlplane-resource-with-purpose-exposure>Contract: <code>ControlPlane</code> Resource with Purpose <code>exposure</code></h1><p>Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser.
Before using ControlPlane resources with purpose <code>exposure</code>, Gardener was having different Helm charts for the deployments for the various providers.
Now, Gardener commissions an external, provider-specific controller to take over this task.</p><h2 id=which-control-plane-resources-are-required>Which control plane resources are required?</h2><p>As mentioned in the <a href=/docs/gardener/extensions/controlplane/>controlplane</a> document, Gardener shall not deploy any other provider-specific component.
Instead, it creates a <code>ControlPlane</code> CRD with purpose <code>exposure</code> that should be picked up by provider extensions.
Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster that are needed to expose the kube-apiserver.</p><p>The shoot cluster&rsquo;s kube-apiserver are exposed via a <code>Service</code> of type <code>LoadBalancer</code> from the shoot provider (you may run the control plane of an Azure shoot in a GCP seed). It&rsquo;s the seed provider extension controller that should act on the <code>ControlPlane</code> resources with purpose <code>exposure</code>.</p><p>If <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>SNI</a> is enabled, then the <code>Service</code> from above is of type <code>ClusterIP</code> and Gardner will not create <code>ControlPlane</code> resources with purpose <code>exposure</code>.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What needs to be implemented to support a new infrastructure provider?</h2><p>As part of the shoot flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlane
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: control-plane-exposure
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  purpose: exposure
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: cloudprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster.
It is most likely not needed, however, still added for some potential corner cases.
If you don&rsquo;t need it, then just ignore it.
The <code>.spec.region</code> contains the region of the seed cluster.</p><p>In order to support a control plane provider with purpose <code>exposure</code>, you need to write a controller or expand the existing <a href=/docs/gardener/extensions/controlplane/>controlplane controller</a> that watches all <code>ControlPlane</code>s with <code>.spec.type=&lt;my-provider-name></code> and purpose <code>exposure</code>.
You can take a look at the below referenced example implementation for the AWS provider.</p><h2 id=non-provider-specific-information-required-for-infrastructure-creation>Non-Provider Specific Information Required for Infrastructure Creation</h2><p>Most providers might require further information that is not provider specific but already part of the shoot resource.
As Gardener cannot know which information is required by providers, it simply mirrors the <code>Shoot</code>, <code>Seed</code>, and <code>CloudProfile</code> resources into the seed.
They are part of the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> extension resource</a> and can be used to extract information.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_controlplane.go><code>ControlPlane</code> API (Golang Specification)</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/controlplane>Exemplary Implementation for the AWS Provider</a></li><li><a href=https://github.com/gardener/aws-lb-readvertiser>AWS Load Balancer Readvertiser</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b8aa124fe8bedbc5e039bbe785060c38>1.5.11 - ControlPlane Webhooks</h1><h1 id=controlplane-customization-webhooks>ControlPlane Customization Webhooks</h1><p>Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:</p><ul><li>Deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace.</li><li>Initiates the deployment of custom controlplane components by <a href=/docs/gardener/extensions/controlplane/>ControlPlane controllers</a> by creating a <code>ControlPlane</code> resource in the Shoot namespace.</li></ul><p>In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-cloud-provider>What needs to be implemented to support a new cloud provider?</h2><p>In order to support a new cloud provider, you should install &ldquo;controlplane&rdquo; mutating webhooks for any of the following resources:</p><ul><li>Deployment with name <code>kube-apiserver</code>, <code>kube-controller-manager</code>, or <code>kube-scheduler</code></li><li>Service with name <code>kube-apiserver</code></li><li><code>OperatingSystemConfig</code> with any name, and purpose <code>reconcile</code></li></ul><p>See <a href=#contract-specification>Contract Specification</a> for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.</p><p>You can install 3 different kinds of controlplane webhooks:</p><ul><li><code>Shoot</code>, or <code>controlplane</code> webhooks apply changes needed by the Shoot cloud provider, for example the <code>--cloud-provider</code> command line flag of <code>kube-apiserver</code> and <code>kube-controller-manager</code>. Such webhooks should only operate on Shoot namespaces labeled with <code>shoot.gardener.cloud/provider=&lt;provider></code>.</li><li><code>Seed</code>, or <code>controlplaneexposure</code> webhooks apply changes needed by the Seed cloud provider, for example annotations on the <code>kube-apiserver</code> service to ensure cloud-specific load balancers are correctly provisioned for a service of type <code>LoadBalancer</code>. Such webhooks should only operate on Shoot namespaces labeled with <code>seed.gardener.cloud/provider=&lt;provider></code>.</li></ul><p>The labels <code>shoot.gardener.cloud/provider</code> and <code>seed.gardener.cloud/provider</code> are added by Gardener when it creates the Shoot namespace.</p><h2 id=contract-specification>Contract Specification</h2><p>This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can&rsquo;t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it&rsquo;s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.</p><blockquote><p><strong>Note:</strong> The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for <a href=/docs/gardener/extensions/overview/>Gardener extensibility</a> have been introduced.</p></blockquote><h3 id=kube-apiserver>kube-apiserver</h3><p>To deploy kube-apiserver, Gardener <strong>shall</strong> create a deployment and a service both named <code>kube-apiserver</code> in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.</p><p>The pod template of the <code>kube-apiserver</code> deployment <strong>shall</strong> contain a container named <code>kube-apiserver</code>.</p><p>The <code>command</code> field of the <code>kube-apiserver</code> container <strong>shall</strong> contain the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver command line</a>. It <strong>shall</strong> contain a number of provider-independent flags that should be ignored by webhooks, such as:</p><ul><li>admission plugins (<code>--enable-admission-plugins</code>, <code>--disable-admission-plugins</code>)</li><li>secure communications (<code>--etcd-cafile</code>, <code>--etcd-certfile</code>, <code>--etcd-keyfile</code>, &mldr;)</li><li>audit log (<code>--audit-log-*</code>)</li><li>ports (<code>--secure-port</code>)</li></ul><p>The kube-apiserver command line <strong>shall not</strong> contain any provider-specific flags, such as:</p><ul><li><code>--cloud-provider</code></li><li><code>--cloud-config</code></li></ul><p>These flags can be added by webhooks if needed.</p><p>The <code>kube-apiserver</code> command line <strong>may</strong> contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:</p><ul><li><code>--endpoint-reconciler-type</code></li><li><code>--advertise-address</code></li><li><code>--feature-gates</code></li></ul><p>Gardener <strong>may</strong> use <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>SNI</a> to expose the apiserver (<code>APIServerSNI</code> feature gate). In this case, Gardener <strong>shall</strong> label the <code>kube-apiserver</code>&rsquo;s <code>Deployment</code> with <code>core.gardener.cloud/apiserver-exposure: gardener-managed</code> label and expects that the <code>--endpoint-reconciler-type</code> and <code>--advertise-address</code> flags are not modified.</p><p>The <code>--enable-admission-plugins</code> flag <strong>may</strong> contain admission plugins that are not compatible with CSI plugins such as <code>PersistentVolumeLabel</code>. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).</p><p>The <code>env</code> field of the <code>kube-apiserver</code> container <strong>shall not</strong> contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.</p><p>The <code>volumes</code> field of the pod template of the <code>kube-apiserver</code> deployment, and respectively the <code>volumeMounts</code> field of the <code>kube-apiserver</code> container <strong>shall not</strong> contain any provider-specific <code>Secret</code> or <code>ConfigMap</code> resources. If such resources should be mounted as volumes, this should be done by webhooks.</p><p>The <code>kube-apiserver</code> <code>Service</code> <strong>may</strong> be of type <code>LoadBalancer</code>, but <strong>shall not</strong> contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider&rsquo;s cloud. If any such annotations are needed, they should be added by webhooks (typically <code>controlplaneexposure</code> webhooks).</p><p>The <code>kube-apiserver</code> <code>Service</code> <strong>shall</strong> be of type <code>ClusterIP</code> if Gardener is using <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>SNI</a> to expose the apiserver (<code>APIServerSNI</code> feature gate). In this case, Gardener <strong>shall</strong> label this <code>Service</code> with <code>core.gardener.cloud/apiserver-exposure: gardener-managed</code> label and expects that no mutations happen.</p><h3 id=kube-controller-manager>kube-controller-manager</h3><p>To deploy kube-controller-manager, Gardener <strong>shall</strong> create a deployment named <code>kube-controller-manager</code> in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.</p><p>The pod template of the <code>kube-controller-manager</code> deployment <strong>shall</strong> contain a container named <code>kube-controller-manager</code>.</p><p>The <code>command</code> field of the <code>kube-controller-manager</code> container <strong>shall</strong> contain the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager command line</a>. It <strong>shall</strong> contain a number of provider-independent flags that should be ignored by webhooks, such as:</p><ul><li><code>--kubeconfig</code>, <code>--authentication-kubeconfig</code>, <code>--authorization-kubeconfig</code></li><li><code>--leader-elect</code></li><li>secure communications (<code>--tls-cert-file</code>, <code>--tls-private-key-file</code>, &mldr;)</li><li>cluster CIDR and identity (<code>--cluster-cidr</code>, <code>--cluster-name</code>)</li><li>sync settings (<code>--concurrent-deployment-syncs</code>, <code>--concurrent-replicaset-syncs</code>)</li><li>horizontal pod autoscaler (<code>--horizontal-pod-autoscaler-*</code>)</li><li>ports (<code>--port</code>, <code>--secure-port</code>)</li></ul><p>The kube-controller-manager command line <strong>shall not</strong> contain any provider-specific flags, such as:</p><ul><li><code>--cloud-provider</code></li><li><code>--cloud-config</code></li><li><code>--configure-cloud-routes</code></li><li><code>--external-cloud-volume-plugin</code></li></ul><p>These flags can be added by webhooks if needed.</p><p>The kube-controller-manager command line <strong>may</strong> contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:</p><ul><li><code>--feature-gates</code></li></ul><p>The <code>env</code> field of the <code>kube-controller-manager</code> container <strong>shall not</strong> contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.</p><p>The <code>volumes</code> field of the pod template of the <code>kube-controller-manager</code> deployment, and respectively the <code>volumeMounts</code> field of the <code>kube-controller-manager</code> container <strong>shall not</strong> contain any provider-specific <code>Secret</code> or <code>ConfigMap</code> resources. If such resources should be mounted as volumes, this should be done by webhooks.</p><h3 id=kube-scheduler>kube-scheduler</h3><p>To deploy kube-scheduler, Gardener <strong>shall</strong> create a deployment named <code>kube-scheduler</code> in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.</p><p>The pod template of the <code>kube-scheduler</code> deployment <strong>shall</strong> contain a container named <code>kube-scheduler</code>.</p><p>The <code>command</code> field of the <code>kube-scheduler</code> container <strong>shall</strong> contain the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler command line</a>. It <strong>shall</strong> contain a number of provider-independent flags that should be ignored by webhooks, such as:</p><ul><li><code>--config</code></li><li><code>--authentication-kubeconfig</code>, <code>--authorization-kubeconfig</code></li><li>secure communications (<code>--tls-cert-file</code>, <code>--tls-private-key-file</code>, &mldr;)</li><li>ports (<code>--port</code>, <code>--secure-port</code>)</li></ul><p>The kube-scheduler command line <strong>may</strong> contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:</p><ul><li><code>--feature-gates</code></li></ul><p>The kube-scheduler command line can&rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific <code>Secret</code> or <code>ConfigMap</code> resources as volumes.</p><h3 id=etcd-main-and-etcd-events>etcd-main and etcd-events</h3><p>To deploy etcd, Gardener <strong>shall</strong> create 2 <a href=https://github.com/gardener/etcd-druid/blob/1d427e9167adac1476d1847c0e265c2c09d6bc62/config/samples/druid_v1alpha1_etcd.yaml>Etcd</a> named <code>etcd-main</code> and <code>etcd-events</code> in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.</p><p>Gardener <strong>shall</strong> configure the <code>Etcd</code> resource completely to set up an etcd cluster which uses the default storage class of the seed cluster.</p><h3 id=cloud-controller-manager>cloud-controller-manager</h3><p>Gardener <strong>shall not</strong> deploy a cloud-controller-manager. If it is needed, it should be added by a <a href=/docs/gardener/extensions/controlplane/><code>ControlPlane</code> controller</a></p><h3 id=csi-controllers>CSI Controllers</h3><p>Gardener <strong>shall not</strong> deploy a CSI controller. If it is needed, it should be added by a <a href=/docs/gardener/extensions/controlplane/><code>ControlPlane</code> controller</a></p><h3 id=kubelet>kubelet</h3><p>To specify the kubelet configuration, Gardener <strong>shall</strong> create a <a href=/docs/gardener/extensions/operatingsystemconfig/><code>OperatingSystemConfig</code> resource</a> with any name and purpose <code>reconcile</code> in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener <strong>may</strong> write multiple such resources with different <code>type</code> to the same Shoot namespaces if multiple OSs are used.</p><p>The OSC resource <strong>shall</strong> contain a unit named <code>kubelet.service</code>, containing the corresponding systemd unit configuration file. The <code>[Service]</code> section of this file <strong>shall</strong> contain a single <code>ExecStart</code> option having the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>kubelet command line</a> as its value.</p><p>The OSC resource <strong>shall</strong> contain a file with path <code>/var/lib/kubelet/config/kubelet</code>, which contains a <code>KubeletConfiguration</code> resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.</p><p>The kubelet command line <strong>shall</strong> contain a number of provider-independent flags that should be ignored by webhooks, such as:</p><ul><li><code>--config</code></li><li><code>--bootstrap-kubeconfig</code>, <code>--kubeconfig</code></li><li><code>--network-plugin</code> (and, if it equals <code>cni</code>, also <code>--cni-bin-dir</code> and <code>--cni-conf-dir</code>)</li><li><code>--node-labels</code></li></ul><p>The kubelet command line <strong>shall not</strong> contain any provider-specific flags, such as:</p><ul><li><code>--cloud-provider</code></li><li><code>--cloud-config</code></li><li><code>--provider-id</code></li></ul><p>These flags can be added by webhooks if needed.</p><p>The kubelet command line / configuration <strong>may</strong> contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:</p><ul><li><code>--enable-controller-attach-detach</code> (<code>enableControllerAttachDetach</code>) - should be set to <code>true</code> if CSI plugins are used, but in general can also be ignored since its default value is also <code>true</code>, and this should work both with and without CSI plugins.</li><li><code>--feature-gates</code> (<code>featureGates</code>) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-719af9bdc56f6cf5b7e123daddb6f6a4>1.5.12 - Conventions</h1><h1 id=general-conventions>General Conventions</h1><p>All the extensions that are registered to Gardener are deployed to the seed clusters on which they are required (also see <a href=/docs/gardener/extensions/controllerregistration/>ControllerRegistration</a>).</p><p>Some of these extensions might need to create global resources in the seed (e.g., <code>ClusterRole</code>s), i.e., it&rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don&rsquo;t use the same names.</p><p>Consequently, this page should help answering some general questions that might come up when it comes to developing an extension.</p><h2 id=priorityclasses><code>PriorityClass</code>es</h2><p>Extensions are not supposed to create and use self-defined <code>PriorityClasses</code>.
Instead, they can and should rely on well-known <a href=/docs/gardener/development/priority-classes/><code>PriorityClasses</code></a> managed by gardenlet.</p><h2 id=high-availability-of-deployed-components>High Availability of Deployed Components</h2><p>Extensions might deploy components via <code>Deployment</code>s, <code>StatefulSet</code>s, etc., as part of the shoot control plane, or the seed or shoot system components.
In case a seed or shoot cluster is highly available, there are various failure tolerance types. For more information, see <a href=/docs/gardener/usage/shoot_high_availability/>Highly Available Shoot Control Plane</a>.
Accordingly, the <code>replicas</code>, <code>topologySpreadConstraints</code> or <code>affinity</code> settings of the deployed components might need to be adapted.</p><p>Instead of doing this one-by-one for each and every component, extensions can rely on a mutating webhook provided by Gardener.
Please refer to <a href=/docs/gardener/development/high-availability/>High Availability of Deployed Components</a> for details.</p><h2 id=is-there-a-naming-scheme-for-global-resources>Is there a naming scheme for (global) resources?</h2><p>As there is no formal process to validate non-existence of conflicts between two extensions, please follow these naming schemes when creating resources (especially, when creating global resources, but it&rsquo;s in general a good idea for most created resources):</p><p><em>The resource name should be prefixed with <code>extensions.gardener.cloud:&lt;extension-type>-&lt;extension-name>:&lt;resource-name></code></em>, for example:</p><ul><li><code>extensions.gardener.cloud:provider-aws:machine-controller-manager</code></li><li><code>extensions.gardener.cloud:extension-certificate-service:cert-broker</code></li></ul><h2 id=how-to-create-resources-in-the-shoot-cluster>How to create resources in the shoot cluster?</h2><p>Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a <code>ServiceAccount</code> and the required RBAC permissions when it gets installed to the seed.
However, there are no credentials for the shoot for every extension.</p><p>Extensions are supposed to use <a href=/docs/gardener/concepts/resource-manager/#ManagedResource-controller><code>ManagedResources</code></a> to manage resources in shoot clusters.
gardenlet deploys gardener-resource-manager instances into all shoot control planes, that will reconcile <code>ManagedResources</code> without a specified class (<code>spec.class=null</code>) in shoot clusters. Mind that Gardener acts on <code>ManagedResources</code> with the <code>origin=gardener</code> label. In order to prevent unwanted behavior, extensions should omit the <code>origin</code> label or provide their own unique value for it when creating such resources.</p><p>If you need to deploy a non-DaemonSet resource, Gardener automatically ensures that it only runs on nodes that are allowed to host system components and extensions. For more information, see <a href=/docs/gardener/concepts/resource-manager/#System-Components-Webhook>System Components Webhook</a>.</p><h2 id=how-to-create-kubeconfigs-for-the-shoot-cluster>How to create kubeconfigs for the shoot cluster?</h2><p>Historically, Gardener extensions used to generate kubeconfigs with client certificates for components they deploy into the shoot control plane.
For this, they reused the shoot cluster CA secret (<code>ca</code>) to issue new client certificates.
With <a href=https://github.com/gardener/gardener/issues/4661>gardener/gardener#4661</a> we moved away from using client certificates in favor of short-lived, auto-rotated <code>ServiceAccount</code> tokens. These tokens are managed by gardener-resource-manager&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor><code>TokenRequestor</code></a>.
Extensions are supposed to reuse this mechanism for requesting tokens and a <code>generic-token-kubeconfig</code> for authenticating against shoot clusters.</p><p>With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> (Shoot cluster CA rotation), a dedicated CA will be used for signing client certificates (<a href=https://github.com/gardener/gardener/pull/5779>gardener/gardener#5779</a>) which will be rotated when triggered by the shoot owner.
With this, extensions cannot reuse the <code>ca</code> secret anymore to issue client certificates.
Hence, extensions must switch to short-lived <code>ServiceAccount</code> tokens in order to support the CA rotation feature.</p><p>The <code>generic-token-kubeconfig</code> secret contains the CA bundle for establishing trust to shoot API servers. However, as the secret is immutable, its name changes with the rotation of the cluster CA.
Extensions need to look up the <code>generic-token-kubeconfig.secret.gardener.cloud/name</code> annotation on the respective <a href=/docs/gardener/extensions/cluster/><code>Cluster</code></a> object in order to determine which secret contains the current CA bundle.
The helper function <code>extensionscontroller.GenericTokenKubeconfigSecretNameFromCluster</code> can be used for this task.</p><p>You can take a look at <a href=/docs/gardener/extensions/ca-rotation/>CA Rotation in Extensions</a> for more details on the CA rotation feature in regard to extensions.</p><h2 id=how-to-create-certificates-for-the-shoot-cluster>How to create certificates for the shoot cluster?</h2><p>Gardener creates several certificate authorities (CA) that are used to create server certificates for various components.
For example, the shoot&rsquo;s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster&rsquo;s CA.</p><p>With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> (Shoot cluster CA rotation), extensions are required to do the same and generate dedicated CAs for their components (e.g. for signing a server certificate for cloud-controller-manager). They must not depend on the CA secrets managed by gardenlet.</p><p>Please see <a href=/docs/gardener/extensions/ca-rotation/>CA Rotation in Extensions</a> for the exact requirements that extensions need to fulfill in order to support the CA rotation feature.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6b0bf831ee5197821c0717de33be3996>1.5.13 - DNS Record</h1><h1 id=contract-dnsrecord-resources>Contract: <code>DNSRecord</code> Resources</h1><p>Every shoot cluster requires external DNS records that are publicly resolvable.
The management of these DNS records requires provider-specific knowledge which is to be developed outside the Gardener&rsquo;s core repository.</p><p>Currently, Gardener uses <code>DNSProvider</code> and <code>DNSEntry</code> resources. However, this introduces undesired coupling of Gardener to a controller that does not adhere to the Gardener extension contracts. Because of this, we plan to stop using <code>DNSProvider</code> and <code>DNSEntry</code> resources for Gardener DNS records in the future and use the <code>DNSRecord</code> resources described here instead.</p><h2 id=what-does-gardener-create-dns-records-for>What does Gardener create DNS records for?</h2><h3 id=internal-domain-name>Internal Domain Name</h3><p>Every shoot cluster&rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname).
This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster.
In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the DNS record to another seed cluster), Gardener creates a so-called <em>internal domain name</em> for every shoot cluster.
The <em>internal domain name</em> is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver.
Gardener uses this domain name in the kubeconfigs of all system components, instead of using directly the load balancer endpoint.
This way Gardener does not need to recreate all kubeconfigs if the endpoint changes - it just needs to update the DNS record.</p><h3 id=external-domain-name>External Domain Name</h3><p>The internal domain name is not configurable by end-users directly but configured by the Gardener administrator.
However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes, to access their Kubernetes clusters.
Gardener supports that by creating another DNS record, named <em>external domain name</em>, that actually points to the <em>internal domain name</em>.
The kubeconfig handed out to end-users does contain this <em>external domain name</em>, i.e., users can access their clusters with the DNS name they like to.</p><p>As not every end-user has an own domain, it is possible for Gardener administrators to configure so-called <em>default domains</em>.
If configured, shoots that do not specify a domain explicitly get an <em>external domain name</em> based on a default domain (unless explicitly stated that this shoot should not get an external domain name (<code>.spec.dns.provider=unmanaged</code>).</p><h3 id=ingress-domain-name-deprecated>Ingress Domain Name (Deprecated)</h3><p>Gardener allows to deploy a <code>nginx-ingress-controller</code> into a shoot cluster (deprecated).
This controller is exposed via a public load balancer (again, either IP or hostname).
Gardener creates a wildcard DNS record pointing to this load balancer.
<code>Ingress</code> resources can later use this wildcard DNS record to expose underlying applications.</p><h3 id=seed-ingress>Seed Ingress</h3><p>If <code>.spec.ingress</code> is configured in the Seed, Gardener deploys the ingress controller mentioned in <code>.spec.ingress.controller.kind</code> to the seed cluster. Currently, the only supported kind is &ldquo;nginx&rdquo;. If the ingress field is set, then <code>.spec.dns.provider</code> must also be set. Gardener creates a wildcard DNS record pointing to the load balancer of the ingress controller. The <code>Ingress</code> resources of components like Grafana and Prometheus in the <code>garden</code> namespace and the shoot namespaces use this wildcard DNS record to expose their underlying applications.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-dns-provider>What needs to be implemented to support a new DNS provider?</h2><p>As part of the shoot flow, Gardener will create a number of <code>DNSRecord</code> resources in the seed cluster (one for each of the DNS records mentioned above) that need to be reconciled by an extension controller.
These resources contain the following information:</p><ul><li>The DNS provider type (e.g., <code>aws-route53</code>, <code>google-clouddns</code>, &mldr;)</li><li>A reference to a <code>Secret</code> object that contains the provider-specific credentials used to communicate with the provider&rsquo;s API.</li><li>The fully qualified domain name (FQDN) of the DNS record, e.g. &ldquo;api.&lt;shoot domain>&rdquo;.</li><li>The DNS record type, one of <code>A</code>, <code>AAAA</code>, <code>CNAME</code>, or <code>TXT</code>.</li><li>The DNS record values, that is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.</li></ul><p>Optionally, the <code>DNSRecord</code> resource may contain also the following information:</p><ul><li>The region of the DNS record. If not specified, the region specified in the referenced <code>Secret</code> shall be used. If that is also not specified, the extension controller shall use a certain default region.</li><li>The DNS hosted zone of the DNS record. If not specified, it shall be determined automatically by the extension controller by getting all hosted zones of the account and searching for the longest zone name that is a suffix of the fully qualified domain name (FQDN) mentioned above.</li><li>The TTL of the DNS record in seconds. If not specified, it shall be set by the extension controller to 120.</li></ul><p><strong>Example <code>DNSRecord</code></strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dnsrecord-bar-external
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># aws-route53 specific credentials here</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSRecord
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dnsrecord-external
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws-route53
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: dnsrecord-bar-external
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span><span style=color:green># region: eu-west-1</span>
</span></span><span style=display:flex><span><span style=color:green># zone: ZFOO</span>
</span></span><span style=display:flex><span>  name: api.bar.foo.my-fancy-domain.com
</span></span><span style=display:flex><span>  recordType: A
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>  - 1.2.3.4
</span></span><span style=display:flex><span><span style=color:green># ttl: 600</span>
</span></span></code></pre></div><p>In order to support a new DNS record provider, you need to write a controller that watches all <code>DNSRecord</code>s with <code>.spec.type=&lt;my-provider-name></code>.
You can take a look at the below referenced example implementation for the AWS route53 provider.</p><h2 id=key-names-in-secrets-containing-provider-specific-credentials>Key Names in Secrets Containing Provider-Specific Credentials</h2><p>For compatibility with existing setups, extension controllers shall support two different namings of keys in secrets containing provider-specific credentials:</p><ul><li>The naming used by the <a href=https://github.com/gardener/external-dns-management>external-dns-management DNS controller</a>. For example, on AWS the key names are <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_REGION</code>.</li><li>The naming used by other provider-specific extension controllers, e.g. for <a href=/docs/gardener/extensions/infrastructure/>infrastructure</a>. For example, on AWS the key names are <code>accessKeyId</code>, <code>secretAccessKey</code>, and <code>region</code>.</li></ul><h2 id=avoiding-reading-the-dns-hosted-zones>Avoiding Reading the DNS Hosted Zones</h2><p>If the DNS hosted zone is not specified in the <code>DNSRecord</code> resource, during the first reconciliation the extension controller shall determine the correct DNS hosted zone for the specified FQDN and write it to the status of the resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSRecord
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dnsrecord-external
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  zone: ZFOO
</span></span></code></pre></div><p>On subsequent reconciliations, the extension controller shall use the zone from the status and avoid reading the DNS hosted zones from the provider.
If the <code>DNSRecord</code> resource specifies a zone in <code>.spec.zone</code> and the extension controller has written a value to <code>.status.zone</code>, the first one shall be considered with higher priority by the extension controller.</p><h2 id=non-provider-specific-information-required-for-dns-record-creation>Non-Provider Specific Information Required for DNS Record Creation</h2><p>Some providers might require further information that is not provider specific but already part of the shoot resource.
As Gardener cannot know which information is required by providers, it simply mirrors the <code>Shoot</code>, <code>Seed</code>, and <code>CloudProfile</code> resources into the seed.
They are part of the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> extension resource</a> and can be used to extract information that is not part of the <code>DNSRecord</code> resource itself.</p><h2 id=using-dnsrecord-resources>Using <code>DNSRecord</code> Resources</h2><p>gardenlet manages <code>DNSRecord</code> resources for all three DNS records mentioned above (internal, external, and ingress).
In order to successfully reconcile a shoot with the feature gate enabled, extension controllers for <code>DNSRecord</code> resources for types used in the default, internal, and custom domain secrets should be registered via <code>ControllerRegistration</code> resources.</p><blockquote><p><strong>Note:</strong> For compatibility reasons, the <code>spec.dns.providers</code> section is still used to specify additional providers. Only the one marked as <code>primary: true</code> will be used for <code>DNSRecord</code>. All others are considered by the <code>shoot-dns-service</code> extension only (if deployed).</p></blockquote><h3 id=support-for-dnsrecord-resources-in-the-provider-extensions>Support for <code>DNSRecord</code> Resources in the Provider Extensions</h3><p>The following table contains information about the provider extension version that adds support for <code>DNSRecord</code> resources:</p><table><thead><tr><th>Extension</th><th>Version</th></tr></thead><tbody><tr><td>provider-alicloud</td><td><code>v1.26.0</code></td></tr><tr><td>provider-aws</td><td><code>v1.27.0</code></td></tr><tr><td>provider-azure</td><td><code>v1.21.0</code></td></tr><tr><td>provider-gcp</td><td><code>v1.18.0</code></td></tr><tr><td>provider-openstack</td><td><code>v1.21.0</code></td></tr><tr><td>provider-vsphere</td><td>N/A</td></tr><tr><td>provider-equinix-metal</td><td>N/A</td></tr><tr><td>provider-kubevirt</td><td>N/A</td></tr><tr><td>provider-openshift</td><td>N/A</td></tr></tbody></table><h3 id=support-for-dnsrecord-ipv6-recordtype-aaaa-in-the-provider-extensions>Support for <code>DNSRecord</code> IPv6 <code>recordType: AAAA</code> in the Provider Extensions</h3><p>The following table contains information about the provider extension version that adds support for <code>DNSRecord</code> IPv6 <code>recordType: AAAA</code>:</p><table><thead><tr><th>Extension</th><th>Version</th></tr></thead><tbody><tr><td>provider-alicloud</td><td>N/A</td></tr><tr><td>provider-aws</td><td>N/A</td></tr><tr><td>provider-azure</td><td>N/A</td></tr><tr><td>provider-gcp</td><td>N/A</td></tr><tr><td>provider-openstack</td><td>N/A</td></tr><tr><td>provider-vsphere</td><td>N/A</td></tr><tr><td>provider-equinix-metal</td><td>N/A</td></tr><tr><td>provider-kubevirt</td><td>N/A</td></tr><tr><td>provider-openshift</td><td>N/A</td></tr><tr><td>provider-local</td><td><code>v1.63.0</code></td></tr></tbody></table><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_dnsrecord.go><code>DNSRecord</code> API (Golang specification)</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/dnsrecord>Sample Implementation for the AWS Route53 Provider</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8d41cfc635f0777d1f5003241ea79535>1.5.14 - Extension</h1><h1 id=contract-extension-resource>Contract: <code>Extension</code> Resource</h1><p>Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like <code>Infrastructure</code>, <code>OperatingSystemConfig</code> or <code>DNS</code>. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc.</p><p>But, some requirements don&rsquo;t match with those special resources or don&rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the <code>Extension</code> resource.</p><h2 id=what-is-required-to-register-and-support-an-extension-type>What is required to register and support an Extension type?</h2><p>Gardener creates one <code>Extension</code> resource per registered extension type in <code>ControllerRegistration</code> per shoot.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-example
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - kind: Extension
</span></span><span style=display:flex><span>    type: example
</span></span><span style=display:flex><span>    globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>If <code>spec.resources[].globallyEnabled</code> is <code>true</code>, then the <code>Extension</code> resources of the given <code>type</code> is created for every shoot cluster. Set to <code>false</code>, the <code>Extension</code> resource is only created if configured in the <code>Shoot</code> manifest.</p><p>The <code>Extension</code> resources are created in the shoot namespace of the seed cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: example
</span></span><span style=display:flex><span>  providerConfig: {}
</span></span></code></pre></div><p>Your controller needs to reconcile <code>extensions.extensions.gardener.cloud</code>. Since there can exist multiple <code>Extension</code> resources per shoot, each one holds a <code>spec.type</code> field to let controllers check their responsibility (similar to all other extension resources of Gardener).</p><h2 id=providerconfig>ProviderConfig</h2><p>It is possible to provide data in the <code>Shoot</code> resource which is copied to <code>spec.providerConfig</code> of the <code>Extension</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: bar
</span></span><span style=display:flex><span>  namespace: garden-foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: example
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>results in</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: example
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    foo: bar
</span></span></code></pre></div><h2 id=shoot-reconciliation-flow-and-extension-status>Shoot Reconciliation Flow and Extension Status</h2><p>Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the <a href=/docs/gardener/extensions/cluster/>Cluster</a> resource exists before the <code>Extension</code> resource is created. <code>Extension</code>s can be reconciled at different stages during Shoot reconciliation depending on the defined extension lifecycle strategy in the respective <a href=/docs/gardener/extensions/controllerregistration/>ControllerRegistration</a> resource. Please consult the <a href=/docs/gardener/extensions/controllerregistration/#extension-lifecycle>Extension Lifecycle</a> section for more information.</p><p>For an <code>Extension</code> controller it is crucial to maintain the <code>Extension</code>&rsquo;s status correctly. At the end Gardener checks the status of each <code>Extension</code> and only reports a successful shoot reconciliation if the state of the last operation is <code>Succeeded</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  generation: 1
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: example
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    state: Succeeded
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-079331a6be1955be3c4c16acf0c583c1>1.5.15 - Healthcheck Library</h1><h1 id=health-check-library>Health Check Library</h1><h2 id=goal>Goal</h2><p>Typically, an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates / modifies resources in the cluster (via helm, managed resources, kubectl, &mldr;).
We call these API Objects &lsquo;dependent objects&rsquo; - as they are bound to the lifecycle of the extension.</p><p>The goal of this library is to enable extensions to setup health checks for their &lsquo;dependent objects&rsquo; with minimal effort.</p><h2 id=usage>Usage</h2><p>The library provides a generic controller with the ability to register any resource that satisfies the <a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types.go>extension object interface</a>.
An example is <a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_worker.go>the <code>Worker</code> CRD</a>.</p><p>Health check functions for commonly used dependent objects can be reused and registered with the controller, such as:</p><ul><li>Deployment</li><li>DaemonSet</li><li>StatefulSet</li><li>ManagedResource (Gardener specific)</li></ul><p>See the below example <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/controller/healthcheck/add.go>taken from the provider-aws</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>health.DefaultRegisterExtensionForHealthCheck(
</span></span><span style=display:flex><span>               aws.Type,
</span></span><span style=display:flex><span>               extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource),
</span></span><span style=display:flex><span>               <span style=color:#00f>func</span>() runtime.Object { <span style=color:#00f>return</span> &amp;extensionsv1alpha1.Worker{} },
</span></span><span style=display:flex><span>               mgr, <span style=color:green>// controller runtime manager
</span></span></span><span style=display:flex><span><span style=color:green></span>               opts, <span style=color:green>// options for the health check controller
</span></span></span><span style=display:flex><span><span style=color:green></span>               <span style=color:#00f>nil</span>, <span style=color:green>// custom predicates
</span></span></span><span style=display:flex><span><span style=color:green></span>               <span style=color:#00f>map</span>[extensionshealthcheckcontroller.HealthCheck]<span style=color:#2b91af>string</span>{
</span></span><span style=display:flex><span>                       general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy),
</span></span><span style=display:flex><span>                       general.CheckSeedDeployment(aws.MachineControllerManagerName):      string(gardencorev1beta1.ShootEveryNodeReady),
</span></span><span style=display:flex><span>                       worker.SufficientNodesAvailable():                                  string(gardencorev1beta1.ShootEveryNodeReady),
</span></span><span style=display:flex><span>               })
</span></span></code></pre></div><p>This creates a health check controller that reconciles the <code>extensions.gardener.cloud/v1alpha1.Worker</code> resource with the spec.type &lsquo;aws&rsquo;.
Three health check functions are registered that are executed during reconciliation.
Each health check is mapped to a single <code>HealthConditionType</code> that results in conditions with the same <code>condition.type</code> (see below).
To contribute to the Shoot&rsquo;s health, the following can be used: <code>SystemComponentsHealthy</code>, <code>EveryNodeReady</code>, <code>ControlPlaneHealthy</code>.
The Gardener/Gardenlet checks each extension for conditions matching these types.
However, extensions are free to choose any <code>HealthConditionType</code>.
For more information, see <a href=/docs/gardener/extensions/shoot-health-status-conditions/>Contributing to Shoot Health Status Conditions</a>.</p><p>A health check has to <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/healthcheck/actuator.go>satisfy the below interface</a>.
You can find implementation examples in the <a href=https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/healthcheck/general>healtcheck folder</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> HealthCheck <span style=color:#00f>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// Check is the function that executes the actual health check
</span></span></span><span style=display:flex><span><span style=color:green></span>    Check(context.Context, types.NamespacedName) (*SingleCheckResult, <span style=color:#2b91af>error</span>)
</span></span><span style=display:flex><span>    <span style=color:green>// InjectSeedClient injects the seed client
</span></span></span><span style=display:flex><span><span style=color:green></span>    InjectSeedClient(client.Client)
</span></span><span style=display:flex><span>    <span style=color:green>// InjectShootClient injects the shoot client
</span></span></span><span style=display:flex><span><span style=color:green></span>    InjectShootClient(client.Client)
</span></span><span style=display:flex><span>    <span style=color:green>// SetLoggerSuffix injects the logger
</span></span></span><span style=display:flex><span><span style=color:green></span>    SetLoggerSuffix(<span style=color:#2b91af>string</span>, <span style=color:#2b91af>string</span>)
</span></span><span style=display:flex><span>    <span style=color:green>// DeepCopy clones the healthCheck
</span></span></span><span style=display:flex><span><span style=color:green></span>    DeepCopy() HealthCheck
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The health check controller regularly (default: <code>30s</code>) reconciles the extension resource and executes the registered health checks for the dependent objects.
As a result, the controller writes condition(s) to the status of the extension containing the health check result.
In our example, two checks are mapped to <code>ShootEveryNodeReady</code> and one to <code>ShootSystemComponentsHealthy</code>, leading to conditions with two distinct <code>HealthConditionTypes</code> (condition.type):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>    - lastTransitionTime: <span style=color:#a31515>&#34;20XX-10-28T08:17:21Z&#34;</span>
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;20XX-11-28T08:17:21Z&#34;</span>
</span></span><span style=display:flex><span>      message: (1/1) Health checks successful
</span></span><span style=display:flex><span>      reason: HealthCheckSuccessful
</span></span><span style=display:flex><span>      status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>      type: SystemComponentsHealthy
</span></span><span style=display:flex><span>    - lastTransitionTime: <span style=color:#a31515>&#34;20XX-10-28T08:17:21Z&#34;</span>
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;20XX-11-28T08:17:21Z&#34;</span>
</span></span><span style=display:flex><span>      message: (2/2) Health checks successful
</span></span><span style=display:flex><span>      reason: HealthCheckSuccessful
</span></span><span style=display:flex><span>      status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>      type: EveryNodeReady
</span></span></code></pre></div><p>Please note that there are four statuses: <code>True</code>, <code>False</code>, <code>Unknown</code>, and <code>Progressing</code>.</p><ul><li><code>True</code> should be used for successful health checks.</li><li><code>False</code> should be used for unsuccessful/failing health checks.</li><li><code>Unknown</code> should be used when there was an error trying to determine the health status.</li><li><code>Progressing</code> should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the <code>Machines</code>, however, it&rsquo;s actually an expected situation and known to be completed within a few minutes.)</li></ul><p>Health checks that report <code>Progressing</code> should also provide a timeout, after which this &ldquo;progressing situation&rdquo; is expected to be completed.
The health check library will automatically transition the status to <code>False</code> if the timeout was exceeded.</p><h2 id=additional-considerations>Additional Considerations</h2><p>It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of <code>managed-resources</code> for trivial checks.
By <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/worker/genericactuator/machine_controller_manager.go>deploying the depending resources via managed resources</a>, the <a href=https://github.com/gardener/gardener-resource-manager>gardener resource manager</a> conducts basic checks for different API objects out-of-the-box (e.g <code>Deployments</code>, <code>DaemonSets</code>, &mldr;) - and writes health conditions.
By default, Gardener performs health check for all the <code>ManagedResource</code>s with <code>.spec.class=nil</code> created in the shoot namespaces.</p><p>More sophisticated health checks should be implemented by the extension controller itself (implementing the <code>HealthCheck</code> interface).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0e4beafbeaa3ceeadf6e6af4e27a26ae>1.5.16 - Heartbeat</h1><h1 id=heartbeat-controller>Heartbeat Controller</h1><p>The heartbeat controller renews a dedicated <code>Lease</code> object named <code>gardener-extension-heartbeat</code> at regular 30 second intervals by default. This <code>Lease</code> is used for heartbeats similar to how <code>gardenlet</code> uses <code>Lease</code> objects for seed heartbeats (see <a href=/docs/gardener/concepts/gardenlet/#heartbeats>gardenlet heartbeats</a>).</p><p>The <code>gardener-extension-heartbeat</code> <code>Lease</code> can be checked by other controllers to verify that the corresponding extension controller is still running. Currently, <code>gardenlet</code> checks this <code>Lease</code> when performing shoot health checks and expects to find the <code>Lease</code> inside the namespace where the extension controller is deployed by the corresponding <code>ControllerInstallation</code>. For each extension resource deployed in the Shoot control plane, <code>gardenlet</code> finds the corresponding <code>gardener-extension-heartbeat</code> <code>Lease</code> resource and checks whether the <code>Lease</code>&rsquo;s <code>.spec.renewTime</code> is older than the allowed threshold for stale extension health checks - in this case, <code>gardenlet</code> considers the health check report for an extension resource as &ldquo;outdated&rdquo; and reflects this in the <code>Shoot</code> status.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-21a79d22424671ff092806dd096f0b8e>1.5.17 - Infrastructure</h1><h1 id=contract-infrastructure-resource>Contract: <code>Infrastructure</code> Resource</h1><p>Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly.
Examples for that are networks, routing entries, security groups, IAM roles, etc.
Before introducing the <code>Infrastructure</code> extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see <a href=https://github.com/gardener/gardener/tree/0.20.0/charts/seed-terraformer/charts/aws-infra>here</a>).
Now, Gardener commissions an external, provider-specific controller to take over this task.</p><h2 id=which-infrastructure-resources-are-required>Which infrastructure resources are required?</h2><p>Unfortunately, there is no general answer to this question as it is highly provider specific.
Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs.
Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-infrastructure-provider>What needs to be implemented to support a new infrastructure provider?</h2><p>As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: cloudprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureConfig
</span></span><span style=display:flex><span>    resourceGroup:
</span></span><span style=display:flex><span>      name: mygroup
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      vnet: <span style=color:green># specify either &#39;name&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>      <span style=color:green># name: my-vnet</span>
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>      workers: 10.250.0.0/19
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used to create the needed resources.
However, the most important section is the <code>.spec.providerConfig</code>.
It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself).
You are responsible for designing how this configuration looks like.
Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the <code>Shoot</code> resource.</p><p>After your controller has created the required resources in your provider&rsquo;s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps.
An example for that is the <code>Worker</code> extension resource controller.
It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure.
Everything that it needs to know in order to do that (e.g. the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the <code>Infrastructure</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  providerStatus:
</span></span><span style=display:flex><span>    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    resourceGroup:
</span></span><span style=display:flex><span>      name: mygroup
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      vnet:
</span></span><span style=display:flex><span>        name: my-vnet
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - purpose: nodes
</span></span><span style=display:flex><span>        name: my-subnet
</span></span><span style=display:flex><span>    availabilitySets:
</span></span><span style=display:flex><span>    - purpose: nodes
</span></span><span style=display:flex><span>      id: av-set-id
</span></span><span style=display:flex><span>      name: av-set-name
</span></span><span style=display:flex><span>    routeTables:
</span></span><span style=display:flex><span>    - purpose: nodes
</span></span><span style=display:flex><span>      name: route-table-name
</span></span><span style=display:flex><span>    securityGroups:
</span></span><span style=display:flex><span>    - purpose: nodes
</span></span><span style=display:flex><span>      name: sec-group-name
</span></span></code></pre></div><p>In order to support a new infrastructure provider you need to write a controller that watches all <code>Infrastructure</code>s with <code>.spec.type=&lt;my-provider-name></code>.
You can take a look at the below referenced example implementation for the Azure provider.</p><h2 id=dynamic-nodes-network-for-shoot-clusters>Dynamic nodes network for shoot clusters</h2><p>Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes.
In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the <code>status</code> of the <code>Infrastructure</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  providerStatus: ...
</span></span><span style=display:flex><span>  nodesCIDR: 10.250.0.0/16
</span></span></code></pre></div><p>Gardener will pick this <code>nodesCIDR</code> and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes.
If the <code>Shoot</code> resource already specifies a nodes CIDR in <code>.spec.networking.nodes</code> and the extension controller provides also a value in <code>.status.nodesCIDR</code> in the <code>Infrastructure</code> resource then the latter one will always be considered with higher priority by Gardener.</p><h2 id=non-provider-specific-information-required-for-infrastructure-creation>Non-provider specific information required for infrastructure creation</h2><p>Some providers might require further information that is not provider specific but already part of the shoot resource.
One example for this is the <a href=https://github.com/gardener/gardener-extension-provider-gcp/tree/master/pkg/controller/infrastructure>GCP infrastructure controller</a> which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly.
As Gardener cannot know which information is required by providers it simply mirrors the <code>Shoot</code>, <code>Seed</code>, and <code>CloudProfile</code> resources into the seed.
They are part of the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> extension resource</a> and can be used to extract information that is not part of the <code>Infrastructure</code> resource itself.</p><h2 id=implementation-details>Implementation details</h2><h3 id=actuator-interface><code>Actuator</code> interface</h3><p>Most existing infrastructure controller implementations follow a common pattern where a generic <code>Reconciler</code> delegates to <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/infrastructure/actuator.go>an <code>Actuator</code> interface</a> that contains the methods <code>Reconcile</code>, <code>Delete</code>, <code>Migrate</code>, and <code>Restore</code>. These methods are called by the generic <code>Reconciler</code> for the respective operations, and should be implemented by the extension according to the contract described here and the <a href=/docs/gardener/extensions/migration/>migration guidelines</a>.</p><h3 id=configvalidator-interface><code>ConfigValidator</code> interface</h3><p>For infrastructure controllers, the generic <code>Reconciler</code> also delegates to <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/infrastructure/configvalidator.go>a <code>ConfigValidator</code> interface</a> that contains a single <code>Validate</code> method. This method is called by the generic <code>Reconciler</code> at the beginning of every reconciliation, and can be implemented by the extension to validate the <code>.spec.providerConfig</code> part of the <code>Infrastructure</code> resource with the respective cloud provider, typically the existence and validity of cloud provider resources such as AWS VPCs or GCP Cloud NAT IPs.</p><p>The <code>Validate</code> method returns a list of errors. If this list is non-empty, the generic <code>Reconciler</code> will fail with an error. This error will have the error code <code>ERR_CONFIGURATION_PROBLEM</code>, unless there is at least one error in the list that has its <code>ErrorType</code> field set to <code>field.ErrorTypeInternal</code>.</p><h2 id=references-and-additional-resources>References and additional resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_infrastructure.go><code>Infrastructure</code> API (Golang specification)</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/infrastructure>Sample implementation for the Azure provider</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/infrastructure/configvalidator.go>Sample ConfigValidator implementation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-95017ccbee8525055e43da8a42034f57>1.5.18 - Logging And Monitoring</h1><h1 id=logging-and-monitoring-for-extensions>Logging and Monitoring for Extensions</h1><p>Gardener provides an integrated logging and monitoring stack for alerting, monitoring, and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for <a href=https://github.com/gardener/logging/tree/master/docs/usage/README.md>exploring logs</a> and for <a href=https://grafana.com/docs/grafana/latest/getting-started/getting-started/#all-users>monitoring with Grafana</a>.</p><p>The components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys <a href=https://prometheus.io/>Prometheus</a>, <a href=https://prometheus.io/docs/alerting/latest/alertmanager/>Alertmanager</a>, and <a href=https://grafana.com/>Grafana</a> into the <code>garden</code> namespace of all seed clusters. If the logging is enabled in the <code>gardenlet</code> configuration (<code>logging.enabled</code>), it will deploy <a href=https://fluentbit.io/>fluent-bit</a> and <a href=https://grafana.com/oss/loki/>Loki</a> in the <code>garden</code> namespace too.</p><p>Each shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the logging is enabled in the <code>gardenlet</code> configuration (<code>logging.enabled</code>) and the <a href=/docs/gardener/usage/shoot_purposes/#behavioral-differences>shoot purpose</a> is not <code>testing</code>, it deploys a shoot-specific Loki in the shoot namespace too.</p><p>The logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in <code>ConfigMap</code>s for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.</p><p>This guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:</p><ul><li><a href=#monitoring>Monitoring</a></li><li><a href=#logging>Logging</a></li></ul><h2 id=monitoring>Monitoring</h2><p>The central Prometheus instance in the <code>garden</code> namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods.
It uses the <a href=https://prometheus.io/docs/prometheus/latest/federation/>federation</a> concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for.
This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.</p><p>The shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.</p><p>Extension controllers might deploy components as part of their reconciliation next to the shoot&rsquo;s control plane.
Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.</p><h3 id=extensions-monitoring-integration>Extensions Monitoring Integration</h3><p>Before deploying the shoot-specific Prometheus instance, Gardener will read all <code>ConfigMap</code>s in the shoot namespace, which are labeled with <code>extensions.gardener.cloud/configuration=monitoring</code>.
Such <code>ConfigMap</code>s may contain four fields in their <code>data</code>:</p><ul><li><code>scrape_config</code>: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped.</li><li><code>alerting_rules</code>: This field contains Alertmanager rules for alerts that shall be raised.</li><li><code>dashboard_operators</code>: This field contains a Grafana dashboard in JSON. Note that the former field name was kept for backwards compatibility but the dashboard is going to be shown both for Gardener operators and for shoot owners because the monitoring stack no longer distinguishes the two roles.</li><li><code>dashboard_users</code>: This field contains a Grafana dashboard in JSON. Note that the former field name was kept for backwards compatibility but the dashboard is going to be shown both for Gardener operators and for shoot owners because the monitoring stack no longer distinguishes the two roles.</li></ul><p><strong>Example:</strong> A <code>ControlPlane</code> controller deploying a <code>cloud-controller-manager</code> into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules, dashboards, and logging configuration for exposing logs to the end users.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-controlplane-monitoring-ccm
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    extensions.gardener.cloud/configuration: monitoring
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  scrape_config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - job_name: cloud-controller-manager
</span></span></span><span style=display:flex><span><span style=color:#a31515>      scheme: https
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tls_config:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        insecure_skip_verify: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>      authorization:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        type: Bearer
</span></span></span><span style=display:flex><span><span style=color:#a31515>        credentials_file: /var/run/secrets/gardener.cloud/shoot/token/token
</span></span></span><span style=display:flex><span><span style=color:#a31515>      honor_labels: false
</span></span></span><span style=display:flex><span><span style=color:#a31515>      kubernetes_sd_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - role: endpoints
</span></span></span><span style=display:flex><span><span style=color:#a31515>        namespaces:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          names: [shoot--project--name]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      relabel_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - source_labels:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - __meta_kubernetes_service_name
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - __meta_kubernetes_endpoint_port_name
</span></span></span><span style=display:flex><span><span style=color:#a31515>        action: keep
</span></span></span><span style=display:flex><span><span style=color:#a31515>        regex: cloud-controller-manager;metrics
</span></span></span><span style=display:flex><span><span style=color:#a31515>      # common metrics
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - action: labelmap
</span></span></span><span style=display:flex><span><span style=color:#a31515>        regex: __meta_kubernetes_service_label_(.+)
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - source_labels: [ __meta_kubernetes_pod_name ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>        target_label: pod
</span></span></span><span style=display:flex><span><span style=color:#a31515>      metric_relabel_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - process_max_fds
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - process_open_fds</span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  alerting_rules: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    cloud-controller-manager.rules.yaml: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      groups:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - name: cloud-controller-manager.rules
</span></span></span><span style=display:flex><span><span style=color:#a31515>        rules:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - alert: CloudControllerManagerDown
</span></span></span><span style=display:flex><span><span style=color:#a31515>          expr: absent(up{job=&#34;cloud-controller-manager&#34;} == 1)
</span></span></span><span style=display:flex><span><span style=color:#a31515>          for: 15m
</span></span></span><span style=display:flex><span><span style=color:#a31515>          labels:
</span></span></span><span style=display:flex><span><span style=color:#a31515>            service: cloud-controller-manager
</span></span></span><span style=display:flex><span><span style=color:#a31515>            severity: critical
</span></span></span><span style=display:flex><span><span style=color:#a31515>            type: seed
</span></span></span><span style=display:flex><span><span style=color:#a31515>            visibility: all
</span></span></span><span style=display:flex><span><span style=color:#a31515>          annotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>            description: All infrastructure specific operations cannot be completed (e.g. creating load balancers or persistent volumes).
</span></span></span><span style=display:flex><span><span style=color:#a31515>            summary: Cloud controller manager is down.</span>    
</span></span></code></pre></div><h2 id=logging>Logging</h2><p>In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.</p><p><img src=/__resources/logging-architecture_c8dc32.png alt="&amp;ldquo;Cluster Logging Topology&amp;rdquo;" title="Cluster Logging Topology"></p><p>Gardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:</p><ul><li>Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the logging is enabled in the <code>gardenlet</code> configuration (<code>logging.enabled</code>) and the <a href=/docs/gardener/usage/shoot_purposes/#behavioral-differences>shoot purpose</a> is not <code>testing</code>, and one in the <code>garden</code> namespace. The shoot instances store logs from the control plane components hosted there. The <code>garden</code> Loki instance is responsible for logs from the rest of the seed namespaces - <code>kube-system</code>, <code>garden</code>, <code>extension-*</code>, and others.</li><li>Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards.</li><li>Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the corresponding Loki instances, sharing the same namespace as data providers. There is one Grafana Deployment in the <code>garden</code> namespace and one Deployment per shoot namespace (exposed to the end users and to the operators).</li></ul><p>Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable <a href=https://docs.fluentbit.io/manual/concepts/data-pipeline>data pipeline</a> to address that problem. Gardener provides such <a href=https://github.com/gardener/gardener/blob/master/charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml>configuration</a> for logs produced by all its core managed components as a <code>ConfigMap</code>. Extensions can contribute their own, specific configurations as <code>ConfigMap</code>s too. See for example the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/charts/gardener-extension-provider-aws/templates/configmap-logging.yaml>logging configuration</a> for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.</p><h3 id=fluent-bit-log-parsers-and-filters>Fluent-bit Log Parsers and Filters</h3><p>To integrate with Gardener logging, extensions can and <em>should</em> specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a <em>parser</em> for the specific logging format, if none of the available is applicable, and a <em>filter</em> defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit&rsquo;s <a href=https://docs.fluentbit.io/manual/>documentation</a>.</p><blockquote><p><strong>Note:</strong> At the moment, only <em>parser</em> and <em>filter</em> configurations are supported.</p></blockquote><p>To contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a <code>ConfigMap</code> labeled <code>extensions.gardener.cloud/configuration=logging</code> and deployed in the seed&rsquo;s <code>garden</code> namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a <em>single</em> configuration <code>ConfigMap</code> is sufficient and it applies to all fluent-bit agents in the seed. Its <code>data</code> field can have the following properties:</p><ul><li><code>filter-kubernetes.conf</code> - configuration for data pipeline <a href=https://docs.fluentbit.io/manual/concepts/data-pipeline/filter>filters</a></li><li><code>parser.conf</code> - configuration for data pipeline <a href=https://docs.fluentbit.io/manual/concepts/data-pipeline/parser>parsers</a></li></ul><blockquote><p><strong>Note:</strong> Take care to provide the correct data pipeline elements in the corresponding data field and not to mix them.</p></blockquote><p><strong>Example:</strong> Logging configuration for provider-specific (OpenStack) worker controller deploying a <code>machine-controller-manager</code> component into a shoot namespace that reuses the <code>kubeapiserverParser</code> defined in <a href=https://github.com/gardener/gardener/blob/master/charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml#L304-L309>fluent-bit-configmap.yaml</a> to parse the component logs:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-extension-provider-openstack-logging-config
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    extensions.gardener.cloud/configuration: logging
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  filter-kubernetes.conf: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    [FILTER]
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Name                parser
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Match               kubernetes.machine-controller-manager*openstack-machine-controller-manager*
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Key_Name            log
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Parser              kubeapiserverParser
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Reserve_Data        True</span>    
</span></span></code></pre></div><p>Further details how to define parsers and use them with examples can be found in the following <a href=/docs/gardener/development/log_parsers/>guide</a>.</p><h3 id=grafana>Grafana</h3><p>The two types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:</p><ul><li>Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters<ul><li><a href=https://github.com/gardener/gardener/blob/master/charts/seed-bootstrap/dashboards/pod-logs.json>Pod Logs</a></li><li><a href=https://github.com/gardener/gardener/blob/master/charts/seed-bootstrap/dashboards/extensions-dashboard.json>Extensions</a></li><li><a href=https://github.com/gardener/gardener/blob/master/charts/seed-bootstrap/dashboards/systemd-logs.json>Systemd Logs</a></li></ul></li><li>Shoot Grafana dashboards expose logs from the shoot cluster namespace where they belong<ul><li>Kube Apiserver</li><li>Kube Controller Manager</li><li>Kube Scheduler</li><li>Cluster Autoscaler</li><li>VPA components</li><li><a href=https://github.com/gardener/gardener/blob/master/charts/seed-monitoring/charts/grafana/dashboards/owners/kubernetes-pods-dashboard.json>Kubernetes Pods</a></li></ul></li></ul><p>If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the corresponding instance dashboard configurations.</p><h2 id=tips>Tips</h2><ul><li>Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form <code>kubernetes.pod_name.&lt;metadata>.container_name</code>. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don&rsquo;t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using <code>myextension-container</code> as container name and a regular expression <code>kubernetes.mypod.*myextension-container</code> will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, <code>kubernetes.systemd.*systemd.*</code> will match both <code>systemd-service</code> and <code>systemd-monitor-service</code>. You will want to be as specific as possible.</li><li>It&rsquo;s a good idea to put the logging configuration into the Helm chart that also deploys the extension <em>controller</em>, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the <em>component</em> managed by the controller.</li></ul><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/issues/1351>GitHub Issue Describing the Concept</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/charts/internal/seed-controlplane/charts/cloud-controller-manager/templates/configmap-observability.yaml>Exemplary Implementation (Monitoring) for the GCP Provider</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-openstack/blob/master/charts/gardener-extension-provider-openstack/templates/configmap-logging.yaml>Exemplary Implementation (Logging) for the OpenStack Provider</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-107acf6f6b69c9ac1c403a4f60e455d7>1.5.19 - Managedresources</h1><h1 id=deploy-resources-to-the-shoot-cluster>Deploy Resources to the Shoot Cluster</h1><p>We have introduced a component called <a href=/docs/gardener/concepts/resource-manager/><code>gardener-resource-manager</code></a> that is deployed as part of every shoot control plane in the seed.
One of its tasks is to manage CRDs, so called <code>ManagedResource</code>s.
Managed resources contain Kubernetes resources that shall be created, reconciled, updated, and deleted by the gardener-resource-manager.</p><p>Extension controllers may create these <code>ManagedResource</code>s in the shoot namespace if they need to create any resource in the shoot cluster itself, for example RBAC roles (or anything else).</p><h2 id=where-can-i-find-more-examples-and-more-information-how-to-use-managedresources>Where can I find more examples and more information how to use <code>ManagedResource</code>s?</h2><p>Please take a look at the <a href=/docs/gardener/concepts/resource-manager/>respective documentation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fb6f6dc9c480f0bfbe05fce71647dcf7>1.5.20 - Migration</h1><h1 id=control-plane-migration>Control Plane Migration</h1><p><em>Control Plane Migration</em> is a new Gardener feature that has been recently implemented as proposed in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>GEP-7 Shoot Control Plane Migration</a>. It should be properly supported by all extensions controllers. This document outlines some important points that extension maintainers should keep in mind to properly support migration in their extensions.</p><h2 id=overall-principles>Overall Principles</h2><p>The following principles should always be upheld:</p><ul><li>All states maintained by the extension that is external from the seed cluster, for example infrastructure resources in a cloud provider, DNS entries, etc., should be kept during the migration. No such state should be deleted and then recreated, as this might cause disruption in the availability of the shoot cluster.</li><li>All Kubernetes resources maintained by the extension in the shoot cluster itself should also be kept during the migration. No such resources should be deleted and then recreated.</li></ul><h2 id=migrate-and-restore-operations>Migrate and Restore Operations</h2><p>Two new operations have been introduced in Gardener. They can be specified as values of the <code>gardener.cloud/operation</code> annotation on an extension resource to indicate that an operation different from a normal <code>reconcile</code> should be performed by the corresponding extension controller:</p><ul><li>The <code>migrate</code> operation is used to ask the extension controller in the source seed to stop reconciling extension resources (in case they are requeued due to errors) and perform cleanup activities, if such are required. These cleanup activities might involve removing finalizers on resources in the shoot namespace that have been previously created by the extension controller and deleting them without actually deleting any resources external to the seed cluster.</li><li>The <code>restore</code> operation is used to ask the extension controller in the destination seed to restore any state saved in the extension resource <code>status</code>, before performing the actual reconciliation.</li></ul><p>Unlike the <a href=/docs/gardener/extensions/reconcile-trigger/>reconcile operation</a>, extension controllers must remove the <code>gardener.cloud/operation</code> annotation at the end of a successful reconciliation when the current operation is <code>migrate</code> or <code>restore</code>, not at the beginning of a reconciliation.</p><h2 id=cleaning-up-source-seed-resources>Cleaning-Up Source Seed Resources</h2><p>All resources in the source seed that have been created by an extension controller, for example secrets, config maps, <a href=/docs/gardener/extensions/managedresources/>managed resources</a>, etc., should be properly cleaned up by the extension controller when the current operation is <code>migrate</code>. As mentioned above, such resources should be deleted without actually deleting any resources external to the seed cluster.</p><p>For many custom resources, for example MCM resources, the above requirement means in practice that any finalizers should be removed before deleting the resource, in addition to ensuring that the resource deletion is not reconciled by its respective controller if there is no finalizer. For managed resources, the above requirement means in practice that the <code>spec.keepObjects</code> field should be set to <code>true</code> before deleting the extension resource.</p><p>Here it is assumed that any resources that contain state needed by the extension controller can be safely deleted, since any such state has been saved as described in <a href=#saving-and-restoring-extension-states>Saving and Restoring Extension States</a> at the end of the last successful reconciliation.</p><h2 id=saving-and-restoring-extension-states>Saving and Restoring Extension States</h2><p>Some extension controllers create and maintain their own state when reconciling extension resources. For example, most infrastructure controllers use Terraform and maintain the terraform state in a special config map in the shoot namespace. This state must be properly migrated to the new seed cluster during control plane migration, so that subsequent reconciliations in the new seed could find and use it appropriately.</p><p>All extension controllers that require such state migration must save their state in the <code>status.state</code> field of their extension resource at the end of a successful reconciliation. They must also restore their state from that same field upon reconciling an extension resource when the current operation is <code>restore</code>, as specified by the <code>gardener.cloud/operation</code> annotation, before performing the actual reconciliation.</p><p>As an example, an infrastructure controller that uses Terraform must save the terraform state in the <code>status.state</code> field of the <code>Infrastructure</code> resource. An <code>Infrastructure</code> resource with a properly saved state might look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: cloudprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureConfig
</span></span><span style=display:flex><span>    resourceGroup:
</span></span><span style=display:flex><span>      name: mygroup
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  state: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    {
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;version&#34;: 3,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;terraform_version&#34;: &#34;0.11.14&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;serial&#34;: 2,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;lineage&#34;: &#34;3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ...
</span></span></span><span style=display:flex><span><span style=color:#a31515>    }</span>    
</span></span></code></pre></div><p>Extension controllers that do not use a saved state and therefore do not require state migration could leave the <code>status.state</code> field as <code>nil</code> at the end of a successful reconciliation, and just perform a normal reconciliation when the current operation is <code>restore</code>.</p><p>In addition, extension controllers that use <a href=/docs/gardener/extensions/referenced-resources/>referenced resources</a> (usually secrets) must also make sure that these resources are added to the <code>status.resources</code> field of their extension resource at the end of a successful reconciliation, so they could be properly migrated by Gardener to the destination seed.</p><h2 id=implementation-details>Implementation Details</h2><h3 id=migrate-and-restore-actuator-methods>Migrate and Restore Actuator Methods</h3><p>Most extension controller implementations follow a common pattern where a generic <code>Reconciler</code> implementation delegates to an <code>Actuator</code> interface that contains the methods <code>Reconcile</code> and <code>Delete</code>, provided by the extension. The two new methods <code>Migrate</code> and <code>Restore</code> have been added to all such <code>Actuator</code> interfaces, see <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/infrastructure/actuator.go>the infrastructure <code>Actuator</code> interface</a> as an example. These methods are called by the generic reconcilers for the <a href=#migrate-and-restore-operations>migrate and restore operations</a> respectively, and should be implemented by the extension according to the above guidelines.</p><h3 id=owner-checks>Owner Checks</h3><p>The so called &ldquo;bad case&rdquo; scenario for control plane migration proposed in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/17-shoot-control-plane-migration-bad-case.md>GEP-17</a> introduced the requirement for extension controllers to check whether they are currently operating in the source or destination seed during reconciliations to avoid the case in which controllers from different seeds can operate on the same IaaS resources (split brain scenario). To that end, a special &ldquo;owner checking&rdquo; mechanism has been added to the <code>Reconciler</code> implementations of all extension controllers. For an example usage of this mechanism see <a href=https://github.com/gardener/gardener/blob/7ac4b04feec409f3e5a5208cd06af9a10c755337/extensions/pkg/controller/infrastructure/reconciler.go#L109-L121>the infrastructure Reconciler implementation</a>. The purpose of the owner check is to interrupt reconciliations of extension controllers that do not operate in the seed that is currently configured to host the shoot&rsquo;s control plane. Note that <code>Migrate</code> operations must not be interrupted, as they are required to clean up Kubernetes resources left in the shoot&rsquo;s control plane namespace and do not act on IaaS resources.</p><h3 id=extension-controllers-based-on-generic-actuators>Extension Controllers Based on Generic Actuators</h3><p>In practice, the implementation of many extension controllers (for example, the controlplane and worker controllers in most provider extensions) are based on a <em>generic <code>Actuator</code> implementation</em> that only delegates to extension methods for behavior that is truly provider specific. In all such cases, the <code>Migrate</code> and <code>Restore</code> methods have already been implemented properly in the generic actuators and there is nothing more to do in the extension itself.</p><p>In some rare cases, extension controllers based on a generic actuator might still introduce a custom <code>Actuator</code> implementation to override some of the generic actuator methods in order to enhance or change their behavior in a certain way. In such cases, the <code>Migrate</code> and <code>Restore</code> methods might need to be overridden as well, see the <a href=https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/controlplane>Azure controlplane controller</a> as an example.</p><h3 id=extension-controllers-not-based-on-generic-actuators>Extension Controllers Not Based on Generic Actuators</h3><p>The implementation of some extension controllers (for example, the infrastructure controllers in all provider extensions) are not based on a generic <code>Actuator</code> implementation. Such extension controllers must always provide a proper implementation of the <code>Migrate</code> and <code>Restore</code> methods according to the above guidelines, see the <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/infrastructure>AWS infrastructure controller</a> as an example. In practice, this might result in code duplication between the different extensions, since the <code>Migrate</code> and <code>Restore</code> code is usually not provider or OS-specific.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6abc753b67fb6ebd6b625067c168bcb1>1.5.21 - Network</h1><h1 id=gardener-network-extension>Gardener Network Extension</h1><p>Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:</p><ul><li>Managed: end-users only request a Kubernetes cluster (Clusters-as-a-Service)</li><li>Hosted: operators utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)</li></ul><p>Whether a user is an operator or an end-user, it makes sense to provide choice. For example, for an end-user it might make sense to
choose a network-plugin that would support enforcing network policies (some plugins does not come with network-policy support by default).
For operators however, choice only matters for delegation purposes i.e., when providing an own managed-service, it becomes important to also provide choice over which network-plugins to use.</p><p>Furthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, <a href=https://docs.projectcalico.org/v3.0/reference/public-cloud/azure>Azure does not support Calico Networking</a>. This leads to the introduction of manual exceptions in static add-on charts which is error prone and can lead to failures during upgrades.</p><p>Finally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provide better performance. Consistency does not necessarily lie in the implementation but in the interface.</p><h2 id=motivation>Motivation</h2><p>Prior to the <code>Network Extensibility</code> concept, Gardener followed a mono network-plugin support model (i.e., Calico). Although this seemed to be the easier approach, it did not completely reflect the real use-case.
The goal of the Gardener Network Extensions is to support different network plugins, therefore, the specification for the network resource won&rsquo;t be fixed and will be customized based on the underlying network plugin.</p><p>To do so, a <code>ProviderConfig</code> field in the spec will be provided where each plugin will define. Below is an example for how to deploy Calico as the cluster network plugin.</p><h2 id=the-network-extensions-resource>The Network Extensions Resource</h2><p>Here is what a typical <code>Network</code> resource would look-like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-network
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ipFamilies:
</span></span><span style=display:flex><span>  - IPv4
</span></span><span style=display:flex><span>  podCIDR: 100.244.0.0/16
</span></span><span style=display:flex><span>  serviceCIDR: 100.32.0.0/13
</span></span><span style=display:flex><span>  type: calico
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span>    backend: bird
</span></span><span style=display:flex><span>    ipam:
</span></span><span style=display:flex><span>      cidr: usePodCIDR
</span></span><span style=display:flex><span>      type: host-local
</span></span></code></pre></div><p>The above resources is divided into two parts (more information can be found at <a href=/docs/extensions/network-extensions/gardener-extension-networking-calico/docs/usage-as-end-user/>Using the Networking Calico Extension</a>):</p><ul><li>global configuration (e.g., podCIDR, serviceCIDR, and type)</li><li>provider specific config (e.g., for calico we can choose to configure a <code>bird</code> backend)</li></ul><blockquote><p><strong>Note</strong>: Certain cloud-provider extensions might have webhooks that would modify the network-resource to fit into their network specific context. As previously mentioned, Azure does not support IPIP, as a result, the <a href=https://github.com/gardener/gardener-extension-provider-azure>Azure provider extension</a> implements a <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/master/pkg/webhook/network/mutate.go>webhook</a> to mutate the backend and set it to <code>None</code> instead of <code>bird</code>.</p></blockquote><h2 id=supporting-a-new-network-extension-provider>Supporting a New Network Extension Provider</h2><p>To add support for another networking provider (e.g., weave, Cilium, Flannel) a network extension controller needs to be implemented which would optionally have its own custom configuration specified in the <code>spec.providerConfig</code> in the <code>Network</code> resource. For example, if support for a network plugin named <code>gardenet</code> is required, the following <code>Network</code> resource would be created:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-network
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ipFamilies:
</span></span><span style=display:flex><span>  - IPv4
</span></span><span style=display:flex><span>  podCIDR: 100.244.0.0/16
</span></span><span style=display:flex><span>  serviceCIDR: 100.32.0.0/13
</span></span><span style=display:flex><span>  type: gardenet
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: gardenet.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span>    gardenetCustomConfigField: &lt;value&gt;
</span></span><span style=display:flex><span>    ipam:
</span></span><span style=display:flex><span>      cidr: usePodCIDR
</span></span><span style=display:flex><span>      type: host-local
</span></span></code></pre></div><p>Once applied, the presumably implemented <code>Gardenet</code> extension controller would pick the configuration up, parse the <code>providerConfig</code>, and create the necessary resources in the shoot.</p><p>For additional reference, please have a look at the <a href=https://github.com/gardener/gardener-extension-networking-calico>networking-calico</a> provider extension, which provides more information on how to configure the necessary charts, as well as the actuators required to reconcile networking inside the <code>Shoot</code> cluster to the desired state.</p><h2 id=supporting-kube-proxy-less-service-routing>Supporting <code>kube-proxy</code>-less Service Routing</h2><p>Some networking extensions support service routing without the <code>kube-proxy</code> component. This is why Gardener supports disabling of <code>kube-proxy</code> for service routing by setting <code>.spec.kubernetes.kubeproxy.enabled</code> to <code>false</code> in the <code>Shoot</code> specification. The implicit contract of the flag is:</p><p><em>If <code>kube-proxy</code> is disabled, then the networking extension is responsible for the service routing.</em></p><p>The networking extensions need to handle this twofold:</p><ol><li>During the reconciliation of the networking resources, the extension needs to check whether <code>kube-proxy</code> takes care of the service routing or the networking extension itself should handle it. In case the networking extension should be responsible according to <code>.spec.kubernetes.kubeproxy.enabled</code> (but is unable to perform the service routing), it should raise an error during the reconciliation. If the networking extension should handle the service routing, it may reconfigure itself accordingly.</li><li>(Optional) In case the networking extension does not support taking over the service routing (in some scenarios), it is recommended to also provide a validating admission webhook to reject corresponding changes early on. The validation may take the current operating mode of the networking extension into consideration.</li></ol><h2 id=related-links>Related Links</h2><ul><li><a href=https://docs.projectcalico.org/v3.0/reference/public-cloud/azure>Azure Support for Calico Networking</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d758265b318aa3c4194c99f8fe8e2760>1.5.22 - Operatingsystemconfig</h1><h1 id=contract-operatingsystemconfig-resource>Contract: <code>OperatingSystemConfig</code> Resource</h1><p>Gardener uses the machine API and leverages the functionalities of the <a href=https://github.com/gardener/machine-controller-manager>machine-controller-manager</a> (MCM) in order to manage the worker nodes of a shoot cluster.
The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider&rsquo;s API when creating VMs.
MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine&rsquo;s configuration in any way - it just creates/deletes machines with the provided metadata.</p><p>Consequently, Gardener needs to provide this information when interacting with the machine-controller-manager.
This means that basically every operating system is possible to be used, as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.</p><p>⚠️ Currently, there are a few requirements:</p><ol><li>The operating system must have built-in <a href=https://www.docker.com/>Docker</a> support.</li><li>The operating system must have <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> support.</li><li>The operating system must have <a href=https://www.gnu.org/software/wget/><code>wget</code></a> pre-installed.</li><li>The operating system must have <a href=https://stedolan.github.io/jq/><code>jq</code></a> pre-installed.</li></ol><p>The reasons for that will become evident later.</p><h2 id=what-does-the-user-data-bootstrapping-the-machines-contain>What does the user-data bootstrapping the machines contain?</h2><p>Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster.
There is the <code>kubelet</code> process, some scripts for continuously checking the health of <code>kubelet</code> and <code>docker</code>, but also configuration for log rotation, CA certificates, etc.
The complete configuration you can find <a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original/components>at the components folder</a>. We are calling this the &ldquo;original&rdquo; user-data.</p><h2 id=how-does-gardener-bootstrap-the-machines>How does Gardener bootstrap the machines?</h2><p>Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time.
However, some providers do have a size limitation (around ~16KB) for that user-data.
That&rsquo;s why we do not send the &ldquo;original&rdquo; user-data to the machine-controller-manager (who then forwards it to the provider&rsquo;s API).
Instead, we only send a small script that downloads the &ldquo;original&rdquo; data and applies it on the machine directly.
This way we can extend the &ldquo;original&rdquo; user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).</p><p>The high-level flow is as follows:</p><ol><li><p>For every worker pool <code>X</code> in the <code>Shoot</code> specification, Gardener creates a <code>Secret</code> named <code>cloud-config-&lt;X></code> in the <code>kube-system</code> namespace of the shoot cluster. The secret contains the &ldquo;original&rdquo; user-data.</p></li><li><p>Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the <code>downloader</code> script later.</p></li><li><p>Gardener provides the <code>downloader</code> script, the kubeconfig, and the machine image stated in the <code>Shoot</code> specification to the machine-controller-manager.</p></li><li><p>Based on this information, the machine-controller-manager creates the VM.</p></li><li><p>After the VM has been provisioned, the <code>downloader</code> script starts and fetches the appropriate <code>Secret</code> for its worker pool (containing the &ldquo;original&rdquo; user-data), and applies it.</p></li></ol><h3 id=detailed-bootstrap-flow-with-a-worker-generated-bootstrap-token>Detailed Bootstrap Flow with a Worker Generated bootstrap-token</h3><p>With gardener v1.23 a file with the content <code>&lt;&lt;BOOTSTRAP_TOKEN>></code> is added to the <code>cloud-config-&lt;worker-group>-downloader</code> <code>OperatingSystemConfig</code> (part of step 2 in the graphic below).
Via the OS extension, the new file (with its content in clear-text) gets passed to the corresponding <code>Worker</code> resource.</p><p>The <code>Worker</code> controller has to guarantee that:</p><ul><li>a bootstrap token is created.</li><li>the <code>&lt;&lt;BOOTSTRAP_TOKEN>></code> in the user data is replaced by the generated token.</li></ul><p>One implementation of that is depicted in the diagram below, where the machine-controller-manager creates a temporary token and replaces the placeholder.</p><p>As part of the user-data, the bootstrap-token is placed on the newly created VM under a defined path.
The cloud-config-script will then refer to the file path of the added bootstrap token in the kubelet-bootstrap script.</p><p><img src=/__resources/bootstrap_token_c9a050.png alt="Bootstrap flow with shortlived bootstrapTokens"></p><h3 id=compatibility-matrix-for-node-bootstrap-token>Compatibility Matrix for Node bootstrap-token</h3><p>With Gardener v1.23, we replaced the long-valid bootstrap-token shared between nodes with a short-lived token unique for each node, ref: <a href=https://github.com/gardener/gardener/issues/3898>#3898</a>.</p><p>❗ When updating to Gardener version >=1.35, the old bootstrap-token will be removed. You are required to update your extensions to the following versions when updating Gardener:</p><table><thead><tr><th>Extension</th><th>Version</th><th>Release Date</th><th>Pull Request</th></tr></thead><tbody><tr><td>os-gardenlinux</td><td>v0.9.0</td><td>2 Jul</td><td><a href=https://github.com/gardener/gardener-extension-os-gardenlinux/pull/29>https://github.com/gardener/gardener-extension-os-gardenlinux/pull/29</a></td></tr><tr><td>os-suse-chost</td><td>v1.11.0</td><td>2 Jul</td><td><a href=https://github.com/gardener/gardener-extension-os-suse-chost/pull/41>https://github.com/gardener/gardener-extension-os-suse-chost/pull/41</a></td></tr><tr><td>os-ubuntu</td><td>v1.11.0</td><td>2 Jul</td><td><a href=https://github.com/gardener/gardener-extension-os-ubuntu/pull/42>https://github.com/gardener/gardener-extension-os-ubuntu/pull/42</a></td></tr><tr><td>os-flatcar</td><td>v1.7.0</td><td>2 Jul</td><td><a href=https://github.com/gardener/gardener-extension-os-coreos/pull/24>https://github.com/gardener/gardener-extension-os-coreos/pull/24</a></td></tr><tr><td>infrastructure-provider using Machine Controller Manager</td><td>varies</td><td>~ end of 2019</td><td><a href=https://github.com/gardener/machine-controller-manager/pull/351>https://github.com/gardener/machine-controller-manager/pull/351</a></td></tr></tbody></table><p>⚠️ If you run a provider extension that does not use Machine Controller Manager (MCM), you need to implement the functionality of creating a temporary bootstrap-token before updating your Gardener version to v1.35 or higher.
All provider extensions maintained in the <a href=https://github.com/gardener/>gardener GitHub repo</a> use MCM.</p><h2 id=how-does-gardener-update-the-user-data-on-already-existing-machines>How does Gardener update the user-data on already existing machines?</h2><p>With ongoing development and new releases of Gardener, some new components could be required to get installed onto every shoot worker VM, or existing components might need to be changed.
Gardener achieves that by simply updating the user-data inside the <code>Secret</code>s mentioned above (step 1).
The <code>downloader</code> script is continuously (every 30s) reading the secret&rsquo;s content (which might include an updated user-data) and storing it onto the disk.
In order to re-apply the (new) downloaded data, the secrets do not only contain the &ldquo;original&rdquo; user-data but also another short script (called an &ldquo;execution&rdquo; script).
This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it.
After that it uses <code>systemctl</code> to restart the installed <code>systemd</code> units.</p><p>With the help of the execution script, Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic.
However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and <code>systemd</code>.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-operating-system>What needs to be implemented to support a new operating system?</h2><p>As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: &lt;my-operating-system&gt;
</span></span><span style=display:flex><span>  purpose: reconcile
</span></span><span style=display:flex><span>  reloadConfigFilePath: /var/lib/cloud-config-downloader/cloud-config
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - name: docker.service
</span></span><span style=display:flex><span>    dropIns:
</span></span><span style=display:flex><span>    - name: 10-docker-opts.conf
</span></span><span style=display:flex><span>      content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>        [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Environment=&#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3&#34;</span>        
</span></span><span style=display:flex><span>  - name: docker-monitor.service
</span></span><span style=display:flex><span>    command: start
</span></span><span style=display:flex><span>    enable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Unit]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Description=Docker-monitor daemon
</span></span></span><span style=display:flex><span><span style=color:#a31515>      After=kubelet.service
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Install]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      WantedBy=multi-user.target
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Restart=always
</span></span></span><span style=display:flex><span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStart=/opt/bin/health-monitor docker</span>      
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>  - path: /var/lib/kubelet/ca.crt
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    encoding: b64
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: default-token-5dtjz
</span></span><span style=display:flex><span>        dataKey: token
</span></span><span style=display:flex><span>  - path: /etc/sysctl.d/99-k8s-general.conf
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      inline:
</span></span><span style=display:flex><span>        data: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          # A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users
</span></span></span><span style=display:flex><span><span style=color:#a31515>          # See https://github.com/kubernetes/kops/issues/1340
</span></span></span><span style=display:flex><span><span style=color:#a31515>          vm.max_map_count = 135217728</span>          
</span></span></code></pre></div><p>In order to support a new operating system, you need to write a controller that watches all <code>OperatingSystemConfig</code>s with <code>.spec.type=&lt;my-operating-system></code>.
For those it shall generate a configuration blob that fits to your operating system.
For example, a CoreOS controller might generate a <a href=https://coreos.com/os/docs/latest/cloud-config.html>CoreOS cloud-config</a> or <a href=https://coreos.com/ignition/docs/latest/what-is-ignition.html>Ignition</a>, SLES might generate <a href=https://cloudinit.readthedocs.io/en/latest/>cloud-init</a>, and others might simply generate a bash script translating the <code>.spec.units</code> into <code>systemd</code> units, and <code>.spec.files</code> into real files on the disk.</p><p><code>OperatingSystemConfig</code>s can have two purposes which can be used (or ignored) by the extension controllers: either <code>provision</code> or <code>reconcile</code>.</p><ul><li>The <code>provision</code> purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider&rsquo;s API) when creating new VMs. It contains the <code>downloader</code> unit.</li><li>The <code>reconcile</code> purpose contains the &ldquo;original&rdquo; user-data (that is then stored in <code>Secret</code>s in the shoot&rsquo;s <code>kube-system</code> namespace (see step 1). This is downloaded and applies late (see step 5).</li></ul><p>As described above, the &ldquo;original&rdquo; user-data must be re-applicable to allow in-place updates.
The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is <code>/usr/bin/coreos-cloudinit --from-file=&lt;path></code>, whereas SLES would run <code>cloud-init --file &lt;path> single -n write_files --frequency=once</code>).
Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data.
As visible in the mentioned examples, the command requires a path to the user-data file.
Gardener will provide the path to the file in the <code>OperatingSystemConfig</code>s <code>.spec.reloadConfigFilePath</code> field (only if <code>.spec.purpose=reconcile</code>).
As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the <code>.status.units[]</code> list (see the below example). The same logic applies during the very first application of the whole configuration.</p><p>After generation, extension controllers are asked to store their OS config inside a <code>Secret</code> (as it might contain confidential data) in the same namespace.
The secret&rsquo;s <code>.data</code> could look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: osc-result-pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: OperatingSystemConfig
</span></span><span style=display:flex><span>    name: pool-01-original
</span></span><span style=display:flex><span>    uid: 99c0c5ca-19b9-11e9-9ebd-d67077b40f82
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  cloud_config: base64(generated-user-data)
</span></span></code></pre></div><p>Finally, the secret&rsquo;s metadata, the OS-specific command to re-apply the configuration, and the list of <code>systemd</code> units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the <code>OperatingSystemConfig</code>&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/coreos-cloudinit --from-file=/var/lib/cloud-config-downloader/cloud-config
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Successfully generated cloud config
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2019-01-23T07:45:23Z&#34;</span>
</span></span><span style=display:flex><span>    progress: 100
</span></span><span style=display:flex><span>    state: Succeeded
</span></span><span style=display:flex><span>    type: Reconcile
</span></span><span style=display:flex><span>  observedGeneration: 5
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span></code></pre></div><p>(The <code>.status.command</code> field is optional and must only be provided if <code>.spec.reloadConfigFilePath</code> exists).</p><p>Once the <code>.status</code> indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.</p><h2 id=cri-support>CRI Support</h2><p>Gardener supports specifying a Container Runtime Interface (CRI) configuration in the <code>OperatingSystemConfig</code> resource. If the <code>.spec.cri</code> section exists, then the <code>name</code> property is mandatory. The only supported values for <code>cri.name</code> at the moment are: <code>containerd</code> and <code>docker</code>, which uses the in-tree dockershim.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: &lt;my-operating-system&gt;
</span></span><span style=display:flex><span>  purpose: reconcile
</span></span><span style=display:flex><span>  reloadConfigFilePath: /var/lib/cloud-config-downloader/cloud-config
</span></span><span style=display:flex><span>  cri:
</span></span><span style=display:flex><span>    name: containerd
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To support ContainerD, an OS extension must satisfy the following criteria:</p><ol><li>The operating system must have built-in <a href=https://containerd.io/>ContainerD</a> and the <a href=https://github.com/projectatomic/containerd/blob/master/docs/cli.md/>Client CLI</a>.</li><li>ContainerD must listen on its default socket path: <code>unix:///run/containerd/containerd.sock</code></li><li>ContainerD must be configured to work with the default configuration file in: <code>/etc/containerd/config.toml</code> (Created by Gardener).</li></ol><p>If CRI configurations are not supported, it is recommended to create a validating webhook running in the garden cluster that prevents specifying the <code>.spec.providers.workers[].cri</code> section in the <code>Shoot</code> objects.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_operatingsystemconfig.go><code>OperatingSystemConfig</code> API (Golang Specification)</a></li><li><a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/downloader/templates/scripts/download-cloud-config.tpl.sh><code>downloader</code> Script</a> (fetching the &ldquo;original&rdquo; user-data and the execution script)</li><li><a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original/components>Original User-Data Templates</a></li><li><a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/executor/templates/scripts/execute-cloud-config.tpl.sh>Execution Script</a> (applying the &ldquo;original&rdquo; user-data)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5809410418b357cb13756988bdf0a8b8>1.5.23 - Overview</h1><h1 id=extensibility-overview>Extensibility Overview</h1><p>Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself.
But as the project grew, it got more and more difficult to add new providers and maintain the existing code base.
As a consequence and in order to become agile and flexible again, we proposed <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> (Gardener Enhancement Proposal).
The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with <a href=https://github.com/kubernetes/enhancements/issues/88>out-of-tree cloud providers</a> or with <a href=https://github.com/kubernetes/community/pull/1258>CSI volume plugins</a>).</p><h2 id=basic-concepts>Basic Concepts</h2><p>Gardener keeps running in the &ldquo;garden cluster&rdquo; and implements the core logic of shoot cluster reconciliation / deletion.
Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters.
As usual, we try to use Kubernetes wherever applicable.
We rely on Kubernetes extension concepts in order to enable extensibility for Gardener.
The main ideas of GEP-1 are the following:</p><ol><li><p>During the shoot reconciliation process, Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the <code>.spec</code>) and report whether everything went well or errors occurred in the CRD&rsquo;s <code>.status</code> field.</p></li><li><p>Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.</p></li></ol><p><strong>Example 1</strong>:</p><p>Gardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.).
It writes the following CRD into the seed cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureConfig
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      vpc:
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>      internal:
</span></span><span style=display:flex><span>      - 10.250.112.0/22
</span></span><span style=display:flex><span>      public:
</span></span><span style=display:flex><span>      - 10.250.96.0/22
</span></span><span style=display:flex><span>      workers:
</span></span><span style=display:flex><span>      - 10.250.0.0/19
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-west-1a
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    apiserver: api.aws-01.core.example.com
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-aws-credentials
</span></span><span style=display:flex><span>  sshPublicKey: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    </span>    base64(key)
</span></span></code></pre></div><p>Please note that the <code>.spec.providerConfig</code> is a raw blob and not evaluated or known in any way by Gardener.
Instead, it was specified by the user (in the <code>Shoot</code> resource) and just &ldquo;forwarded&rdquo; to the extension controller.
Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure.
It reports in the <code>.status</code> field the result:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  providerStatus:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    vpc:
</span></span><span style=display:flex><span>      id: vpc-1234
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - id: subnet-acbd1234
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>        zone: eu-west-1
</span></span><span style=display:flex><span>      securityGroups:
</span></span><span style=display:flex><span>      - id: sg-xyz12345
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>    iam:
</span></span><span style=display:flex><span>      nodesRoleARN: &lt;some-arn&gt;
</span></span><span style=display:flex><span>      instanceProfileName: foo
</span></span><span style=display:flex><span>    ec2:
</span></span><span style=display:flex><span>      keyName: bar
</span></span></code></pre></div><p>Gardener waits until the <code>.status.lastOperation</code> / <code>.status.lastError</code> indicates that the operation reached a final state and either continuous with the next step, or stops and reports the potential error.
The extension-specific output in <code>.status.providerStatus</code> is - similar to <code>.spec.providerConfig</code> - not evaluated, and simply forwarded to CRDs in subsequent steps.</p><p><strong>Example 2</strong>:</p><p>Gardener deploys the control plane components into the seed cluster, e.g. the <code>kube-controller-manager</code> deployment with the following flags:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - command:
</span></span><span style=display:flex><span>        - /usr/local/bin/kube-controller-manager
</span></span><span style=display:flex><span>        - --allocate-node-cidrs=true
</span></span><span style=display:flex><span>        - --attach-detach-reconcile-sync-period=1m0s
</span></span><span style=display:flex><span>        - --controllers=*,bootstrapsigner,tokencleaner
</span></span><span style=display:flex><span>        - --cluster-cidr=100.96.0.0/11
</span></span><span style=display:flex><span>        - --cluster-name=shoot--core--aws-01
</span></span><span style=display:flex><span>        - --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt
</span></span><span style=display:flex><span>        - --cluster-signing-key-file=/srv/kubernetes/ca/ca.key
</span></span><span style=display:flex><span>        - --concurrent-deployment-syncs=10
</span></span><span style=display:flex><span>        - --concurrent-replicaset-syncs=10
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>The AWS controller requires some additional flags in order to make the cluster functional.
It needs to provide a Kubernetes cloud-config and also some cloud-specific flags.
Consequently, it registers a <code>MutatingWebhookConfiguration</code> on <code>Deployment</code>s and adds these flags to the container:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>        - --cloud-provider=external
</span></span><span style=display:flex><span>        - --external-cloud-volume-plugin=aws
</span></span><span style=display:flex><span>        - --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf
</span></span></code></pre></div><p>Of course, it would have needed to create a <code>ConfigMap</code> containing the cloud config and to add the proper <code>volume</code> and <code>volumeMounts</code> to the manifest as well.</p><p>(Please note for this special example: The Kubernetes community is also working on making the <code>kube-controller-manager</code> provider-independent.
However, there will most probably be still components other than the <code>kube-controller-manager</code> which need to be adapted by extensions.)</p><p>If you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts, please read <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a>.
We are truly looking forward to your feedback!</p><h2 id=current-status>Current Status</h2><p>Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the <a href=https://github.com/gardener/gardener/tree/master/extensions#known-extension-implementations>Gardener Extensions Library</a> repo.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d20ec57f1c788e4c86ed68fe624544eb>1.5.24 - Project Roles</h1><h1 id=extending-project-roles>Extending Project Roles</h1><p>The <code>Project</code> resource allows to specify a list of roles for every member (<code>.spec.members[*].roles</code>).
There are a few standard roles defined by Gardener itself.
Please consult <a href=/docs/gardener/usage/projects/>Projects</a> for further information.</p><p>However, extension controllers running in the garden cluster may also create <code>CustomResourceDefinition</code>s that project members might be able to CRUD.
For this purpose, Gardener also allows to specify extension roles.</p><p>An extension role is prefixed with <code>extension:</code>, e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Project
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: alice.doe@example.com
</span></span><span style=display:flex><span>    role: admin
</span></span><span style=display:flex><span>    roles:
</span></span><span style=display:flex><span>    - owner
</span></span><span style=display:flex><span>    - extension:foo
</span></span></code></pre></div><p>The project controller will, for every extension role, create a <code>ClusterRole</code> with name <code>gardener.cloud:extension:project:&lt;projectName>:&lt;roleName></code>, i.e., for the above example: <code>gardener.cloud:extension:project:dev:foo</code>.
This <code>ClusterRole</code> aggregates other <code>ClusterRole</code>s that are labeled with <code>rbac.gardener.cloud/aggregate-to-extension-role=foo</code> which might be created by extension controllers.</p><p>An extension that might want to contribute to the core <code>admin</code> or <code>viewer</code> roles can use the labels <code>rbac.gardener.cloud/aggregate-to-project-member=true</code> or <code>rbac.gardener.cloud/aggregate-to-project-viewer=true</code>, respectively.</p><p>Please note that the names of the extension roles are restricted to 20 characters!</p><p>Moreover, the project controller will also create a corresponding <code>RoleBinding</code> with the same name in the project namespace.
It will automatically assign all members that are assigned to this extension role.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-45f5e5c8792c123831d7123647c5db0c>1.5.25 - Provider Local</h1><h1 id=local-provider-extension>Local Provider Extension</h1><p>The &ldquo;local provider&rdquo; extension is used to allow the usage of seed and shoot clusters which run entirely locally without any real infrastructure or cloud provider involved.
It implements Gardener&rsquo;s extension contract (<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a>) and thus comprises several controllers and webhooks acting on resources in seed and shoot clusters.</p><p>The code is maintained in <a href=https://github.com/gardener/gardener/tree/master/pkg/provider-local><code>pkg/provider-local</code></a>.</p><h2 id=motivation>Motivation</h2><p>The motivation for maintaining such extension is the following:</p><ul><li>🛡 Output Qualification: Run fast and cost-efficient end-to-end tests, locally and in CI systems (increased confidence ⛑ before merging pull requests)</li><li>⚙️ Development Experience: Develop Gardener entirely on a local machine without any external resources involved (improved costs 💰 and productivity 🚀)</li><li>🤝 Open Source: Quick and easy setup for a first evaluation of Gardener and a good basis for first contributions</li></ul><h2 id=current-limitations>Current Limitations</h2><p>The following enlists the current limitations of the implementation.
Please note that all of them are not technical limitations/blockers, but simply advanced scenarios that we haven&rsquo;t had invested yet into.</p><ol><li><p>Shoot clusters can only have multiple nodes, but inter-pod communication for pods on different nodes does not work.</p><p><em>We are using the <a href=https://github.com/gardener/gardener-extension-networking-calico/><code>networking-calico</code></a> extension for the CNI plugin in shoot clusters, however, it doesn&rsquo;t seem to be configured correctly yet to support this scenario.</em></p></li><li><p>No owner TXT <code>DNSRecord</code>s (hence, no <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/17-shoot-control-plane-migration-bad-case.md>&ldquo;bad-case&rdquo; control plane migration</a>).</p><p><em>In order to realize DNS (see the <a href=#implementation-details>Implementation Details</a> section below), the <code>/etc/hosts</code> file is manipulated. This does not work for TXT records. In the future, we could look into using <a href=https://coredns.io/>CoreDNS</a> instead.</em></p></li><li><p>No load balancers for Shoot clusters.</p><p>_We have not yet developed a <code>cloud-controller-manager</code> which could reconcile load balancer <code>Service</code>s in the shoot cluster.</p></li><li><p>In case a seed cluster with multiple availability zones, i.e. multiple entries in <code>.spec.provider.zones</code>, is used in conjunction with a single-zone shoot control plane, i.e. a shoot cluster without <code>.spec.controlPlane.highAvailability</code> or with <code>.spec.controlPlane.highAvailability.failureTolerance.type</code> set to <code>node</code>, the local address of the API server endpoint needs to be determined manually or via the in-cluster <code>coredns</code>.</p><p><em>As the different istio ingress gateway loadbalancers have individual external IP addresses, single-zone shoot control planes can end up in a random availability zone. Having the local host use the <code>coredns</code> in the cluster as name resolver would form a name resolution cycle. The tests mitigate the issue by adapting the DNS configuration inside the affected test.</em></p></li></ol><h2 id=managedseeds><code>ManagedSeed</code>s</h2><p>It is possible to deploy <a href=/docs/gardener/usage/managed_seed/><code>ManagedSeed</code>s</a> with <code>provider-local</code> by first creating a <a href=https://github.com/gardener/gardener/blob/master/example/provider-local/managedseeds/shoot-managedseed.yaml><code>Shoot</code> in the <code>garden</code> namespace</a> and then creating a referencing <a href=https://github.com/gardener/gardener/blob/master/example/provider-local/managedseeds/managedseed.yaml><code>ManagedSeed</code> object</a>.</p><blockquote><p>Please note that this is only supported by the <a href=/docs/gardener/deployment/getting_started_locally/><code>Skaffold</code>-based setup</a>.</p></blockquote><p>The corresponding e2e test can be run via:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/test-e2e-local.sh --label-filter <span style=color:#a31515>&#34;ManagedSeed&#34;</span>
</span></span></code></pre></div><h3 id=implementation-details>Implementation Details</h3><p>The images locally built by <code>Skaffold</code> for the Gardener components which are deployed to this shoot cluster are managed by a container registry in the <code>registry</code> namespace in the kind cluster.
<code>provider-local</code> configures this registry as mirror for the shoot by mutating the <code>OperatingSystemConfig</code> and using the <a href=/docs/gardener/usage/custom-containerd-config/>default contract for extending the <code>containerd</code> configuration</a>.</p><p>In order to bootstrap a seed cluster, the <code>gardenlet</code> deploys <code>PersistentVolumeClaim</code>s and <code>Service</code>s of type <code>LoadBalancer</code>.
While storage is supported in shoot clusters by using the <a href=https://github.com/rancher/local-path-provisioner><code>local-path-provisioner</code></a>, load balancers are not supported yet.
However, <code>provider-local</code> runs a <code>Service</code> controller which specifically reconciles the seed-related <code>Service</code>s of type <code>LoadBalancer</code>.
This way, they get an IP and <code>gardenlet</code> can finish its bootstrapping process.
Note that these IPs are not reachable, however for the sake of developing <code>ManagedSeed</code>s this is sufficient for now.</p><p>Also, please note that the <code>provider-local</code> extension only gets deployed because of the <code>Always</code> deployment policy in its corresponding <code>ControllerRegistration</code> and because the DNS provider type of the seed is set to <code>local</code>.</p><h2 id=implementation-details-1>Implementation Details</h2><p>This section contains information about how the respective controllers and webhooks in <code>provider-local</code> are implemented and what their purpose is.</p><h3 id=bootstrapping>Bootstrapping</h3><p>The Helm chart of the <code>provider-local</code> extension defined in its <a href=/docs/gardener/extensions/controllerregistration/><code>ControllerDeployment</code></a> contains a special deployment for a <a href=https://coredns.io/>CoreDNS</a> instance in a <code>gardener-extension-provider-local-coredns</code> namespace in the seed cluster.</p><p>This CoreDNS instance is responsible for enabling the components running in the shoot clusters to be able to resolve the DNS names when they communicate with their <code>kube-apiserver</code>s.</p><p>It contains a static configuration to resolve the DNS names based on <code>local.gardener.cloud</code> to either the <code>istio-ingressgateway.istio-ingress.svc</code> or the <code>kube-apiserver.&lt;shoot-namespace>.svc</code> addresses (depending on whether the <code>--apiserver-sni-enabled</code> flag is set to <code>true</code> or <code>false</code>).</p><h3 id=controllers>Controllers</h3><p>There are controllers for all resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group except for <code>BackupBucket</code> and <code>BackupEntry</code>s.</p><h4 id=controlplane><code>ControlPlane</code></h4><p>This controller is deploying the <a href=https://github.com/rancher/local-path-provisioner>local-path-provisioner</a> as well as a related <code>StorageClass</code> in order to support <code>PersistentVolumeClaim</code>s in the local shoot cluster.
Additionally, it creates a few (currently unused) dummy secrets (CA, server and client certificate, basic auth credentials) for the sake of testing the secrets manager integration in the extensions library.</p><h4 id=dnsrecord><code>DNSRecord</code></h4><p>This controller manipulates the <code>/etc/hosts</code> file and adds a new line for each <code>DNSRecord</code> it observes.
This enables accessing the shoot clusters from the respective machine, however, it also requires to run the extension with elevated privileges (<code>sudo</code>).</p><p>The <code>/etc/hosts</code> would be extended as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span># Begin of gardener-extension-provider-local section
</span></span><span style=display:flex><span>10.84.23.24 api.local.local.external.local.gardener.cloud
</span></span><span style=display:flex><span>10.84.23.24 api.local.local.internal.local.gardener.cloud
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span># End of gardener-extension-provider-local section
</span></span></code></pre></div><p>In addition to that, the controller also adapts the cluster internal DNS configuration by extending the <code>coredns</code> configuration for every observed <code>DNSRecord</code>. It will add two corresponding entries in the custom DNS configuration per shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  api.local.local.external.local.gardener.cloud.override: |
</span></span><span style=display:flex><span>    rewrite stop name regex api.local.local.external.local.gardener.cloud istio-ingressgateway.istio-ingress.svc.cluster.local answer auto
</span></span><span style=display:flex><span>  api.local.local.internal.local.gardener.cloud.override: |
</span></span><span style=display:flex><span>    rewrite stop name regex api.local.local.internal.local.gardener.cloud istio-ingressgateway.istio-ingress.svc.cluster.local answer auto
</span></span></code></pre></div><h4 id=infrastructure><code>Infrastructure</code></h4><p>This controller generates a <code>NetworkPolicy</code> which allows the control plane pods (like <code>kube-apiserver</code>) to communicate with the worker machine pods (see <a href=#worker><code>Worker</code> section</a>)).</p><h4 id=network><code>Network</code></h4><p>This controller is not implemented anymore. In the initial version of <code>provider-local</code>, there was a <code>Network</code> controller deploying <a href=https://github.com/kubernetes-sigs/kind/blob/main/images/kindnetd/README.md>kindnetd</a> (see <a href=https://github.com/gardener/gardener/tree/v1.44.1/pkg/provider-local/controller/network>release v1.44.1</a>).
However, we decided to drop it because this setup prevented us from using <code>NetworkPolicy</code>s (kindnetd does not ship a <code>NetworkPolicy</code> controller).
In addition, we had issues with shoot clusters having more than one node (hence, we couldn&rsquo;t support rolling updates, see <a href=https://github.com/gardener/gardener/pull/5666/commits/491b3cd16e40e5c20ef02367fda93a34ff9465eb>PR #5666</a>).</p><h4 id=operatingsystemconfig><code>OperatingSystemConfig</code></h4><p>This controller leverages the standard <a href=https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/operatingsystemconfig/oscommon><code>oscommon</code> library</a> in order to render a simple cloud-init template which can later be executed by the shoot worker nodes.</p><p>The shoot worker nodes are <code>Pod</code>s with a container based on the <code>kindest/node</code> image. This is maintained in the <a href=https://github.com/gardener/machine-controller-manager-provider-local/tree/master/node>gardener/machine-controller-manager-provider-local repository</a> and has a special <code>run-userdata</code> systemd service which executes the cloud-init generated earlier by the <code>OperatingSystemConfig</code> controller.</p><h4 id=worker><code>Worker</code></h4><p>This controller leverages the standard <a href=https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/worker/genericactuator>generic <code>Worker</code> actuator</a> in order to deploy the <a href=https://github.com/gardener/machine-controller-manager><code>machine-controller-manager</code></a> as well as the <a href=https://github.com/gardener/machine-controller-manager-provider-local><code>machine-controller-manager-provider-local</code></a>.</p><p>Additionally, it generates the <a href=https://github.com/gardener/machine-controller-manager-provider-local/blob/master/kubernetes/machine-class.yaml><code>MachineClass</code>es</a> and the <code>MachineDeployment</code>s based on the specification of the <code>Worker</code> resources.</p><h4 id=ingress><code>Ingress</code></h4><p>Gardenlet creates a wildcard DNS record for the Seed&rsquo;s ingress domain pointing to the <code>nginx-ingress-controller</code>&rsquo;s LoadBalancer.
This domain is commonly used by all <code>Ingress</code> objects created in the Seed for Seed and Shoot components.
However, provider-local implements the <code>DNSRecord</code> extension API by writing the DNS record to <code>/etc/hosts</code>, which doesn&rsquo;t support wildcard entries.
To make <code>Ingress</code> domains resolvable on the host, this controller reconciles all <code>Ingresses</code> and creates <code>DNSRecords</code> of type <code>local</code> for each host included in <code>spec.rules</code>.</p><h4 id=service><code>Service</code></h4><p>This controller reconciles <code>Services</code> of type <code>LoadBalancer</code> in the local <code>Seed</code> cluster.
Since the local Kubernetes clusters used as Seed clusters typically don&rsquo;t support such services, this controller sets the <code>.status.ingress.loadBalancer.ip[0]</code> to the IP of the host.
It makes important LoadBalancer Services (e.g. <code>istio-ingress/istio-ingressgateway</code> and <code>garden/nginx-ingress-controller</code>) available to the host by setting <code>spec.ports[].nodePort</code> to well-known ports that are mapped to <code>hostPorts</code> in the kind cluster configuration.</p><p>If the <code>--apiserver-sni-enabled</code> flag is set to <code>true</code> (default), <code>istio-ingress/istio-ingressgateway</code> is set to be exposed on <code>nodePort</code> <code>30433</code> by this controller. Otherwise, the <code>kube-apiserver</code> <code>Service</code> in the shoot namespaces in the seed cluster needs to be patched to be exposed on <code>30443</code> by the <a href=#control-plane-exposure>Control Plane Exposure Webhook</a>.</p><p>In case the seed has multiple availability zones (<code>.spec.provider.zones</code>) and it uses SNI, the different zone-specific <code>istio-ingressgateway</code> loadbalancers are exposed via different IP addresses. Per default, IP addresses <code>127.0.0.10</code>, <code>127.0.0.11</code>, and <code>127.0.0.12</code> are used for the zones <code>0</code>, <code>1</code>, and <code>2</code> respectively.</p><h4 id=etcd-backups>ETCD Backups</h4><p>This controller reconciles the <code>BackuBucket</code> and <code>BackupEntry</code> of the shoot allowing the <code>etcd-backup-restore</code> to create and copy backups using the <code>local</code> provider functionality. The backups are stored on the host file system. This is achieved by mounting that directory to the <code>etcd-backup-restore</code> container.</p><h4 id=extension-seed>Extension Seed</h4><p>This controller reconciles <code>Extensions</code> of type <code>local-ext-seed</code>. It creates a single <code>serviceaccount</code> named <code>local-ext-seed</code> in the shoot&rsquo;s namespace in the seed. The extension is reconciled before the <code>kube-apiserver</code>. More on extension lifecycle strategies can be read in <a href=/docs/gardener/extensions/controllerregistration/#extension-lifecycle>Registering Extension Controllers</a>.</p><h4 id=extension-shoot>Extension Shoot</h4><p>This controller reconciles <code>Extensions</code> of type <code>local-ext-shoot</code>. It creates a single <code>serviceaccount</code> named <code>local-ext-shoot</code> in the <code>kube-system</code> namespace of the shoot. The extension is reconciled after the <code>kube-apiserver</code>. More on extension lifecycle strategies can be read <a href=/docs/gardener/extensions/controllerregistration/#extension-lifecycle>Registering Extension Controllers</a>.</p><h4 id=health-checks>Health Checks</h4><p>The health check controller leverages the <a href=/docs/gardener/extensions/healthcheck-library/>health check library</a> in order to:</p><ul><li>check the health of the <code>ManagedResource/extension-controlplane-shoot-webhooks</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>ControlPlane</code> resource.</li><li>check the health of the <code>ManagedResource/extension-networking-local</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>Network</code> resource.</li><li>check the health of the <code>ManagedResource/extension-worker-mcm-shoot</code> and populate the <code>SystemComponentsHealthy</code> condition in the <code>Worker</code> resource.</li><li>check the health of the <code>Deployment/machine-controller-manager</code> and populate the <code>ControlPlaneHealthy</code> condition in the <code>Worker</code> resource.</li><li>check the health of the <code>Node</code>s and populate the <code>EveryNodeReady</code> condition in the <code>Worker</code> resource.</li></ul><h3 id=webhooks>Webhooks</h3><h4 id=control-plane>Control Plane</h4><p>This webhook reacts on the <code>OperatingSystemConfig</code> containing the configuration of the kubelet and sets the <code>failSwapOn</code> to <code>false</code> (independent of what is configured in the <code>Shoot</code> spec) (<a href=https://github.com/kubernetes-sigs/kind/blob/b6bc112522651d98c81823df56b7afa511459a3b/site/content/docs/design/node-image.md#design>ref</a>).</p><h4 id=control-plane-exposure>Control Plane Exposure</h4><p>This webhook reacts on the <code>kube-apiserver</code> <code>Service</code> in shoot namespaces in the seed in case the gardenlet&rsquo;s <code>APIServerSNI</code> feature gate is disabled.
It sets the <code>nodePort</code> to <code>30443</code> to enable communication from the host (this requires a port mapping to work when creating the local cluster).</p><h4 id=dns-config>DNS Config</h4><p>This webhook reacts on events for the <code>dependency-watchdog-probe</code> <code>Deployment</code>, the <code>prometheus</code> <code>StatefulSet</code> as well as on events for <code>Pod</code>s created when the <code>machine-controller-manager</code> reconciles <code>Machine</code>s.
All these pods need to be able to resolve the DNS names for shoot clusters.
It sets the <code>.spec.dnsPolicy=None</code> and <code>.spec.dnsConfig.nameServers</code> to the cluster IP of the <code>coredns</code> <code>Service</code> created in the <code>gardener-extension-provider-local-coredns</code> namespaces so that these pods can resolve the DNS records for shoot clusters (see the <a href=#bootstrapping>Bootstrapping section</a> for more details).</p><h4 id=node>Node</h4><p>This webhook reacts on updates to <code>nodes/status</code> in both seed and shoot clusters and sets the <code>.status.{allocatable,capacity}.cpu="100"</code> and <code>.status.{allocatable,capacity}.memory="100Gi"</code> fields.</p><p>Background: Typically, the <code>.status.{capacity,allocatable}</code> values are determined by the resources configured for the Docker daemon (see for example the <a href=https://docs.docker.com/desktop/mac/#resources>docker Quick Start Guide</a> for Mac).
Since many of the <code>Pod</code>s deployed by Gardener have quite high <code>.spec.resources.requests</code>, the <code>Node</code>s easily get filled up and only a few <code>Pod</code>s can be scheduled (even if they barely consume any of their reserved resources).
In order to improve the user experience, on startup/leader election the provider-local extension submits an empty patch which triggers the &ldquo;node webhook&rdquo; (see the below section) for the seed cluster.
The webhook will increase the capacity of the <code>Node</code>s to allow all <code>Pod</code>s to be scheduled.
For the shoot clusters, this empty patch trigger is not needed since the <code>MutatingWebhookConfiguration</code> is reconciled by the <code>ControlPlane</code> controller and exists before the <code>Node</code> object gets registered.</p><h4 id=shoot>Shoot</h4><p>This webhook reacts on the <code>ConfigMap</code> used by the <code>kube-proxy</code> and sets the <code>maxPerCore</code> field to <code>0</code> since other values don&rsquo;t work well in conjunction with the <code>kindest/node</code> image which is used as base for the shoot worker machine pods (<a href=https://github.com/kubernetes-sigs/kind/blob/fa7d86470f4c0e924fc4c2e767ec8491c45f4304/pkg/cluster/internal/kubeadm/config.go#L283-L285>ref</a>).</p><h3 id=dns-configuration-for-multi-zonal-seeds>DNS Configuration for Multi-Zonal Seeds</h3><p>In case a seed cluster has multiple availability zones as specified in <code>.spec.provider.zones</code>, multiple istio ingress gateways are deployed, one per availability zone in addition to the default deployment. The result is that single-zone shoot control planes, i.e. shoot clusters with <code>.spec.controlPlane.highAvailability</code> set or with <code>.spec.controlPlane.highAvailability.failureTolerance.type</code> set to <code>node</code>, may be exposed via any of the zone-specific istio ingress gateways. Previously, the endpoints were statically mapped via <code>/etc/hosts</code>. Unfortunately, this is no longer possible due to the aforementioned dynamic in the endpoint selection.</p><p>For multi-zonal seed clusters, there is an additional configuration following <code>coredns</code>&rsquo;s <a href=https://github.com/coredns/coredns/tree/master/plugin/view>view plugin</a> mapping the external IP addresses of the zone-specific loadbalancers to the corresponding internal istio ingress gateway domain names. This configuration is only in place for requests from outside of the seed cluster. Those requests are currently being identified by the protocol. UDP requests are interpreted as originating from within the seed cluster while TCP requests are assumed to come from outside the cluster via the docker hostport mapping.</p><p>The corresponding test sets the DNS configuration accordingly so that the name resolution during the test use <code>coredns</code> in the cluster.</p><h2 id=future-work>Future Work</h2><p>Future work could mostly focus on resolving the above listed <a href=#limitations>limitations</a>, i.e.:</p><ul><li>Implement a <code>cloud-controller-manager</code> and deploy it via the <a href=#controlplane><code>ControlPlane</code> controller</a>.</li><li>Properly implement <code>.spec.machineTypes</code> in the <code>CloudProfile</code>s (i.e., configure <code>.spec.resources</code> properly for the created shoot worker machine pods).</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-874f35cd0bbeb8acb3c1033b641e3d43>1.5.26 - Reconcile Trigger</h1><h1 id=reconcile-trigger>Reconcile Trigger</h1><p>Gardener dictates the time of reconciliation for resources of the API group <code>extensions.gardener.cloud</code>.
It does that by annotating the respected resource with <code>gardener.cloud/operation=reconcile</code>.
Extension controllers shall react to this annotation and start reconciling the resource.
They have to remove this annotation as soon as they begin with their reconcile operation and maintain the <code>status</code> of the extension resource accordingly.</p><p>The reason for this behaviour is that it is possible to configure Gardener to reconcile only in the shoots&rsquo; maintenance time windows.
In order to avoid that, extension controllers reconcile outside of the shoot&rsquo;s maintenance time window we have introduced this contract.
This way extension controllers don&rsquo;t need to care about when the shoot maintenance time window happens.
Gardener keeps control and decides when the shoot shall be reconciled/updated.</p><p>Our <a href=https://github.com/gardener/gardener/blob/master/extensions>extension controller library</a> provides all the required utilities to conveniently implement this behaviour.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-df45cc83f21d97b2fdcceee7d9127056>1.5.27 - Referenced Resources</h1><h1 id=referenced-resources>Referenced Resources</h1><p>The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in the extension <code>providerConfig</code> and other Shoot sections, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: foobar
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: foobar.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: FooBarConfig
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>      secretRef: foobar-secret
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - name: foobar-secret
</span></span><span style=display:flex><span>    resourceRef:
</span></span><span style=display:flex><span>      apiVersion: v1
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: my-foobar-secret
</span></span></code></pre></div><p>Gardener expects to find these referenced resources in the project namespace (e.g. <code>garden-dev</code>) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener&rsquo;s own resources.</p><p>Extension controllers can resolve the references to these resources by accessing the Shoot via the <code>Cluster</code> resource. To properly read a referenced resources, extension controllers should use the utility function <code>GetObjectByReference</code> from the <code>extensions/pkg/controller</code> package, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    ref = &amp;autoscalingv1.CrossVersionObjectReference{
</span></span><span style=display:flex><span>        APIVersion: <span style=color:#a31515>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>        Kind:       <span style=color:#a31515>&#34;Secret&#34;</span>,
</span></span><span style=display:flex><span>        Name:       <span style=color:#a31515>&#34;foo&#34;</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    secret := &amp;corev1.Secret{}
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> err := controller.GetObjectByReference(ctx, client, ref, <span style=color:#a31515>&#34;shoot--test--foo&#34;</span>, secret); err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> err
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:green>// Use secret
</span></span></span><span style=display:flex><span><span style=color:green></span>    ...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0a10283207035fd65241bff1e92982ef>1.5.28 - Shoot Health Status Conditions</h1><h1 id=contributing-to-shoot-health-status-conditions>Contributing to Shoot Health Status Conditions</h1><p>Gardener checks regularly (every minute by default) the health status of all shoot clusters.
It categorizes its checks into four different types:</p><ul><li><code>APIServerAvailable</code>: This type indicates whether the shoot&rsquo;s kube-apiserver is available or not.</li><li><code>ControlPlaneHealthy</code>: This type indicates whether the core components of the Shoot controlplane (ETCD, KAPI, KCM..) are healthy.</li><li><code>EveryNodeReady</code>: This type indicates whether all <code>Node</code>s and all <code>Machine</code> objects report healthiness.</li><li><code>ObservabilityComponentsHealthy</code>: This type indicates whether the observability components of the Shoot control plane (Prometheus, Loki, Grafana..) are healthy.</li><li><code>SystemComponentsHealthy</code>: This type indicates whether all system components deployed to the <code>kube-system</code> namespace in the shoot do exist and are running fine.</li></ul><p>Every <code>Shoot</code> resource has a <code>status.conditions[]</code> list that contains the mentioned types, together with a <code>status</code> (<code>True</code>/<code>False</code>) and a descriptive message/explanation of the <code>status</code>.</p><p>Most extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster.
A prominent example for this is the <code>ControlPlane</code> controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane.
Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener&rsquo;s checks mentioned above.</p><h2 id=what-can-extensions-do-to-contribute-to-gardeners-health-checks>What can extensions do to contribute to Gardener&rsquo;s health checks?</h2><p>Every extension resource in Gardener&rsquo;s <code>extensions.gardener.cloud/v1alpha1</code> API group also has a <code>status.conditions[]</code> list (like the <code>Shoot</code>).
Extension controllers can write conditions to the resource they are acting on and use a type that also exists in the shoot&rsquo;s conditions.
One exception is that <code>APIServerAvailable</code> and <code>ObservabilityComponentsHealthy</code> can&rsquo;t be used, as Gardener clearly can identify the status of this condition and it doesn&rsquo;t make sense for extensions to try to contribute/modify it.</p><p>As an example for the <code>ControlPlane</code> controller, let&rsquo;s take a look at the following resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlane
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: control-plane
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: ControlPlaneHealthy
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    reason: DeploymentUnhealthy
</span></span><span style=display:flex><span>    message: &#39;Deployment cloud-controller-manager is unhealthy: condition &#34;Available&#34; has
</span></span><span style=display:flex><span>      invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment
</span></span><span style=display:flex><span>      does not have minimum availability.&#39;
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2014-05-25T12:44:27Z&#34;</span>
</span></span><span style=display:flex><span>  - type: ConfigComputedSuccessfully
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    reason: ConfigCreated
</span></span><span style=display:flex><span>    message: The cloud-provider-config has been successfully computed.
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2014-05-25T12:43:27Z&#34;</span>
</span></span></code></pre></div><p>The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy.
Also, it has written a second condition using a type that is unknown by Gardener.</p><p>Gardener will pick the list of conditions and recognize that there is one with a type <code>ControlPlaneHealthy</code>.
It will merge it with its own <code>ControlPlaneHealthy</code> condition and report it back to the <code>Shoot</code>&rsquo;s status:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    shoot.gardener.cloud/status: unhealthy
</span></span><span style=display:flex><span>  name: some-shoot
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: APIServerAvailable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    reason: HealthzRequestSucceeded
</span></span><span style=display:flex><span>    message: API server /healthz endpoint responded with success status code. [response_time:31ms]
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2014-05-23T08:26:52Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#34;2014-05-25T12:45:13Z&#34;</span>
</span></span><span style=display:flex><span>  - type: ControlPlaneHealthy
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    reason: ControlPlaneUnhealthyReport
</span></span><span style=display:flex><span>    message: &#39;Deployment cloud-controller-manager is unhealthy: condition &#34;Available&#34; has
</span></span><span style=display:flex><span>      invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment
</span></span><span style=display:flex><span>      does not have minimum availability.&#39;
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2014-05-25T12:45:13Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#34;2014-05-25T12:45:13Z&#34;</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing.
This can be accomplished using the <a href=/docs/gardener/extensions/healthcheck-library/>health check library for extensions</a>.</p><h2 id=error-codes>Error Codes</h2><p>The Gardener API includes some well-defined error codes, e.g., <code>ERR_INFRA_UNAUTHORIZED</code>, <code>ERR_INFRA_DEPENDENCIES</code>, etc.
Extension may set these error codes in the <code>.status.conditions[].codes[]</code> list in case it makes sense.
Gardener will pick them up and will similarly merge them into the <code>.status.conditions[].codes[]</code> list in the <code>Shoot</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: ControlPlaneHealthy
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    reason: DeploymentUnhealthy
</span></span><span style=display:flex><span>    message: &#39;Deployment cloud-controller-manager is unhealthy: condition &#34;Available&#34; has
</span></span><span style=display:flex><span>      invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment
</span></span><span style=display:flex><span>      does not have minimum availability.&#39;
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2014-05-25T12:44:27Z&#34;</span>
</span></span><span style=display:flex><span>    codes:
</span></span><span style=display:flex><span>    - ERR_INFRA_UNAUTHORIZED 
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a2e99bb1eae354d266924368862541f8>1.5.29 - Shoot Maintenance</h1><h1 id=shoot-maintenance>Shoot Maintenance</h1><p>There is a general <a href=/docs/gardener/usage/shoot_maintenance/>document about shoot maintenance</a> that you might want to read.
Here, we describe how you can influence certain operations that happen during a shoot maintenance.</p><h2 id=restart-control-plane-controllers>Restart Control Plane Controllers</h2><p>As outlined in the above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.</p><p>Extension controllers can extend the amount of pods being affected by these restarts.
If your Gardener extension manages pods of a shoot&rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart, please consider labeling it with <code>maintenance.gardener.cloud/restart=true</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-be099eb6107ce0525e229d5cfa5bf30b>1.5.30 - Shoot Webhooks</h1><h1 id=shoot-resource-customization-webhooks>Shoot Resource Customization Webhooks</h1><p>Gardener deploys several components/resources into the shoot cluster.
Some of these resources are essential (like the <code>kube-proxy</code>), others are optional addons (like the <code>kubernetes-dashboard</code> or the <code>nginx-ingress-controller</code>).
In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.</p><h2 id=whats-the-approach-to-implement-such-mutations>What&rsquo;s the approach to implement such mutations?</h2><p>Similar to how <a href=/docs/gardener/extensions/controlplane-webhooks/>control plane components in the seed</a> are modified, we are using <code>MutatingWebhookConfiguration</code>s to achieve the same for resources in the shoot.
Both the provider extension and the kube-apiserver of the shoot cluster are running in the same seed.
Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook, which makes such operations even faster.</p><h2 id=how-is-the-mutatingwebhookconfiguration-object-created-in-the-shoot>How is the <code>MutatingWebhookConfiguration</code> object created in the shoot?</h2><p>The preferred approach is to use a <code>ManagedResource</code> (see also <a href=/docs/gardener/extensions/managedresources/>Deploy Resources to the Shoot Cluster</a>) in the seed cluster.
This way the <code>gardener-resource-manager</code> ensures that end-users cannot delete/modify the webhook configuration.
The provider extension doesn&rsquo;t need to care about the same.</p><h2 id=what-else-is-needed>What else is needed?</h2><p>The shoot&rsquo;s kube-apiserver must be allowed to talk to the provider extension.
To achieve this, you need to create a <code>NetworkPolicy</code> in the shoot namespace.
Our <a href=https://github.com/gardener/gardener/blob/master/extensions>extension controller library</a> provides easy-to-use utilities and hooks to implement such a webhook.
Please find an exemplary implementation <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/webhook/shoot>here</a> and <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/566fe4dd588c93821bc9d22c452203867457c930/cmd/gardener-extension-provider-aws/app/app.go#L170-L174>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa66bdf689454ea90b099d33515cef79>1.5.31 - Worker</h1><h1 id=contract-worker-resource>Contract: <code>Worker</code> Resource</h1><p>While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account.
The Gardener project features a sub-project called <a href=https://github.com/gardener/machine-controller-manager>machine-controller-manager</a>.
This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as <code>Machine</code> objects inside a Kubernetes system.
This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.</p><h2 id=what-is-the-machine-controller-manager-doing-exactly>What is the machine-controller-manager doing exactly?</h2><p>Generally, there are provider-specific <code>MachineClass</code> objects (<code>AWSMachineClass</code>, <code>AzureMachineClass</code>, etc.; similar to <code>StorageClass</code>), and <code>MachineDeployment</code>, <code>MachineSet</code>, and <code>Machine</code> objects (similar to <code>Deployment</code>, <code>ReplicaSet</code>, and <code>Pod</code>).
A machine class describes <strong>where</strong> and <strong>how</strong> to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.), while a <code>Machine</code> results in an actual virtual machine.
You can read up <a href=https://github.com/gardener/machine-controller-manager>more information</a> in the machine-controller-manager&rsquo;s <a href=https://github.com/gardener/machine-controller-manager>repository</a>.</p><p>Before the introduction of the <code>Worker</code> extension resource, Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself.
Now, Gardener commissions an external, provider-specific controller to take over these tasks.</p><h2 id=what-needs-to-be-implemented-to-support-a-new-worker-provider>What needs to be implemented to support a new worker provider?</h2><p>As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Worker
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: bar
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: cloudprovider
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  infrastructureProviderStatus:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    ec2:
</span></span><span style=display:flex><span>      keyName: shoot--foo--bar-ssh-publickey
</span></span><span style=display:flex><span>    iam:
</span></span><span style=display:flex><span>      instanceProfiles:
</span></span><span style=display:flex><span>      - name: shoot--foo--bar-nodes
</span></span><span style=display:flex><span>        purpose: nodes
</span></span><span style=display:flex><span>      roles:
</span></span><span style=display:flex><span>      - arn: arn:aws:iam::0123456789:role/shoot--foo--bar-nodes
</span></span><span style=display:flex><span>        purpose: nodes
</span></span><span style=display:flex><span>    vpc:
</span></span><span style=display:flex><span>      id: vpc-0123456789
</span></span><span style=display:flex><span>      securityGroups:
</span></span><span style=display:flex><span>      - id: sg-1234567890
</span></span><span style=display:flex><span>        purpose: nodes
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - id: subnet-01234
</span></span><span style=display:flex><span>        purpose: nodes
</span></span><span style=display:flex><span>        zone: eu-west-1b
</span></span><span style=display:flex><span>      - id: subnet-56789
</span></span><span style=display:flex><span>        purpose: public
</span></span><span style=display:flex><span>        zone: eu-west-1b
</span></span><span style=display:flex><span>      - id: subnet-0123a
</span></span><span style=display:flex><span>        purpose: nodes
</span></span><span style=display:flex><span>        zone: eu-west-1c
</span></span><span style=display:flex><span>      - id: subnet-5678a
</span></span><span style=display:flex><span>        purpose: public
</span></span><span style=display:flex><span>        zone: eu-west-1c
</span></span><span style=display:flex><span>  pools:
</span></span><span style=display:flex><span>  - name: cpu-worker
</span></span><span style=display:flex><span>    minimum: 3
</span></span><span style=display:flex><span>    maximum: 5
</span></span><span style=display:flex><span>    maxSurge: 1
</span></span><span style=display:flex><span>    maxUnavailable: 0
</span></span><span style=display:flex><span>    machineType: m4.large
</span></span><span style=display:flex><span>    machineImage:
</span></span><span style=display:flex><span>      name: coreos
</span></span><span style=display:flex><span>      version: 1967.5.0
</span></span><span style=display:flex><span>    nodeTemplate:
</span></span><span style=display:flex><span>      capacity:
</span></span><span style=display:flex><span>        cpu: 2
</span></span><span style=display:flex><span>        gpu: 0
</span></span><span style=display:flex><span>        memory: 8Gi
</span></span><span style=display:flex><span>    labels:
</span></span><span style=display:flex><span>      node.kubernetes.io/role: node
</span></span><span style=display:flex><span>      worker.gardener.cloud/cri-name: containerd
</span></span><span style=display:flex><span>      worker.gardener.cloud/pool: cpu-worker
</span></span><span style=display:flex><span>      worker.gardener.cloud/system-components: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K
</span></span><span style=display:flex><span>    volume:
</span></span><span style=display:flex><span>      size: 20Gi
</span></span><span style=display:flex><span>      type: gp2
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-west-1b
</span></span><span style=display:flex><span>    - eu-west-1c
</span></span><span style=display:flex><span>    machineControllerManager:
</span></span><span style=display:flex><span>      drainTimeout: 10m
</span></span><span style=display:flex><span>      healthTimeout: 10m
</span></span><span style=display:flex><span>      creationTimeout: 10m
</span></span><span style=display:flex><span>      maxEvictRetries: 30
</span></span><span style=display:flex><span>      nodeConditions:
</span></span><span style=display:flex><span>      - ReadonlyFilesystem
</span></span><span style=display:flex><span>      - DiskPressure
</span></span><span style=display:flex><span>      - KernelDeadlock
</span></span></code></pre></div><p>The <code>.spec.secretRef</code> contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines.
Also, as you can see, Gardener copies the output of the infrastructure creation (<code>.spec.infrastructureProviderStatus</code>, see <a href=/docs/gardener/extensions/infrastructure/><code>Infrastructure</code> resource</a>), into the <code>.spec</code>.</p><p>In the <code>.spec.pools[]</code> field, the desired worker pools are listed.
In the above example, one pool with machine type <code>m4.large</code> and <code>min=3</code>, <code>max=5</code> machines shall be spread over two availability zones (<code>eu-west-1b</code>, <code>eu-west-1c</code>).
This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.</p><p>The <code>spec.pools[].labels</code> map contains all labels that should be added to all nodes of the corresponding worker pool.
Gardener configures kubelet&rsquo;s <code>--node-labels</code> flag to contain all labels that are mentioned here and allowed by the <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> admission plugin</a>.
This makes sure that kubelet adds all user-specified and gardener-managed labels to the new <code>Node</code> object when registering a new machine with the API server.
Nevertheless, this is only effective when bootstrapping new nodes.
The provider extension (respectively, machine-controller-manager) is still responsible for updating the labels of existing <code>Nodes</code> when the worker specification changes.</p><p>The <code>spec.pools[].nodeTemplate.capacity</code> field contains the resource information of the machine like <code>cpu</code>, <code>gpu</code>, and <code>memory</code>. This info is used by Cluster Autoscaler to generate <code>nodeTemplate</code> during scaling the <code>nodeGroup</code> from zero.</p><p>The <code>spec.pools[].machineControllerManager</code> field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34>fields</a> in MachineDeployment.</p><p>When seeing such a resource, your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster.
After that, it must compute the desired machine classes and the desired machine deployments.
Typically, one class maps to one deployment, and one class/deployment is created per availability zone.
Following this convention, the created resource would look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z1-3db65
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/purpose: machineclass
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  providerAccessKeyId: eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=
</span></span><span style=display:flex><span>  providerSecretAccessKey: eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkK
</span></span><span style=display:flex><span>  userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: AWSMachineClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z1-3db65
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ami: ami-0123456789 <span style=color:green># Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.</span>
</span></span><span style=display:flex><span>  blockDevices:
</span></span><span style=display:flex><span>  - ebs:
</span></span><span style=display:flex><span>      volumeSize: 20
</span></span><span style=display:flex><span>      volumeType: gp2
</span></span><span style=display:flex><span>  iam:
</span></span><span style=display:flex><span>    name: shoot--foo--bar-nodes
</span></span><span style=display:flex><span>  keyName: shoot--foo--bar-ssh-publickey
</span></span><span style=display:flex><span>  machineType: m4.large
</span></span><span style=display:flex><span>  networkInterfaces:
</span></span><span style=display:flex><span>  - securityGroupIDs:
</span></span><span style=display:flex><span>    - sg-1234567890
</span></span><span style=display:flex><span>    subnetID: subnet-01234
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: shoot--foo--bar-cpu-worker-z1-3db65
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  tags:
</span></span><span style=display:flex><span>    kubernetes.io/cluster/shoot--foo--bar: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubernetes.io/role/node: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 2
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: shoot--foo--bar-cpu-worker-z1-3db65
</span></span></code></pre></div><p>for the first availability zone <code>eu-west-1b</code>, and</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z2-5z6as
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/purpose: machineclass
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  providerAccessKeyId: eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=
</span></span><span style=display:flex><span>  providerSecretAccessKey: eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkK
</span></span><span style=display:flex><span>  userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: AWSMachineClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z2-5z6as
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ami: ami-0123456789 <span style=color:green># Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.</span>
</span></span><span style=display:flex><span>  blockDevices:
</span></span><span style=display:flex><span>  - ebs:
</span></span><span style=display:flex><span>      volumeSize: 20
</span></span><span style=display:flex><span>      volumeType: gp2
</span></span><span style=display:flex><span>  iam:
</span></span><span style=display:flex><span>    name: shoot--foo--bar-nodes
</span></span><span style=display:flex><span>  keyName: shoot--foo--bar-ssh-publickey
</span></span><span style=display:flex><span>  machineType: m4.large
</span></span><span style=display:flex><span>  networkInterfaces:
</span></span><span style=display:flex><span>  - securityGroupIDs:
</span></span><span style=display:flex><span>    - sg-1234567890
</span></span><span style=display:flex><span>    subnetID: subnet-0123a
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: shoot--foo--bar-cpu-worker-z2-5z6as
</span></span><span style=display:flex><span>    namespace: shoot--foo--bar
</span></span><span style=display:flex><span>  tags:
</span></span><span style=display:flex><span>    kubernetes.io/cluster/shoot--foo--bar: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubernetes.io/role/node: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: shoot--foo--bar-cpu-worker-z2-5z6as
</span></span></code></pre></div><p>for the second availability zone <code>eu-west-1c</code>.</p><p>Another convention is the 5-letter hash at the end of the machine class names.
Most controllers compute a checksum out of the specification of the machine class.
This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes.
In this case, a new checksum will be generated which results in the creation of a new machine class.
The <code>MachineDeployment</code>&rsquo;s machine class reference (<code>.spec.template.spec.class.name</code>) is updated, which triggers the rolling update process in the machine-controller-manager.
However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.</p><p>After the machine classes and machine deployments have been created, the machine-controller-manager will start talking to the provider&rsquo;s IaaS API and create the virtual machines.
Gardener makes sure that the content of the <code>userData</code> field that is used to bootstrap the machines contains the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster.
The <code>Worker</code> extension controller shall wait until all the created <code>MachineDeployment</code>s indicate healthiness/readiness before it ends the control loop.</p><h2 id=does-gardener-need-some-information-that-must-be-returned-back>Does Gardener need some information that must be returned back?</h2><p>Another important benefit of the machine-controller-manager&rsquo;s design principles (extending the Kubernetes API using CRDs) is that the <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a> can be used <strong>without</strong> any provider-specific implementation.
We have forked the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>upstream Kubernetes community&rsquo;s cluster-autoscaler</a> and extended it so that it understands the machine API.
Definitely, we will merge it back into the community&rsquo;s versions once it has been adapted properly.</p><p>Our cluster-autoscaler only needs to know the minimum and maximum number of replicas <strong>per</strong> <code>MachineDeployment</code> and is ready to act. Without knowing that, it needs to talk to the provider APIs (it just modifies the <code>.spec.replicas</code> field in the <code>MachineDeployment</code> object).
Gardener deploys this autoscaler if there is at least one worker pool that specifies <code>max>min</code>.
In order to know how it needs to configure it, the provider-specific <code>Worker</code> extension controller must expose which <code>MachineDeployment</code>s it has created and how the <code>min</code>/<code>max</code> numbers should look like.</p><p>Consequently, your controller should write this information into the <code>Worker</code> resource&rsquo;s <code>.status.machineDeployments</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Worker
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: worker
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  machineDeployments:
</span></span><span style=display:flex><span>  - name: shoot--foo--bar-cpu-worker-z1
</span></span><span style=display:flex><span>    minimum: 2
</span></span><span style=display:flex><span>    maximum: 3
</span></span><span style=display:flex><span>  - name: shoot--foo--bar-cpu-worker-z2
</span></span><span style=display:flex><span>    minimum: 1
</span></span><span style=display:flex><span>    maximum: 2
</span></span></code></pre></div><p>In order to support a new worker provider, you need to write a controller that watches all <code>Worker</code>s with <code>.spec.type=&lt;my-provider-name></code>.
You can take a look at the below referenced example implementation for the AWS provider.</p><h2 id=that-sounds-like-a-lot-that-needs-to-be-done-can-you-help-me>That sounds like a lot that needs to be done, can you help me?</h2><p>All of the described behaviour is mostly the same for every provider.
The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself.
You can take a look at our <a href=https://github.com/gardener/gardener/blob/master/extensions>extension library</a>, especially the <a href=https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/worker>worker controller</a> part where you will find a lot of utilities that you can use.
Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented.
Take a look at the <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/worker>AWS worker controller</a> for finding an example.</p><h2 id=non-provider-specific-information-required-for-worker-creation>Non-provider specific information required for worker creation</h2><p>All the providers require further information that is not provider specific but already part of the shoot resource.
One example for such information is whether the shoot is hibernated or not.
In this case, all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down.
You can take a look at the <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/worker>AWS worker controller</a> to see how it reads this information and how it is used.
As Gardener cannot know which information is required by providers, it simply mirrors the <code>Shoot</code>, <code>Seed</code>, and <code>CloudProfile</code> resources into the seed.
They are part of the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> extension resource</a> and can be used to extract information that is not part of the <code>Worker</code> resource itself.</p><h2 id=references-and-additional-resources>References and Additional Resources</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/pkg/apis/extensions/v1alpha1/types_worker.go><code>Worker</code> API (Golang Specification)</a></li><li><a href=https://github.com/gardener/gardener/blob/master/extensions>Extension Controller Library</a></li><li><a href=https://github.com/gardener/gardener/tree/master/extensions/pkg/controller/worker>Generic Worker Controller</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller/worker>Exemplary Implementation for the AWS Provider</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-62245990be65d8c8a7f72ab7834620e9>1.6 - Monitoring</h1><h1 id=monitoring>Monitoring</h1><h2 id=roles-of-the-different-prometheus-instances>Roles of the different Prometheus instances</h2><p><img src=/__resources/monitoring_06c124.png alt=monitoring></p><h3 id=prometheus>Prometheus</h3><p>Deployed in the <code>garden</code> namespace. Important scrape targets:</p><ul><li>cadvisor</li><li>node-exporter</li><li>kube-state-metrics</li></ul><p><strong>Purpose</strong>: Acts as a cache for other Prometheus instances. The metrics are kept for a short amount of time (~2 hours) due to the high cardinality. For example if another Prometheus needs access to cadvisor metrics it will query this Prometheus instead of the cadvisor. This also reduces load on the kubelets and API Server.</p><p>Some of the high cardinality metrics are aggregated with recording rules. These <em>pre-aggregated</em> metrics are scraped by the <a href=#aggregate-prometheus>Aggregate Prometheus</a>.</p><p>This Prometheus is not used for alerting.</p><h3 id=aggregate-prometheus>Aggregate Prometheus</h3><p>Deployed in the <code>garden</code> namespace. Important scrape targets:</p><ul><li>other prometheus instances</li><li>logging components</li></ul><p><strong>Purpose</strong>: Store pre-aggregated data from <a href=#prometheus>prometheus</a> and <a href=#shoot-prometheus>shoot prometheus</a>. An ingress exposes this Prometheus allowing it to be scraped from another cluster.</p><h3 id=seed-prometheus>Seed Prometheus</h3><p>Deployed in the <code>garden</code> namespace. Important scrape targets:</p><ul><li>pods in extension namespaces annotated with:</li></ul><pre tabindex=0><code>prometheus.io/scrape=true
prometheus.io/port=&lt;port&gt;
prometheus.io/name=&lt;name&gt;
</code></pre><ul><li>cadvisor metrics from pods in the garden and extension namespaces</li></ul><p>The job name label will be applied to all metrics from that service.
<strong>Purpose</strong>: Entrypoint for operators when debugging issues with extensions or other garden components.</p><h3 id=shoot-prometheus>Shoot Prometheus</h3><p>Deployed in the shoot control plane namespace. Important scrape targets:</p><ul><li>control plane components</li><li>shoot nodes (node-exporter)</li><li>blackbox-exporter used to measure <a href=/docs/gardener/monitoring/connectivity/>connectivity</a></li></ul><p><strong>Purpose</strong>: Monitor all relevant components belonging to a shoot cluster managed by Gardener. Shoot owners can view the metrics in Grafana dashboards and receive <a href=/docs/gardener/monitoring/user_alerts/>alerts</a> based on these metrics. Gardener operators will receive a different set of <a href=/docs/gardener/monitoring/operator_alerts/>alerts</a>. For alerting internals refer to <a href=/docs/gardener/monitoring/alerting/>this</a> document.</p><h2 id=collect-all-shoot-prometheus-with-remote-write>Collect all Shoot Prometheus with remote write</h2><p>An optional collection of all Shoot Prometheus metrics to a central prometheus (or cortex) instance is possible with the <code>monitoring.shoot</code> setting in <code>GardenletConfiguration</code>:</p><pre tabindex=0><code>monitoring:
  shoot:
    remoteWrite:
      url: https://remoteWriteUrl # remote write URL
      keep:# metrics that should be forwarded to the external write endpoint. If empty all metrics get forwarded
      - kube_pod_container_info
      queueConfig: | # queue_config of prometheus remote write as multiline string
        max_shards: 100
        batch_send_deadline: 20s
        min_backoff: 500ms
        max_backoff: 60s
    externalLabels: # add additional labels to metrics to identify it on the central instance
      additional: label
</code></pre><p>If basic auth is needed it can be set via secret in garden namespace (Gardener API Server). <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-remote-write.yaml>Example secret</a></p><h2 id=disable-gardener-monitoring>Disable Gardener Monitoring</h2><p>If you wish to disable metric collection for every shoot and roll your own then you can simply set.</p><pre tabindex=0><code>monitoring:
  shoot:
    enabled: false
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-954b7364e06db9899aa6264a5b0a9e91>1.6.1 - Alerting</h1><h1 id=alerting>Alerting</h1><p>Gardener uses <a href=https://prometheus.io/>Prometheus</a> to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an <a href=https://prometheus.io/docs/alerting/alertmanager/>Alertmanager</a>. The Alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:</p><ul><li><a href=#Alerting-for-Users>end-users/stakeholders/customers</a></li><li><a href=#Alerting-for-Operators>operators/administrators</a></li></ul><h1 id=alerting-for-users>Alerting for Users</h1><p>To receive email alerts as a user, set the following values in the shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  monitoring:
</span></span><span style=display:flex><span>    alerting:
</span></span><span style=display:flex><span>      emailReceivers:
</span></span><span style=display:flex><span>      - john.doe@example.com
</span></span></code></pre></div><p><code>emailReceivers</code> is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found in the <a href=/docs/gardener/monitoring/user_alerts/>User Alerts</a> topic.</p><h1 id=alerting-for-operators>Alerting for Operators</h1><p>Currently, Gardener supports two options for alerting:</p><ul><li><a href=#Email-Alerting>Email Alerting</a></li><li><a href=#External-Alertmanager>Sending Alerts to an External Alertmanager</a></li></ul><p>A list of operator alerts can be found in the <a href=/docs/gardener/monitoring/operator_alerts/>Operator Alerts</a> topic.</p><h2 id=email-alerting>Email Alerting</h2><p>Gardener provides the option to deploy an Alertmanager into each seed. This Alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the Alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values <code>alerting</code>. See <a href=/docs/gardener/usage/configuration/>Gardener Configuration and Usage</a> on how to configure the Gardener&rsquo;s SMTP secret. If the values are set, a secret with the label <code>gardener.cloud/role: alerting</code> will be created in the garden namespace of the garden cluster. This secret will be used by each Alertmanager in each seed.</p><h2 id=external-alertmanager>External Alertmanager</h2><p>The Alertmanager supports different kinds of <a href=https://prometheus.io/docs/alerting/configuration/>alerting configurations</a>. The Alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external Alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external Alertmanager. This external Alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this Alertmanager). To configure sending alerts to an external Alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: <code>gardener.cloud/role: alerting</code>. This secret needs to contain a URL to the external Alertmanager and information regarding authentication. Supported authentication types are:</p><ul><li>No Authentication (none)</li><li>Basic Authentication (basic)</li><li>Mutual TLS (certificate)</li></ul><h3 id=remote-alertmanager-examples>Remote Alertmanager Examples</h3><blockquote><p><strong>Note:</strong> The <code>url</code> value cannot be prepended with <code>http</code> or <code>https</code>.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># No Authentication</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/role: alerting
</span></span><span style=display:flex><span>  name: alerting-auth
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># No Authentication</span>
</span></span><span style=display:flex><span>  auth_type: base64(none)
</span></span><span style=display:flex><span>  url: base64(external.alertmanager.foo)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Basic Auth</span>
</span></span><span style=display:flex><span>  auth_type: base64(basic)
</span></span><span style=display:flex><span>  url: base64(extenal.alertmanager.foo)
</span></span><span style=display:flex><span>  username: base64(admin)
</span></span><span style=display:flex><span>  password: base64(password)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Mutual TLS</span>
</span></span><span style=display:flex><span>  auth_type: base64(certificate)
</span></span><span style=display:flex><span>  url: base64(external.alertmanager.foo)
</span></span><span style=display:flex><span>  ca.crt: base64(ca)
</span></span><span style=display:flex><span>  tls.crt: base64(certificate)
</span></span><span style=display:flex><span>  tls.key: base64(key)
</span></span><span style=display:flex><span>  insecure_skip_verify: base64(false)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Email Alerts (internal alertmanager)</span>
</span></span><span style=display:flex><span>  auth_type: base64(smtp)
</span></span><span style=display:flex><span>  auth_identity: base64(internal.alertmanager.auth_identity)
</span></span><span style=display:flex><span>  auth_password: base64(internal.alertmanager.auth_password)
</span></span><span style=display:flex><span>  auth_username: base64(internal.alertmanager.auth_username)
</span></span><span style=display:flex><span>  from: base64(internal.alertmanager.from)
</span></span><span style=display:flex><span>  smarthost: base64(internal.alertmanager.smarthost)
</span></span><span style=display:flex><span>  to: base64(internal.alertmanager.to)
</span></span><span style=display:flex><span>type: Opaque
</span></span></code></pre></div><h3 id=configuring-your-external-alertmanager>Configuring Your External Alertmanager</h3><p>Please refer to the <a href=https://prometheus.io/docs/alerting/alertmanager/>Alertmanager</a> documentation on how to configure an Alertmanager.</p><p>We recommend you use at least the following inhibition rules in your Alertmanager configuration to prevent excessive alerts:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>inhibit_rules:
</span></span><span style=display:flex><span><span style=color:green># Apply inhibition if the alert name is the same.</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    severity: critical
</span></span><span style=display:flex><span>  target_match:
</span></span><span style=display:flex><span>    severity: warning
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;alertname&#39;</span>, <span style=color:#a31515>&#39;service&#39;</span>, <span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Stop all alerts for type=shoot if there are VPN problems.</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    service: vpn
</span></span><span style=display:flex><span>  target_match_re:
</span></span><span style=display:flex><span>    type: shoot
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;type&#39;</span>, <span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Stop warning and critical alerts if there is a blocker</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    severity: blocker
</span></span><span style=display:flex><span>  target_match_re:
</span></span><span style=display:flex><span>    severity: ^(critical|warning)$
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    service: kube-apiserver
</span></span><span style=display:flex><span>  target_match_re:
</span></span><span style=display:flex><span>    service: nodes
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># If API server is down inhibit kube-state-metrics alerts.</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    service: kube-apiserver
</span></span><span style=display:flex><span>  target_match_re:
</span></span><span style=display:flex><span>    severity: info
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.</span>
</span></span><span style=display:flex><span>- source_match:
</span></span><span style=display:flex><span>    service: kube-state-metrics-shoot
</span></span><span style=display:flex><span>  target_match_re:
</span></span><span style=display:flex><span>    service: nodes
</span></span><span style=display:flex><span>  equal: [<span style=color:#a31515>&#39;cluster&#39;</span>]
</span></span></code></pre></div><p>Below is a graph visualizing the inhibition rules:</p><p><img src=/__resources/alertInhibitionGraph_ceaef0.png alt=inhibitionGraph></p></div><div class=td-content style=page-break-before:always><h1 id=pg-7c33abee93181fcc22c3692c1a9c8f2a>1.6.2 - Connectivity</h1><h1 id=connectivity>Connectivity</h1><h2 id=shoot-connectivity>Shoot Connectivity</h2><p>We measure the connectivity from the shoot to the API Server. This is done via the <code>blackbox exporter</code> which is deployed in the shoot&rsquo;s <code>kube-system</code> namespace. Prometheus will scrape the <code>blackbox exporter</code> and then the exporter will try to access the API Server. Metrics are exposed if the connection was successful or not. This can be seen in the <code>Kubernetes Control Plane Status</code> dashboard under the <code>API Server Connectivity</code> panel. The <code>shoot</code> line represents the connectivity from the shoot.</p><p><img src=/__resources/panel_393a41.png alt=image></p><h2 id=seed-connectivity>Seed Connectivity</h2><p>In addition to the shoot connectivity, we also measure the seed connectivity. This means trying to reach the API Server from the seed via the external fully qualified domain name of the API server. The connectivity is also displayed in the above panel as the <code>seed</code> line. Both <code>seed</code> and <code>shoot</code> connectivity are shown below.</p><p><img src=/__resources/connectivity_b79584.png alt=image></p></div><div class=td-content style=page-break-before:always><h1 id=pg-52cd1f12c4a525b9382c2ae0184ec4c3>1.6.3 - Operator Alerts</h1><h1 id=operator-alerts>Operator Alerts</h1><table><thead><tr><th>Alertname</th><th>Severity</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>ApiServerUnreachableViaKubernetesService</td><td>critical</td><td>shoot</td><td><code>The Api server has been unreachable for 15 minutes via the kubernetes service in the shoot.</code></td></tr><tr><td>KubeletTooManyOpenFileDescriptorsSeed</td><td>critical</td><td>seed</td><td><code>Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.</code></td></tr><tr><td>KubePersistentVolumeUsageCritical</td><td>critical</td><td>seed</td><td><code>The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf "%0.2f" $value }}% free.</code></td></tr><tr><td>KubePersistentVolumeFullInFourDays</td><td>warning</td><td>seed</td><td><code>Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf "%0.2f" $value }}% is available.</code></td></tr><tr><td>KubePodPendingControlPlane</td><td>warning</td><td>seed</td><td><code>Pod {{ $labels.pod }} is stuck in "Pending" state for more than 30 minutes.</code></td></tr><tr><td>KubePodNotReadyControlPlane</td><td>warning</td><td></td><td><code>Pod {{ $labels.pod }} is not ready for more than 30 minutes.</code></td></tr><tr><td>PrometheusCantScrape</td><td>warning</td><td>seed</td><td><code>Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.</code></td></tr><tr><td>PrometheusConfigurationFailure</td><td>warning</td><td>seed</td><td><code>Latest Prometheus configuration is broken and Prometheus is using the previous one.</code></td></tr><tr><td>VPNProbeAPIServerProxyFailed</td><td>critical</td><td>shoot</td><td><code>The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.</code></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-4bd3553c7141772a5c5be0814ce3145b>1.6.4 - Profiling</h1><h1 id=profiling-gardener-components>Profiling Gardener Components</h1><p>Similar to Kubernetes, Gardener components support profiling using <a href=https://golang.org/doc/diagnostics#profiling>standard Go tools</a> for analyzing CPU and memory usage by different code sections and more.
This document shows how to enable and use profiling handlers with Gardener components.</p><p>Enabling profiling handlers and the ports on which they are exposed differs between components.
However, once the handlers are enabled, they provide profiles via the same HTTP endpoint paths, from which you can retrieve them via <code>curl</code>/<code>wget</code> or directly using <code>go tool pprof</code>.
(You might need to use <code>kubectl port-forward</code> in order to access HTTP endpoints of Gardener components running in clusters.)</p><p>For example (gardener-controller-manager):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ curl http://localhost:2718/debug/pprof/heap &gt; /tmp/heap-controller-manager
</span></span><span style=display:flex><span>$ go tool pprof /tmp/heap-controller-manager
</span></span><span style=display:flex><span>Type: inuse_space
</span></span><span style=display:flex><span>Time: Sep 3, 2021 at 10:05am (CEST)
</span></span><span style=display:flex><span>Entering interactive mode (type <span style=color:#a31515>&#34;help&#34;</span> <span style=color:#00f>for</span> commands, <span style=color:#a31515>&#34;o&#34;</span> <span style=color:#00f>for</span> options)
</span></span><span style=display:flex><span>(pprof)
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ go tool pprof http://localhost:2718/debug/pprof/heap
</span></span><span style=display:flex><span>Fetching profile over HTTP from http://localhost:2718/debug/pprof/heap
</span></span><span style=display:flex><span>Saved profile in /Users/timebertt/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz
</span></span><span style=display:flex><span>Type: inuse_space
</span></span><span style=display:flex><span>Time: Sep 3, 2021 at 10:05am (CEST)
</span></span><span style=display:flex><span>Entering interactive mode (type <span style=color:#a31515>&#34;help&#34;</span> <span style=color:#00f>for</span> commands, <span style=color:#a31515>&#34;o&#34;</span> <span style=color:#00f>for</span> options)
</span></span><span style=display:flex><span>(pprof)
</span></span></code></pre></div><h2 id=gardener-apiserver>gardener-apiserver</h2><p><code>gardener-apiserver</code> provides the same flags as <code>kube-apiserver</code> for enabling profiling handlers (enabled by default):</p><pre tabindex=0><code>--contention-profiling    Enable lock contention profiling, if profiling is enabled
--profiling               Enable profiling via web interface host:port/debug/pprof/ (default true)
</code></pre><p>The handlers are served on the same port as the API endpoints (configured via <code>--secure-port</code>).
This means that you will also have to authenticate against the API server according to the configured authentication and authorization policy.</p><p>For example, in the <a href=/docs/gardener/development/local_setup/>local-setup</a> you can use:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ curl -k --cert ./hack/local-development/local-garden/certificates/certs/default-admin.crt --key ./hack/local-development/local-garden/certificates/keys/default-admin.key https://localhost:8443/debug/pprof/heap &gt; /tmp/heap
</span></span><span style=display:flex><span>$ go tool pprof /tmp/heap
</span></span></code></pre></div><h2 id=gardener-admission-controllercontroller-managerschedulerresource-manager-gardenlet>gardener-{admission-controller,controller-manager,scheduler,resource-manager}, gardenlet</h2><p><code>gardener-controller-manager</code>, <code>gardener-admission-controller</code>, <code>gardener-scheduler</code>, <code>gardener-resource-manager</code> and <code>gardenlet</code> also allow enabling profiling handlers via their respective component configs (currently disabled by default).
Here is an example for the <code>gardener-admission-controller</code>&rsquo;s configuration and how to enable it (it looks similar for the other components):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: admissioncontroller.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: AdmissionControllerConfiguration
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>    port: 2723
</span></span><span style=display:flex><span>debugging:
</span></span><span style=display:flex><span>  enableProfiling: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  enableContentionProfiling: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>However, the handlers are served on the same port as configured in <code>server.metrics.port</code> via HTTP.</p><p>For example (gardener-admission-controller):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ curl http://localhost:2723/debug/pprof/heap &gt; /tmp/heap
</span></span><span style=display:flex><span>$ go tool pprof /tmp/heap
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e5728941ab1558f050d903ea1fc6ece4>1.6.5 - User Alerts</h1><h1 id=user-alerts>User Alerts</h1><table><thead><tr><th>Alertname</th><th>Severity</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>ApiServerUnreachableViaKubernetesService</td><td>critical</td><td>shoot</td><td><code>The Api server has been unreachable for 15 minutes via the kubernetes service in the shoot.</code></td></tr><tr><td>KubeKubeletNodeDown</td><td>warning</td><td>shoot</td><td><code>The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.</code></td></tr><tr><td>KubeletTooManyOpenFileDescriptorsShoot</td><td>warning</td><td>shoot</td><td><code>Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.</code></td></tr><tr><td>KubeletTooManyOpenFileDescriptorsShoot</td><td>critical</td><td>shoot</td><td><code>Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.</code></td></tr><tr><td>KubePodPendingShoot</td><td>warning</td><td>shoot</td><td><code>Pod {{ $labels.pod }} is stuck in "Pending" state for more than 1 hour.</code></td></tr><tr><td>KubePodNotReadyShoot</td><td>warning</td><td>shoot</td><td><code>Pod {{ $labels.pod }} is not ready for more than 1 hour.</code></td></tr><tr><td>NodeExporterDown</td><td>warning</td><td>shoot</td><td><code>The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.</code></td></tr><tr><td>K8SNodeOutOfDisk</td><td>critical</td><td>shoot</td><td><code>Node {{ $labels.node }} has run out of disk space.</code></td></tr><tr><td>K8SNodeMemoryPressure</td><td>warning</td><td>shoot</td><td><code>Node {{ $labels.node }} is under memory pressure.</code></td></tr><tr><td>K8SNodeDiskPressure</td><td>warning</td><td>shoot</td><td><code>Node {{ $labels.node }} is under disk pressure</code></td></tr><tr><td>VMRootfsFull</td><td>critical</td><td>shoot</td><td><code>Root filesystem device on instance {{ $labels.instance }} is almost full.</code></td></tr><tr><td>VMConntrackTableFull</td><td>critical</td><td>shoot</td><td><code>The nf_conntrack table is {{ $value }}% full.</code></td></tr><tr><td>VPNProbeAPIServerProxyFailed</td><td>critical</td><td>shoot</td><td><code>The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.</code></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-d9bbacfebdbb87bb1858bcd155c5647f>1.7 - Usage</h1></div><div class=td-content><h1 id=pg-5092bba8dce7cbbc8af74c9d34288317>1.7.1 - Hibernate a Cluster</h1><h1 id=hibernate-a-cluster>Hibernate a Cluster</h1><p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save a lot of money if you scale-down your Kubernetes resources whenever you don&rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.</p><p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button, or by defining a hibernation schedule.</p><blockquote><p>To save costs, it&rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&rsquo;s a schedule for its hibernation.</p></blockquote><ul><li><a href=#hibernate-a-cluster>Hibernate a Cluster</a><ul><li><a href=#what-is-hibernation>What Is Hibernation?</a></li><li><a href=#what-isnt-affected-by-the-hibernation>What Isn’t Affected by the Hibernation?</a></li><li><a href=#hibernate-your-cluster-manually>Hibernate Your Cluster Manually</a></li><li><a href=#wake-up-your-cluster-manually>Wake Up Your Cluster Manually</a></li><li><a href=#create-a-schedule-to-hibernate-your-cluster>Create a Schedule to Hibernate Your Cluster</a></li></ul></li></ul><h2 id=what-is-hibernation>What Is Hibernation?</h2><p>When a cluster is hibernated, Gardener scales down the worker nodes and the cluster&rsquo;s control plane to free resources at the IaaS provider. This affects:</p><ul><li>Your workload, for example, pods, deployments, custom resources.</li><li>The virtual machines running your workload.</li><li>The resources of the control plane of your cluster.</li></ul><h2 id=what-isnt-affected-by-the-hibernation>What Isn’t Affected by the Hibernation?</h2><p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in <code>etcd</code> is also preserved.</p><h2 id=hibernate-your-cluster-manually>Hibernate Your Cluster Manually</h2><p>The <code>.spec.hibernation.enabled</code> field specifies whether the cluster needs to be hibernated or not. If the field is set to <code>true</code>, the cluster&rsquo;s desired state is to be hibernated. If it is set to <code>false</code> or not specified at all, the cluster&rsquo;s desired state is to be awakened.</p><p>To hibernate your cluster, you can run the following <code>kubectl</code> command:</p><pre tabindex=0><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &#39;{&#34;spec&#34;:{&#34;hibernation&#34;:{&#34;enabled&#34;: true}}}&#39;
</code></pre><h2 id=wake-up-your-cluster-manually>Wake Up Your Cluster Manually</h2><p>To wake up your cluster, you can run the following <code>kubectl</code> command:</p><pre tabindex=0><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &#39;{&#34;spec&#34;:{&#34;hibernation&#34;:{&#34;enabled&#34;: false}}}&#39;
</code></pre><h2 id=create-a-schedule-to-hibernate-your-cluster>Create a Schedule to Hibernate Your Cluster</h2><p>You can specify a hibernation schedule to automatically hibernate/wake up a cluster.</p><p>Let&rsquo;s have a look into the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  hibernation:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    schedules:
</span></span><span style=display:flex><span>    - start: <span style=color:#a31515>&#34;0 20 * * *&#34;</span> <span style=color:green># Start hibernation every day at 8PM</span>
</span></span><span style=display:flex><span>      end: <span style=color:#a31515>&#34;0 6 * * *&#34;</span>    <span style=color:green># Stop hibernation every day at 6AM</span>
</span></span><span style=display:flex><span>      location: <span style=color:#a31515>&#34;America/Los_Angeles&#34;</span> <span style=color:green># Specify a location for the cron to run in</span>
</span></span></code></pre></div><p>The above section configures a hibernation schedule that hibernates the cluster every day at 08:00 PM and wakes it up at 06:00 AM. The <code>start</code> or <code>end</code> fields can be omitted, though at least one of them has to be specified. Hence, it is possible to configure a hibernation schedule that only hibernates or wakes up a cluster. The <code>location</code> field is the time location used to evaluate the cron expressions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc140bc78ef125e0e6636c2d7cd748a>1.7.2 - APIServer SNI Injection</h1><h1 id=apiserversni-environment-variable-injection>APIServerSNI Environment Variable Injection</h1><p>If the Gardener administrator has enabled the <code>APIServerSNI</code> feature gate for a particular Seed cluster, then in each Shoot cluster&rsquo;s <code>kube-system</code> namespace a <code>DaemonSet</code> called <code>apiserver-proxy</code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>APIServer SNI GEP</a> for more details.</p><p>To skip this extra network hop, a <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>mutating webhook</a> called <code>apiserver-proxy.networking.gardener.cloud</code> is deployed next to the API server in the Seed. It adds a <code>KUBERNETES_SERVICE_HOST</code> environment variable to each container and init container that do not specify it. See the webhook <a href=https://github.com/gardener/apiserver-proxy/>repository</a> for more information.</p><h2 id=opt-out-of-pod-injection>Opt-Out of Pod Injection</h2><p>In some cases it&rsquo;s desirable to opt-out of Pod injection:</p><ul><li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.</li><li>Want to test the <code>kube-proxy</code> and <code>kubelet</code> in-cluster discovery.</li></ul><h3 id=opt-out-of-pod-injection-for-specific-pods>Opt-Out of Pod Injection for Specific Pods</h3><p>To opt out of the injection, the Pod should be labeled with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: nginx
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: nginx
</span></span><span style=display:flex><span>        apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: nginx
</span></span><span style=display:flex><span>        image: nginx:1.14.2
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 80
</span></span></code></pre></div><h3 id=opt-out-of-pod-injection-on-namespace-level>Opt-Out of Pod Injection on Namespace Level</h3><p>To opt out of the injection of <strong>all</strong> Pods in a namespace, you should label your namespace with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Namespace
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>  name: my-namespace
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a namespace level and enable it for individual pods in it.</p></blockquote><h3 id=opt-out-of-pod-injection-for-the-entire-cluster>Opt-Out of Pod Injection for the Entire Cluster</h3><p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the <code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector</code> annotation with value <code>disable</code> on the <code>Shoot</code> resource itself:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: <span style=color:#a31515>&#39;disable&#39;</span>
</span></span><span style=display:flex><span>  name: my-cluster
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a cluster level and enable it for individual pods in it.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-7e7022c7d80b227a07794be1c66352e7>1.7.3 - Configuration</h1><h1 id=gardener-configuration-and-usage>Gardener Configuration and Usage</h1><p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.</p><p>This document describes the:</p><ol><li><a href=#configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and usage of Gardener as operator/administrator</a>.</li><li><a href=#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and usage of Gardener as end-user/stakeholder/customer</a>.</li></ol><h2 id=configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and Usage of Gardener as Operator/Administrator</h2><p>When we use the terms &ldquo;operator/administrator&rdquo;, we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:</p><ol><li><code>gardener-apiserver</code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like <code>Seed</code>s and <code>Shoot</code>s), and a component that contains multiple admission plugins.</li><li><code>gardener-admission-controller</code>, an HTTP(S) server with several handlers to be used in a <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml>ValidatingWebhookConfiguration</a>.</li><li><code>gardener-controller-manager</code>, a component consisting of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining <code>Shoot</code>s, reconciling <code>Project</code>s).</li><li><code>gardener-scheduler</code>, a component that assigns newly created <code>Shoot</code> clusters to appropriate <code>Seed</code> clusters.</li><li><code>gardenlet</code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of <code>Shoot</code>s).</li></ol><p>Each of these components have various configuration options.
The <code>gardener-apiserver</code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.</p><h3 id=configuration-file-for-gardener-admission-controller>Configuration File for Gardener Admission Controller</h3><p>The Gardener admission controller only supports one command line flag, which should be a path to a valid admission-controller configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-controller-manager>Configuration File for Gardener Controller Manager</h3><p>The Gardener controller manager only supports one command line flag, which should be a path to a valid controller-manager configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-scheduler>Configuration File for Gardener Scheduler</h3><p>The Gardener scheduler also only supports one command line flag, which should be a path to a valid scheduler configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>example configuration</a>.
Information about the concepts of the Gardener scheduler can be found at <a href=/docs/gardener/concepts/scheduler/>Gardener Scheduler</a>.</p><h3 id=configuration-file-for-gardenlet>Configuration File for gardenlet</h3><p>The gardenlet also only supports one command line flag, which should be a path to a valid gardenlet configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>example configuration</a>.
Information about the concepts of the Gardenlet can be found at <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a>.</p><h3 id=system-configuration>System Configuration</h3><p>After successful deployment of the four components, you need to setup the system.
Let&rsquo;s first focus on some &ldquo;static&rdquo; configuration.
When the <code>gardenlet</code> starts, it scans the <code>garden</code> namespace of the garden cluster for <code>Secret</code>s that have influence on its reconciliation loops, mainly the <code>Shoot</code> reconciliation:</p><ul><li><p><strong>Internal domain secret</strong> - contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete the so-called &ldquo;internal&rdquo; DNS records for the Shoot clusters, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml>yaml file</a> for an example.</p><ul><li>This secret is used in order to establish a stable endpoint for shoot clusters, which is used internally by all control plane components.</li><li>The DNS records are normal DNS records but called &ldquo;internal&rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.</li><li>It is forbidden to change the internal domain secret if there are existing shoot clusters.</li></ul></li><li><p><strong>Default domain secrets</strong> (optional) - contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., <code>example.com</code>), please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml>yaml file</a> for an example.</p><ul><li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.</li><li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&rsquo;t specify their own domain.</li><li>If you have multiple default domain secrets defined you can add a priority as an annotation (<code>dns.gardener.cloud/domain-default-priority</code>) to select which domain should be used for new shoots during creation. The domain with the highest priority is selected during shoot creation. If there is no annotation defined, the default priority is <code>0</code>, also all non integer values are considered as priority <code>0</code>.</li></ul></li><li><p><strong>Alerting secrets</strong> (optional) - contain the alerting configuration and credentials for the <a href=https://prometheus.io/docs/alerting/alertmanager/>AlertManager</a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>yaml file</a> for an example.</p><ul><li>If email alerting is configured:<ul><li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.</li><li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.</li><li>The AlertManager will send emails to the configured email address in case any alerts are firing.</li></ul></li><li>If an external AlertManager is configured:<ul><li>Each shoot has a <a href=https://prometheus.io/docs/introduction/overview/>Prometheus</a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.</li><li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.</li><li>Supported authentication types are no authentication, basic, or mutual TLS.</li></ul></li></ul></li><li><p><strong>OpenVPN Diffie-Hellmann Key secret</strong> (optional) - contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-openvpn-diffie-hellman.yaml>yaml file</a> for an example.</p><ul><li>If you don&rsquo;t specify a custom key, then a default key is used, but for productive landscapes it&rsquo;s recommend to create a landscape-specific key and define it.</li></ul></li><li><p><strong>Global monitoring secrets</strong> (optional) - contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.</p><ul><li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.</li></ul></li></ul><p>Apart from this &ldquo;static&rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator, you have to configure some of them to make the system work.</p><h3 id=configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and Usage of Gardener as End-User/Stakeholder/Customer</h3><p>As an end-user/stakeholder/customer, you are using a Gardener landscape that has been setup for you by another team.
You don&rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at <a href=/docs/gardener/concepts/apiserver/>Gardener API Server</a> - the topic describes which resources are offered by Gardener.
You may want to have a more detailed look for <code>Project</code>s, <code>SecretBinding</code>s, <code>Shoot</code>s, and <code>(Cluster)OpenIDConnectPreset</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d39ff1936115af2f29f4e58ff5cabc9d>1.7.4 - Control Plane Endpoints And Ports</h1><h1 id=endpoints-and-ports-of-a-shoot-control-plane>Endpoints and Ports of a Shoot Control-Plane</h1><p>With the <a href=/docs/gardener/usage/reversed-vpn-tunnel/>reversed VPN</a> tunnel, there are no endpoints with open ports in the shoot cluster required by Gardener.
In order to allow communication to the shoots control-plane in the seed cluster, there are endpoints shared by multiple shoots of a seed cluster.
Depending on the configured zones or <a href=/docs/gardener/usage/exposureclasses/>exposure classes</a>, there are different endpoints in a seed cluster. The IP address(es) can be determined by a DNS query for the API Server URL.
The main entry-point into the seed cluster is the load balancer of the Istio ingress-gateway service. Depending on the infrastructure provider, there can be one IP address per zone.</p><p>The load balancer of the Istio ingress-gateway service exposes the following TCP ports:</p><ul><li><strong>443</strong> for requests to the shoot API Server. The request is dispatched according to the set TLS SNI extension.</li><li><strong>8443</strong> for requests to the shoot API Server via <code>api-server-proxy</code>, dispatched based on the proxy protocol target, which is the IP address of <code>kubernetes.default.svc.cluster.local</code> in the shoot.</li><li><strong>8132</strong> to establish the reversed VPN connection. It&rsquo;s dispatched according to an HTTP header value.</li></ul><h2 id=kube-apiserver-via-sni><code>kube-apiserver</code> via SNI</h2><p><img src=/__resources/api-server-sni_feb16f.png alt="kube-apiserver via SNI"></p><p>DNS entries for <code>api.&lt;external-domain></code> and <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code> point to the load balancer of an Istio ingress-gateway service.
The Kubernetes client sets the server name to <code>api.&lt;external-domain></code> or <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code>.
Based on SNI, the connection is forwarded to the respective API Server at TCP layer. There is no TLS termination at the Istio ingress-gateway.
TLS termination happens on the shoots API Server. Traffic is end-to-end encrypted between the client and the API Server. The certificate authority and authentication are defined in the corresponding <code>kubeconfig</code>.
Details can be found in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>GEP-08</a>.</p><h2 id=kube-apiserver-via-apiserver-proxy><code>kube-apiserver</code> via <code>apiserver-proxy</code></h2><p><img src=/__resources/api-server-proxy_b419fc.png alt=apiserver-proxy></p><p>Inside the shoot cluster, the API Server can also be reached by the cluster internal name <code>kubernetes.default.svc.cluster.local</code>.
The pods <code>apiserver-proxy</code> are deployed in the host network as daemonset and intercept connections to the Kubernetes service IP address.
The destination address is changed to the cluster IP address of the service <code>kube-apiserver.&lt;shoot-namespace>.svc.cluster.local</code> in the seed cluster.
The connections are forwarded via the <a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol>HaProxy Proxy Protocol</a> to the Istio ingress-gateway in the seed cluster.
The Istio ingress-gateway forwards the connection to the respective shoot API Server by it&rsquo;s cluster IP address.
As TLS termination happens at the API Server, the traffic is end-to-end encrypted the same way as with SNI.</p><p>Details can be found in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/11-apiserver-network-proxy.md>GEP-11</a>.</p><h2 id=reversed-vpn-tunnel>Reversed VPN Tunnel</h2><p><img src=/__resources/reversed-vpn_e89ad6.png alt="Reversed VPN"></p><p>As the API Server has to be able to connect to endpoints in the shoot cluster, a VPN connection is established.
This VPN connection is initiated from a VPN client in the shoot cluster.
The VPN client connects to the Istio ingress-gateway and is forwarded to the VPN server in the control-plane namespace of the shoot.
Once the VPN tunnel between the VPN client in the shoot and the VPN server in the seed cluster is established, the API Server can connect to nodes, services and pods in the shoot cluster.</p><p>More details can be found in the <a href=/docs/gardener/usage/reversed-vpn-tunnel/>usage document</a> and <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>GEP-14</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-557189bc540e3079a8a290a1db4e319f>1.7.5 - Control Plane Migration</h1><h1 id=control-plane-migration>Control Plane Migration</h1><h2 id=prerequisites>Prerequisites</h2><p>To be able to use this feature, the <code>SeedChange</code> feature gate has to be enabled on your <code>gardener-apiserver</code>.</p><p>Also, the involved Seeds need to have enabled <code>BackupBucket</code>s.</p><h2 id=shootstate>ShootState</h2><p><code>ShootState</code> is an API resource which stores non-reconstructible state and data required to completely recreate a <code>Shoot</code>&rsquo;s control plane on a new <code>Seed</code>. The <code>ShootState</code> resource is created on <code>Shoot</code> creation in its <code>Project</code> namespace and the required state/data is persisted during <code>Shoot</code> creation or reconciliation.</p><h2 id=shoot-control-plane-migration>Shoot Control Plane Migration</h2><p>Triggering the migration is done by changing the <code>Shoot</code>&rsquo;s <code>.spec.seedName</code> to a <code>Seed</code> that differs from the <code>.status.seedName</code>, we call this <code>Seed</code> a <code>"Destination Seed"</code>. This action can only be performed by an operator with the necessary RBAC. If the Destination <code>Seed</code> does not have a backup and restore configuration, the the change to <code>spec.seedName</code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.</p><p>If the <code>Shoot</code> has different <code>.spec.seedName</code> and <code>.status.seedName</code>, a process is started to prepare the Control Plane for migration:</p><ol><li><code>.status.lastOperation</code> is changed to <code>Migrate</code>.</li><li>Kubernetes API Server is stopped and the extension resources are annotated with <code>gardener.cloud/operation=migrate</code>.</li><li>Full snapshot of the ETCD is created and terminating of the Control Plane in the <code>Source Seed</code> is initiated.</li></ol><p>If the process is successful, we update the status of the <code>Shoot</code> by setting the <code>.status.seedName</code> to the null value. That way, a restoration is triggered in the <code>Destination Seed</code> and <code>.status.lastOperation</code> is changed to <code>Restore</code>. The control plane migration is completed when the <code>Restore</code> operation has completed successfully.</p><p>When the <code>CopyEtcdBackupsDuringControlPlaneMigration</code> feature gate is enabled on the <code>gardenlet</code>, the etcd backups will be copied over to the <code>BackupBucket</code> of the <code>Destination Seed</code> during control plane migration and any future backups will be uploaded there. Otherwise, backups will continue to be uploaded to the <code>BackupBucket</code> of the <code>Source Seed</code>.</p><h2 id=triggering-the-migration>Triggering the Migration</h2><p>For controlplane migration, operators with the necessary RBAC can use the <a href=/docs/gardener/concepts/scheduler/#shootsbinding-subresource><code>shoots/binding</code></a> subresource to change the <code>.spec.seedName</code>, with the following commands:</p><pre tabindex=0><code>export NAMESPACE=my-namespace
export SHOOT_NAME=my-shoot
kubectl get --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME} | jq -c &#39;.spec.seedName = &#34;&lt;destination-seed&gt;&#34;&#39; | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME}/binding -f - | jq -r &#39;.spec.seedName&#39;
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-3a01742bcc319ec4dfb0ee2cdce04deb>1.7.6 - CSI Components</h1><h1 id=custom-csi-components>(Custom) CSI Components</h1><p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).</p><p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the <code>VolumeSnapshot</code> custom resources (each responsible for those instances associated with their respective driver provisioner types).</p><p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!</p><h2 id=recommendations>Recommendations</h2><p>Custom CSI components are typically regular <code>Deployment</code>s running in the shoot clusters.</p><p><strong>Please label them with the <code>shoot.gardener.cloud/no-cleanup=true</code> label.</strong></p><h2 id=background-information>Background Information</h2><p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (<code>Deployment</code>s, <code>DaemonSet</code>s, <code>StatefulSet</code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.</p><p>This can result in <code>VolumeSnapshot</code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2ff8f948af545c542d2859a2fd5223e4>1.7.7 - Custom containerd Configuration</h1><h1 id=custom-containerd-configuration>Custom <code>containerd</code> Configuration</h1><p>In case a <code>Shoot</code> cluster uses <code>containerd</code> (see <a href=/docs/gardener/usage/docker-shim-removal/>Kubernetes dockershim Removal</a>) for more information), it is possible to make the <code>containerd</code> process load custom configuration files.
Gardener initializes <code>contaienerd</code> with the following statement:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>imports = [<span style=color:#a31515>&#34;/etc/containerd/conf.d/*.toml&#34;</span>]
</span></span></code></pre></div><p>This means that all <code>*.toml</code> files in the <code>/etc/containerd/conf.d</code> directory will be imported and merged with the default configuration.
To prevent unintended configuration overwrites, please be aware that containerd merges config sections, not individual keys (see <a href=https://github.com/containerd/containerd/issues/5837#issuecomment-894840240>here</a> and <a href=https://github.com/gardener/gardener/pull/7316>here</a>).
Please consult the <a href=https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md#format>upstream <code>containerd</code> documentation</a> for more information.</p><blockquote><p>⚠️ Note that this only applies to nodes which were newly created after <code>gardener/gardener@v1.51</code> was deployed. Existing nodes are not affected.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-0886d3122ac4a522338baa665bcd836e>1.7.8 - Custom DNS Configuration</h1><h1 id=custom-dns-configuration>Custom DNS Configuration</h1><p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.</p><p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.</p><p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &ldquo;undo&rdquo;, we utilize the <code>import</code> plugin from CoreDNS [1].
which enables in-line configuration changes.</p><h2 id=how-to-use>How to use</h2><p>To customize your CoreDNS cluster config, you can simply edit a <code>ConfigMap</code> named <code>coredns-custom</code> in the <code>kube-system</code> namespace.
By editing, this <code>ConfigMap</code>, you are modifying CoreDNS configuration, therefore care is advised.</p><p>For example, to apply new config to CoreDNS that would point all <code>.global</code> DNS requests to another DNS pod, simply edit the configuration as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: coredns-custom
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  istio.server: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    global:8053 {
</span></span></span><span style=display:flex><span><span style=color:#a31515>            errors
</span></span></span><span style=display:flex><span><span style=color:#a31515>            cache 30
</span></span></span><span style=display:flex><span><span style=color:#a31515>            forward . 1.2.3.4
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }</span>    
</span></span><span style=display:flex><span>  corefile.override: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>         # &lt;some-plugin&gt; &lt;some-plugin-config&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>         debug
</span></span></span><span style=display:flex><span><span style=color:#a31515>         whoami</span>         
</span></span></code></pre></div><p>The port number 8053 in <code>global:8053</code> is the specific port that CoreDNS is bound to and cannot be changed to any other port if it should act on ordinary name resolution requests from pods. Otherwise, CoreDNS will open a second port, but you are responsible to direct the traffic to this port. <code>kube-dns</code> service in <code>kube-system</code> namespace will direct name resolution requests within the cluster to port 8053 on the CoreDNS pods.
Moreover, additional network policies are needed to allow corresponding ingress traffic to CoreDNS pods.
In order for the destination DNS server to be reachable, it must listen on port 53 as it is required by network policies. Other ports are only possible if additional network policies allow corresponding egress traffic from CoreDNS pods.</p><p>It is important to have the <code>ConfigMap</code> keys ending with <code>*.server</code> (if you would like to add a new server) or <code>*.override</code>
if you want to customize the current server configuration (it is optional setting both).</p><h2 id=optional-reload-coredns>[Optional] Reload CoreDNS</h2><p>As Gardener is configuring the <code>reload</code> <a href=https://coredns.io/plugins/reload/>plugin</a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate <code>ConfigMap</code> changes. However, if you don&rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system rollout restart deploy coredns
</span></span></code></pre></div><p>This will reload the config into CoreDNS.</p><p>The approach we follow here was inspired by AKS&rsquo;s approach [2].</p><h2 id=anti-pattern>Anti-Pattern</h2><p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).</p><p>If incompatible changes are applied by mistake, simply delete the content of the <code>ConfigMap</code> and re-apply.
This should bring the cluster DNS back to functioning state.</p><h2 id=node-local-dns>Node Local DNS</h2><p>Custom DNS configuration] may not work as expected in conjunction with <code>NodeLocalDNS</code>.
With <code>NodeLocalDNS</code>, ordinary DNS queries targeted at the upstream DNS servers, i.e. non-kubernetes domains,
will not end up at CoreDNS, but will instead be directly sent to the upstream DNS server. Therefore, configuration
applying to non-kubernetes entities, e.g. the <code>istio.server</code> block in the
<a href=/docs/gardener/usage/custom-dns-config/>custom DNS configuration</a> example, may not have any effect with <code>NodeLocalDNS</code> enabled.
If this kind of custom configuration is required, forwarding to upstream DNS has to be disabled.
This can be done by setting the option (<code>spec.systemComponents.nodeLocalDNS.disableForwardToUpstreamDNS</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      disableForwardToUpstreamDNS: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=references>References</h2><p>[1] <a href=https://github.com/coredns/coredns/tree/master/plugin/import>Import plugin</a>
[2] <a href=https://docs.microsoft.com/en-us/azure/aks/coredns-custom>AKS Custom DNS</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c5ac85b19cff8076310937381264d967>1.7.9 - Default Seccomp Profile</h1><h1 id=default-seccomp-profile-and-configuration>Default Seccomp Profile and Configuration</h1><p>This is a short guide describing how to enable the defaulting of seccomp profiles for Gardener managed workloads in the seed.</p><h2 id=default-kubernetes-behavior>Default Kubernetes Behavior</h2><p>The state of Kubernetes in versions &lt; 1.25 is such that all workloads by default run in <code>Unconfined</code> (seccomp disabled) mode. This is undesirable since this is the least restrictive profile. Also, mind that any privileged container will always run as <code>Unconfined</code>. More information about seccomp can be found in this <a href=https://kubernetes.io/docs/tutorials/security/seccomp/>Kubernetes tutorial</a>.</p><h2 id=setting-the-seccomp-profile-to-runtimedefault-for-seed-clusters>Setting the Seccomp Profile to RuntimeDefault for Seed Clusters</h2><p>To address the above issue, Gardener provides a webhook that is capable of mutating pods in the seed clusters, explicitly providing them with a seccomp profile type of <code>RuntimeDefault</code>. This profile is defined by the container runtime and represents a set of default syscalls that are allowed or not.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  securityContext:
</span></span><span style=display:flex><span>    seccompProfile:
</span></span><span style=display:flex><span>      type: RuntimeDefault
</span></span></code></pre></div><p>A <code>Pod</code> is mutated when all of the following preconditions are fulfilled:</p><ol><li>The <code>Pod</code> is created in a Gardener managed namespace.</li><li>The <code>Pod</code> is NOT labeled with <code>seccompprofile.resources.gardener.cloud/skip</code>.</li><li>The <code>Pod</code> does NOT explicitly specify <code>.spec.securityContext.seccompProfile.type</code>.</li></ol><h3 id=how-to-configure>How to Configure</h3><p>To enable this feature, the gardenlet <code>DefaultSeccompProfile</code> feature gate must be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  DefaultSeccompProfile: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Please refer to the examples in this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>yaml file</a> for more information.</p><p>Once the feature gate is enabled, the webhook will be registered and configured for the seed cluster. Newly created pods will be mutated to have their seccomp profile set to <code>RuntimeDefault</code>.</p><blockquote><p><strong>Note:</strong> Please note that this feature is still in Alpha, so you might see instabilities every now and then.</p></blockquote><h2 id=setting-the-seccomp-profile-to-runtimedefault-for-shoot-clusters>Setting the Seccomp Profile to RuntimeDefault for Shoot Clusters</h2><p>For Kubernetes shoot versions >= 1.25, you can enable the use of <code>RuntimeDefault</code> as the default seccomp profile for all workloads. If enabled, the kubelet will use the <code>RuntimeDefault</code> seccomp profile by default, which is defined by the container runtime, instead of using the <code>Unconfined</code> mode. More information for this feature can be found in the <a href=https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads>Kubernetes documentation</a>.</p><p>To use seccomp profile defaulting, you must run the kubelet with the <code>SeccompDefault</code> feature gate enabled (this is the default for k8s versions >= 1.25).</p><h3 id=how-to-configure-1>How to Configure</h3><p>To enable this feature, the kubelet <code>seccompDefault</code> configuration parameter must be set to <code>true</code> in the shoot&rsquo;s spec.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.25.0
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      seccompDefault: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Please refer to the examples in this <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>yaml file</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-da5e6c1f64dc67e784358df56a1ec88d>1.7.10 - DNS Autoscaling</h1><h1 id=dns-autoscaling>DNS Autoscaling</h1><p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.</p><h2 id=background>Background</h2><p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).</li><li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.</li><li>and more &mldr;</li></ul><p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a>, which helps mitigate potential DNS bottlenecks (see <a href=#trade-offs-in-conjunction-with-nodelocaldns>Trade-offs in conjunction with NodeLocalDNS</a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).</p><h2 id=configuring-cluster-proportional-dns-autoscaling>Configuring Cluster-Proportional DNS Autoscaling</h2><p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (<code>spec.systemComponents.coreDNS.autoscaling.mode</code>) in the <code>Shoot</code> resource to <code>cluster-proportional</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: cluster-proportional
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To switch back to horizontal DNS autoscaling, you can set the <code>spec.systemComponents.coreDNS.autoscaling.mode</code> to <code>horizontal</code> (or remove the <code>coreDNS</code> section).</p><p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called <code>coredns-autoscaler</code> will be created in the <code>kube-system</code> namespace with the default settings. The content will be similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>linear: <span style=color:#a31515>&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:2,&#34;nodesPerReplica&#34;:16}&#39;</span>
</span></span></code></pre></div><p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:</p><pre tabindex=0><code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
</code></pre><p>Depending on your needs, you can adjust <code>coresPerReplica</code> or <code>nodesPerReplica</code>, but it is also possible to override <code>min</code> if required.</p><h2 id=trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>Trade-Offs of Horizontal and Cluster-Proportional DNS Autoscaling</h2><p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e., you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas, meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead, with its more flexible configuration options.</p><p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e., if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g., CPU consumption, into account.</p><p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).</p><h2 id=trade-offs-in-conjunction-with-nodelocaldns>Trade-Offs in Conjunction with NodeLocalDNS</h2><p>Using a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> reduces the amount of traffic being sent to the cluster&rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.</p><p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0de9e36d58f4dc635d78712b0907f162>1.7.11 - DNS Search Path Optimization</h1><h1 id=dns-search-path-optimization>DNS Search Path Optimization</h1><h2 id=dns-search-path>DNS Search Path</h2><p>Using fully qualified names has some downsides, e.g., it may become harder to move deployments from one landscape to the
next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context
they are used in.</p><p>The DNS search path allows for the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local
names to create a fully qualified name.</p><p>If a short/local name should be resolved, each entry is appended to it one by one to check whether it can be resolved. The
process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search
path, the short/local name is attempted to be resolved on it own.</p><h2 id=dns-option-ndots>DNS Option <code>ndots</code></h2><p>As explained in the <a href=#dns-search-path>section above</a>, the DNS search path is used for short/local names to create fully
qualified names. The DNS option <code>ndots</code> specifies how many dots (<code>.</code>) a name needs to have to be considered fully qualified.
For names with less than <code>ndots</code> dots (<code>.</code>), the <a href=#dns-search-path>DNS search path</a> will be applied.</p><h2 id=dns-search-path-ndots-and-kubernetes>DNS Search Path, <code>ndots</code>, and Kubernetes</h2><p>Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a
service, most notably by its name directly, using the namespace as suffix, utilizing <code>&lt;namespace>.svc</code> as suffix or as a
fully qualified name as <code>&lt;service>.&lt;namespace>.svc.cluster.local</code> (assuming <code>cluster.local</code> to be the cluster domain).</p><p>This is why the DNS search path is fairly long in Kubernetes, usually consisting of <code>&lt;namespace>.svc.cluster.local</code>,
<code>svc.cluster.local</code>, <code>cluster.local</code>, and potentially some additional entries coming from the local network of the cluster.
For various reasons, the default <code>ndots</code> value in the context of Kubernetes is with <code>5</code>, also fairly large. See
<a href=https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056>this comment</a> for a more detailed description.</p><h2 id=dns-search-pathndots-problem-in-kubernetes>DNS Search Path/<code>ndots</code> Problem in Kubernetes</h2><p>As the DNS search path is long and <code>ndots</code> is large, a lot of DNS queries might traverse the DNS search path. This results
in an explosion of DNS requests.</p><p>For example, consider the name resolution of the default kubernetes service <code>kubernetes.default.svc.cluster.local</code>. As this
name has only four dots, it is not considered a fully qualified name according to the default <code>ndots=5</code> setting. Therefore,
the DNS search path is applied, resulting in the following queries being created</p><ul><li><code>kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.network-domain</code></li><li>&mldr;</li></ul><p>In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.</p><h2 id=general-workaroundsmitigations>General Workarounds/Mitigations</h2><p>Kubernetes provides the capability to set the DNS options for each pod (see
<a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config>Pod DNS config</a> for details).
However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be
useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general
solution may difficult to impossible.</p><p>Another approach is to use always fully qualified names and append a dot (<code>.</code>) to the name to prevent the name resolution
system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the
trailing dot (<code>.</code>). Furthermore, it makes moving to different landscapes more difficult/error-prone.</p><h2 id=gardener-specific-workaroundsmitigations>Gardener Specific Workarounds/Mitigations</h2><p>Gardener allows users to <a href=/docs/gardener/usage/custom-dns-config/>customize their DNS configuration</a>. CoreDNS allows several approaches to deal with
the requests generated by the DNS search path. <a href=https://coredns.io/plugins/cache/>Caching</a> is possible as well as
<a href=https://coredns.io/plugins/rewrite/>query rewriting</a>. There are also several other <a href=https://coredns.io/plugins/>plugins</a>
available, which may mitigate the situation.</p><h2 id=gardener-dns-query-rewriting>Gardener DNS Query Rewriting</h2><p>As explained <a href=#dns-search-path-ndots-and-kubernetes>above</a>, the application of the DNS search path may lead to the undesired
creation of DNS requests. Especially with the default setting of <code>ndots=5</code>, seemingly fully qualified names pointing to
services in the cluster may trigger the DNS search path application.</p><p>Gardener allows to automatically rewrite some obviously incorrect DNS names, which stem from an application of the DNS search
path to the most likely desired name. The feature can be enabled by setting the Gardenlet feature gate <code>CoreDNSQueryRewriting</code> to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  CoreDNSQueryRewriting: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>In case the feature is enabled in the gardenlet, it can be disabled per shoot cluster by setting the annotation
<code>alpha.featuregates.shoot.gardener.cloud/core-dns-rewriting-disabled</code> to any value.</p><p>This will automatically rewrite requests like <code>service.namespace.svc.cluster.local.svc.cluster.local</code> to
<code>service.namespace.svc.cluster.local</code>.</p><p>In case the applications also target services for name resolution, which are outside of the cluster and have less than <code>ndots</code> dots,
it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the
<code>commonSuffixes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      rewriting:
</span></span><span style=display:flex><span>        commonSuffixes:
</span></span><span style=display:flex><span>        - gardener.cloud
</span></span><span style=display:flex><span>        - example.com
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>DNS requests containing a common suffix and ending in <code>.svc.cluster.local</code> are assumed to be incorrect application of the DNS
search path. Therefore, they are rewritten to everything ending in the common suffix. For example, <code>www.gardener.cloud.svc.cluster.local</code>
would be rewritten to <code>www.gardener.cloud</code>.</p><p>Please note that the common suffixes should be long enough and include enough dots (<code>.</code>) to prevent random overlap with
other DNS queries. For example, it would be a bad idea to simply put <code>com</code> on the list of common suffixes, as there may be
services/namespaces which have <code>com</code> as part of their name. The effect would be seemingly random DNS requests. Gardener
requires that common suffixes contain at least one dot (.) and adds a second dot at the beginning. For instance, a common
suffix of <code>example.com</code> in the configuration would match <code>*.example.com</code>.</p><p>Since some clients verify the host in the response of a DNS query, the host must also be rewritten.
For that reason, we can&rsquo;t rewrite a query for <code>service.dst-namespace.svc.cluster.local.src-namespace.svc.cluster.local</code> or
<code>www.example.com.src-namespace.svc.cluster.local</code>, as for an answer rewrite <code>src-namespace</code> would not be known.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2c8dcee576a95f02378e3b89966d6410>1.7.12 - Docker Shim Removal</h1><h1 id=kubernetes-dockershim-removal>Kubernetes dockershim Removal</h1><h2 id=whats-happening>What&rsquo;s Happening?</h2><p>With Kubernetes v1.20, the built-in dockershim <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#dockershim-deprecation>was deprecated</a> and is scheduled to be <a href=https://github.com/kubernetes/enhancements/issues/2221>removed with v1.24</a>.
Don&rsquo;t Panic! The Kubernetes community has <a href=https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/>published a blogpost</a> and an <a href=https://kubernetes.io/blog/2022/02/17/dockershim-faq/>FAQ</a> with more information.</p><p>Gardener also needs to switch from using the built-in dockershim to <code>containerd</code>.
Gardener will not change already running Shoot clusters. But changes to the container runtime will be coupled to the K8s version selected by the Shoot:</p><ul><li>Starting with K8s version 1.22, Shoots not explicitly selecting a container runtime will get <code>containerd</code> instead of <code>docker</code>. Shoots can still select <code>docker</code> explicitly if needed.</li><li>Starting with K8s version 1.23 <code>docker</code> can no longer be selected.</li></ul><p>At this point in time, we have no plans to support other container runtimes, such as <code>cri-o</code>.</p><h2 id=what-should-i-do>What Should I Do?</h2><p>As a Gardener operator:</p><ul><li>Add <code>containerd</code> and <code>docker</code> to <code>.spec.machineImages[].versions[].cri.name</code> in your CloudProfile to allow users selecting a container runtime for their Shoots (see below).</li></ul><blockquote><p><strong>Note:</strong> Please take a look at our detailed information regarding <a href=#container-runtime-support-in-gardener-operating-system-extensions>container runtime support in Gardener Operating System Extensions</a>.</p></blockquote><ul><li>Update your cloud provider extensions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>.</li></ul><blockquote><p><strong>Note:</strong> Please take a look at our detailed information regarding <a href=#stable-worker-node-hash-support-in-gardener-provider-extensions>stable Worker node hash support in Gardener Provider Extensions</a>.</p></blockquote><p>As a shoot owner:</p><ul><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies>Check if you have dependencies to the <code>docker</code> container runtime</a>.</li></ul><blockquote><p><strong>Note:</strong> This is not only about your actual workload, but also concerns ops tooling as well as logging, monitoring, and metric agents installed on the nodes.</p></blockquote><ul><li>Test with <code>containerd</code>:<ul><li>Create a new Shoot or add a Worker Pool to an existing one.</li><li><a href=/docs/gardener/api-reference/core/#cri>Set <code>.spec.provider.workers[].cri.name: containerd</code></a> for your Shoot.</li></ul></li><li>Once testing is successful, switch to <code>containerd</code> with your production workload. You don&rsquo;t need to wait for kubernetes v1.22, <code>containerd</code> is considered production ready as of today.</li><li>If you find dependencies to <code>docker</code>, set <code>.spec.provider.workers[].cri.name: docker</code> explicitly to avoid defaulting to <code>containerd</code> once you update your Shoot to kubernetes v1.22.</li></ul><h2 id=timeline>Timeline</h2><ul><li><strong>2021-08-04:</strong> Kubernetes v1.22 released. Shoots using this version get <code>containerd</code> as default container runtime. Shoots can still select <code>docker</code> explicitly if needed.</li><li><strong>2021-12-07:</strong> Kubernetes v1.23 released. Shoots using this version can no longer select <code>docker</code> as container runtime.</li><li><strong>2022-06-28:</strong> Kubernetes v1.21 goes out of maintenance. This is the last version not affected by these changes. Make sure you have tested thoroughly and set the correct configuration for your Shoots!</li><li><strong>2022-10-28:</strong> Kubernetes v1.22 goes out of maintenance. This is the last version that you can use with <code>docker</code> as container runtime. Make sure you have removed any dependencies to <code>docker</code> as container runtime!</li></ul><p>See <a href=https://kubernetes.io/releases/>the official kubernetes documentation</a> for the exact dates for all releases.</p><h2 id=container-runtime-support-in-gardener-operating-system-extensions>Container Runtime Support in Gardener Operating System Extensions</h2><table><thead><tr><th>Operating System</th><th>docker support</th><th>containerd support</th></tr></thead><tbody><tr><td>GardenLinux</td><td>✅</td><td>>= v0.3.0</td></tr><tr><td>Ubuntu</td><td>✅</td><td>>= v1.4.0</td></tr><tr><td>SuSE CHost</td><td>✅</td><td>>= v1.14.0</td></tr><tr><td>CoreOS/FlatCar</td><td>✅</td><td>>= v1.8.0</td></tr></tbody></table><blockquote><p><strong>Note</strong>: If you&rsquo;re using a different Operating System Extension, start evaluating now if it provides support for <code>containerd</code>. Please refer to <a href=/docs/gardener/extensions/operatingsystemconfig/#cri-support>our documentation of the <code>operatingsystemconfig</code> contract</a> to understand how to support <code>containerd</code> for an Operating System Extension.</p></blockquote><h2 id=stable-worker-node-hash-support-in-gardener-provider-extensions>Stable Worker Node Hash Support in Gardener Provider Extensions</h2><p>Upgrade to these versions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>.</p><table><thead><tr><th>Provider Extension</th><th>Stable worker hash support</th></tr></thead><tbody><tr><td>Alicloud</td><td>>= 1.26.0</td></tr><tr><td>AWS</td><td>>= 1.27.0</td></tr><tr><td>Azure</td><td>>= 1.21.0</td></tr><tr><td>GCP</td><td>>= 1.18.0</td></tr><tr><td>OpenStack</td><td>>= 1.21.0</td></tr><tr><td>vSphere</td><td>>= 0.11.0</td></tr></tbody></table><blockquote><p><strong>Note</strong>: If you&rsquo;re using a different Provider Extension, start evaluating now if it keeps the worker hash stable when switching from <code>.spec.provider.workers[].cri: nil</code> to <code>.spec.provider.workers[].cri.name: docker</code>. This doesn&rsquo;t impact functional correctness, however, a node rollout will be triggered when users decide to configure <code>docker</code> for their shoots.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-c846de8dc75ecb3d656ca901da206be3>1.7.13 - ExposureClasses</h1><h1 id=exposureclasses>ExposureClasses</h1><p>The Gardener API server provides a cluster-scoped <code>ExposureClass</code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ, etc.</p><h2 id=background>Background</h2><p>The <code>ExposureClass</code> resource is based on the concept for the <code>RuntimeClass</code> resource in Kubernetes.</p><p>A <code>RuntimeClass</code> abstracts the installation of a certain container runtime (e.g., gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>Runtime Class</a> for more information.</p><p>In contrast, an <code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g., corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.</p><p>Example: <code>RuntimeClass</code> and <code>ExposureClass</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: node.k8s.io/v1
</span></span><span style=display:flex><span>kind: RuntimeClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gvisor
</span></span><span style=display:flex><span>handler: gvisorconfig
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   nodeSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     env: prod</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: internet
</span></span><span style=display:flex><span>handler: internet-config
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   seedSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     matchLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#       network/env: internet</span>
</span></span></code></pre></div><p>Similar to <code>RuntimeClasses</code>, <code>ExposureClasses</code> also define a <code>.handler</code> field reflecting the name reference for the corresponding CRI configuration of the <code>RuntimeClass</code> and the control plane exposure configuration for the <code>ExposureClass</code>.</p><p>The CRI handler for <code>RuntimeClasses</code> is usually installed by an administrator (e.g., via a <code>DaemonSet</code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for <code>ExposureClasses</code> will be also provided by an administrator.
This exposure configuration is part of the gardenlet configuration, as this component is responsible to configure the control plane accordingly.
See the <a href=#gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers</a> section for more information.</p><p>The <code>RuntimeClass</code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its <code>.scheduling</code> section.
The <code>ExposureClass</code> also supports the selection of a subset of available Seed clusters whose gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its <code>.scheduling</code> section.</p><h2 id=usage-by-a-shoot>Usage by a <code>Shoot</code></h2><p>A <code>Shoot</code> can reference an <code>ExposureClass</code> via the <code>.spec.exposureClassName</code> field.</p><blockquote><p>⚠️ When creating a <code>Shoot</code> resource, the Gardener scheduler will try to assign the <code>Shoot</code> to a <code>Seed</code> which will host its control plane.</p></blockquote><p>The scheduling behaviour can be influenced via the <code>.spec.seedSelectors</code> and/or <code>.spec.tolerations</code> fields in the <code>Shoot</code>.
<code>ExposureClass</code>es can also contain scheduling instructions.
If a <code>Shoot</code> is referencing an <code>ExposureClass</code>, then the scheduling instructions of both will be merged into the <code>Shoot</code>.
Those unions of scheduling instructions might lead to a selection of a <code>Seed</code> which is not able to deal with the <code>handler</code> of the <code>ExposureClass</code> and the <code>Shoot</code> creation might end up in an error.
In such case, the <code>Shoot</code> scheduling instructions should be revisited to check that they are not interfering with the ones from the <code>ExposureClass</code>.
If this is not feasible, then the combination with the <code>ExposureClass</code> might not be possible and you need to contact your Gardener administrator.</p><details><summary>Example: Shoot and ExposureClass scheduling instructions merge flow</summary><ol><li>Assuming there is the following <code>Shoot</code> which is referencing the <code>ExposureClass</code> below:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>handler: abc
</span></span><span style=display:flex><span>scheduling:
</span></span><span style=display:flex><span>  seedSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=2><li>Both <code>seedSelectors</code> would be merged into the <code>Shoot</code>. The result would be the following:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=3><li>Now the Gardener Scheduler would try to find a <code>Seed</code> with those labels.</li></ol><ul><li>If there are <strong>no</strong> Seeds with matching labels for the seed selector, then the <code>Shoot</code> will be unschedulable.</li><li>If there are Seeds with matching labels for the seed selector, then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see <a href=/docs/gardener/concepts/scheduler/#algorithm-overview>Gardener Scheduler</a>.<ul><li>If the <code>Seed</code> is <strong>not</strong> able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the Shoot will end up in error state.</li><li>If the <code>Seed</code> is able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the <code>Shoot</code> will be created.</li></ul></li></ul></details><h2 id=gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers</h2><p>The gardenlet is responsible to realize the control plane exposure strategy defined in the referenced <code>ExposureClass</code> of a <code>Shoot</code>.</p><p>Therefore, the <code>GardenletConfiguration</code> can contain an <code>.exposureClassHandlers</code> list with the respective configuration.</p><p>Example of the <code>GardenletConfiguration</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>exposureClassHandlers:
</span></span><span style=display:flex><span>- name: internet-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internet
</span></span><span style=display:flex><span>- name: internal-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internal
</span></span><span style=display:flex><span>  sni:
</span></span><span style=display:flex><span>    ingress:
</span></span><span style=display:flex><span>      namespace: ingress-internal
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        network: internal
</span></span></code></pre></div><p>Each gardenlet can define how the handler of a certain <code>ExposureClass</code> needs to be implemented for the Seed(s) where it is responsible for.</p><p>The <code>.name</code> is the name of the handler config and it must match to the <code>.handler</code> in the <code>ExposureClass</code>.</p><p>All control planes on a <code>Seed</code> are exposed via a load balancer, either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified, which can be done via the <code>.loadBalancerService</code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.</p><p>In case the gardenlet runs with activated <code>APIServerSNI</code> feature flag (default), the control planes on a <code>Seed</code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the <code>Seed</code>.
The configuration of the ingress gateways can be controlled via the <code>.sni</code> section in the same way like for the default ingress gateways.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-455bea128db452349701d12b6d003975>1.7.14 - Ipv6</h1><h1 id=ipv6-in-gardener-clusters>IPv6 in Gardener Clusters</h1><blockquote><p>🚧 IPv6 networking is currently under development.</p></blockquote><h2 id=ipv6-single-stack-networking>IPv6 Single-Stack Networking</h2><p><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/21-ipv6-singlestack-local.md>GEP-21</a> proposes IPv6 Single-Stack Support in the local Gardener environment.
This documentation will be enhanced while implementing GEP-21, see <a href=https://github.com/gardener/gardener/issues/7051>gardener/gardener#7051</a>.</p><p>To use IPv6 single-stack networking, the <a href=/docs/gardener/deployment/feature_gates/>feature gate</a> <code>IPv6SingleStack</code> must be enabled on gardener-apiserver.</p><h2 id=development-setup>Development Setup</h2><p>Developing or testing IPv6-related features requires a Linux machine (docker only supports IPv6 on Linux) and native IPv6 connectivity to the internet.
If you&rsquo;re on a different OS or don&rsquo;t have IPv6 connectivity in your office environment or via your home ISP, make sure to check out <a href=https://github.com/gardener-community/dev-box-gcp>gardener-community/dev-box-gcp</a>, which allows you to circumvent these limitations.</p><h2 id=container-images>Container Images</h2><p>If you plan on using custom images, make sure your registry supports IPv6 access.
The <code>docker.io</code> registry doesn&rsquo;t support pulling images over IPv6 (see <a href=https://www.docker.com/blog/beta-ipv6-support-on-docker-hub-registry/>Beta IPv6 Support on Docker Hub Registry</a>).
Use the <a href=https://cloud.google.com/container-registry/docs/pulling-cached-images>Google Mirror</a> of Docker Hub instead which supports dual-stack network access.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e25b3c63361fa049e4ac90bb38e50ff1>1.7.15 - Istio</h1><h1 id=istio>Istio</h1><p><a href=https://istio.io>Istio</a> offers a service mesh implementation with focus on several important features - traffic, observability, security, and policy.</p><h2 id=gardener-managedistio-feature-gate>Gardener <code>ManagedIstio</code> Feature Gate</h2><p>When enabled in gardenlet, the <code>ManagedIstio</code> feature gate can be used to deploy a Gardener-tailored Istio installation in Seed clusters. Its main usage is to enable features such as <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>Shoot API server SNI</a>. This feature should not be enabled on a Seed cluster where Istio is already deployed.</p><p>However, this feature gate is deprecated, turned on by default and will be removed in a future version of Gardener.
This means that Gardener will unconditionally deploy Istio with its desired configuration to seed clusters.
Consequently, existing/bring-your-own Istio deployments will no longer be supported.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> enabled.</li><li>Kubernetes 1.16+</li></ul><h2 id=differences-with-istios-default-profile>Differences with Istio&rsquo;s Default Profile</h2><p>The <a href=https://istio.io/docs/setup/additional-setup/config-profiles/>default profile</a> which is recommended for production deployment, is not suitable for the Gardener use case, as it offers more functionality than desired. The current installation goes through heavy refactorings due to the <code>IstioOperator</code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:</p><ul><li>Telemetry is not deployed.</li><li><code>istiod</code> is deployed.</li><li><code>istio-ingress-gateway</code> is deployed in a separate <code>istio-ingress</code> namespace.</li><li><code>istio-egress-gateway</code> is not deployed.</li><li>None of the Istio addons are deployed.</li><li>Mixer (deprecated) is not deployed.</li><li>Mixer CDRs are not deployed.</li><li>Kubernetes <code>Service</code>, Istio&rsquo;s <code>VirtualService</code> and <code>ServiceEntry</code> are <strong>NOT</strong> advertised in the service mesh. This means that if a <code>Service</code> needs to be accessed directly from the Istio Ingress Gateway, it should have <code>networking.istio.io/exportTo: "*"</code> annotation. <code>VirtualService</code> and <code>ServiceEntry</code> must have <code>.spec.exportTo: ["*"]</code> set on them respectively.</li><li>Istio injector is not enabled.</li><li>mTLS is enabled by default.</li></ul><h2 id=handling-multiple-availability-zones-with-istio>Handling Multiple Availability Zones with Istio</h2><p>For various reasons, e.g., improved resiliency to certain failures, it may be beneficial to use multiple availability zones in a seed cluster. While availability zones have advantages in being able to cover some failure domains, they also come with some additional challenges. Most notably, the latency across availability zone boundaries is higher than within an availability zone. Furthermore, there might be additional cost implied by network traffic crossing an availability zone boundary. Therefore, it may be useful to try to keep traffic within an availability zone if possible. The istio deployment as part of Gardener has been adapted to allow this.</p><p>A seed cluster spanning multiple availability zones may be used for <a href=/docs/gardener/usage/shoot_high_availability/>highly-available shoot control planes</a>. Those control planes may use a single or multiple availability zones. In addition to that, ordinary non-highly-available shoot control planes may be scheduled to such a seed cluster as well. The result is that the seed cluster may have control planes spanning multiple availability zones and control planes that are pinned to exactly one availability zone. These two types need to be handled differently when trying to prevent unnecessary cross-zonal traffic.</p><p>The goal is achieved by using multiple istio ingress gateways. The default istio ingress gateway spans all availability zones. It is used for multi-zonal shoot control planes. For each availability zone, there is an additional istio ingress gateway, which is utilized only for single-zone shoot control planes pinned to this availability zone. This is illustrated in the following diagram.</p><p><img src=/__resources/multi-zonal-istio_bf47f6.png alt="Multi Availability Zone Handling in Istio"></p><p>Please note that operators may need to perform additional tuning to prevent cross-zonal traffic completely. The <a href=/docs/gardener/usage/seed_settings/#load-balancer-services>loadbalancer settings in the seed specification</a> offer various options, e.g., by setting the external traffic policy to <code>local</code> or using infrastructure specific loadbalancer annotations.</p><p>Furthermore, note that this approach is also taken in case <a href=/docs/gardener/usage/exposureclasses/><code>ExposureClass</code>es</a> are used. For each exposure class, additional zonal istio ingress gateways may be deployed to cover for single-zone shoot control planes using the exposure class.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c25983cd195ac0d27b90213377bdf169>1.7.16 - Logging</h1><h1 id=logging-stack>Logging Stack</h1><h2 id=motivation>Motivation</h2><p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components by introducing its own logging stack.</p><h2 id=components>Components</h2><ul><li>A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Loki instances.</li><li>One Loki Statefulset in the <code>garden</code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&rsquo;s controlplane.</li><li>One Grafana Deployment in <code>garden</code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Grafana is the UI component used in the logging stack.</li></ul><p><img src=/__resources/logging-architecture_c8dc32.png alt></p><h2 id=container-logs-rotation-and-retention>Container Logs Rotation and Retention</h2><p>Container <a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation>log rotation</a> in Kubernetes describes a subtile but important implementation detail depending on the type of the used high-level container runtime. When the used container runtime is not CRI compliant (such as <code>dockershim</code>), then the <code>kubelet</code> does not provide any rotation or retention implementations, hence leaving those aspects to the downstream components. When the used container runtime is CRI compliant (such as <code>containerd</code>), then the <code>kubelet</code> provides the necessary implementation with two configuration options:</p><ul><li><code>ContainerLogMaxSize</code> for rotation</li><li><code>ContainerLogMaxFiles</code> for retention</li></ul><h3 id=docker-container-runtime>Docker Container Runtime</h3><p>In this case, the log rotation and retention is implemented by a <code>logrotate</code> service provisioned by Gardener, which rotates logs once <code>100M</code> size is reached. Logs are compressed on daily basis and retained for a maximum period of <code>14d</code>.</p><h3 id=containerd-runtime>ContainerD Runtime</h3><p>In this case, it is possible to configure the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields in the Shoot specification. Both fields are optional and if nothing is specified, then the <code>kubelet</code> rotates on the same size <code>100M</code> as in the <code>docker</code> container runtime. Those fields are part of provider&rsquo;s workers definition. Here is an example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>      - cri:
</span></span><span style=display:flex><span>          name: containerd
</span></span><span style=display:flex><span>        kubernetes:
</span></span><span style=display:flex><span>          kubelet:
</span></span><span style=display:flex><span>            <span style=color:green># accepted values are of resource.Quantity</span>
</span></span><span style=display:flex><span>            containerLogMaxSize: 150Mi
</span></span><span style=display:flex><span>            containerLogMaxFiles: 10
</span></span></code></pre></div><p>The values of the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields need to be considered with care since container log files claim disk space from the host. On the opposite side, log rotations on too small sizes may result in frequent rotations which can be missed by other components (log shippers) observing these rotations.</p><p>In the majority of the cases, the defaults should do just fine. Custom configuration might be of use under rare conditions.</p><h2 id=extension-of-the-logging-stack>Extension of the Logging Stack</h2><p>The logging stack is extended to scrape logs from the systemd services of each shoots&rsquo; nodes and from all Gardener components in the shoot <code>kube-system</code> namespace. These logs are exposed only to the Gardener operators.</p><p>Also, in the shoot control plane an <code>event-logger</code> pod is deployed, which scrapes events from the shoot <code>kube-system</code> namespace and shoot <code>control-plane</code> namespace in the seed. The <code>event-logger</code> logs the events to the standard output. Then the <code>fluent-bit</code> gets these events as container logs and sends them to the Loki in the shoot control plane (similar to how it works for any other control plane component).
<img src=/__resources/shoot-node-logging-architecture_23c018.png alt></p><h2 id=how-to-access-the-logs>How to Access the Logs</h2><p>The logs are accessible via Grafana. To access them:</p><ol><li><p>Authenticate via basic auth to gain access to Grafana.
The Grafana URL can be found in the <code>Logging and Monitoring</code> section of a cluster in the Gardener Dashboard alongside the credentials.
The secret containing the credentials is stored in the project namespace following the naming pattern <code>&lt;shoot-name>.monitoring</code>.
For Gardener operators, the credentials are also stored in the control-plane (<code>shoot--&lt;project-name>--&lt;shoot-name></code>) namespace in the <code>observability-ingress-users-&lt;hash></code> secret in the seed.</p></li><li><p>Grafana contains several dashboards that aim to facilitate the work of operators and users.
From the <code>Explore</code> tab, users and operators have unlimited abilities to extract and manipulate logs.</p></li></ol><blockquote><p><strong>Note:</strong> Gardener Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!</p></blockquote><h3 id=how-to-use-the-explore-tab>How to Use the <code>Explore</code> Tab</h3><p>If you click on the <code>Log browser ></code> button, you will see all of the available labels.
Clicking on the label, you can see all of its available values for the given period of time you have specified.
If you are searching for logs for the past one hour, do not expect to see labels or values for which there were no logs for that period of time.
By clicking on a value, Grafana automatically eliminates all other labels and/or values with which no valid log stream can be made.
After choosing the right labels and their values, click on the <code>Show logs</code> button.
This will build <code>Log query</code> and execute it.
This approach is convenient when you don&rsquo;t know the labels names or they values.
<img src=/__resources/explore-button-usage_0dfdca.png alt></p><p>Once you feel comfortable, you can start to use the <a href=https://grafana.com/docs/loki/latest/logql/log_queries/>LogQL</a> language to search for logs.
Next to the <code>Log browser ></code> button is the place where you can type log queries.</p><p>Examples:</p><ol><li><p>If you want to get logs for <code>calico-node-&lt;hash></code> pod in the cluster <code>kube-system</code>:
The name of the node on which <code>calico-node</code> was running is known, but not the hash suffix of the <code>calico-node</code> pod.
Also we want to search for errors in the logs.</p><p><code>{pod_name=~"calico-node-.+", nodename="ip-10-222-31-182.eu-central-1.compute.internal"} |~ "error"</code></p><p>Here, you will get as much help as possible from the Grafana by giving you suggestions and auto-completion.</p></li><li><p>If you want to get the logs from <code>kubelet</code> systemd service of a given node and search for a pod name in the logs:</p><p><code>{unit="kubelet.service", nodename="ip-10-222-31-182.eu-central-1.compute.internal"} |~ "pod name"</code></p></li></ol><blockquote><p><strong>Note:</strong> Under <code>unit</code> label there is only the <code>docker</code>, <code>containerd</code>, <code>kubelet</code> and <code>kernel</code> logs.</p></blockquote><ol start=3><li><p>If you want to get the logs from <code>cloud-config-downloader</code> systemd service of a given node and search for a string in the logs:</p><p><code>{job="systemd-combine-journal",nodename="ip-10-222-31-182.eu-central-1.compute.internal"} | unpack | unit="cloud-config-downloader.service" |~ "last execution was"</code></p></li></ol><blockquote><p><strong>Note:</strong> <code>{job="systemd-combine-journal",nodename="&lt;node name>"}</code> stream <a href=https://grafana.com/docs/loki/latest/clients/promtail/stages/pack/>pack</a> all logs from systemd services except <code>docker</code>, <code>containerd</code>, <code>kubelet</code>, and <code>kernel</code>. To filter those log by unit, you have to <a href=https://grafana.com/docs/loki/latest/logql/log_queries/#unpack>unpack</a> them first.</p></blockquote><ol start=4><li>Retrieving events:</li></ol><ul><li><p>If you want to get the events from the shoot <code>kube-system</code> namespace generated by <code>kubelet</code> and related to the <code>node-problem-detector</code>:</p><p><code>{job="event-logging"} | unpack | origin_extracted="shoot",source="kubelet",object=~".*node-problem-detector.*"</code></p></li><li><p>If you want to get the events generated by MCM in the shoot control plane in the seed:</p><p><code>{job="event-logging"} | unpack | origin_extracted="seed",source=~".*machine-controller-manager.*"</code></p></li></ul><blockquote><p><strong>Note:</strong> In order to group events by origin, one has to specify <code>origin_extracted</code> because the <code>origin</code> label is reserved for all of the logs from the seed and the <code>event-logger</code> resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked, the embedded <code>origin</code> becomes <code>origin_extracted</code>.</p></blockquote><h2 id=expose-logs-for-component-to-user-grafana>Expose Logs for Component to User Grafana</h2><p>Exposing logs for a new component to the User&rsquo;s Grafana is described in the <a href=/docs/gardener/extensions/logging-and-monitoring/#how-to-expose-logs-to-the-users>How to Expose Logs to the Users</a> section.</p><h2 id=configuration>Configuration</h2><h3 id=fluent-bit>Fluent-bit</h3><p>The Fluent-bit configurations can be found on <code>charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml</code>
There are five different specifications:</p><ul><li>SERVICE: Defines the location of the server specifications.</li><li>INPUT: Defines the location of the input stream of the logs.</li><li>OUTPUT: Defines the location of the output source (Loki for example).</li><li>FILTER: Defines filters which match specific keys.</li><li>PARSER: Defines parsers which are used by the filters.</li></ul><h3 id=loki>Loki</h3><p>The Loki configurations can be found on <code>charts/seed-bootstrap/charts/loki/templates/loki-configmap.yaml</code></p><p>The main specifications there are:</p><ul><li>Index configuration: Currently the following one is used:</li></ul><pre tabindex=0><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
</code></pre><ul><li><code>from</code>: Is the date from which logs collection is started. Using a date in the past is okay.</li><li><code>store</code>: The DB used for storing the index.</li><li><code>object_store</code>: Where the data is stored.</li><li><code>schema</code>: Schema version which should be used (v11 is currently recommended).</li><li><code>index.prefix</code>: The prefix for the index.</li><li><code>index.period</code>: The period for updating the indices.</li></ul><p><strong>Adding a new index happens with new config block definition. The <code>from</code> field should start from the current day + previous <code>index.period</code> and should not overlap with the current index. The <code>prefix</code> also should be different.</strong></p><pre tabindex=0><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
      - from: 2020-06-18
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_new_
          period: 24h
</code></pre><ul><li>chunk_store_config Configuration</li></ul><pre tabindex=0><code>    chunk_store_config:
      max_look_back_period: 336h
</code></pre><p><strong><code>chunk_store_config.max_look_back_period</code> should be the same as the <code>retention_period</code></strong></p><ul><li>table_manager Configuration</li></ul><pre tabindex=0><code>    table_manager:
      retention_deletes_enabled: true
      retention_period: 336h
</code></pre><p><code>table_manager.retention_period</code> is the living time for each log message. Loki will keep messages for (<code>table_manager.retention_period</code> - <code>index.period</code>) time due to specification in the Loki implementation.</p><h3 id=grafana>Grafana</h3><p>The Grafana configurations can be found on <code>charts/seed-bootstrap/charts/templates/grafana/grafana-datasources-configmap.yaml</code> and
<code>charts/seed-monitoring/charts/grafana/tempates/grafana-datasources-configmap.yaml</code></p><p>This is the Loki configuration that Grafana uses:</p><pre tabindex=0><code>    - name: loki
      type: loki
      access: proxy
      url: http://loki.{{ .Release.Namespace }}.svc:3100
      jsonData:
        maxLines: 5000
</code></pre><ul><li><code>name</code>: Is the name of the datasource.</li><li><code>type</code>: Is the type of the datasource.</li><li><code>access</code>: Should be set to proxy.</li><li><code>url</code>: Loki&rsquo;s url</li><li><code>svc</code>: Loki&rsquo;s port</li><li><code>jsonData.maxLines</code>: The limit of the log messages which Grafana will show to the users.</li></ul><p><strong>Decrease this value if the browser works slowly!</strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b03cef45b6b25f644cf6ab64d3c5c66c>1.7.17 - Managed Seed</h1><h1 id=register-shoot-as-seed>Register Shoot as Seed</h1><p>An existing shoot can be registered as a seed by creating a <code>ManagedSeed</code> resource. This resource contains:</p><ul><li>The name of the shoot that should be registered as seed.</li><li>A <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p><code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section.</p><blockquote><p><strong>Note:</strong> Earlier Gardener allowed specifying a <code>seedTemplate</code> directly in the <code>ManagedSeed</code> resource. This feature is discontinued, any seed configuration must be via the <code>GardenletConfiguration</code>.</p></blockquote><p>Note the following important aspects:</p><ul><li>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. Currently, managed seeds are restricted to the <code>garden</code> namespace.</li><li>The newly created <code>Seed</code> resource always has the same name as the <code>ManagedSeed</code> resource. Attempting to specify a different name in the <code>seedConfig</code> will fail.</li><li>The <code>ManagedSeed</code> resource must always refer to an existing shoot. Attempting to create a <code>ManagedSeed</code> referring to a non-existing shoot will fail.</li><li>A shoot that is being referred to by a <code>ManagedSeed</code> cannot be deleted. Attempting to delete such a shoot will fail.</li><li>You can omit practically everything from the <code>gardenlet</code> section, including all or most of the <code>Seed</code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the <code>Shoot</code> resource.</li><li>Also, if your seed is configured to host HA shoot control planes, then <code>gardenlet</code> will be deployed with multiple replicas across nodes or availability zones by default.</li><li>Some <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.</li></ul><h2 id=deploying-gardenlet-to-the-shoot>Deploying gardenlet to the Shoot</h2><p>To register a shoot as a seed and deploy <code>gardenlet</code> to the shoot using a default configuration, create a <code>ManagedSeed</code> resource similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-managed-seed
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    name: crazy-botany
</span></span><span style=display:flex><span>  gardenlet: {}
</span></span></code></pre></div><p>For an example that uses non-default configuration, see <a href=https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml>55-managed-seed-gardenlet.yaml</a></p><h3 id=renewing-the-gardenlet-kubeconfig-secret>Renewing the Gardenlet Kubeconfig Secret</h3><p>In order to make the <code>ManagedSeed</code> controller renew the gardenlet&rsquo;s kubeconfig secret, annotate the <code>ManagedSeed</code> with <code>gardener.cloud/operation=renew-kubeconfig</code>. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).</p><p>It is also possible to trigger the renewal on the secret directly, see <a href=/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig>Rotate Certificates Using Bootstrap kubeconfig</a>.</p><h3 id=specifying-apiserver-replicas-and-autoscaler-options>Specifying <code>apiServer</code> <code>replicas</code> and <code>autoscaler</code> Options</h3><p>There are few configuration options that are not supported in a <code>Shoot</code> resource but due to backward compatibility reasons it is possible to specify them for a <code>Shoot</code> that is referred by a <code>ManagedSeed</code>. These options are:</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>apiServer.autoscaler.minReplicas</code></td><td>Controls the minimum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.autoscaler.maxReplicas</code></td><td>Controls the maximum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.replicas</code></td><td>Controls how many <code>kube-apiserver</code> replicas the shoot registered as seed cluster gets by default.</td></tr></tbody></table><p>It is possible to specify these options via the <code>shoot.gardener.cloud/managed-seed-api-server</code> annotation on the Shoot resource. Example configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    shoot.gardener.cloud/managed-seed-api-server: <span style=color:#a31515>&#34;apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6&#34;</span>
</span></span></code></pre></div><h3 id=enforced-configuration-options>Enforced Configuration Options</h3><p>The following configuration options are enforced by Gardener API server for the ManagedSeed resources:</p><ol><li><p>The vertical pod autoscaler should be enabled from the Shoot specification.</p><p>The vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed <a href=/docs/gardener/usage/seed_settings/#vertical-pod-autoscaler>(using the Seed spec)</a> and for a Shoot <a href=/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling>(using the Shoot spec)</a>. In context of <code>ManagedSeed</code>s, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.</p></li><li><p>The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.</p><p>An Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster, it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-34837cc04520de863b3e8452aabb4fe7>1.7.18 - Node Readiness</h1><h1 id=readiness-of-shoot-worker-nodes>Readiness of Shoot Worker Nodes</h1><h2 id=background>Background</h2><p>When registering new <code>Nodes</code>, kubelet adds the <code>node.kubernetes.io/not-ready</code> taint to prevent scheduling workload Pods to the <code>Node</code> until the <code>Ready</code> condition gets <code>True</code>.
However, the kubelet does not consider the readiness of node-critical Pods.
Hence, the <code>Ready</code> condition might get <code>True</code> and the <code>node.kubernetes.io/not-ready</code> taint might get removed, for example, before the CNI daemon Pod (e.g., <code>calico-node</code>) has successfully placed the CNI binaries on the machine.</p><p>This problem has been discussed extensively in kubernetes, e.g., in <a href=https://github.com/kubernetes/kubernetes/issues/75890>kubernetes/kubernetes#75890</a>.
However, several proposals have been rejected because the problem can be solved by using the <code>--register-with-taints</code> kubelet flag and dedicated controllers (<a href=https://github.com/kubernetes/enhancements/pull/1003#issuecomment-619087019>ref</a>).</p><h2 id=implementation-in-gardener>Implementation in Gardener</h2><p>Gardener makes sure that workload Pods are only scheduled to <code>Nodes</code> where all node-critical components required for running workload Pods are ready.
For this, Gardener follows the proposed solution by the Kubernetes community and registers new <code>Node</code> objects with the <code>node.gardener.cloud/critical-components-not-ready</code> taint (effect <code>NoSchedule</code>).
gardener-resource-manager&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#node-controller><code>Node</code> controller</a> reacts on newly created <code>Node</code> objects that have this taint.
The controller removes the taint once all node-critical Pods are ready (determined by checking the Pods&rsquo; <code>Ready</code> conditions).</p><p>The <code>Node</code> controller considers all <code>DaemonSets</code> and <code>Pods</code> with the label <code>node.gardener.cloud/critical-component=true</code> as node-critical.
If there are <code>DaemonSets</code> that contain the <code>node.gardener.cloud/critical-component=true</code> label in their metadata and in their Pod template, the <code>Node</code> controller waits for corresponding daemon Pods to be scheduled and to get ready before removing the taint.</p><h2 id=marking-node-critical-components>Marking Node-Critical Components</h2><p>To make use of this feature, node-critical DaemonSets and Pods need to:</p><ul><li>Tolerate the <code>node.gardener.cloud/critical-components-not-ready</code> <code>NoSchedule</code> taint.</li><li>Be labelled with <code>node.gardener.cloud/critical-component=true</code>.</li></ul><p>Gardener already marks components like kube-proxy, apiserver-proxy and node-local-dns as node-critical.
Provider extensions mark components like csi-driver-node as node-critical.
Network extensions mark components responsible for setting up CNI on worker Nodes (e.g., <code>calico-node</code>) as node-critical.
If shoot owners manage any additional node-critical components, they can make use of this feature as well.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8d5f560b3aa48903f7142745f2944770>1.7.19 - NodeLocalDNS Configuration</h1><h1 id=nodelocaldns-configuration>NodeLocalDNS Configuration</h1><p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.</p><h2 id=background>Background</h2><p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)</li><li>and more &mldr;</li></ul><p>To workaround the issues described above, <code>node-local-dns</code> was introduced. The architecture is described below. The idea is simple:</p><ul><li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.</li><li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.</li></ul><p><img src=/__resources/node-local-dns_6d452a.png alt=node-local-dns-architecture></p><h2 id=configuring-nodelocaldns>Configuring NodeLocalDNS</h2><p>All that needs to be done to enable the usage of the <code>node-local-dns</code> feature is to set the corresponding option (<code>spec.systemComponents.nodeLocalDNS.enabled</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>It is worth noting that:</p><ul><li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.</li><li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.</li><li>The annotation will take effect during the next shoot reconciliation. This happens automatically once per day in the maintenance period (unless you have disabled it).</li><li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, DNS requests are repeated for some time as UDP is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.</li><li>If a short DNS outage is not a big issue, you can <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>trigger reconciliation</a> directly after setting the annotation.</li><li>Switching node-local-dns off by removing the annotation can be a rather destructive operation that will result in pods without a working DNS configuration.</li></ul><p>For more information about <code>node-local-dns</code>, please refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md>KEP</a> or to the <a href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/>usage documentation</a>.</p><h2 id=known-issues>Known Issues</h2><p>Custom DNS configuration may not work as expected in conjunction with <code>NodeLocalDNS</code>.
Please refer to <a href=/docs/gardener/usage/custom-dns-config/#node-local-dns>Custom DNS Configuration</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6aba6b44b714cf2748c3009e8eadd0c0>1.7.20 - OpenIDConnect Presets</h1><h1 id=clusteropenidconnectpreset-and-openidconnectpreset>ClusterOpenIDConnectPreset and OpenIDConnectPreset</h1><p>This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting <a href=https://openid.net/connect/>OpenIDConnect Configuration</a> into <code>Shoot</code> at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.</p><h2 id=openidconnectpreset>OpenIDConnectPreset</h2><p>An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the <code>Shoot</code> to which a given OpenIDConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in their <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for OpenIDConnectPreset.</p><h3 id=how-openidconnectpreset-works>How OpenIDConnectPreset Works</h3><p>Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all OpenIDConnectPreset available for use in the <code>Shoot</code> namespace.</p></li><li><p>Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names (e.g., <code>002preset</code> > <code>001preset</code>)</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code>, then no mutation occurs.</p></li></ul><h3 id=simple-openidconnectpreset-example>Simple OpenIDConnectPreset Example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: settings.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OpenIDConnectPreset
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name:  test-1
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      oidc: enabled
</span></span><span style=display:flex><span>  server:
</span></span><span style=display:flex><span>    clientID: test-1
</span></span><span style=display:flex><span>    issuerURL: https://foo.bar
</span></span><span style=display:flex><span>    <span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   Li4u</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    groupsClaim: groups-claim
</span></span><span style=display:flex><span>    groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>    usernameClaim: username-claim
</span></span><span style=display:flex><span>    usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    signingAlgs:
</span></span><span style=display:flex><span>    - RS256
</span></span><span style=display:flex><span>    requiredClaims:
</span></span><span style=display:flex><span>      key: value
</span></span><span style=display:flex><span>  client:
</span></span><span style=display:flex><span>    secret: oidc-client-secret
</span></span><span style=display:flex><span>    extraConfig:
</span></span><span style=display:flex><span>      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  weight: 90
</span></span></code></pre></div><p>Create the OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f preset.yaml
</span></span></code></pre></div><p>Examine the created OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get openidconnectpresets
</span></span><span style=display:flex><span>NAME     ISSUER            SHOOT-SELECTOR   AGE
</span></span><span style=display:flex><span>test-1   https://foo.bar   oidc=enabled     1s
</span></span></code></pre></div><p>Simple <code>Shoot</code> example:</p><p>This is a sample of a <code>Shoot</code> with some fields omitted:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><p>Create the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f shoot.yaml
</span></span></code></pre></div><p>Examine the created Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get shoot preset -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientAuthentication:
</span></span><span style=display:flex><span>          extraConfig:
</span></span><span style=display:flex><span>            extra-scopes: email,offline_access,profile
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>          secret: oidc-client-secret
</span></span><span style=display:flex><span>        clientID: test-1
</span></span><span style=display:flex><span>        groupsClaim: groups-claim
</span></span><span style=display:flex><span>        groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>        issuerURL: https://foo.bar
</span></span><span style=display:flex><span>        requiredClaims:
</span></span><span style=display:flex><span>          key: value
</span></span><span style=display:flex><span>        signingAlgs:
</span></span><span style=display:flex><span>        - RS256
</span></span><span style=display:flex><span>        usernameClaim: username-claim
</span></span><span style=display:flex><span>        usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><h3 id=disable-openidconnectpreset>Disable OpenIDConnectPreset</h3><p>The OpenIDConnectPreset admission control is enabled by default. To disable it, use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>--disable-admission-plugins=OpenIDConnectPreset
</span></span></code></pre></div><h2 id=clusteropenidconnectpreset>ClusterOpenIDConnectPreset</h2><p>A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect, it&rsquo;s a cluster-scoped resource. You use label selectors to specify the <code>Project</code> and <code>Shoot</code> to which a given OpenIDCConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in specific <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for ClusterOpenIDConnectPreset.</p><h3 id=how-clusteropenidconnectpreset-works>How ClusterOpenIDConnectPreset Works</h3><p>Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all ClusterOpenIDConnectPresets available.</p></li><li><p>Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the <code>Project</code> in which the <code>Shoot</code> is being created.</p></li><li><p>Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the <code>Shoot</code> being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names ( e.g. <code>002preset</code> > <code>001preset</code> )</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> then no mutation occurs.</p></li></ul><blockquote><p><strong>Note:</strong> Due to the previous requirement, if a <code>Shoot</code> is matched by both <code>OpenIDConnectPreset</code> and <code>ClusterOpenIDConnectPreset</code>, then <code>OpenIDConnectPreset</code> takes precedence over <code>ClusterOpenIDConnectPreset</code>.</p></blockquote><h3 id=simple-clusteropenidconnectpreset-example>Simple ClusterOpenIDConnectPreset Example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: settings.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ClusterOpenIDConnectPreset
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name:  test
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      oidc: enabled
</span></span><span style=display:flex><span>  projectSelector: {} <span style=color:green># selects all projects.</span>
</span></span><span style=display:flex><span>  server:
</span></span><span style=display:flex><span>    clientID: cluster-preset
</span></span><span style=display:flex><span>    issuerURL: https://foo.bar
</span></span><span style=display:flex><span>    <span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   Li4u</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    groupsClaim: groups-claim
</span></span><span style=display:flex><span>    groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>    usernameClaim: username-claim
</span></span><span style=display:flex><span>    usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    signingAlgs:
</span></span><span style=display:flex><span>    - RS256
</span></span><span style=display:flex><span>    requiredClaims:
</span></span><span style=display:flex><span>      key: value
</span></span><span style=display:flex><span>  client:
</span></span><span style=display:flex><span>    secret: oidc-client-secret
</span></span><span style=display:flex><span>    extraConfig:
</span></span><span style=display:flex><span>      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  weight: 90
</span></span></code></pre></div><p>Create the ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f preset.yaml
</span></span></code></pre></div><p>Examine the created ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get clusteropenidconnectpresets
</span></span><span style=display:flex><span>NAME     ISSUER            PROJECT-SELECTOR   SHOOT-SELECTOR   AGE
</span></span><span style=display:flex><span>test     https://foo.bar   &lt;none&gt;             oidc=enabled     1s
</span></span></code></pre></div><p>This is a sample of a <code>Shoot</code>, with some fields omitted:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><p>Create the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f shoot.yaml
</span></span></code></pre></div><p>Examine the created Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get shoot preset -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientAuthentication:
</span></span><span style=display:flex><span>          extraConfig:
</span></span><span style=display:flex><span>            extra-scopes: email,offline_access,profile
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>          secret: oidc-client-secret
</span></span><span style=display:flex><span>        clientID: cluster-preset
</span></span><span style=display:flex><span>        groupsClaim: groups-claim
</span></span><span style=display:flex><span>        groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>        issuerURL: https://foo.bar
</span></span><span style=display:flex><span>        requiredClaims:
</span></span><span style=display:flex><span>          key: value
</span></span><span style=display:flex><span>        signingAlgs:
</span></span><span style=display:flex><span>        - RS256
</span></span><span style=display:flex><span>        usernameClaim: username-claim
</span></span><span style=display:flex><span>        usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><h3 id=disable-clusteropenidconnectpreset>Disable ClusterOpenIDConnectPreset</h3><p>The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it, use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>--disable-admission-plugins=ClusterOpenIDConnectPreset
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-3b70dcc12d5f6585d051e97f92c4a62e>1.7.21 - Pod Security</h1><h1 id=migrating-from-podsecuritypolicys-to-podsecurity-admission-controller>Migrating from <code>PodSecurityPolicy</code>s to PodSecurity Admission Controller</h1><p>Kubernetes has deprecated the <code>PodSecurityPolicy</code> API in <code>v1.21</code> and it will be removed in <code>v1.25</code>. With <code>v1.23</code>, a new feature called <a href=https://kubernetes.io/docs/concepts/security/pod-security-admission/><code>PodSecurity</code></a> was promoted to beta. From <code>v1.25</code> onwards, there will be no API serving <code>PodSecurityPolicy</code>s, so you have to cleanup all the existing PSPs before upgrading your cluster. Detailed migration steps are described in <a href=https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.</p><p>After migration, you should disable the <code>PodSecurityPolicy</code> admission plugin. To do so, you have to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>admissionPlugins:
</span></span><span style=display:flex><span>- name: PodSecurityPolicy
</span></span><span style=display:flex><span>  disabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>in <code>spec.kubernetes.kubeAPIServer.admissionPlugins</code> field in the <code>Shoot</code> resource. Please refer the example <code>Shoot</code> manifest in <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>90-shoot.yaml</a>.</p><p>Only if the <code>PodSecurityPolicy</code> admission plugin is disabled the cluster can be upgraded to <code>v1.25</code>.</p><blockquote><p>⚠️ You should disable the admission plugin and wait until Gardener finishes at least one <code>Shoot</code> reconciliation before upgrading to <code>v1.25</code>. This is to make sure all the <code>PodSecurityPolicy</code> related resources deployed by Gardener are cleaned up.</p></blockquote><h2 id=admission-configuration-for-the-podsecurity-admission-plugin>Admission Configuration for the <code>PodSecurity</code> Admission Plugin</h2><p>If you wish to add your custom configuration for the <code>PodSecurity</code> plugin and your cluster version is <code>v1.23+</code>, you can do so in the Shoot spec under <code>.spec.kubernetes.kubeAPIServer.admissionPlugins</code> by adding:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>admissionPlugins:
</span></span><span style=display:flex><span>- name: PodSecurity
</span></span><span style=display:flex><span>  config:
</span></span><span style=display:flex><span>    apiVersion: pod-security.admission.config.k8s.io/v1
</span></span><span style=display:flex><span>    kind: PodSecurityConfiguration
</span></span><span style=display:flex><span>    <span style=color:green># Defaults applied when a mode label is not set.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Level label values must be one of:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;privileged&#34; (default)</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;baseline&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;restricted&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Version label values must be one of:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;latest&#34; (default) </span>
</span></span><span style=display:flex><span>    <span style=color:green># - specific version like &#34;v1.25&#34;</span>
</span></span><span style=display:flex><span>    defaults:
</span></span><span style=display:flex><span>      enforce: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      enforce-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>      audit: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      audit-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>      warn: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      warn-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>    exemptions:
</span></span><span style=display:flex><span>      <span style=color:green># Array of authenticated usernames to exempt.</span>
</span></span><span style=display:flex><span>      usernames: []
</span></span><span style=display:flex><span>      <span style=color:green># Array of runtime class names to exempt.</span>
</span></span><span style=display:flex><span>      runtimeClasses: []
</span></span><span style=display:flex><span>      <span style=color:green># Array of namespaces to exempt.</span>
</span></span><span style=display:flex><span>      namespaces: []
</span></span></code></pre></div><p>⚠️ Note that the <code>pod-security.admission.config.k8s.io/v1</code> configuration requires <code>v1.25</code>+. For <code>v1.23</code> and <code>v1.24</code>, use <code>pod-security.admission.config.k8s.io/v1beta1</code>. For <code>v1.22</code>, use <code>pod-security.admission.config.k8s.io/v1alpha1</code>.</p><p>Also note that in <code>v1.22</code>, the feature gate <code>PodSecurity</code> is not enabled by default. You have to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  PodSecurity: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>under <code>.spec.kubernetes.kubeAPIServer</code>.</p><p>For proper functioning of Gardener, <code>kube-system</code> namespace will also be automatically added to the <code>exemptions.namespaces</code> list.</p><h2 id=speckubernetesallowprivilegedcontainers-in-the-shoot-spec><code>.spec.kubernetes.allowPrivilegedContainers</code> in the Shoot Spec</h2><p>If this field is set to <code>true</code>, then all authenticated users can use the &ldquo;gardener.privileged&rdquo; <code>PodSecurityPolicy</code>, allowing full unrestricted access to Pod features. However, the <code>PodSecurityPolicy</code> admission plugin is removed in Kubernetes <code>v1.25</code> and <code>PodSecurity</code> has taken its place as its successor. Therefore, this field doesn&rsquo;t have any relevance in versions <code>>= v1.25</code> anymore. If you need to set a default pod admission level for your cluster, follow <a href=#admission-configuration-for-the-podsecurity-admission-plugin>this documentation</a>.</p><blockquote><p><strong>Note:</strong> You should remove this field from the <code>Shoot</code> spec for <code>v1.24</code> clusters after migrating to the new <code>PodSecurity</code> admission controller, before upgrading your cluster to <code>v1.25</code>.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-5312b08cd06fa7b827ab36d682e935b0>1.7.22 - Projects</h1><h1 id=projects>Projects</h1><p>The Gardener API server supports a cluster-scoped <code>Project</code> resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.</p><p>Each <code>Project</code> is backed by a Kubernetes <code>Namespace</code> that contains the actual related Kubernetes resources, like <code>Secret</code>s or <code>Shoot</code>s.</p><p><strong>Example resource:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Project
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>  description: <span style=color:#a31515>&#34;This is my first project&#34;</span>
</span></span><span style=display:flex><span>  purpose: <span style=color:#a31515>&#34;Experimenting with Gardener&#34;</span>
</span></span><span style=display:flex><span>  owner:
</span></span><span style=display:flex><span>    apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: john.doe@example.com
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: alice.doe@example.com
</span></span><span style=display:flex><span>    role: admin
</span></span><span style=display:flex><span>  <span style=color:green># roles:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - viewer </span>
</span></span><span style=display:flex><span>  <span style=color:green># - uam</span>
</span></span><span style=display:flex><span>  <span style=color:green># - serviceaccountmanager</span>
</span></span><span style=display:flex><span>  <span style=color:green># - extension:foo</span>
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: bob.doe@example.com
</span></span><span style=display:flex><span>    role: viewer
</span></span><span style=display:flex><span><span style=color:green># tolerations:</span>
</span></span><span style=display:flex><span><span style=color:green>#   defaults:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - key: &lt;some-key&gt;</span>
</span></span><span style=display:flex><span><span style=color:green>#   whitelist:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - key: &lt;some-key&gt;</span>
</span></span></code></pre></div><p>The <code>.spec.namespace</code> field is optional and is initialized if unset.
The name of the resulting namespace will be determined based on the <code>Project</code> name and UID, e.g., <code>garden-dev-5aef3</code>.
It&rsquo;s also possible to adopt existing namespaces by labeling them <code>gardener.cloud/role=project</code> and <code>project.gardener.cloud/name=dev</code> beforehand (otherwise, they cannot be adopted).</p><p>When deleting a Project resource, the corresponding namespace is also deleted.
To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with <code>namespace.gardener.cloud/keep-after-project-deletion</code>.</p><p>The <code>spec.description</code> and <code>.spec.purpose</code> fields can be used to describe to fellow team members and Gardener operators what this project is used for.</p><p>Each project has one dedicated owner, configured in <code>.spec.owner</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type.
The owner is the main contact person for Gardener operators.
Please note that the <code>.spec.owner</code> field is deprecated and will be removed in future API versions in favor of the <code>owner</code> role, see below.</p><p>The list of members (again a list in <code>.spec.members[]</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type) contains all the people that are associated with the project in any way.
Each project member must have at least one role (currently described in <code>.spec.members[].role</code>, additional roles can be added to <code>.spec.members[].roles[]</code>). The following roles exist:</p><ul><li><code>admin</code>: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar). Mind that the <code>admin</code> role has read only access to service accounts.</li><li><code>serviceaccountmanager</code>: This allows to fully manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the <code>admin</code> role. Please refer to <a href=/docs/gardener/usage/service-account-manager/>Service Account Manager</a>.</li><li><code>uam</code>: This allows to add/modify/remove human users or groups to/from the project member list.</li><li><code>viewer</code>: This allows to read all resources inside the project except secrets.</li><li><code>owner</code>: This combines the <code>admin</code>, <code>uam</code>, and <code>serviceaccountmanager</code> roles.</li><li>Extension roles (prefixed with <code>extension:</code>): Please refer to <a href=/docs/gardener/extensions/project-roles/>Extending Project Roles</a>.</li></ul><p>The <a href=/docs/gardener/concepts/controller-manager/#project-controller>project controller</a> inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.</p><p>There are three central <code>ClusterRole</code>s <code>gardener.cloud:system:project-member</code>, <code>gardener.cloud:system:project-viewer</code>, and <code>gardener.cloud:system:project-serviceaccountmanager</code> that grant the permissions for namespaced resources (e.g., <code>Secret</code>s, <code>Shoot</code>s, <code>ServiceAccount</code>s).
Via referring <code>RoleBinding</code>s created in the respective namespace the project members get bound to these <code>ClusterRole</code>s and, thus, the needed permissions.
There are also project-specific <code>ClusterRole</code>s granting the permissions for cluster-scoped resources, e.g., the <code>Namespace</code> or <code>Project</code> itself.<br>For each role, the following <code>ClusterRole</code>s, <code>ClusterRoleBinding</code>s, and <code>RoleBinding</code>s are created:</p><table><thead><tr><th>Role</th><th><code>ClusterRole</code></th><th><code>ClusterRoleBinding</code></th><th><code>RoleBinding</code></th></tr></thead><tbody><tr><td><code>admin</code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member</code></td></tr><tr><td><code>serviceaccountmanager</code></td><td></td><td></td><td><code>gardener.cloud:system:project-serviceaccountmanager</code></td></tr><tr><td><code>uam</code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td></td></tr><tr><td><code>viewer</code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer</code></td></tr><tr><td><code>owner</code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td></td></tr><tr><td><code>extension:*</code></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td><td></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td></tr></tbody></table><h2 id=user-access-management>User Access Management</h2><p>For <code>Project</code>s created before Gardener v1.8, all admins were allowed to manage other members.
Beginning with v1.8, the new <code>uam</code> role is being introduced.
It is backed by the <code>manage-members</code> custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list.
Human users are subjects with <code>kind=User</code> and <code>name!=system:serviceaccount:*</code>, and groups are subjects with <code>kind=Group</code>.
The management of service account subjects (<code>kind=ServiceAccount</code> or <code>name=system:serviceaccount:*</code>) is not controlled via the <code>uam</code> custom verb but with the standard <code>update</code>/<code>patch</code> verbs for projects.</p><p>All newly created projects will only bind the owner to the <code>uam</code> role.
The owner can still grant the <code>uam</code> role to other members if desired.
For projects created before Gardener v1.8, the Gardener Controller Manager will migrate all projects to also assign the <code>uam</code> role to all <code>admin</code> members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13.
The project owner can gradually remove these roles if desired.</p><h2 id=stale-projects>Stale Projects</h2><p>When a project is not actively used for some period of time, it is marked as &ldquo;stale&rdquo;. This is done by a controller called <a href=/docs/gardener/concepts/controller-manager/#stale-projects-reconciler>&ldquo;Stale Projects Reconciler&rdquo;</a>. Once the project is marked as stale, there is a time frame in which if not used it will be deleted by that controller.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-51f43840ca475366ea7f9ff5a443fac5>1.7.23 - Reversed VPN Tunnel</h1><h1 id=reversed-vpn-tunnel-setup-and-configuration>Reversed VPN Tunnel Setup and Configuration</h1><p>The Reversed VPN Tunnel is enabled by default.
A highly available VPN connection is automatically deployed in all shoots that configure an HA control-plane.</p><h2 id=reversed-vpn-tunnel>Reversed VPN Tunnel</h2><p>In the first VPN solution, connection establishment was initiated by a VPN client in the seed cluster.
Due to several issues with this solution, the tunnel establishment direction has been reverted.
The client is deployed in the shoot and initiates the connection from there. This way, there is no need to deploy a special purpose
loadbalancer for the sake of addressing the data-plane, in addition to saving costs, this is considered the more secure alternative.
For more information on how this is achieved, please have a look at the following <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>GEP</a>.</p><p>Connection establishment with a reversed tunnel:</p><p><code>APIServer --> Envoy-Proxy | VPN-Seed-Server &lt;-- Istio/Envoy-Proxy &lt;-- SNI API Server Endpoint &lt;-- LB (one for all clusters of a seed) &lt;--- internet &lt;--- VPN-Shoot-Client --> Pods | Nodes | Services</code></p><p>The reversed VPN tunnel is always deployed.
The feature gate <code>ReversedVPN</code> is GA and will be removed in a future release.</p><h2 id=high-availability-for-reversed-vpn-tunnel>High Availability for Reversed VPN Tunnel</h2><p>Shoots which define <code>spec.controlPlane.highAvailability.failureTolerance: {node, zone}</code> get an HA control-plane, including a
highly available VPN connection by deploying redundant VPN servers and clients.</p><p>Please note that it is not possible to move an open connection to another VPN tunnel. Especially long-running
commands like <code>kubectl exec -it ...</code> or <code>kubectl logs -f ...</code> will still break if the routing path must be switched
because either VPN server or client are not reachable anymore. A new request should be possible within seconds.</p><h3 id=ha-architecture-for-vpn>HA Architecture for VPN</h3><p>Establishing a connection from the VPN client on the shoot to the server in the control plane works nearly the same
way as in the non-HA case. The only difference is that the VPN client targets one of two VPN servers, represented by two services
<code>vpn-seed-server-0</code> and <code>vpn-seed-server-1</code> with endpoints in pods with the same name.
The VPN tunnel is used by a <code>kube-apiserver</code> to reach nodes, services, or pods in the shoot cluster.
In the non-HA case, a kube-apiserver uses an HTTP proxy running as a side-car in the VPN server to address
the shoot networks via the VPN tunnel and the <code>vpn-shoot</code> acts as a router.
In the HA case, the setup is more complicated. Instead of an HTTP proxy in the VPN server, the kube-apiserver has
additional side-cars, one side-car for each VPN client to connect to the corresponding VPN server.
On the shoot side, there are now two <code>vpn-shoot</code> pods, each with two VPN clients for each VPN server.
With this setup, there would be four possible routes, but only one can be used. Switching the route kills all
open connections. Therefore, another layer is introduced: link aggregation, also named <a href=https://www.kernel.org/doc/Documentation/networking/bonding.txt>bonding</a>.
In Linux, you can create a network link by using several other links as slaves. Bonding here is used with
active-backup mode. This means the traffic only goes through the active sublink and is only changed if the active one
becomes unavailable. Switching happens in the bonding network driver without changing any routes. So with this layer,
vpn-seed-server pods can be rolled without disrupting open connections.</p><p><img src=/__resources/vpn-ha-architecture_2f71aa.png alt="VPN HA Architecture"></p><p>With bonding, there are 2 possible routing paths, ensuring that there is at least one routing path intact even if
one <code>vpn-seed-server</code> pod and one <code>vpn-shoot</code> pod are unavailable at the same time.</p><p>As it is not possible to use multi-path routing, one routing path must be configured explicitly.
For this purpose, the <code>path-controller</code> script is running in another side-car of the kube-apiserver pod.
It pings all shoot-side VPN clients regularly every few seconds. If the active routing path is not responsive anymore,
the routing is switched to the other responsive routing path.</p><p><img src=/__resources/vpn-ha-routing-paths_aa0d8d.png alt="Four possible routing paths"></p><p>For general information about HA control-plane, see <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/20-ha-control-planes.md>GEP-20</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-42e1bcea9f49e68fd1fe7ce23506a673>1.7.24 - Seed Bootstrapping</h1><h1 id=seed-bootstrapping>Seed Bootstrapping</h1><p>Whenever the gardenlet is responsible for a new <code>Seed</code> resource its &ldquo;seed controller&rdquo; is being activated.
One part of this controller&rsquo;s reconciliation logic is deploying certain components into the <code>garden</code> namespace of the seed cluster itself.
These components are required to spawn and manage control planes for shoot clusters later on.
This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>The dependency watchdog (abbreviation: DWD) is a component developed separately in the <a href=https://github.com/gardener/dependency-watchdog>gardener/dependency-watchdog</a> GitHub repository.
Gardener is using it for two purposes:</p><ol><li>Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running.</li><li>Fast recovery times for crash-looping pods when depending pods are again available.</li></ol><p>For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.</p><h3 id=probe>Probe</h3><p>The <code>dependency-watchdog-probe</code> deployment is responsible for above mentioned first point.</p><p>The <code>kube-apiserver</code> of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via <code>kubectl</code>).
While end-users are talking to their clusters via this load balancer, other control plane components like the <code>kube-controller-manager</code> or <code>kube-scheduler</code> run in the same namespace/same cluster, so they can communicate via the in-cluster <code>Service</code> directly instead of using the detour with the load balancer.
However, the worker nodes of shoot clusters run in isolated, distinct networks.
This means that the <code>kubelet</code>s and <code>kube-proxy</code>s also have to talk to the control plane via the load balancer.</p><p>The <code>kube-controller-manager</code> has a special control loop called <a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/nodelifecycle><code>nodelifecycle</code></a> which will set the status of <code>Node</code>s to <code>NotReady</code> in case the kubelet stops to regularly renew its lease/to send its heartbeat.
This will trigger other self-healing capabilities of Kubernetes, for example, the eviction of pods from such &ldquo;unready&rdquo; nodes to healthy nodes.
Similarly, the <code>cloud-controller-manager</code> has a control loop that will disconnect load balancers from &ldquo;unready&rdquo; nodes, i.e., such workload would no longer be accessible until moved to a healthy node.</p><p>While these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener&rsquo;s architecture:
When the <code>kube-apiserver</code> load balancer fails for whatever reason, then the <code>kubelet</code>s can&rsquo;t talk to the <code>kube-apiserver</code> to renew their lease anymore.
After a minute or so the <code>kube-controller-manager</code> will get the impression that all nodes have died and will mark them as <code>NotReady</code>.
This will trigger above mentioned eviction as well as detachment of load balancers.
As a result, the customer&rsquo;s workload will go down and become unreachable.</p><p>This is exactly the situation that the DWD prevents:
It regularly tries to talk to the <code>kube-apiserver</code>s of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster <code>Service</code>.
If it detects that the <code>kube-apiserver</code> is reachable internally but not externally, it scales down the <code>kube-controller-manager</code> to <code>0</code>.
This will prevent it from marking the shoot worker nodes as &ldquo;unready&rdquo;.
As soon as the <code>kube-apiserver</code> is reachable externally again, the <code>kube-controller-manager</code> will be scaled up to <code>1</code> again.</p><h3 id=endpoint>Endpoint</h3><p>The <code>dependency-watchdog-endpoint</code> deployment is responsible for the above mentioned second point.</p><p>Kubernetes is restarting failing pods with an exponentially increasing backoff time.
While this is a great strategy to prevent system overloads, it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.</p><p>In the Gardener context, we are deploying many components that are depending on other components.
For example, the <code>kube-apiserver</code> is depending on a running <code>etcd</code>, or the <code>kube-controller-manager</code> and <code>kube-scheduler</code> are depending on a running <code>kube-apiserver</code>.
In case such a &ldquo;higher-level&rdquo; component fails for whatever reason, the dependent pods will fail and end-up in crash-loops.
As Kubernetes does not know anything about these hierarchies, it won&rsquo;t recognize that such pods can be restarted faster as soon as their dependents are up and running again.</p><p>This is exactly the situation in which the DWD will become active:
If it detects that a certain <code>Service</code> is available again (e.g., after the <code>etcd</code> was temporarily down while being moved to another seed node), then DWD will restart all crash-looping dependant pods.
These dependant pods are detected via a pre-configured label selector.</p><p>As of today, the DWD is configured to restart a crash-looping <code>kube-apiserver</code> after <code>etcd</code> became available again, or any pod depending on the <code>kube-apiserver</code> that has a <code>gardener.cloud/role=controlplane</code> label (e.g., <code>kube-controller-manager</code>, <code>kube-scheduler</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a4713902bcc1223db8a52f790b8cd0ef>1.7.25 - Seed Settings</h1><h1 id=settings-for-seeds>Settings for <code>Seed</code>s</h1><p>The <code>Seed</code> resource offers a few settings that are used to control the behaviour of certain Gardener components.
This document provides an overview over the available settings:</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>gardenlet can deploy two instances of the <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> into the <code>garden</code> namespace of the seed cluster.
One instance only activates the <code>endpoint</code> controller, while the second instance only activates the <code>probe</code> controller.</p><h3 id=endpoint-controller>Endpoint Controller</h3><p>The <code>endpoint</code> controller helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in CrashLoopBackoff status and restarting them once their dependants become ready and available again.
For example, if <code>etcd</code> goes down, then also <code>kube-apiserver</code> goes down (and into a <code>CrashLoopBackoff</code> state). If <code>etcd</code> comes up again, then (without the <code>endpoint</code> controller) it might take some time until <code>kube-apiserver</code> gets restarted as well.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> field.
It defaults to <code>true</code>.</p><h3 id=probe-controller>Probe Controller</h3><p>The <code>probe</code> controller scales down the <code>kube-controller-manager</code> of shoot clusters in case their respective <code>kube-apiserver</code> is not reachable via its external ingress.
This is in order to avoid melt-down situations, since the <code>kube-controller-manager</code> uses in-cluster communication when talking to the <code>kube-apiserver</code>, i.e., it wouldn&rsquo;t be affected if the external access to the <code>kube-apiserver</code> is interrupted for whatever reason.
The <code>kubelet</code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the <code>kube-apiserver</code>.
Hence, without scaling down <code>kube-controller-manager</code>, the nodes might be marked as <code>NotReady</code> and eventually replaced (since the <code>kubelet</code>s cannot report their status anymore).
To prevent such unnecessary turbulences, <code>kube-controller-manager</code> is being scaled down until the external ingress becomes available again.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.probe.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=reserve-excess-capacity>Reserve Excess Capacity</h2><p>If the excess capacity reservation is enabled, then the gardenlet will deploy a special <code>Deployment</code> into the <code>garden</code> namespace of the seed cluster.
This <code>Deployment</code>&rsquo;s pod template has only one container, the <code>pause</code> container, which simply runs in an infinite loop.
The priority of the deployment is very low, so any other pod will preempt these <code>pause</code> pods.
This is especially useful if new shoot control planes are created in the seed.
In case the seed cluster runs at its capacity, then there is no waiting time required during the scale-up.
Instead, the low-priority <code>pause</code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast.
In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted <code>pause</code> pods want to run again.
However, this delay doesn&rsquo;t affect the important shoot control plane pods, which will improve the user experience.</p><p>It can be enabled/disabled via the <code>.spec.settings.excessCapacityReservation.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=scheduling>Scheduling</h2><p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created.
However, administrators/operators might want to exclude some of them from being considered by the scheduler.
Therefore, seed clusters can be marked as &ldquo;invisible&rdquo;.
In this case, the scheduler simply ignores them as if they wouldn&rsquo;t exist.
Shoots can still use the invisible seed but only by explicitly specifying the name in their <code>.spec.seedName</code> field.</p><p>Seed clusters can be marked visible/invisible via the <code>.spec.settings.scheduling.visible</code> field.
It defaults to <code>true</code>.</p><p>ℹ️ In previous Gardener versions (&lt; 1.5) these settings were controlled via taint keys (<code>seed.gardener.cloud/{disable-capacity-reservation,invisible}</code>).
The taint keys are no longer supported and removed in version 1.12.
The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations.
More information about it can be found in <a href=https://github.com/gardener/gardener/issues/2193>#2193</a>.</p><h2 id=load-balancer-services>Load Balancer Services</h2><p>Gardener creates certain Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> in the seed cluster.
Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters.
In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations, which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h3 id=external-traffic-policy>External Traffic Policy</h3><p>Setting the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip>external traffic policy</a> to <code>Local</code> can be beneficial as it
preserves the source IP address of client requests. In addition to that, it removes one hop in the data path and hence reduces request latency. On some cloud infrastructures, it can furthermore be
used in conjunction with <code>Service</code> annotations as described above to prevent cross-zonal traffic from the load balancer to the backend pod.</p><p>The default external traffic policy is <code>Cluster</code>, meaning that all traffic from the load balancer will be sent to any cluster node, which then itself will redirect the traffic to the actual receiving pod.
This approach adds a node to the data path, may cross the zone boundaries twice, and replaces the source IP with one of the cluster nodes.</p><p><img src=/__resources/external-traffic-policy-cluster_350bbe.png alt="External Traffic Policy Cluster"></p><p>Using external traffic policy <code>Local</code> drops the additional node, i.e., only cluster nodes with corresponding backend pods will be in the list of backends of the load balancer. However, this has multiple implications.
The health check port in this scenario is exposed by <code>kube-proxy</code> , i.e., if <code>kube-proxy</code> is not working on a node a corresponding pod on the node will not receive traffic from
the load balancer as the load balancer will see a failing health check. (This is quite different from ordinary service routing where <code>kube-proxy</code> is only responsible for setup, but does not need to
run for its operation.) Furthermore, load balancing may become imbalanced if multiple pods run on the same node because load balancers will split the load equally among the nodes and not among the pods. This is mitigated by corresponding node anti affinities.</p><p><img src=/__resources/external-traffic-policy-local_b36f63.png alt="External Traffic Policy Local"></p><p>Operators need to take these implications into account when considering switching external traffic policy to <code>Local</code>.</p><h3 id=zone-specific-settings>Zone-Specific Settings</h3><p>In case a seed cluster is configured to use multiple zones via <code>.spec.provider.zones</code>, it may be necessary to configure the load balancers in individual zones in different way, e.g., by utilizing
different annotations. One reason may be to reduce cross-zonal traffic and have zone-specific load balancers in place. Zone-specific load balancers may then be bound to zone-specific subnets or
availability zones in the cloud infrastructure.</p><p>Besides the load balancer annotations, it is also possible to set the <a href=#external-traffic-policy>external traffic policy</a> for each zone-specific load balancer individually.</p><h2 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h2><p>Gardener heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the seed controller deploys the VPA components into the <code>garden</code> namespace of the respective seed clusters.
In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of Gardener.
Otherwise, you might end up with two VPAs, which will cause erratic behaviour.
By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code>, you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.</p><h2 id=owner-checks>Owner Checks</h2><p>When a shoot is scheduled to a seed and actually reconciled, Gardener appoints the seed as the current &ldquo;owner&rdquo; of the shoot by creating a special &ldquo;owner DNS record&rdquo; and checking against it if the seed still owns the shoot in order to guard against &ldquo;split brain scenario&rdquo; during control plane migration, as described in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/17-shoot-control-plane-migration-bad-case.md>GEP-17 Shoot Control Plane Migration &ldquo;Bad Case&rdquo; Scenario</a>.
This mechanism relies on the DNS resolution of TXT DNS records being possible and highly reliable, since if the owner check fails, the shoot will be effectively disabled for the duration of the failure.
In environments where resolving TXT DNS records is either not possible or not considered reliable enough, it may be necessary to disable the owner check mechanism, in order to avoid shoots failing to reconcile or temporary outages due to transient DNS failures.
By setting the <code>.spec.settings.ownerChecks.enabled=false</code> (default is <code>true</code>), the creation and checking of owner DNS records can be disabled for all shoots scheduled on this seed. Note that if owner checks are disabled, migrating shoots scheduled on this seed to other seeds should be considered unsafe, and in the future will be disabled as well.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ad3a46d969ae07bfe655009c5cfb0f36>1.7.26 - Service Account Manager</h1><h1 id=service-account-manager>Service Account Manager</h1><h2 id=overview>Overview</h2><p>With Gardener <code>v1.47</code>, a new role called <code>serviceaccountmanager</code> was introduced. This role allows to fully manage <code>ServiceAccount</code>&rsquo;s in the project namespace and request tokens for them. This is the preferred way of managing the access to a project namespace, as it aims to replace the usage of the default <code>ServiceAccount</code> secrets that will no longer be generated automatically with Kubernetes <code>v1.24+</code>.</p><h2 id=actions>Actions</h2><p>Once assigned the <code>serviceaccountmanager</code> role, a user can create/update/delete <code>ServiceAccount</code>s in the project namespace.</p><h3 id=create-a-service-account>Create a Service Account</h3><p>In order to create a <code>ServiceAccount</code> named &ldquo;robot-user&rdquo;, run the following <code>kubectl</code> command:</p><pre tabindex=0><code class=language-code data-lang=code>kubectl -n project-abc create sa robot-user
</code></pre><h3 id=request-a-token-for-a-service-account>Request a Token for a Service Account</h3><p>A token for the &ldquo;robot-user&rdquo; <code>ServiceAccount</code> can be requested via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/>TokenRequest API</a> in several ways:</p><ul><li>using <code>kubectl</code> >= v1.24</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n project-abc create token robot-user --duration=3600s
</span></span></code></pre></div><ul><li>using <code>kubectl</code> &lt; v1.24</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt;EOF | kubectl create -f - --raw /api/v1/namespaces/project-abc/serviceaccounts/robot-user/token
</span></span></span><span style=display:flex><span><span style=color:#a31515>{
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;apiVersion&#34;: &#34;authentication.k8s.io/v1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;kind&#34;: &#34;TokenRequest&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;spec&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>    &#34;expirationSeconds&#34;: 3600
</span></span></span><span style=display:flex><span><span style=color:#a31515>  }
</span></span></span><span style=display:flex><span><span style=color:#a31515>}
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><ul><li>directly calling the Kubernetes HTTP API</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST https://api.gardener/api/v1/namespaces/project-abc/serviceaccounts/robot-user/token <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -H <span style=color:#a31515>&#34;Authorization: Bearer &lt;auth-token&gt;&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -H <span style=color:#a31515>&#34;Content-Type: application/json&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -d <span style=color:#a31515>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;apiVersion&#34;: &#34;authentication.k8s.io/v1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;kind&#34;: &#34;TokenRequest&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;spec&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;expirationSeconds&#34;: 3600
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      }&#39;</span>
</span></span></code></pre></div><p>Mind that the returned token is not stored within the Kubernetes cluster, will be valid for <code>3600</code> seconds, and will be invalidated if the &ldquo;robot-user&rdquo; <code>ServiceAccount</code> is deleted. Although <code>expirationSeconds</code> can be modified depending on the needs, the returned token&rsquo;s validity will not exceed the configured <code>service-account-max-token-expiration</code> duration for the garden cluster. It is advised that the actual <code>expirationTimestamp</code> is verified so that expectations are met. This can be done by asserting the <code>expirationTimestamp</code> in the <code>TokenRequestStatus</code> or the <code>exp</code> claim in the token itself.</p><h3 id=delete-a-service-account>Delete a Service Account</h3><p>In order to delete the <code>ServiceAccount</code> named &ldquo;robot-user&rdquo;, run the following <code>kubectl</code> command:</p><pre tabindex=0><code class=language-code data-lang=code>kubectl -n project-abc delete sa robot-user
</code></pre><p>This will invalidate all existing tokens for the &ldquo;robot-user&rdquo; <code>ServiceAccount</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d133339effd8d7bc8dfe83c9e64bb2e>1.7.27 - Shoot Access</h1><h1 id=accessing-shoot-clusters>Accessing Shoot Clusters</h1><p>After creation of a shoot cluster, end-users require a <code>kubeconfig</code> to access it. There are several options available to get to such <code>kubeconfig</code>.</p><h2 id=static-token-kubeconfig>Static Token kubeconfig</h2><p>This <code>kubeconfig</code> contains a <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file>static token</a> and provides <code>cluster-admin</code> privileges.
It is created by default and persisted in the <code>&lt;shoot-name>.kubeconfig</code> secret in the project namespace in the garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    enableStaticTokenKubeconfig: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>It is <strong>not</strong> the recommended method to access the shoot cluster, as the static token <code>kubeconfig</code> has some security flaws associated with it:</p><ul><li>The static token in the <code>kubeconfig</code> doesn&rsquo;t have any expiration date. Read <a href=/docs/gardener/usage/shoot_credentials_rotation/#kubeconfig>this document</a> to learn how to rotate the static token.</li><li>The static token doesn&rsquo;t have any user identity associated with it. The user in that token will always be <code>system:cluster-admin</code>, irrespective of the person accessing the cluster. Hence, it is impossible to audit the events in cluster.</li></ul><p>When <code>enableStaticTokenKubeconfig</code> field is not explicitly set in the Shoot spec:</p><ul><li>for Shoot clusters using Kubernetes version &lt; 1.26 the field is defaulted to <code>true</code>.</li><li>for Shoot clusters using Kubernetes version >= 1.26 the field is defaulted to <code>false</code>.</li></ul><blockquote><p><strong>Note:</strong> Starting with Kubernetes 1.27, the <code>enableStaticTokenKubeconfig</code> field will be locked to <code>false</code>. The <a href=#shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> subresource</a> should be used instead.</p></blockquote><h2 id=shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> Subresource</h2><p>The <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/16-adminkubeconfig-subresource.md><code>shoots/adminkubeconfig</code></a> subresource allows users to dynamically generate temporary <code>kubeconfig</code>s that can be used to access shoot cluster with <code>cluster-admin</code> privileges. The credentials associated with this <code>kubeconfig</code> are client certificates which have a very short validity and must be renewed before they expire (by calling the subresource endpoint again).</p><p>The username associated with such <code>kubeconfig</code> will be the same which is used for authenticating to the Gardener API. Apart from this advantage, the created <code>kubeconfig</code> will not be persisted anywhere.</p><p>In order to request such a <code>kubeconfig</code>, you can run the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NAMESPACE=my-namespace
</span></span><span style=display:flex><span>export SHOOT_NAME=my-shoot
</span></span><span style=display:flex><span>kubectl create <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -f &lt;path&gt;/&lt;to&gt;/kubeconfig-request.json <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --raw /apis/core.gardener.cloud/v1beta1/namespaces/<span style=color:#a31515>${</span>NAMESPACE<span style=color:#a31515>}</span>/shoots/<span style=color:#a31515>${</span>SHOOT_NAME<span style=color:#a31515>}</span>/adminkubeconfig | jq -r <span style=color:#a31515>&#34;.status.kubeconfig&#34;</span> | base64 -d
</span></span></code></pre></div><p>Here, the <code>kubeconfig-request.json</code> has the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.gardener.cloud/v1alpha1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;AdminKubeconfigRequest&#34;</span>,
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>        &#34;expirationSeconds&#34;: 1000
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> The <a href=https://github.com/gardener/gardenctl-v2/><code>gardenctl-v2</code></a> tool makes it easy to target shoot clusters and automatically renews such <code>kubeconfig</code> when required.</p></blockquote><h2 id=openid-connect>OpenID Connect</h2><p>The <code>kube-apiserver</code> of shoot clusters can be provided with <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OpenID Connect configuration</a> via the Shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    oidcConfig:
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>It is the end-user&rsquo;s responsibility to incorporate the OpenID Connect configurations in <code>kubeconfig</code> for accessing the cluster (i.e., Gardener will not automatically generate <code>kubeconfig</code> based on these OIDC settings).
The recommended way is using the <code>kubectl</code> plugin called <a href=https://github.com/int128/kubelogin><code>kubectl oidc-login</code></a> for OIDC authentication.</p><p>If you want to use the same OIDC configuration for all your shoots by default, then you can use the <code>ClusterOpenIDConnectPreset</code> and <code>OpenIDConnectPreset</code> API resources. They allow defaulting the <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> fields for newly created <code>Shoot</code>s such that you don&rsquo;t have to repeat yourself every time (similar to <code>PodPreset</code> resources in Kubernetes).
<code>ClusterOpenIDConnectPreset</code> specified OIDC configuration applies to <code>Projects</code> and <code>Shoots</code> cluster-wide (hence, only available to Gardener operators) while <code>OpenIDConnectPreset</code> is <code>Project</code>-scoped.
Shoots have to &ldquo;opt-in&rdquo; for such defaulting by using the <code>oidc=enable</code> label.</p><p>For further information on <code>(Cluster)OpenIDConnectPreset</code>, refer to <a href=/docs/gardener/usage/openidconnect-presets/>ClusterOpenIDConnectPreset and OpenIDConnectPreset</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ac6a66f3733149171ad802e94f5dc7e7>1.7.28 - Shoot Auditpolicy</h1><h1 id=audit-a-kubernetes-cluster>Audit a Kubernetes Cluster</h1><p>The shoot cluster is a Kubernetes cluster and its <code>kube-apiserver</code> handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the Kubernetes API server. You could find more information about auditing a kubernetes cluster in the <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/>Auditing</a> topic.</p><h2 id=default-audit-policy>Default Audit Policy</h2><p>By default, the Gardener will deploy the shoot cluster with audit policy defined in the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/kubeapiserver/secrets.go>kube-apiserver package</a>.</p><h2 id=custom-audit-policy>Custom Audit Policy</h2><p>If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as <code>ConfigMap</code> resource and set up your shoot to refer this <code>ConfigMap</code>. Note that the policy must be stored under the key <code>policy</code> in the data section of the <code>ConfigMap</code>.</p><p>For example, deploy the auditpolicy <code>ConfigMap</code> in the same namespace as your <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f example/95-configmap-custom-audit-policy.yaml
</span></span></code></pre></div><p>then set your shoot to refer that <code>ConfigMap</code> (only related fields are shown):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      auditConfig:
</span></span><span style=display:flex><span>        auditPolicy:
</span></span><span style=display:flex><span>          configMapRef:
</span></span><span style=display:flex><span>            name: auditpolicy
</span></span></code></pre></div><p>Gardener validate the <code>Shoot</code> resource to refer only existing <code>ConfigMap</code> containing valid audit policy, and rejects the <code>Shoot</code> on failure.
If you want to switch back to the default audit policy, you have to remove the section</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>auditPolicy:
</span></span><span style=display:flex><span>  configMapRef:
</span></span><span style=display:flex><span>    name: &lt;configmap-name&gt;
</span></span></code></pre></div><p>from the shoot spec.</p><h2 id=rolling-out-changes-to-the-audit-policy>Rolling Out Changes to the Audit Policy</h2><p>Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc.
Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap.
If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>triggering an immediate reconciliation</a>.
This is similar to changes to the cloud provider secret referenced by Shoots.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fca47cfc40d1ae8a466ee0cce9f8b9e3>1.7.29 - Shoot Autoscaling</h1><h1 id=auto-scaling-in-shoot-clusters>Auto-Scaling in Shoot Clusters</h1><p>There are two parts that relate to auto-scaling in Kubernetes clusters in general:</p><ul><li>Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes.</li><li>Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods.</li></ul><p>This document provides an overview of both scenarios.</p><h2 id=horizontal-node-auto-scaling>Horizontal Node Auto-Scaling</h2><p>Every shoot cluster that has at least one worker pool with <code>minimum &lt; maximum</code> nodes configuration will get a <code>cluster-autoscaler</code> deployment.
Gardener is leveraging the upstream community Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler><code>cluster-autoscaler</code> component</a>.
We have forked it to <a href=https://github.com/gardener/autoscaler/>gardener/autoscaler</a> so that it supports the way how Gardener manages the worker nodes (leveraging <a href=https://github.com/gardener/machine-controller-manager>gardener/machine-controller-manager</a>).
However, we have not touched the logic how it performs auto-scaling decisions.
Consequently, please refer to the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#faqdocumentation>offical documentation</a> for this component.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>cluster-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterAdd</code> defines how long after scale up that scale down evaluation resumes (default: <code>1h</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterDelete</code> defines how long after node deletion that scale down evaluation resumes (defaults to <code>ScanInterval</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterFailure</code> defines how long after scale down failure that scale down evaluation resumes (default: <code>3m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownUnneededTime</code> defines how long a node should be unneeded before it is eligible for scale down (default: <code>30m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownUtilizationThreshold</code> defines the threshold under which a node is being removed (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScanInterval</code> defines how often cluster is reevaluated for scale up or down (default: <code>10s</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.IgnoreTaints</code> specifies a list of taint keys to ignore in node templates when considering to scale a node group (default: <code>nil</code>).</li></ul><h2 id=vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</h2><p>This form of auto-scaling is not enabled by default and must be explicitly enabled in the <code>Shoot</code> by setting <code>.spec.kubernetes.verticalPodAutoscaler.enabled=true</code>.
The reason is that it was only introduced lately, and some end-users might have already deployed their own VPA into their clusters, i.e., enabling it by default would interfere with such custom deployments and lead to issues, eventually.</p><p>Gardener is also leveraging an upstream community tool, i.e., the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
If enabled, Gardener will deploy it as part of the control plane into the seed cluster.
It will also be used for the vertical autoscaling of Gardener&rsquo;s system components deployed into the <code>kube-system</code> namespace of shoot clusters, for example, <code>kube-proxy</code> or <code>metrics-server</code>.</p><p>You might want to refer to the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md>official documentation</a> for this component to get more information how to use it.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>vertical-pod-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.verticalPodAutoscaler.evictAfterOOMThreshold</code> defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: <code>10m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateBurst</code> defines the burst of pods that can be evicted (default: <code>1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateLimit</code> defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: <code>-1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionTolerance</code> defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.recommendationMarginFraction</code> is the fraction of usage added as the safety margin to the recommended request (default: <code>0.15</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.updaterInterval</code> is the interval how often the updater should run (default: <code>1m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.recommenderInterval</code> is the interval how often metrics should be fetched (default: <code>1m0s</code>).</li></ul><p>⚠️ Please note that if you disable the VPA again, then the related <code>CustomResourceDefinition</code>s will remain in your shoot cluster (although, nobody will act on them).
This will also keep all existing <code>VerticalPodAutoscaler</code> objects in the system, including those that might be created by you. You can delete the <code>CustomResourceDefinition</code>s yourself using <code>kubectl delete crd</code> if you want to get rid of them.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cdc20eb6ababa209934b6e4bb03138d6>1.7.30 - Shoot Cleanup</h1><h1 id=cleanup-of-shoot-clusters-in-deletion>Cleanup of Shoot Clusters in Deletion</h1><p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artefacts remain after the shoot deletion.</p><p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.</p><p><strong>Cleanup steps:</strong></p><ol><li>All <code>ValidatingWebhookConfiguration</code>s and <code>MutatingWebhookConfiguration</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.</li><li>All <code>APIService</code>s and <code>CustomResourceDefinition</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li><li>All <code>CronJob</code>s, <code>DaemonSet</code>s, <code>Deployment</code>s, <code>Ingress</code>s, <code>Job</code>s, <code>Pod</code>s, <code>ReplicaSet</code>s, <code>ReplicationController</code>s, <code>Service</code>s, <code>StatefulSet</code>s, <code>PersistentVolumeClaim</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.<blockquote><p>If the <code>Shoot</code> is annotated with <code>shoot.gardener.cloud/skip-cleanup=true</code>, then only <code>Service</code>s and <code>PersistentVolumeClaim</code>s are considered.</p></blockquote></li><li>All <code>VolumeSnapshot</code>s and <code>VolumeSnapshotContent</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li><li>All <code>Namespace</code>s are deleted without any grace period. Forceful finalization happens after <code>5m</code>.</li></ol><p>It is possible to override the finalization grace periods via annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds</code> (for the resources handled in step 1)</li><li><code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds</code> (for the resources handled in step 2)</li><li><code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds</code> (for the resources handled in step 3)</li><li><code>shoot.gardener.cloud/cleanup-namespaces-finalize-grace-period-seconds</code> (for the resources handled in step 4)</li></ul><p>⚠️ If <code>"0"</code> is provided, then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artefacts.</p><h2 id=infrastructure-cleanup-wait-period>Infrastructure Cleanup Wait Period</h2><p>After all above cleanup steps have been performed and the <code>Infrastructure</code> extension resource has been deleted, the gardenlet waits for a certain duration to allow controllers to properly cleanup infrastructure resources.</p><p>By default, this duration is set to <code>5m</code>. Only after this time has passed, the shoot deletion flow continues with the entire tear-down of the remaining control plane components (including <code>kube-apiserver</code>s, etc.).</p><p>It is also possible to override this wait period via an annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/infrastructure-cleanup-wait-period-seconds</code></li></ul><blockquote><p>ℹ️️ All provided period values larger than the above mentioned defaults are ignored.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-be2e4b54fd8d4a9659ea3ba10c6cee2a>1.7.31 - Shoot Credentials Rotation</h1><h1 id=credentials-rotation-for-shoot-clusters>Credentials Rotation for Shoot Clusters</h1><p>There are a lot of different credentials for <code>Shoot</code>s to make sure that the various components can communicate with each other and to make sure it is usable and operable.</p><p>This page explains how the varieties of credentials can be rotated so that the cluster can be considered secure.</p><h2 id=user-provided-credentials>User-Provided Credentials</h2><h3 id=cloud-provider-keys>Cloud Provider Keys</h3><p>End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations.
For example, Gardener uses them to setup and maintain the networks, security groups, subnets, etc., while the <a href=https://kubernetes.io/docs/concepts/architecture/cloud-controller/>cloud-controller-manager</a> uses them to reconcile load balancers and routes, and the <a href=https://kubernetes-csi.github.io/docs/>CSI controller</a> uses them to reconcile volumes and disks.</p><p>Depending on the cloud provider, the required <a href=https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml>data keys of the <code>Secret</code> differ</a>.
Please consult the documentation of the respective provider extension documentation to get to know the concrete data keys (e.g., <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#provider-secret-data>this document for AWS</a>).</p><p><strong>It is the responsibility of the end-user to regularly rotate those credentials.</strong>
The following steps are required to perform the rotation:</p><ol><li>Update the data in the <code>Secret</code> with new credentials.</li><li>⚠️ Wait until all <code>Shoot</code>s using the <code>Secret</code> are reconciled before you disable the old credentials in your cloud provider account! Otherwise, the <code>Shoot</code>s will no longer work as expected. Check out <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>this document</a> to learn how to trigger a reconciliation of your <code>Shoot</code>s.</li><li>After all <code>Shoot</code>s using the <code>Secret</code> were reconciled, you can go ahead and deactivate the old credentials in your provider account.</li></ol><h2 id=gardener-provided-credentials>Gardener-Provided Credentials</h2><p>The below credentials are generated by Gardener when shoot clusters are being created.
Those include:</p><ul><li>kubeconfig (if enabled)</li><li>certificate authorities (and related server and client certificates)</li><li>observability passwords for Grafana</li><li>SSH key pair for worker nodes</li><li>ETCD encryption key</li><li><code>ServiceAccount</code> token signing key</li><li>&mldr;</li></ul><p><strong>🚨 There is no auto-rotation of those credentials, and it is the responsibility of the end-user to regularly rotate them.</strong></p><p>While it is possible to rotate them one by one, there is also a convenient method to combine the rotation of all of those credentials.
The rotation happens in two phases since it might be required to update some API clients (e.g., when CAs are rotated).
In order to start the rotation (first phase), you have to annotate the shoot with the <code>rotate-credentials-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-credentials-start
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> You can check the <code>.status.credentials.rotation</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><p>Kindly consider the detailed descriptions below to learn how the rotation is performed and what your responsibilities are.
Please note that all respective individual actions apply for this combined rotation as well (e.g., worker nodes are rolled out in the first phase).</p><p>You can complete the rotation (second phase) by annotating the shoot with the <code>rotate-credentials-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-credentials-complete
</span></span></code></pre></div><h3 id=kubeconfig>Kubeconfig</h3><p>If the <code>.spec.kubernetes.enableStaticTokenKubeconfig</code> field is set to <code>true</code> (default), then Gardener generates a <code>kubeconfig</code> with <code>cluster-admin</code> privileges for the <code>Shoot</code>s containing credentials for communication with the <code>kube-apiserver</code> (see <a href=/docs/gardener/usage/shoot_access/#static-token-kubeconfig>this document</a> for more information).</p><p>This <code>Secret</code> is stored with the name <code>&lt;shoot-name>.kubeconfig</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>kubeconfig</code>: the completed kubeconfig</li><li><code>ca.crt</code>: the CA bundle for establishing trust to the API server (same as in the <a href=#cluster-certificate-authority-bundle>Cluster CA bundle secret</a>)</li></ul><blockquote><p><code>Shoots</code> created with Gardener &lt;= 0.28 used to have a <code>kubeconfig</code> based on a client certificate instead of a static token. With the first kubeconfig rotation, such clusters will get a static token as well.</p><p>⚠️ This does not invalidate the old client certificate. In order to do this, you should perform a rotation of the CAs (see section below).</p></blockquote><p><strong>It is the responsibility of the end-user to regularly rotate those credentials (or disable this <code>kubeconfig</code> entirely).</strong>
In order to rotate the <code>token</code> in this <code>kubeconfig</code>, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-kubeconfig-credentials</code>.
This operation is not allowed for <code>Shoot</code>s that are already marked for deletion.
Please note that only the token (and basic auth password, if enabled) are exchanged.
The CA certificate remains the same (see section below for information about the rotation).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-kubeconfig-credentials
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.kubeconfig</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><h3 id=certificate-authorities>Certificate Authorities</h3><p>Gardener generates several certificate authorities (CAs) to ensure secured communication between the various components and actors.
Most of those CAs are used for internal communication (e.g., <code>kube-apiserver</code> talks to etcd, <code>vpn-shoot</code> talks to the <code>vpn-seed-server</code>, <code>kubelet</code> talks to <code>kube-apiserver</code>).
However, there is also the &ldquo;cluster CA&rdquo; which is part of all <code>kubeconfig</code>s and used to sign the server certificate exposed by the <code>kube-apiserver</code>.</p><p>Gardener populates a <code>Secret</code> with the name <code>&lt;shoot-name>.ca-cluster</code> in the project namespace in the garden cluster which contains the following data keys:</p><ul><li><code>ca.crt</code>: the CA bundle of the cluster</li></ul><p>This bundle contains one or multiple CAs which are used for signing serving certificates of the <code>Shoot</code>&rsquo;s API server.
Hence, the certificates contained in this <code>Secret</code> can be used to verify the API server&rsquo;s identity when communicating with its public endpoint (e.g., as <code>certificate-authority-data</code> in a <code>kubeconfig</code>).
This is the same certificate that is also contained in the <code>kubeconfig</code>&rsquo;s <code>certificate-authority-data</code> field.</p><blockquote><p><code>Shoot</code>s created with Gardener >= v1.45 have a dedicated client CA which verifies the legitimacy of client certificates. For older <code>Shoot</code>s, the client CA is equal to the cluster CA. With the first CA rotation, such clusters will get a dedicated client CA as well.</p></blockquote><p>All of the certificates are valid for 10 years.
Since it requires adaptation for the consumers of the <code>Shoot</code>, there is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the CA certificates.</strong></p><p>The rotation happens in three stages (see also <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> for the full details):</p><ul><li>In stage one, new CAs are created and added to the bundle (together with the old CAs). Client certificates are re-issued immediately.</li><li>In stage two, end-users update all cluster API clients that communicate with the control plane.</li><li>In stage three, the old CAs are dropped from the bundle and server certificate are re-issued.</li></ul><p>Technically, the <code>Preparing</code> phase indicates stage one.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage two.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.certificateAuthorities</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-ca-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ca-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs stage one.
After it is completed, the <code>.status.credentials.rotation.certificateAuthorities.phase</code> is set to <code>Prepared</code>.</p><p>Now you must update all API clients outside the cluster (such as the <code>kubeconfig</code>s on developer machines) to use the newly issued CA bundle in the <code>&lt;shoot-name>.ca-cluster</code> <code>Secret</code>.
Please also note that client certificates must be re-issued now.</p><p>After updating all API clients, you can complete the rotation by annotating the shoot with the <code>rotate-ca-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ca-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.certificateAuthorities.phase</code> is set to <code>Completed</code>.
You could update your API clients again and drop the old CA from their bundle.</p><blockquote><p>Note that the CA rotation also rotates all internal CAs and signed certificates.
Hence, most of the components need to be restarted (including etcd and <code>kube-apiserver</code>).</p><p>⚠️ In stage one, all worker nodes of the <code>Shoot</code> will be rolled out to ensure that the <code>Pod</code>s as well as the <code>kubelet</code>s get the updated credentials as well.</p></blockquote><h3 id=observability-passwords-for-grafana-and-prometheus>Observability Password(s) For Grafana and Prometheus</h3><p>For <code>Shoot</code>s with <code>.spec.purpose!=testing</code>, Gardener deploys an observability stack with Prometheus for monitoring, Alertmanager for alerting (optional), Loki for logging, and Grafana for visualization.
The Grafana instance is exposed via <code>Ingress</code> and accessible for end-users via basic authentication credentials generated and managed by Gardener.</p><p>Those credentials are stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.monitoring</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>username</code>: the user name</li><li><code>password</code>: the password</li><li><code>auth</code>: the user name with SHA-1 representation of the password</li></ul><p><strong>It is the responsibility of the end-user to regularly rotate those credentials.</strong>
In order to rotate the <code>password</code>, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-observability-credentials</code>.
This operation is not allowed for <code>Shoot</code>s that are already marked for deletion.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-observability-credentials
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.observability</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><h3 id=ssh-key-pair-for-worker-nodes>SSH Key Pair for Worker Nodes</h3><p>Gardener generates an SSH key pair whose public key is propagated to all worker nodes of the <code>Shoot</code>.
The private key can be used to establish an SSH connection to the workers for troubleshooting purposes.
It is recommended to use <a href=https://github.com/gardener/gardenctl-v2/><code>gardenctl-v2</code></a> and its <code>gardenctl ssh</code> command since it is required to first open up the security groups and create a bastion VM (no direct SSH access to the worker nodes is possible).</p><p>The private key is stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.ssh-keypair</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>id_rsa</code>: the private key</li><li><code>id_rsa.pub</code>: the public key for SSH</li></ul><p>In order to rotate the keys, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-ssh-keypair</code>.
This will propagate a new key to all worker nodes while keeping the old key active and valid as well (it will only be invalidated/removed with the next rotation).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ssh-keypair
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.sshKeypair</code> field in the <code>Shoot</code> to see when the rotation was last initiated or last completed.</p></blockquote><p>The old key is stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.ssh-keypair.old</code> in the project namespace in the garden cluster and has the same data keys as the regular <code>Secret</code>.</p><h3 id=etcd-encryption-key>ETCD Encryption Key</h3><p>This key is used to encrypt the data of <code>Secret</code> resources inside etcd (see <a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>upstream Kubernetes documentation</a>).</p><p>The encryption key has no expiration date.
There is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the encryption key.</strong></p><p>The rotation happens in three stages:</p><ul><li>In stage one, a new encryption key is created and added to the bundle (together with the old encryption key).</li><li>In stage two, all <code>Secret</code>s in the cluster are rewritten by the <code>kube-apiserver</code> so that they become encrypted with the new encryption key.</li><li>In stage three, the old encryption is dropped from the bundle.</li></ul><p>Technically, the <code>Preparing</code> phase indicates the stages one and two.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage three.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.etcdEncryptionKey</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-etcd-encryption-key-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-etcd-encryption-key-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs the stages one and two.
After it is completed, the <code>.status.credentials.rotation.etcdEncryptionKey.phase</code> is set to <code>Prepared</code>.
Now you can complete the rotation by annotating the shoot with the <code>rotate-etcd-encryption-key-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-etcd-encryption-key-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.etcdEncryptionKey.phase</code> is set to <code>Completed</code>.</p><h3 id=serviceaccount-token-signing-key><code>ServiceAccount</code> Token Signing Key</h3><p>Gardener generates a key which is used to sign the tokens for <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/><code>ServiceAccount</code>s</a>.
Those tokens are typically used by workload <code>Pod</code>s running inside the cluster in order to authenticate themselves with the <code>kube-apiserver</code>.
This also includes system components running in the <code>kube-system</code> namespace.</p><p>The token signing key has no expiration date.
Since it might require adaptation for the consumers of the <code>Shoot</code>, there is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the signing key.</strong></p><p>The rotation happens in three stages, similar to how the <a href=#certificate-authorities>CA certificates</a> are rotated:</p><ul><li>In stage one, a new signing key is created and added to the bundle (together with the old signing key).</li><li>In stage two, end-users update all out-of-cluster API clients that communicate with the control plane via <code>ServiceAccount</code> tokens.</li><li>In stage three, the old signing key is dropped from the bundle.</li></ul><p>Technically, the <code>Preparing</code> phase indicates stage one.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage two.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.serviceAccountKey</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-serviceaccount-key-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-serviceaccount-key-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs stage one.
After it is completed, the <code>.status.credentials.rotation.serviceAccountKey.phase</code> is set to <code>Prepared</code>.</p><p>Now you must update all API clients outside the cluster using a <code>ServiceAccount</code> token (such as the <code>kubeconfig</code>s on developer machines) to use a token issued by the new signing key.
Gardener already generates new static token secrets for all <code>ServiceAccount</code>s in the cluster.
However, if you need to create it manually, you can check out <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token>this document</a> for instructions.</p><p>After updating all API clients, you can complete the rotation by annotating the shoot with the <code>rotate-serviceaccount-key-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-serviceaccount-key-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.serviceAccountKey.phase</code> is set to <code>Completed</code>.</p><blockquote><p>⚠️ In stage one, all worker nodes of the <code>Shoot</code> will be rolled out to ensure that the <code>Pod</code>s use a new token.</p></blockquote><h3 id=openvpn-tls-auth-keys>OpenVPN TLS Auth Keys</h3><p>This key is used to ensure encrypted communication for the VPN connection between the control plane in the seed cluster and the shoot cluster.
It is currently <strong>not</strong> rotated automatically and there is no way to trigger it manually.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-99132edc0ae6e88780688fe9c346c413>1.7.32 - Shoot High Availability</h1><h1 id=highly-available-shoot-control-plane>Highly Available Shoot Control Plane</h1><p>Shoot resource offers a way to request for a highly available control plane.</p><h2 id=failure-tolerance-types>Failure Tolerance Types</h2><p>A highly available shoot control plane can be setup with either a failure tolerance of <code>zone</code> or <code>node</code>.</p><h3 id=node-failure-tolerance><code>Node</code> Failure Tolerance</h3><p>The failure tolerance of a <code>node</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.</li><li><code>Worker pool</code> should have a minimum of 3 nodes.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different node within a single availability zone.</li></ul><h3 id=zone-failure-tolerance><code>Zone</code> Failure Tolerance</h3><p>The failure tolerance of a <code>zone</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.</li><li>Gardener scheduler will automatically select a <code>seed</code> which has a minimum of 3 zones to host the shoot control plane.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different zone.</li></ul><h2 id=shoot-spec>Shoot Spec</h2><p>To request for a highly available shoot control plane Gardener provides the following configuration in the shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: &lt;node | zone&gt;
</span></span></code></pre></div><p><strong>Allowed Transitions</strong></p><p>If you already have a shoot cluster with non-HA control plane, then the following upgrades are possible:</p><ul><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>node</code> failure tolerance.</li><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>zone</code> failure tolerance. However, it is essential that the <code>seed</code> which is currently hosting the shoot control plane should be <code>multi-zonal</code>. If it is not, then the request to upgrade will be rejected.</li></ul><blockquote><p><strong>Note:</strong> There will be a small downtime during the upgrade, especially for etcd, which will transition from a single node etcd cluster to a multi-node etcd cluster.</p></blockquote><p><strong>Disallowed Transitions</strong></p><p>If you have already set-up an HA shoot control plane with <code>node</code> failure tolerance, then an upgrade to a <code>zone</code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.</p><h2 id=zone-outage-situation>Zone Outage Situation</h2><p>An availability zone outage might lead to the requirement to change your cluster setup temporarily and on short notice, in order to compensate failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (<code>shoot.spec.provider.workers[]</code>) and infrastructure (<code>shoot.spec.provider.infrastructureConfig</code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.</p><p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the <em>readiness</em> of prior ones.
During a zone outage, this can block the entire flow, e.g., because all three <code>etcd</code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation <code>shoot.gardener.cloud/skip-readiness</code> helps to skip any readiness checks in the flow.</p><blockquote><p>The <code>shoot.gardener.cloud/skip-readiness</code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-6e28d817b7a970b189d7b89840fa8925>1.7.33 - Shoot Info Configmap</h1><h1 id=shoot-info-configmap>Shoot Info <code>ConfigMap</code></h1><h2 id=overview>Overview</h2><p>The gardenlet maintains a <a href=https://kubernetes.io/docs/concepts/configuration/configmap/>ConfigMap</a> inside the Shoot cluster that contains information about the cluster itself. The ConfigMap is named <code>shoot-info</code> and located in the <code>kube-system</code> namespace.</p><h2 id=fields>Fields</h2><p>The following fields are provided:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot-info
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  domain: crazy-botany.core.my-custom-domain.com     <span style=color:green># .spec.dns.domain field from the Shoot resource</span>
</span></span><span style=display:flex><span>  extensions: foobar,foobaz                          <span style=color:green># List of extensions that are enabled</span>
</span></span><span style=display:flex><span>  kubernetesVersion: 1.20.1                          <span style=color:green># .spec.kubernetes.version field from the Shoot resource</span>
</span></span><span style=display:flex><span>  maintenanceBegin: 220000+0100                      <span style=color:green># .spec.maintenance.timeWindow.begin field from the Shoot resource</span>
</span></span><span style=display:flex><span>  maintenanceEnd: 230000+0100                        <span style=color:green># .spec.maintenance.timeWindow.end field from the Shoot resource</span>
</span></span><span style=display:flex><span>  nodeNetwork: 10.250.0.0/16                         <span style=color:green># .spec.networking.nodes field from the Shoot resource</span>
</span></span><span style=display:flex><span>  podNetwork: 100.96.0.0/11                          <span style=color:green># .spec.networking.pods field from the Shoot resource</span>
</span></span><span style=display:flex><span>  projectName: dev                                   <span style=color:green># .metadata.name of the Project</span>
</span></span><span style=display:flex><span>  provider: &lt;some-provider-name&gt;                     <span style=color:green># .spec.provider.type field from the Shoot resource</span>
</span></span><span style=display:flex><span>  region: europe-central-1                           <span style=color:green># .spec.region field from the Shoot resource</span>
</span></span><span style=display:flex><span>  serviceNetwork: 100.64.0.0/13                      <span style=color:green># .spec.networking.services field from the Shoot resource</span>
</span></span><span style=display:flex><span>  shootName: crazy-botany                            <span style=color:green># .metadata.name from the Shoot resource</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-530324ec62b83dd4fb4dbc1c64e21ea4>1.7.34 - Shoot Maintenance</h1><h1 id=shoot-maintenance>Shoot Maintenance</h1><p>Shoots configure a maintenance time window in which Gardener performs certain operations that may restart the control plane, roll out the nodes, result in higher network traffic, etc. A summary of what was changed in the last maintenance time window in shoot specification is kept in the shoot status <code>.status.lastMaintenance</code> field.</p><p>This document outlines what happens during a shoot maintenance.</p><h2 id=time-window>Time Window</h2><p>Via the <code>.spec.maintenance.timeWindow</code> field in the shoot specification, end-users can configure the time window in which maintenance operations are executed.
Gardener runs one maintenance operation per day in this time window:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span></code></pre></div><p>The offset (<code>+0100</code>) is considered with respect to UTC time.
The minimum time window is <code>30m</code> and the maximum is <code>6h</code>.</p><p>⚠️ Please note that there is no guarantee that a maintenance operation that, e.g., starts a node roll-out will finish <em>within</em> the time window.
Especially for large clusters, it may take several hours until a graceful rolling update of the worker nodes succeeds (also depending on the workload and the configured pod disruption budgets/termination grace periods).</p><p>Internally, Gardener is subtracting <code>15m</code> from the end of the time window to (best-effort) try to finish the maintenance until the end is reached, however, this might not work in all cases.</p><p>If you don&rsquo;t specify a time window, then Gardener will randomly compute it.
You can change it later, of course.</p><h2 id=automatic-version-updates>Automatic Version Updates</h2><p>The <code>.spec.maintenance.autoUpdate</code> field in the shoot specification allows you to control how/whether automatic updates of Kubernetes patch and machine image versions are performed.
Machine image versions are updated per worker pool.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>During the daily maintenance, the Gardener Controller Manager updates the Shoot&rsquo;s Kubernetes and machine image version if any of the following criteria applies:</p><ul><li>There is a higher version available and the Shoot opted-in for automatic version updates.</li><li>The currently used version is <code>expired</code>.</li></ul><p>Gardener creates events with the type <code>MaintenanceDone</code> on the Shoot describing the action performed during maintenance, including the reason why an update has been triggered.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>MaintenanceDone  Updated image of worker-pool &#39;coreos-xy&#39; from &#39;coreos&#39; version &#39;xy&#39; to version &#39;abc&#39;. Reason: AutoUpdate of MachineImage configured.
</span></span><span style=display:flex><span>MaintenanceDone  Updated Kubernetes version &#39;0.0.1&#39; to version &#39;0.0.5&#39;. This is an increase in the patch level. Reason: AutoUpdate of Kubernetes version configured.
</span></span><span style=display:flex><span>MaintenanceDone  Updated Kubernetes version &#39;0.0.5&#39; to version &#39;0.1.5&#39;. This is an increase in the minor level. Reason: Kubernetes version expired - force update required.
</span></span></code></pre></div><p>Please refer to the <a href=/docs/gardener/usage/shoot_versions/>Shoot Kubernetes and Operating System Versioning in Gardener</a> topic for more information about Kubernetes and machine image versions in Gardener.</p><h2 id=cluster-reconciliation>Cluster Reconciliation</h2><p>Gardener administrators/operators can configure the gardenlet in a way that it only reconciles shoot clusters during their maintenance time windows.
This behaviour is not controllable by end-users but might make sense for large Gardener installations.
Concretely, your shoot will be reconciled regularly during its maintenance time window.
Outside of the maintenance time window it will only reconcile if you change the specification or if you explicitly trigger it, see also <a href=/docs/gardener/usage/shoot_operations/>Trigger Shoot Operations</a>.</p><h2 id=confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out</h2><p>Via the <code>.spec.maintenance.confineSpecUpdateRollout</code> field you can control whether you want to make Gardener roll out changes/updates to your shoot specification only during the maintenance time window.
It is <code>false</code> by default, i.e., any change to your shoot specification triggers a reconciliation (even outside of the maintenance time window).
This is helpful if you want to update your shoot but don&rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window.
Any update to the specification will not increase the <code>.metadata.generation</code> of the <code>Shoot</code>, which is something you should be aware of.
Also, even if Gardener administrators/operators have not enabled the &ldquo;reconciliation in maintenance time window only&rdquo; configuration (as mentioned above), then your shoot will only reconcile in the maintenance time window.
The reason is that Gardener cannot differentiate between create/update/reconcile operations.</p><p>⚠️ If <code>confineSpecUpdateRollout=true</code>, please note that if you change the maintenance time window itself, then it will only be effective after the upcoming maintenance.</p><p>⚠️ As exceptions to the above rules, <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>manually triggered reconciliations</a> and changes to the <code>.spec.hibernation.enabled</code> field trigger immediate rollouts.
I.e., if you hibernate or wake-up your shoot, or you explicitly tell Gardener to reconcile your shoot, then Gardener gets active right away.</p><h2 id=shoot-operations>Shoot Operations</h2><p>In case you would like to perform a <a href=/docs/gardener/usage/shoot_operations/#credentials-rotation-operations>shoot credential rotation</a> or a <code>reconcile</code> operation during your maintenance time window, you can annotate the <code>Shoot</code> with</p><pre tabindex=0><code>maintenance.gardener.cloud/operation=&lt;operation&gt;
</code></pre><p>This will execute the specified <code>&lt;operation></code> during the next maintenance reconciliation.
Note that Gardener will remove this annotation after it has been performed in the maintenance reconciliation.</p><blockquote><p>⚠️ This is skipped when the <code>Shoot</code>&rsquo;s <code>.status.lastOperation.state=Failed</code>. Make sure to <a href=/docs/gardener/usage/shoot_operations/#retry-failed-reconciliation>retry</a> your shoot reconciliation beforehand.</p></blockquote><h2 id=special-operations-during-maintenance>Special Operations During Maintenance</h2><p>The shoot maintenance controller triggers special operations that are performed as part of the shoot reconciliation.</p><h3 id=infrastructure-and-dnsrecord-reconciliation><code>Infrastructure</code> and <code>DNSRecord</code> Reconciliation</h3><p>The reconciliation of the <code>Infrastructure</code> and <code>DNSRecord</code> extension resources is only demanded during the shoot&rsquo;s maintenance time window.
The rationale behind it is to prevent sending too many requests against the cloud provider APIs, especially on large landscapes or if a user has many shoot clusters in the same cloud provider account.</p><h3 id=restart-control-plane-controllers>Restart Control Plane Controllers</h3><p>Gardener operators can make Gardener restart/delete certain control plane pods during a shoot maintenance.
This feature helps to automatically solve service denials of controllers due to stale caches, dead-locks or starving routines.</p><p>Please note that these are exceptional cases but they are observed from time to time.
Gardener, for example, takes this precautionary measure for <code>kube-controller-manager</code> pods.</p><p>See <a href=/docs/gardener/extensions/shoot-maintenance/>Shoot Maintenance</a> to see how extension developers can extend this behaviour.</p><h3 id=restart-some-core-addons>Restart Some Core Addons</h3><p>Gardener operators can make Gardener restart some core addons (at the moment only CoreDNS) during a shoot maintenance.</p><p>CoreDNS benefits from this feature as it automatically solve problems with clients stuck to single replica of the deployment and thus overloading it.
Please note that these are exceptional cases but they are observed from time to time.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-97d7360c220fe009f695e761863dd64c>1.7.35 - Shoot Network Policies</h1><h2 id=network-policies-in-the-shoot-cluster>Network Policies in the Shoot Cluster</h2><p>In addition to deploying network policies <a href=/docs/gardener/development/seed_network_policies/>into the Seed</a>,
Gardener deploys network policies into the <code>kube-system</code> namespace of the Shoot.
These network policies are used by Shoot system components (that are not part of the control plane).
Other namespaces in the Shoot do not contain network policies deployed by Gardener.</p><p>As a best practice, every pod deployed into the <code>kube-system</code> namespace should use appropriate network policies in order to only allow <strong>required</strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.</p><p>Gardener deploys the following network policies:</p><pre tabindex=0><code>NAME                                       POD-SELECTOR
gardener.cloud--allow-dns                  k8s-app in (kube-dns)
gardener.cloud--allow-from-seed            networking.gardener.cloud/from-seed=allowed
gardener.cloud--allow-to-apiserver         networking.gardener.cloud/to-apiserver=allowed
gardener.cloud--allow-to-dns               networking.gardener.cloud/to-dns=allowed
gardener.cloud--allow-to-from-nginx        app=nginx-ingress
gardener.cloud--allow-to-kubelet           networking.gardener.cloud/to-kubelet=allowed
gardener.cloud--allow-to-public-networks   networking.gardener.cloud/to-public-networks=allowed
gardener.cloud--allow-vpn                  app=vpn-shoot
</code></pre><p>Additionally, there can be network policies deployed by Gardener extensions such as <a href=https://github.com/gardener/gardener-extension-networking-calico>extension-calico</a>.</p><pre tabindex=0><code>NAME                                       POD-SELECTOR
gardener.cloud--allow-from-calico-node     k8s-app=calico-typha
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-ee4728cf7f71a3c0ae90d16a7c9cf79a>1.7.36 - Shoot Networking</h1><h1 id=shoot-networking>Shoot Networking</h1><p>This document contains network related information for Shoot clusters.</p><h2 id=pod-network>Pod Network</h2><p>A Pod network is imperative for any kind of cluster communication with Pods not started within the Node&rsquo;s host network.
More information about the Kubernetes network model can be found in the <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>Cluster Networking</a> topic.</p><p>Gardener allows users to configure the Pod network&rsquo;s CIDR during Shoot creation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: &lt;some-network-extension-name&gt; <span style=color:green># {calico,cilium}</span>
</span></span><span style=display:flex><span>    pods: 100.96.0.0/16
</span></span><span style=display:flex><span>    nodes: ...
</span></span><span style=display:flex><span>    services: ...
</span></span></code></pre></div><blockquote><p>⚠️ The <code>networking.pods</code> IP configuration is immutable and cannot be changed afterwards.
Please consider the following paragraph to choose a configuration which will meet your demands.</p></blockquote><p>One of the network plugin&rsquo;s (CNI) tasks is to assign IP addresses to Pods started in the Pod network.
Different network plugins come with different IP address management (IPAM) features, so we can&rsquo;t give any definite advice how IP ranges should be configured.
Nevertheless, we want to outline the standard configuration.</p><p>Information in <code>.spec.networking.pods</code> matches the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>&ndash;cluster-cidr flag</a> of the Kube-Controller-Manager of your Shoot cluster.
This IP range is divided into smaller subnets, also called <code>podCIDRs</code> (default mask <code>/24</code>) and assigned to Node objects <code>.spec.podCIDR</code>.
Pods get their IP address from this smaller node subnet in a default IPAM setup.
Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.</p><p><em><strong>Example 1</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/16
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 256 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>256 nodes</strong> which are ready to run workload in the Pod network.</p><p><em><strong>Example 2</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 16 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>16 nodes</strong> which are ready to run workload in the Pod network.</p><p>Beside the configuration in <code>.spec.networking.pods</code>, users can tune the <code>nodeCIDRMaskSize</code> used by Kube-Controller-Manager on shoot creation.
A smaller IP range per node means more <code>podCIDRs</code> and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubeControllerManager:
</span></span><span style=display:flex><span>    nodeCIDRMaskSize: 24 (default)
</span></span></code></pre></div><blockquote><p>⚠️ The <code>nodeCIDRMaskSize</code> configuration is immutable and cannot be changed afterwards.</p></blockquote><p><em><strong>Example 3</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /25
-------------------------

Number of podCIDRs: 32 --&gt; max. Node count 
Number of IPs per podCIDRs: 128
</code></pre><p>With the configuration above, a Shoot cluster can at most have <strong>32 nodes</strong> which are ready to run workload in the Pod network.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa95d2c4d5739a992f6e2fe3ce04db73>1.7.37 - Shoot Operations</h1><h1 id=trigger-shoot-operations>Trigger Shoot Operations</h1><p>You can trigger a few explicit operations by annotating the <code>Shoot</code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the <code>Shoot</code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&rsquo;t properly be reflected here.
Note that once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.</p><p>Please note: If <code>.spec.maintenance.confineSpecUpdateRollout=true</code>, then the only way to trigger a shoot reconciliation is by setting the <code>reconcile</code> operation, see below.</p><h2 id=immediate-reconciliation>Immediate Reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=reconcile</code> to make the <code>gardenlet</code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=reconcile
</span></span></code></pre></div><h2 id=immediate-maintenance>Immediate Maintenance</h2><p>Annotate the shoot with <code>gardener.cloud/operation=maintain</code> to make the <code>gardener-controller-manager</code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts, then nothing needs to be maintained:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=maintain
</span></span></code></pre></div><h2 id=retry-failed-reconciliation>Retry Failed Reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=retry</code> to make the <code>gardenlet</code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=retry
</span></span></code></pre></div><h2 id=credentials-rotation-operations>Credentials Rotation Operations</h2><p>Please consult <a href=/docs/gardener/usage/shoot_credentials_rotation/>Credentials Rotation for Shoot Clusters</a> for more information.</p><h2 id=restart-systemd-services-on-particular-worker-nodes>Restart <code>systemd</code> Services on Particular Worker Nodes</h2><p>It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed.
The annotation is not set on the <code>Shoot</code> resource but directly on the <code>Node</code> object you want to target.
For example, the following will restart both the <code>kubelet</code> and the <code>docker</code> services:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate node &lt;node-name&gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker
</span></span></code></pre></div><p>It may take up to a minute until the service is restarted.
The annotation will be removed from the <code>Node</code> object after all specified systemd services have been restarted.
It will also be removed even if the restart of one or more services failed.</p><blockquote><p>ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using <code>kubectl describe node &lt;node-name></code> and looking for such a <code>Starting kubelet</code> event.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-ce24e382e35468e5c98ad64130e0653b>1.7.38 - Shoot Purposes</h1><h1 id=shoot-cluster-purpose>Shoot Cluster Purpose</h1><p>The <code>Shoot</code> resource contains a <code>.spec.purpose</code> field indicating how the shoot is used, whose allowed values are as follows:</p><ul><li><code>evaluation</code> (default): Indicates that the shoot cluster is for evaluation scenarios.</li><li><code>development</code>: Indicates that the shoot cluster is for development scenarios.</li><li><code>testing</code>: Indicates that the shoot cluster is for testing scenarios.</li><li><code>production</code>: Indicates that the shoot cluster is for production scenarios.</li><li><code>infrastructure</code>: Indicates that the shoot cluster is for infrastructure scenarios (only allowed for shoots in the <code>garden</code> namespace).</li></ul><h2 id=behavioral-differences>Behavioral Differences</h2><p>The following enlists the differences in the way the shoot clusters are set up based on the selected purpose:</p><ul><li><code>testing</code> shoot clusters <strong>do not</strong> get a monitoring or a logging stack as part of their control planes.</li><li><code>production</code> shoot clusters get at least two replicas of the <code>kube-apiserver</code> for their control planes.
Auto-scaling scale down of the main ETCD is disabled for such clusters.</li></ul><p>There are also differences with respect to how <code>testing</code> shoots are scheduled after creation, please consult the <a href=/docs/gardener/concepts/scheduler/>Scheduler documentation</a>.</p><h2 id=future-steps>Future Steps</h2><p>We might introduce more behavioral difference depending on the shoot purpose in the future.
As of today, there are no plans yet.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5e55fb76047a446cc19292d575f3a524>1.7.39 - Shoot Scheduling Profiles</h1><h1 id=shoot-scheduling-profiles>Shoot Scheduling Profiles</h1><p>This guide describes the available scheduling profiles and how they can be configured in the Shoot cluster. It also clarifies how a custom scheduling profile can be configured.</p><h2 id=scheduling-profiles>Scheduling Profiles</h2><p>The scheduling process in the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a> happens in a series of stages. A <a href=https://kubernetes.io/docs/reference/scheduling/config/#profiles>scheduling profile</a> allows configuring the different stages of the scheduling.</p><p>As of today, Gardener supports two predefined scheduling profiles:</p><ul><li><p><code>balanced</code> (default)</p><p><strong>Overview</strong></p><p>The <code>balanced</code> profile attempts to spread Pods evenly across Nodes to obtain a more balanced resource usage. This profile provides the default kube-scheduler behavior.</p><p><strong>How it works?</strong></p><p>The kube-scheduler is started without any profiles. In such case, by default, one profile with the scheduler name <code>default-scheduler</code> is created. This profile includes the default plugins. If a Pod doesn&rsquo;t specify the <code>.spec.schedulerName</code> field, kube-apiserver sets it to <code>default-scheduler</code>. Then, the Pod gets scheduled by the <code>default-scheduler</code> accordingly.</p></li><li><p><code>bin-packing</code> (alpha)</p><p><strong>Overview</strong></p><p>The <code>bin-packing</code> profile scores Nodes based on the allocation of resources. It prioritizes Nodes with the most allocated resources. By favoring the Nodes with the most allocation, some of the other Nodes become under-utilized over time (because new Pods keep being scheduled to the most allocated Nodes). Then, the cluster-autoscaler identifies such under-utilized Nodes and removes them from the cluster. In this way, this profile provides a greater overall resource utilization (compared to the <code>balanced</code> profile).</p><blockquote><p><strong>Note:</strong> The decision of when to remove a Node is a trade-off between optimizing for utilization or the availability of resources. Removing under-utilized Nodes improves cluster utilization, but new workloads might have to wait for resources to be provisioned again before they can run.</p></blockquote><blockquote><p><strong>Note:</strong> The <code>bin-packing</code> profile is considered as an alpha feature. Use it only for evaluation purposes.</p></blockquote><p><strong>How it works?</strong></p><p>The kube-scheduler is configured with the following bin packing profile:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: kubescheduler.config.k8s.io/v1beta3
</span></span><span style=display:flex><span>kind: KubeSchedulerConfiguration
</span></span><span style=display:flex><span>profiles:
</span></span><span style=display:flex><span>- schedulerName: bin-packing-scheduler
</span></span><span style=display:flex><span>  pluginConfig:
</span></span><span style=display:flex><span>  - name: NodeResourcesFit
</span></span><span style=display:flex><span>    args:
</span></span><span style=display:flex><span>      scoringStrategy:
</span></span><span style=display:flex><span>        type: MostAllocated
</span></span><span style=display:flex><span>  plugins:
</span></span><span style=display:flex><span>    score:
</span></span><span style=display:flex><span>      disabled:
</span></span><span style=display:flex><span>      - name: NodeResourcesBalancedAllocation
</span></span></code></pre></div><p>To impose the new profile, a <code>MutatingWebhookConfiguration</code> is deployed in the Shoot cluster. The <code>MutatingWebhookConfiguration</code> intercepts <code>CREATE</code> operations for Pods and sets the <code>.spec.schedulerName</code> field to <code>bin-packing-scheduler</code>. Then, the Pod gets scheduled by the <code>bin-packing-scheduler</code> accordingly. Pods that specify a custom scheduler (i.e., having <code>.spec.schedulerName</code> different from <code>default-scheduler</code> and <code>bin-packing-scheduler</code>) are not affected.</p></li></ul><h2 id=configuring-the-scheduling-profile>Configuring the Scheduling Profile</h2><p>The scheduling profile can be configured via the <code>.spec.kubernetes.kubeScheduler.profile</code> field in the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  <span style=color:green># ...</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeScheduler:
</span></span><span style=display:flex><span>      profile: <span style=color:#a31515>&#34;balanced&#34;</span> <span style=color:green># or &#34;bin-packing&#34;</span>
</span></span></code></pre></div><h2 id=custom-scheduling-profiles>Custom Scheduling Profiles</h2><p>The kube-scheduler&rsquo;s component configs allows configuring custom scheduling profiles to match the cluster needs. As of today, Gardener supports only two predefined scheduling profiles. The profile configuration in the component config is quite expressive and it is not possible to easily define profiles that would match the needs of every cluster. Because of these reasons, there are no plans to add support for new predefined scheduling profiles. If a cluster owner wants to use a custom scheduling profile, then they have to deploy (and maintain) a dedicated kube-scheduler deployment in the cluster itself.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f031782d2dc6494fa8d3d01bba9374d9>1.7.40 - Shoot Serviceaccounts</h1><h1 id=serviceaccount-configurations-for-shoot-clusters><code>ServiceAccount</code> Configurations for Shoot Clusters</h1><p>The <code>Shoot</code> specification allows to configure some of the settings for the handling of <code>ServiceAccount</code>s:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      serviceAccountConfig:
</span></span><span style=display:flex><span>        issuer: foo
</span></span><span style=display:flex><span>        acceptedIssuers:
</span></span><span style=display:flex><span>        - foo1
</span></span><span style=display:flex><span>        - foo2
</span></span><span style=display:flex><span>        extendTokenExpiration: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        maxTokenExpiration: 45d
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=issuer-and-accepted-issuers>Issuer and Accepted Issuers</h2><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.{issuer,acceptedIssuers}</code> fields are translated to the <code>--service-account-issuer</code> flag for the <code>kube-apiserver</code>.
The issuer will assert its identifier in the <code>iss</code> claim of the issued tokens.
According to the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>upstream specification</a>, values need to meet the following requirements:</p><blockquote><p>This value is a string or URI. If this option is not a valid URI per the OpenID Discovery 1.0 spec, the ServiceAccountIssuerDiscovery feature will remain disabled, even if the feature gate is set to true. It is highly recommended that this value comply with the OpenID spec: <a href=https://openid.net/specs/openid-connect-discovery-1_0.html>https://openid.net/specs/openid-connect-discovery-1_0.html</a>. In practice, this means that service-account-issuer must be an https URL. It is also highly recommended that this URL be capable of serving OpenID discovery documents at {service-account-issuer}/.well-known/openid-configuration.</p></blockquote><p>By default, Gardener uses the internal cluster domain as issuer (e.g., <code>https://api.foo.bar.example.com</code>).
If you specify the <code>issuer</code>, then this default issuer will always be part of the list of accepted issuers (you don&rsquo;t need to specify it yourself).</p><p>⚠️ Caution: If you change from the default issuer to a custom <code>issuer</code>, all previously issued tokens will still be valid/accepted.
However, if you change from a custom <code>issuer</code> <code>A</code> to another <code>issuer</code> <code>B</code> (custom or default), then you have to add <code>A</code> to the <code>acceptedIssuers</code> so that previously issued tokens are not invalidated.
Otherwise, the control plane components as well as system components and your workload pods might fail.
You can remove <code>A</code> from the <code>acceptedIssuers</code> when all currently active tokens have been issued solely by <code>B</code>.
This can be ensured by using <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>projected token volumes</a> with a short validity, or by rolling out all pods.
Additionally, all <a href=https://kubernetes.io/docs/concepts/configuration/secret/#service-account-token-secrets><code>ServiceAccount</code> token secrets</a> should be recreated.
Apart from this, you should wait for at least <code>12h</code> to make sure the control plane and system components have received a new token from Gardener.</p><h2 id=token-expirations>Token Expirations</h2><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.extendTokenExpiration</code> configures the <code>--service-account-extend-token-expiration</code> flag of the <code>kube-apiserver</code>.
It is enabled by default and has the following specification:</p><blockquote><p>Turns on projected service account expiration extension during token generation, which helps safe transition from legacy token to bound service account token feature. If this flag is enabled, admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition, ignoring value of service-account-max-token-expiration.</p></blockquote><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration</code> configures the <code>--service-account-max-token-expiration</code> flag of the <code>kube-apiserver</code>.
It has the following specification:</p><blockquote><p>The maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value.</p></blockquote><p>⚠️ Note that the value for this field must be in the <code>[30d,90d]</code> range.
The background for this limitation is that all Gardener components rely on the <code>TokenRequest</code> API and the Kubernetes service account token projection feature with short-lived, auto-rotating tokens.
Any values lower than <code>30d</code> risk impacting the SLO for shoot clusters, and any values above <code>90d</code> violate security best practices with respect to maximum validity of credentials before they must be rotated.
Given that the field just specifies the upper bound, end-users can still use lower values for their individual workload by specifying the <code>.spec.volumes[].projected.sources[].serviceAccountToken.expirationSeconds</code> in the <code>PodSpec</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c9d857194e434604d822b4e8baf96323>1.7.41 - Shoot Status</h1><h1 id=shoot-status>Shoot Status</h1><p>This document provides an overview of the <a href=/docs/gardener/api-reference/core/#shootstatus>ShootStatus</a>.</p><h2 id=conditions>Conditions</h2><p>The Shoot status consists of a set of conditions. A <a href=/docs/gardener/api-reference/core/#condition>Condition</a> has the following fields:</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td>Name of the condition.</td></tr><tr><td><code>status</code></td><td>Indicates whether the condition is applicable, with possible values <code>True</code>, <code>False</code>, <code>Unknown</code> or <code>Progressing</code>.</td></tr><tr><td><code>lastTransitionTime</code></td><td>Timestamp for when the condition last transitioned from one status to another.</td></tr><tr><td><code>lastUpdateTime</code></td><td>Timestamp for when the condition was updated. Usually changes when <code>reason</code> or <code>message</code> in condition is updated.</td></tr><tr><td><code>reason</code></td><td>Machine-readable, UpperCamelCase text indicating the reason for the condition&rsquo;s last transition.</td></tr><tr><td><code>message</code></td><td>Human-readable message indicating details about the last status transition.</td></tr><tr><td><code>codes</code></td><td>Well-defined error codes in case the condition reports a problem.</td></tr></tbody></table><p>Currently, the available Shoot condition types are:</p><ul><li><code>APIServerAvailable</code></li><li><code>ControlPlaneHealthy</code></li><li><code>EveryNodeReady</code></li><li><code>ObservabilityComponentsHealthy</code></li><li><code>SystemComponentsHealthy</code></li></ul><p>The Shoot conditions are maintained by the <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/care/reconciler.go>shoot care reconciler</a> of the gardenlet.
Find more information in the <a href=/docs/gardener/concepts/gardenlet/#shoot-controller>gardelent documentation</a>.</p><h3 id=sync-period>Sync Period</h3><p>The condition checks are executed periodically at an interval which is configurable in the <code>GardenletConfiguration</code> (<code>.controllers.shootCare.syncPeriod</code>, defaults to <code>1m</code>).</p><h3 id=condition-thresholds>Condition Thresholds</h3><p>The <code>GardenletConfiguration</code> also allows configuring condition thresholds (<code>controllers.shootCare.conditionThresholds</code>). A condition threshold is the amount of time to consider a condition as <code>Processing</code> on condition status changes.</p><p>Let&rsquo;s check the following example to get a better understanding. Let&rsquo;s say that the <code>APIServerAvailable</code> condition of our Shoot is with status <code>True</code>. If the next condition check fails (for example kube-apiserver becomes unreachable), then the condition first goes to <code>Processing</code> state. Only if this state remains for condition threshold amount of time, then the condition is finally updated to <code>False</code>.</p><h3 id=constraints>Constraints</h3><p>Constraints represent conditions of a Shoot’s current state that constraint some operations on it.
The current constraints are:</p><p><strong><code>HibernationPossible</code></strong>:</p><p>This constraint indicates whether a Shoot is allowed to be hibernated.
The rationale behind this constraint is that a Shoot can have <code>ValidatingWebhookConfiguration</code>s or <code>MutatingWebhookConfiguration</code>s acting on resources that are critical for waking up a cluster.
For example, if a webhook has rules for <code>CREATE/UPDATE</code> Pods or Nodes and <code>failurePolicy=Fail</code>, the webhook will block joining <code>Nodes</code> and creating critical system component Pods and thus block the entire wakeup operation, because the server backing the webhook is not running.</p><p>Even if the <code>failurePolicy</code> is set to <code>Ignore</code>, high timeouts (<code>>15s</code>) can lead to blocking requests of control plane components.
That&rsquo;s because most control-plane API calls are made with a client-side timeout of <code>30s</code>, so if a webhook has <code>timeoutSeconds=30</code>
the overall request might still fail as there is overhead in communication with the API server and potential other webhooks.</p><p>Generally, it&rsquo;s <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts>best practice</a> to specify low timeouts in WebhookConfigs.</p><p>As an effort to correct this common problem, the webhook remediator has been created. This is enabled by setting <code>.controllers.shootCare.webhookRemediatorEnabled=true</code> in the <code>gardenlet</code>&rsquo;s configuration. This feature simply checks whether webhook configurations in shoot clusters match a set of rules described <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/matchers/matcher.go>here</a>. If at least one of the rules matches, it will change set <code>status=False</code> for the <code>.status.constraints</code> of type <code>HibernationPossible</code> and <code>MaintenancePreconditionsSatisfied</code> in the <code>Shoot</code> resource. In addition, the <code>failurePolicy</code> in the affected webhook configurations will be set from <code>Fail</code> to <code>Ignore</code>. Gardenlet will also add an annotation to make it visible to end-users that their webhook configurations were mutated and should be fixed/adapted according to the rules and best practices.</p><p>In most cases, you can avoid this by simply excluding the <code>kube-system</code> namespace from your webhook via the <code>namespaceSelector</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: admissionregistration.k8s.io/v1
</span></span><span style=display:flex><span>kind: MutatingWebhookConfiguration
</span></span><span style=display:flex><span>webhooks:
</span></span><span style=display:flex><span>  - name: my-webhook.example.com
</span></span><span style=display:flex><span>    namespaceSelector:
</span></span><span style=display:flex><span>      matchExpressions:
</span></span><span style=display:flex><span>      - key: gardener.cloud/purpose
</span></span><span style=display:flex><span>        operator: NotIn
</span></span><span style=display:flex><span>        values:
</span></span><span style=display:flex><span>          - kube-system
</span></span><span style=display:flex><span>    rules:
</span></span><span style=display:flex><span>      - operations: [<span style=color:#a31515>&#34;*&#34;</span>]
</span></span><span style=display:flex><span>        apiGroups: [<span style=color:#a31515>&#34;&#34;</span>]
</span></span><span style=display:flex><span>        apiVersions: [<span style=color:#a31515>&#34;v1&#34;</span>]
</span></span><span style=display:flex><span>        resources: [<span style=color:#a31515>&#34;pods&#34;</span>]
</span></span><span style=display:flex><span>        scope: <span style=color:#a31515>&#34;Namespaced&#34;</span>
</span></span></code></pre></div><p>However, some other resources (some of them cluster-scoped) might still trigger the remediator, namely:</p><ul><li>endpoints</li><li>nodes</li><li>podsecuritypolicies</li><li>clusterroles</li><li>clusterrolebindings</li><li>customresourcedefinitions</li><li>apiservices</li><li>certificatesigningrequests</li><li>priorityclasses</li></ul><p>If one of the above resources triggers the remediator, the preferred solution is to remove that particular resource from your webhook&rsquo;s <code>rules</code>. You can also use the <code>objectSelector</code> to reduce the scope of webhook&rsquo;s <code>rules</code>. However, in special cases where a webhook is absolutely needed for the workload, it is possible to add the <code>remediation.webhook.shoot.gardener.cloud/exclude=true</code> label to your webhook so that the remediator ignores it. This label <strong>should not be used to silence an alert</strong>, but rather to confirm that a webhook won&rsquo;t cause problems. Note that all of this is no perfect solution and just done on a best effort basis, and only the owner of the webhook can know whether it indeed is problematic and configured correctly.</p><p>You can also find more help from the <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings>Kubernetes documentation</a></p><p><strong><code>MaintenancePreconditionsSatisfied</code></strong>:</p><p>This constraint indicates whether all preconditions for a safe maintenance operation are satisfied (see <a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a> for more information about what happens during a shoot maintenance).
As of today, the same checks as in the <code>HibernationPossible</code> constraint are being performed (user-deployed webhooks that might interfere with potential rolling updates of shoot worker nodes).
There is no further action being performed on this constraint&rsquo;s status (maintenance is still being performed).
It is meant to make the user aware of potential problems that might occur due to his configurations.</p><p><strong><code>CACertificateValiditiesAcceptable</code></strong>:</p><p>This constraints indicates that there is at least one CA certificate which expires in less than <code>1y</code>.
It will not be added to the <code>.status.constraints</code> if there is no such CA certificate.
However, if it&rsquo;s visible, then a <a href=/docs/gardener/usage/shoot_credentials_rotation/#certificate-authorities>credentials rotation operation</a> should be considered.</p><h3 id=last-operation>Last Operation</h3><p>The Shoot status holds information about the last operation that is performed on the Shoot. The last operation field reflects overall progress and the tasks that are currently being executed. Allowed operation types are <code>Create</code>, <code>Reconcile</code>, <code>Delete</code>, <code>Migrate</code>, and <code>Restore</code>. Allowed operation states are <code>Processing</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code>, <code>Pending</code>, and <code>Aborted</code>. An operation in <code>Error</code> state is an operation that will be retried for a configurable amount of time (<code>controllers.shoot.retryDuration</code> field in <code>GardenletConfiguration</code>, defaults to <code>12h</code>). If the operation cannot complete successfully for the configured retry duration, it will be marked as <code>Failed</code>. An operation in <code>Failed</code> state is an operation that won&rsquo;t be retried automatically (to retry such an operation, see <a href=/docs/gardener/usage/shoot_operations/#retry-failed-operation>Retry failed operation</a>).</p><h3 id=last-errors>Last Errors</h3><p>The Shoot status also contains information about the last occurred error(s) (if any) during an operation. A <a href=/docs/gardener/api-reference/core/#lasterror>LastError</a> consists of identifier of the task returned error, human-readable message of the error and error codes (if any) associated with the error.</p><h3 id=error-codes>Error Codes</h3><p>Known error codes are:</p><ul><li><code>ERR_INFRA_UNAUTHENTICATED</code> - Indicates that the last error occurred due to the client request not being completed because it lacks valid authentication credentials for the requested resource. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_UNAUTHORIZED</code> - Indicates that the last error occurred due to the server understanding the request but refusing to authorize it. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_QUOTA_EXCEEDED</code> - Indicates that the last error occurred due to infrastructure quota limits. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_RATE_LIMITS_EXCEEDED</code> - Indicates that the last error occurred due to exceeded infrastructure request rate limits.</li><li><code>ERR_INFRA_DEPENDENCIES</code> - Indicates that the last error occurred due to dependent objects on the infrastructure level. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_INFRA_DEPENDENCIES</code> - Indicates that the last error occurred due to dependent objects on the infrastructure level, but the operation should be retried.</li><li><code>ERR_INFRA_RESOURCES_DEPLETED</code> - Indicates that the last error occurred due to depleted resource in the infrastructure.</li><li><code>ERR_CLEANUP_CLUSTER_RESOURCES</code> - Indicates that the last error occurred due to resources in the cluster that are stuck in deletion.</li><li><code>ERR_CONFIGURATION_PROBLEM</code> - Indicates that the last error occurred due to a configuration problem. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_CONFIGURATION_PROBLEM</code> - Indicates that the last error occurred due to a retryable configuration problem. &ldquo;Retryable&rdquo; means that the occurred error is likely to be resolved in a ungraceful manner after given period of time.</li><li><code>ERR_PROBLEMATIC_WEBHOOK</code> - Indicates that the last error occurred due to a webhook not following the <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings>Kubernetes best practices</a>.</li></ul><h3 id=status-label>Status Label</h3><p>Shoots will be automatically labeled with the <code>shoot.gardener.cloud/status</code> label.
Its value might either be <code>healthy</code>, <code>progressing</code>, <code>unhealthy</code> or <code>unknown</code> depending on the <code>.status.conditions</code>, <code>.status.lastOperation</code>, and <code>status.lastErrors</code> of the <code>Shoot</code>.
This can be used as an easy filter method to find shoots based on their &ldquo;health&rdquo; status.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3dee6f3ebf812a904f8258912bb1e764>1.7.42 - Shoot Supported Architectures</h1><h1 id=supported-cpu-architectures-for-shoot-worker-nodes>Supported CPU Architectures for Shoot Worker Nodes</h1><p>Users can create shoot clusters with worker groups having virtual machines of different architectures. CPU architecture of each worker pool can be specified in the <code>Shoot</code> specification as follows:</p><h2 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        architecture: &lt;some-cpu-architecture&gt; <span style=color:green># optional</span>
</span></span></code></pre></div><p>If no value is specified for the architecture field, it defaults to <code>amd64</code>. For a valid shoot object, a machine type should be present in the respective <code>CloudProfile</code> with the same CPU architecture as specified in the <code>Shoot</code> yaml. Also, a valid machine image should be present in the <code>CloudProfile</code> that supports the required architecture specified in the <code>Shoot</code> worker pool.</p><h2 id=example-usage-in-a-cloudprofile>Example Usage in a <code>CloudProfile</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: test-image
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - architectures: <span style=color:green># optional</span>
</span></span><span style=display:flex><span>      - &lt;architecture-1&gt;
</span></span><span style=display:flex><span>      - &lt;architecture-2&gt;
</span></span><span style=display:flex><span>      version: 1.2.3
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - architecture: &lt;some-cpu-architecture&gt;
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    name: test-machine
</span></span></code></pre></div><p>Currently, Gardener supports two of the most widely used CPU architectures:</p><ul><li><code>amd64</code></li><li><code>arm64</code></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-442c557a128e8fd2c4f787af32cd900f>1.7.43 - Shoot Updates</h1><h1 id=shoot-updates-and-upgrades>Shoot Updates and Upgrades</h1><p>This document describes what happens during shoot updates (changes incorporated in a newly deployed Gardener version) and during shoot upgrades (changes for version controllable by end-users).</p><h2 id=updates>Updates</h2><p>Updates to all aspects of the shoot cluster happen when the gardenlet reconciles the <code>Shoot</code> resource.</p><h3 id=when-are-reconciliations-triggered>When are Reconciliations Triggered</h3><p>Generally, when you change the specification of your <code>Shoot</code> the reconciliation will start immediately, potentially updating your cluster.
Please note that you can also confine the reconciliation triggered due to your specification updates to the cluster&rsquo;s maintenance time window. Please find more information in <a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out</a>.</p><p>You can also annotate your shoot with special operation annotations (for more information, see <a href=/docs/gardener/usage/shoot_operations/>Trigger Shoot Operations</a>), which will cause the reconciliation to start due to your actions.</p><p>There is also an automatic reconciliation by Gardener.
The period, i.e., how often it is performed, depends on the configuration of the Gardener administrators/operators.
In some Gardener installations the operators might enable &ldquo;reconciliation in maintenance time window only&rdquo; (for more information, see <a href=/docs/gardener/usage/shoot_maintenance/#cluster-reconciliation>Cluster Reconciliation</a>), which will result in at least one reconciliation during the time configured in the <code>Shoot</code>&rsquo;s <code>.spec.maintenance.timeWindow</code> field.</p><h3 id=which-updates-are-applied>Which Updates are Applied</h3><p>As end-users can only control the <code>Shoot</code> resource&rsquo;s specification but not the used Gardener version, they don&rsquo;t have any influence on which of the updates are rolled out (other than those settings configurable in the <code>Shoot</code>).
A Gardener operator can deploy a new Gardener version at any point in time.
Any subsequent reconciliation of <code>Shoot</code>s will update them by rolling out the changes incorporated in this new Gardener version.</p><p>Some examples for such shoot updates are:</p><ul><li>Add a new/remove an old component to/from the shoot&rsquo;s control plane running in the seed, or to/from the shoot&rsquo;s system components running on the worker nodes.</li><li>Change the configuration of an existing control plane/system component.</li><li>Restart of existing control plane/system components (this might result in a short unavailability of the Kubernetes API server, e.g., when etcd or a kube-apiserver itself is being restarted)</li></ul><h3 id=behavioural-changes>Behavioural Changes</h3><p>Generally, some of such updates (e.g., configuration changes) could theoretically result in different behaviour of controllers.
If such changes would be backwards-incompatible, then we usually follow one of those approaches (depends on the concrete change):</p><ul><li>Only apply the change for new clusters.</li><li>Expose a new field in the <code>Shoot</code> resource that lets users control this changed behaviour to enable it at a convenient point in time.</li><li>Put the change behind an alpha feature gate (disabled by default) in the gardenlet (only controllable by Gardener operators), which will be promoted to beta (enabled by default) in subsequent releases (in this case, end-users have no influence on when the behaviour changes - Gardener operators should inform their end-users and provide clear timelines when they will enable the feature gate).</li></ul><h2 id=upgrades>Upgrades</h2><p>We consider shoot upgrades to change either the:</p><ul><li>Kubernetes version (<code>.spec.kubernetes.version</code>)</li><li>Kubernetes version of the worker pool if specified (<code>.spec.provider.workers[].kubernetes.version</code>)</li><li>Machine image version of at least one worker pool (<code>.spec.provider.workers[].machine.image.version</code>)</li></ul><p>Generally, an upgrade is also performed through a reconciliation of the <code>Shoot</code> resource, i.e., the same concepts as for <a href=#updates>shoot updates</a> apply.
If an end-user triggers an upgrade (e.g., by changing the Kubernetes version) after a new Gardener version was deployed but before the shoot was reconciled again, then this upgrade might incorporate the changes delivered with this new Gardener version.</p><h3 id=in-place-vs-rolling-updates>In-Place vs. Rolling Updates</h3><p>If the Kubernetes patch version is changed, then the upgrade happens in-place.
This means that the shoot worker nodes remain untouched and only the <code>kubelet</code> process restarts with the new Kubernetes version binary.
The same applies for configuration changes of the kubelet.</p><p>If the Kubernetes minor version is changed, then the upgrade is done in a &ldquo;rolling update&rdquo; fashion, similar to how pods in Kubernetes are updated (when backed by a <code>Deployment</code>).
The worker nodes will be terminated one after another and replaced by new machines.
The existing workload is gracefully drained and evicted from the old worker nodes to new worker nodes, respecting the configured <code>PodDisruptionBudget</code>s (see <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>Specifying a Disruption Budget for your Application</a>).</p><h4 id=customize-rolling-update-behaviour-of-shoot-worker-nodes>Customize Rolling Update Behaviour of Shoot Worker Nodes</h4><p>The <code>.spec.provider.workers[]</code> list exposes two fields that you might configure based on your workload&rsquo;s needs: <code>maxSurge</code> and <code>maxUnavailable</code>.
The same concepts <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>like in Kubernetes</a> apply.
Additionally, you might customize how the machine-controller-manager (abbrev.: MCM; the component instrumenting this rolling update) is behaving. You can configure the following fields in <code>.spec.provider.worker[].machineControllerManager</code>:</p><ul><li><code>machineDrainTimeout</code>: Timeout (in duration) used while draining of machine before deletion, beyond which MCM forcefully deletes the machine (default: <code>10m</code>).</li><li><code>machineHealthTimeout</code>: Timeout (in duration) used while re-joining (in case of temporary health issues) of a machine before it is declared as failed (default: <code>10m</code>).</li><li><code>machineCreationTimeout</code>: Timeout (in duration) used while joining (during creation) of a machine before it is declared as failed (default: <code>10m</code>).</li><li><code>maxEvictRetries</code>: Maximum number of times evicts would be attempted on a pod before it is forcibly deleted during the draining of a machine (default: <code>10</code>).</li><li><code>nodeConditions</code>: List of case-sensitive node-conditions which will change a machine to a <code>Failed</code> state after the <code>machineHealthTimeout</code> duration. It may further be replaced with a new machine if the machine is backed by a machine-set object (defaults: <code>KernelDeadlock</code>, <code>ReadonlyFilesystem</code> , <code>DiskPressure</code>).</li></ul><h4 id=rolling-update-triggers>Rolling Update Triggers</h4><p>Apart from the above mentioned triggers, a rolling update of the shoot worker nodes is also triggered for some changes to your worker pool specification (<code>.spec.provider.workers[]</code>, even if you don&rsquo;t change the Kubernetes or machine image version).
The complete list of fields that trigger a rolling update:</p><ul><li><code>.spec.kubernetes.version</code> (except for patch version changes)</li><li><code>.spec.provider.workers[].machine.image.name</code></li><li><code>.spec.provider.workers[].machine.image.version</code></li><li><code>.spec.provider.workers[].machine.type</code></li><li><code>.spec.provider.workers[].volume.type</code></li><li><code>.spec.provider.workers[].volume.size</code></li><li><code>.spec.provider.workers[].providerConfig</code></li><li><code>.spec.provider.workers[].cri.name</code></li><li><code>.spec.provider.workers[].kubernetes.version</code> (except for patch version changes)</li><li><code>.status.credentials.rotation.certificateAuthorities.lastInitiationTime</code> (changed by Gardener when a shoot CA rotation is initiated)</li><li><code>.status.credentials.rotation.serviceAccountKey.lastInitiationTime</code> (changed by Gardener when a shoot service account signing key rotation is initiated)</li></ul><p>Generally, the provider extension controllers might have additional constraints for changes leading to rolling updates, so please consult the respective documentation as well.</p><h2 id=related-documentation>Related Documentation</h2><ul><li><a href=/docs/gardener/usage/shoot_operations/>Shoot Operations</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out To Maintenance Time Window</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-76b0c7e5bbbfbafbbf70acec450a467b>1.7.44 - Shoot Versions</h1><h1 id=shoot-kubernetes-and-operating-system-versioning-in-gardener>Shoot Kubernetes and Operating System Versioning in Gardener</h1><h2 id=motivation>Motivation</h2><p>On the one hand-side, Gardener is responsible for managing the Kubernetes and the Operating System (OS) versions of its Shoot clusters.
On the other hand-side, Gardener needs to be configured and updated based on the availability and support of the Kubernetes and Operating System version it provides.
For instance, the Kubernetes community releases <strong>minor</strong> versions roughly every three months and usually maintains <strong>three minor</strong> versions (the current and the last two) with bug fixes and security updates.
Patch releases are done more frequently.</p><p>When using the term <code>Machine image</code> in the following, we refer to the OS version that comes with the machine image of the node/worker pool of a Gardener Shoot cluster.
As such, we are not referring to the <code>CloudProvider</code> specific machine image like the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html><code>AMI</code></a> for AWS.
For more information on how Gardener maps machine image versions to <code>CloudProvider</code> specific machine images, take a look at the individual gardener extension providers, such as the <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-operator/>provider for AWS</a>.</p><p>Gardener should be configured accordingly to reflect the &ldquo;logical state&rdquo; of a version.
It should be possible to define the Kubernetes or Machine image versions that still receive bug fixes and security patches, and also vice-versa to define the version that are out-of-maintenance and are potentially vulnerable.
Moreover, this allows Gardener to &ldquo;understand&rdquo; the current state of a version and act upon it (more information in the following sections).</p><h2 id=overview>Overview</h2><p><strong>As a Gardener operator</strong>:</p><ul><li>I can classify a version based on it&rsquo;s logical state (<code>preview</code>, <code>supported</code>, <code>deprecated</code>, and <code>expired</code>; see <a href=#version-classifications>Version Classification</a>).</li><li>I can define which Machine image and Kubernetes versions are eligible for the auto update of clusters during the maintenance time.</li><li>I can disallow the creation of clusters having a certain version (think of severe security issues).</li></ul><p><strong>As an end-user/Shoot owner of Gardener</strong>:</p><ul><li>I can get information about which Kubernetes and Machine image versions exist and their classification.</li><li>I can determine the time when my Shoot clusters Machine image and Kubernetes version will be forcefully updated to the next patch or minor version (in case the cluster is running a deprecated version with an expiration date).</li><li>I can get this information via API from the <code>CloudProfile</code>.</li></ul><h2 id=version-classifications>Version Classifications</h2><p>Administrators can classify versions into four distinct &ldquo;logical states&rdquo;: <code>preview</code>, <code>supported</code>, <code>deprecated</code>, and <code>expired</code>.
The version classification serves as a &ldquo;point-of-reference&rdquo; for end-users and also has implications during shoot creation and the maintenance time.</p><p>If a version is unclassified, Gardener cannot make those decision based on the &ldquo;logical state&rdquo;.
Nevertheless, Gardener can operate without version classifications and can be added at any time to the Kubernetes and machine image versions in the <code>CloudProfile</code>.</p><p>As a best practice, versions usually start with the classification <code>preview</code>, then are promoted to <code>supported</code>, eventually <code>deprecated</code> and finally <code>expired</code>.
This information is programmatically available in the <code>CloudProfiles</code> of the Garden cluster.</p><ul><li><p><strong>preview:</strong> A <code>preview</code> version is a new version that has not yet undergone thorough testing, possibly a new release, and needs time to be validated.
Due to its short early age, there is a higher probability of undiscovered issues and is therefore not yet recommended for production usage.
A Shoot does not update (neither <code>auto-update</code> or <code>force-update</code>) to a <code>preview</code> version during the maintenance time.
Also, <code>preview</code> versions are not considered for the defaulting to the highest available version when deliberately omitting the patch version during Shoot creation.
Typically, after a fresh release of a new Kubernetes (e.g., v1.25.0) or Machine image version (e.g., suse-chost 15.4.20220818), the operator tags it as <code>preview</code> until he has gained sufficient experience and regards this version to be reliable.
After the operator has gained sufficient trust, the version can be manually promoted to <code>supported</code>.</p></li><li><p><strong>supported:</strong> A <code>supported</code> version is the recommended version for new and existing Shoot clusters. This is the version that new Shoot clusters should use and existing clusters should update to.
Typically for Kubernetes versions, the latest Kubernetes patch versions of the actual (if not still in <code>preview</code>) and the last 3 minor Kubernetes versions are maintained by the community. An operator could define these versions as being <code>supported</code> (e.g., v1.24.6, v1.23.12, and v1.22.15).</p></li><li><p><strong>deprecated:</strong> A <code>deprecated</code> version is a version that approaches the end of its lifecycle and can contain issues which are probably resolved in a supported version.
New Shoots should not use this version anymore.
Existing Shoots will be updated to a newer version if <code>auto-update</code> is enabled (<code>.spec.maintenance.autoUpdate.kubernetesVersion</code> for Kubernetes version <code>auto-update</code>, or <code>.spec.maintenance.autoUpdate.machineImageVersion</code> for machine image version <code>auto-update</code>).
Using automatic upgrades, however, does not guarantee that a Shoot runs a non-deprecated version, as the latest version (overall or of the minor version) can be deprecated as well.
Deprecated versions <strong>should</strong> have an expiration date set for eventual expiration.</p></li><li><p><strong>expired:</strong> An <code>expired</code> versions has an expiration date (based on the <a href=https://golang.org/src/time/time.go>Golang time package</a>) in the past.
New clusters with that version cannot be created and existing clusters are forcefully migrated to a higher version during the maintenance time.</p></li></ul><p>Below is an example how the relevant section of the <code>CloudProfile</code> might look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>      - classification: preview
</span></span><span style=display:flex><span>        version: 1.25.0
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.24.6
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        expirationDate: <span style=color:#a31515>&#34;2022-11-30T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>        version: 1.24.5
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.23.12
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        expirationDate: <span style=color:#a31515>&#34;2023-01-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>        version: 1.23.11
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.22.15
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        version: 1.21.14
</span></span></code></pre></div><h2 id=version-requirements-kubernetes-and-machine-image>Version Requirements (Kubernetes and Machine Image)</h2><p>The Gardener API server enforces the following requirements for versions:</p><h3 id=deletion-of-a-version>Deletion of a Version</h3><ul><li>A version that is in use by a Shoot cannot be deleted from the <code>CloudProfile</code>.</li></ul><h3 id=adding-a-version>Adding a Version</h3><ul><li>A version must not have an expiration date in the past.</li><li>There can be only one <code>supported</code> version per minor version.</li><li>The latest Kubernetes version cannot have an expiration date.</li><li>The latest version for a machine image can have an expiration date. [*]</li></ul><p><sub>[*] Useful for cases in which support for A given machine image needs to be deprecated and removed (for example, the machine image reaches end of life).</sub></p><h2 id=forceful-migration-of-expired-versions>Forceful Migration of Expired Versions</h2><p>If a Shoot is running a version after its expiration date has passed, it will be forcefully migrated during its maintenance time.
This happens <strong>even if the owner has opted out of automatic cluster updates!</strong></p><p>For <strong>Machine images</strong>, the Shoots worker pools will be updated to the latest <code>non-preview</code> version of the pools respective image.</p><p>For <strong>Kubernetes versions</strong>, the forceful update picks the latest <code>non-preview</code> patch version of the current minor version.</p><p>If the cluster is already on the latest patch version and the latest patch version is also expired,
it will continue with the latest patch version of the <strong>next consecutive minor Kubernetes version</strong>,
so <strong>it will result in an update of a minor Kubernetes version!</strong></p><p>Please note that multiple consecutive minor version upgrades are possible.
This can occur if the Shoot is updated to a version that in turn is also <code>expired</code>.
In this case, the version is again upgraded in the <strong>next</strong> maintenance time.</p><p><strong>Depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!</strong></p><p>Kubernetes &ldquo;minor version jumps&rdquo; are not allowed - meaning to skip the update to the consecutive minor version and directly update to any version after that.
For instance, the version <code>1.20.x</code> can only update to a version <code>1.21.x</code>, not to <code>1.22.x</code> or any other version.
This is because Kubernetes does not guarantee upgradability in this case, leading to possibly broken Shoot clusters.
The administrator has to set up the <code>CloudProfile</code> in such a way that consecutive Kubernetes minor versions are available.
Otherwise, Shoot clusters will fail to upgrade during the maintenance time.</p><p>Consider the <code>CloudProfile</code> below with a Shoot using the Kubernetes version <code>1.20.12</code>.
Even though the version is <code>expired</code>, due to missing <code>1.21.x</code> versions, the Gardener Controller Manager cannot upgrade the Shoot&rsquo;s Kubernetes version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.22.8
</span></span><span style=display:flex><span>    - version: 1.22.7
</span></span><span style=display:flex><span>    - version: 1.20.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</span></span></code></pre></div><p>The <code>CloudProfile</code> must specify versions <code>1.21.x</code> of the <strong>consecutive</strong> minor version.
Configuring the <code>CloudProfile</code> in such a way, the Shoot&rsquo;s Kubernetes version will be upgraded to version <code>1.21.10</code> in the next maintenance time.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.22.8
</span></span><span style=display:flex><span>    - version: 1.21.10
</span></span><span style=display:flex><span>    - version: 1.21.09
</span></span><span style=display:flex><span>    - version: 1.20.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</span></span></code></pre></div><h2 id=related-documentation>Related Documentation</h2><p>You might want to read about the <a href=/docs/gardener/usage/shoot_updates/>Shoot Updates and Upgrades</a> procedures to get to know the effects of such operations.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-92a2153b69b5ad751225b35af070429c>1.7.45 - Shoot Workers Settings</h1><h1 id=shoot-worker-nodes-settings>Shoot Worker Nodes Settings</h1><p>Users can configure settings affecting all worker nodes via <code>.spec.provider.workersSettings</code> in the <code>Shoot</code> resource.</p><h2 id=ssh-access>SSH Access</h2><p><code>SSHAccess</code> indicates whether the <code>sshd.service</code> should be running on the worker nodes. This is ensured by a systemd service called <code>sshd-ensurer.service</code> which runs every 15 seconds on each worker node. When set to <code>true</code>, the systemd service ensures that the <code>sshd.service</code> is enabled and running. If it is set to <code>false</code>, the systemd service ensures that <code>sshd.service</code> is stopped and disabled. This also terminates all established SSH connections. In addition, when this value is set to <code>false</code>, existing <code>Bastion</code> resources are deleted during <code>Shoot</code> reconciliation and new ones are prevented from being created, SSH keypairs are not created/rotated, SSH keypair secrets are deleted from the Garden cluster, and the <code>gardener-user.service</code> is not deployed to the worker nodes.</p><p><code>sshAccess.enabled</code> is set to <code>true</code> by default.</p><h3 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workersSettings:
</span></span><span style=display:flex><span>      sshAccess:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ba6ea030a94e576907f8029c24109302>1.7.46 - Supported K8s Versions</h1><h1 id=supported-kubernetes-versions>Supported Kubernetes Versions</h1><p>Currently, Gardener supports the following Kubernetes versions:</p><h2 id=garden-clusters>Garden Clusters</h2><p>The minimum version of a garden cluster that can be used to run Gardener is <strong><code>1.20.x</code></strong>.</p><h2 id=seed-clusters>Seed Clusters</h2><p>The minimum version of a seed cluster that can be connected to Gardener is <strong><code>1.20.x</code></strong>.
Please note that Gardener does not support seeds with version <strong>>= <code>1.25</code></strong> yet when the <code>HVPA</code> or <code>HVPAForShootedSeed</code> feature gate is enabled. For more details, see <a href=https://github.com/gardener/gardener/issues/6893>Issue #6893</a>.</p><h2 id=shoot-clusters>Shoot Clusters</h2><p>Gardener itself is capable of spinning up clusters with Kubernetes versions <strong><code>1.20</code></strong> up to <strong><code>1.26</code></strong>.
However, the concrete versions that can be used for shoot clusters depend on the installed provider extension.
Consequently, please consult the documentation of your provider extension to see which Kubernetes versions are supported for shoot clusters.</p><blockquote><p>👨🏼‍💻 Developers note: The <a href=/docs/gardener/development/new-kubernetes-version/>Adding Support For a New Kubernetes Version</a> topic explains what needs to be done in order to add support for a new Kubernetes version.</p></blockquote><h2 id=support-timeline>Support Timeline</h2><p>The Kubernetes project maintains the most recent three minor releases and releases a new minor version every 4 months.
This means that a release has patch support for approximately 1 year.
See the official <a href=https://kubernetes.io/releases/>Releases</a> topic for the official upstream information.</p><p>In the past, the Gardener project did not have a policy regarding the number of supported Kubernetes versions at the same time.
Beginning with 2023, a new policy has been introduced:</p><blockquote><p>The Gardener project supports the last four Kubernetes minor versions and drops support for the oldest minor version as soon as support for a new minor version has been introduced.</p></blockquote><table><thead><tr><th>Kubernetes Version</th><th>End of Life</th><th>Supported Since</th><th>Support Dropped After</th></tr></thead><tbody><tr><td>1.20</td><td>2022-02-28</td><td>v1.15.0</td><td>2023-01-31</td></tr><tr><td>1.21</td><td>2022-06-28</td><td>v1.21.0</td><td>2023-02-28</td></tr><tr><td>1.22</td><td>2022-10-28</td><td>v1.31.0</td><td>2023-04-30</td></tr><tr><td>1.23</td><td>2023-02-28</td><td>v1.39.0</td><td>1.27 is supported (> 2023-04)</td></tr><tr><td>1.24</td><td>2023-07-28</td><td>v1.48.0</td><td>1.28 is supported (> 2023-08)</td></tr><tr><td>1.25</td><td>2023-10-28</td><td>v1.56.0</td><td>1.29 is supported (> 2023-12)</td></tr><tr><td>1.26</td><td>2024-02-28</td><td>v1.63.0</td><td>1.30 is supported (> 2024-04)</td></tr></tbody></table><p>The three versions 1.20, 1.21, 1.22 (which all are officially out of maintenance already) are handled specially to allow users to adapt to this new policy.
Beginning with 1.23, the support of the oldest version is dropped after the support of a new version was introduced.</p><blockquote><p>⚠️ Note that this guideline only concerns the code of <code>gardener/gardener</code> and is not related to the versions offered in <code>CloudProfile</code>s.
It is recommended to always only offer the last three minor versions with <code>supported</code> classification in <code>CloudProfile</code>s and deprecate the oldest version with an expiration date before a new minor Kubernetes version is released.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-f92740830de9285764daf47f392d76d6>1.7.47 - Tolerations</h1><h1 id=taints-and-tolerations-for-seeds-and-shoots>Taints and Tolerations for <code>Seed</code>s and <code>Shoot</code>s</h1><p>Similar to <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a> for <code>Node</code>s and <code>Pod</code>s in Kubernetes, the <code>Seed</code> resource supports specifying taints (<code>.spec.taints</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml#L48-L55>this example</a>) while the <code>Shoot</code> resource supports specifying tolerations (<code>.spec.tolerations</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L268-L269>this example</a>).
The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.</p><p>Compared to Kubernetes, Gardener&rsquo;s taints and tolerations are very much down-stripped right now and have some behavioral differences.
Please read the following explanations carefully if you plan to use them.</p><h2 id=scheduling>Scheduling</h2><p>When scheduling a new shoot, the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot.
As Gardener&rsquo;s taints/tolerations don&rsquo;t support <code>effect</code>s yet, you can compare this behaviour with using a <code>NoSchedule</code> effect taint in Kubernetes.</p><p>Be reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use <code>.spec.seedSelector</code> in the <code>Shoot</code> to state such desires.</p><p>⚠️ Please note that - unlike how it&rsquo;s implemented in Kubernetes - a certain seed cluster <strong>may</strong> only be used when the shoot tolerates <strong>all</strong> the seed&rsquo;s taints.
This means that specifying <code>.spec.seedName</code> for a seed whose taints are not tolerated will make the gardener-apiserver reject the request.</p><p>Consequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.</p><h2 id=toleration-defaults-and-whitelist>Toleration Defaults and Whitelist</h2><p>The <code>Project</code> resource features a <code>.spec.tolerations</code> object that may carry <code>defaults</code> and a <code>whitelist</code> (see <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml#L33-L37>this example</a>).
The corresponding <code>ShootTolerationRestriction</code> admission plugin (cf. Kubernetes&rsquo; <code>PodTolerationRestriction</code> admission plugin) is responsible for evaluating these settings during creation/update of <code>Shoot</code>s.</p><h3 id=whitelist>Whitelist</h3><p>If a shoot gets created or updated with tolerations, then it is validated that only those tolerations may be used that were added to either a) the <code>Project</code>&rsquo;s <code>.spec.tolerations.whitelist</code>, or b) to the global whitelist in the <code>ShootTolerationRestriction</code>&rsquo;s admission config (see <a href=https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml#L7-L14>this example</a>).</p><p>⚠️ Please note that the tolerations whitelist of <code>Project</code>s can only be changed if the user trying to change it is bound to the <code>modify-spec-tolerations-whitelist</code> custom RBAC role, e.g., via the following <code>ClusterRole</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRole
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: full-project-modification-access
</span></span><span style=display:flex><span>rules:
</span></span><span style=display:flex><span>- apiGroups:
</span></span><span style=display:flex><span>  - core.gardener.cloud
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - projects
</span></span><span style=display:flex><span>  verbs:
</span></span><span style=display:flex><span>  - create
</span></span><span style=display:flex><span>  - patch
</span></span><span style=display:flex><span>  - update
</span></span><span style=display:flex><span>  - modify-spec-tolerations-whitelist
</span></span><span style=display:flex><span>  - delete
</span></span></code></pre></div><h3 id=defaults>Defaults</h3><p>If a shoot gets created, then the default tolerations specified in both the <code>Project</code>&rsquo;s <code>.spec.tolerations.defaults</code> and the global default list in the <code>ShootTolerationRestriction</code> admission plugin&rsquo;s configuration will be added to the <code>.spec.tolerations</code> of the <code>Shoot</code> (unless it already specifies a certain key).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-717af96646de8dc0584626ea13c573c8>1.7.48 - Trouble Shooting Guide</h1><h1 id=troubleshooting-guide>Troubleshooting Guide</h1><h2 id=are-there-really-issues-that-cannot-be-fixed>Are there really issues that cannot be fixed?</h2><p>Well, of course not. With the continuous development of Gardener, over the time its architecture and API might have to be changed to reduce complexity and support more features. In this process, developers are bound to keep the current Gardener version backward compatible with the last two releases. But maintaining backward compatibility is quite the complex and effortful task. So, to save the short term complex effort, it&rsquo;s common practice in open source community to use workarounds or hacky solutions sometimes. This results in rare issues which are supposed to be resolved by human interaction across upgrades of the Gardener version.</p><p>This guide records the issues that are quite possible across upgrade of Gardener version, their root cause and the human action required for graceful resolution of the issue. For a troubleshooting guide of bugs which are not yet fixed, please refer the associated GitHub issue.</p><blockquote><p><strong>Note To Maintainers:</strong> Please only mention the resolution of issues which are by design. For bugs, please report the temporary resolution in the GitHub issue created for the bug.</p></blockquote><h3 id=etcd-main-pod-fails-to-come-up-since-the-backup-restore-sidecar-is-reporting-revisionconsistencycheckerr>Etcd-Main pod fails to come up since the backup-restore sidecar is reporting RevisionConsistencyCheckErr</h3><h4 id=issue>Issue</h4><ul><li>Etcd-main pod goes in <code>CrashLoopBackoff</code>.</li><li>Etcd-backup-restore sidecar reports validation error with RevisionConsistencyCheckErr.</li></ul><h4 id=environment>Environment</h4><ul><li>Gardener version: 0.29.0+</li></ul><h4 id=root-cause>Root Cause</h4><ul><li>From version 0.29.0, Gardener uses a shared backup bucket for storing etcd backups, replacing the old logic of having a single bucket per shoot as per <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md>GEP-02</a>.</li><li>Since there are very rare chances that the etcd data directory will get corrupted while doing this migration, to avoid etcd downtime and implementation effort, we decided to switch directly from the old bucket to the new shared bucket without migrating the snapshot from the old bucket to the new bucket.</li><li>In this case, just to be on the safe side, we added a sanity check in the etcd-backup-restore sidecar of the etcd-main pod, which checks if the etcd data revision is greater than the last snapshot revision from the old bucket.</li><li>If the above check fails, this means that some data corruption has occurred with etcd, so etcd-backup-restore reports error and then etcd-main pod goes in <code>CrashLoopBackoff</code>, creating etcd-main down alerts.</li></ul><h4 id=action>Action</h4><ol><li>Disable the Gardener reconciliation for the Shoot by annotating it with <code>shoot.gardener.cloud/ignore=true</code>.</li><li>Scale down the etcd-main statefulset in the seed cluster.</li><li>Find out the latest full snapshot and delta snapshot from the old backup bucket. The old backup bucket name is the same as the backupInfra resource associated with the Shoot in the Garden cluster.</li><li>Move them manually to new the backup bucket.</li><li>Enable the Gardener reconciliation for the shoot by removing the <code>shoot.gardener.cloud/ignore=true</code> annotation.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-cf607bc71a1ae64925c002dc4829df51>1.7.49 - Trusted Tls For Control Planes</h1><h1 id=trusted-tls-certificate-for-shoot-control-planes>Trusted TLS Certificate for Shoot Control Planes</h1><p>Shoot clusters are composed of several control plane components deployed by Gardener and its corresponding extensions.</p><p>Some components are exposed via <code>Ingress</code> resources, which make them addressable under the HTTPS protocol.</p><p>Examples:</p><ul><li>Alertmanager</li><li>Grafana</li><li>Prometheus</li></ul><p>Gardener generates the backing TLS certificates, which are signed by the shoot cluster&rsquo;s CA by default (self-signed).</p><p>Unlike with a self-contained Kubeconfig file, common internet browsers or operating systems don&rsquo;t trust a shoot&rsquo;s cluster CA and adding it as a trusted root is often undesired in enterprise environments.</p><p>Therefore, Gardener operators can predefine trusted wildcard certificates under which the mentioned endpoints will be served instead.</p><h2 id=register-a-trusted-wildcard-certificate>Register a trusted wildcard certificate</h2><p>Since control plane components are published under the ingress domain (<code>core.gardener.cloud/v1beta1.Seed.spec.ingress.domain</code>) a wildcard certificate is required.</p><p>For example:</p><ul><li>Seed ingress domain: <code>dev.my-seed.example.com</code></li><li><code>CN</code> or <code>SAN</code> for a certificate: <code>*.dev.my-seed.example.com</code></li></ul><p>A wildcard certificate matches exactly one seed. It must be deployed as part of your landscape setup as a Kubernetes <code>Secret</code> inside the <code>garden</code> namespace of the corresponding seed cluster.</p><p>Please ensure that the secret has the <code>gardener.cloud/role</code> label shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  ca.crt: base64-encoded-ca.crt
</span></span><span style=display:flex><span>  tls.crt: base64-encoded-tls.crt
</span></span><span style=display:flex><span>  tls.key: base64-encoded-tls.key
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane-cert
</span></span><span style=display:flex><span>  name: seed-ingress-certificate
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span></code></pre></div><p>Gardener copies the secret during the reconciliation of shoot clusters to the shoot namespace in the seed. Afterwards, the <code>Ingress</code> resources in that namespace for the mentioned components will refer to the wildcard certificate.</p><h2 id=best-practice>Best Practice</h2><p>While it is possible to create the wildcard certificates manually and deploy them to seed clusters, it is recommended to let certificate management components do this job. Often, a seed cluster is also a shoot cluster at the same time (ManagedSeed) and might already provide a certificate service extension.
Otherwise, a Gardener operator may use solutions like <a href=https://github.com/gardener/cert-management>Cert-Management</a> or <a href=https://github.com/jetstack/cert-manager>Cert-Manager</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2a827ed3506304fa29f2a71110704fb0>1.7.50 - Worker Pool K8s Versions</h1><h1 id=controlling-the-kubernetes-versions-for-specific-worker-pools>Controlling the Kubernetes Versions for Specific Worker Pools</h1><p>Since Gardener <code>v1.36</code>, worker pools can have different Kubernetes versions specified than the control plane.</p><p>In earlier Gardener versions, all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).</p><p>In order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process.
In such cases, the Kubernetes version for the worker pools can be pinned (<code>.spec.provider.workers[].kubernetes.version</code>) while the control plane Kubernetes version (<code>.spec.kubernetes.version</code>) is updated.
This results in the nodes being untouched while the control plane is upgraded.
Now a new worker pool (with the version equal to the control plane version) can be added.
Administrators can then reschedule their workloads to the new worker pool according to their upgrade requirements and processes.</p><h2 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.6
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: data1
</span></span><span style=display:flex><span>      kubernetes:
</span></span><span style=display:flex><span>        version: 1.23.13
</span></span><span style=display:flex><span>    - name: data2
</span></span></code></pre></div><ul><li>If <code>.kubernetes.version</code> is not specified in a worker pool, then the Kubernetes version of the kubelet is inherited from the control plane (<code>.spec.kubernetes.version</code>), i.e., in the above example, the <code>data2</code> pool will use <code>1.24.6</code>.</li><li>If <code>.kubernetes.version</code> is specified in a worker pool, then it must meet the following constraints:<ul><li>It must be at most two minor versions lower than the control plane version.</li><li>If it was not specified before, then no downgrade is possible (you cannot set it to <code>1.23.13</code> while <code>.spec.kubernetes.version</code> is already <code>1.24.6</code>). The &ldquo;two minor version skew&rdquo; is only possible if the worker pool version is set to the control plane version and then the control plane was updated gradually by two minor versions.</li><li>If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version <code>1.22.0</code> to <code>1.24.0</code> in one go).</li></ul></li></ul><p>Automatic updates of Kubernetes versions (see <a href=/docs/gardener/usage/shoot_maintenance/#automatic-version-updates>Shoot Maintenance</a>) also apply to worker pool Kubernetes versions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e6e9681e52c6113b4fd564708ed02c67>2 - Gardener Extensions</h1><div class=lead>The infrastructure, networking, OS and other extension components for Gardener</div></div><div class=td-content><h1 id=pg-55e13a93740d6c3c02354485eaf37722>2.1 - Infrastructure Extensions</h1><div class=lead>Gardener extension controllers for the different infrastructures</div></div><div class=td-content><h1 id=pg-936f45ed7bca2e441d2b1f9f2ad32c57>2.1.1 - Provider Alicloud</h1><div class=lead>Gardener extension controller for the Alibaba cloud provider</div><h1 id=gardener-extension-for-alicloud-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Alicloud provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-alicloud><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-alicloud alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Alicloud provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-alicloud/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2629137f136937f8db657fecf465bb0>2.1.1.1 - Create a Kubernetes Cluster on Alibaba Cloud with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Alibaba Cloud.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://www.alibabacloud.com>Alibaba Cloud account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a project.</p><img src=/__resources/new-gardener-project_a2095b.png><blockquote><p>To be able to add shoot clusters to this project, you must first create a technical user on Alibaba Cloud with sufficient permissions.</p></blockquote></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_99aabe.png> and select <em>AliCloud</em>.</p><img src=/__resources/alicloud-create-secret_fe594e.png></li><li><p>To copy the policy for Alibaba Cloud from the Gardener dashboard, click on the help icon <img src=/__resources/help-icon_57882f.png> for Alibaba Cloud secrets, and choose copy <img src=/__resources/copy-icon_276abc.png>.</p><img src=/__resources/alicloud-copy-policy_270ed2.png></li><li><p>Create a custom policy in Alibaba Cloud:</p><ol><li><p>Log on to your Alibaba account and choose <em>RAM</em> > <em>Permissions</em> > <em>Policies</em>.</p><img src=/__resources/alicloud-create-policy_c01478.png></li><li><p>Enter the name of your policy.</p></li><li><p>Select <code>Script</code>.</p></li><li><p>Paste the policy that you copied from the Gardener dashboard to this custom policy.</p></li><li><p>Choose <em>OK</em>.</p><img src=/__resources/alicloud-paste-policy_2c06ea.png></li></ol></li><li><p>In the Alibaba Cloud console, create a new technical user:</p><ol><li><p>Choose <em>RAM</em> > <em>Users</em>.</p></li><li><p>Choose <em>Create User</em>.</p><img src=/__resources/alicloud-create-user_1bcd01.png></li><li><p>Enter a logon and display name for your user.</p></li><li><p>Select <em>Open API Access</em>.</p></li><li><p>Choose <em>OK</em>.</p><img src=/__resources/alicloud-input-user_a2701c.png></li></ol><blockquote><p>After the user is created, <code>AccessKeyId</code> and <code>AccessKeySecret</code> are generated and displayed. Remember to save them. The <code>AccessKey</code> is used later to create secrets for Gardener.</p></blockquote><img src=/__resources/alicloud-user-created_e5639c.png></li><li><p>Assign the policy you created to the technical user:</p><ol><li><p>Choose <em>RAM</em> > <em>Permissions</em> > <em>Grants</em>.</p></li><li><p>Choose <em>Grant Permission</em>.</p><img src=/__resources/alicloud-grant-permission_e7c523.png></li><li><p>Select <em>Alibaba Cloud Account</em>.</p></li><li><p>Assign the policy you’ve created before to the technical user.</p><img src=/__resources/alicloud-assign-policy_8f5061.png></li></ol></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on Alibaba Cloud.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/alicloud-create-secret-1_a0435c.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/alicloud-create-secret-2_5a025a.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/alicloud-new-cluster_5acfe8.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li><p>Select <em>AliCloud</em> in the <em>Infrastructure</em> tab.</p></li><li><p>Type the name of your cluster in the <em>Cluster Details</em> tab.</p></li><li><p>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</p></li><li><p>Choose <em>Create</em>.</p><img src=/__resources/alicloud-create-cluster_1acc19.png></li></ol></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/alicloud-processing-cluster_e882c1.png></li></ol><h2 id=result>Result</h2><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster. With it you can create shoot clusters on Alibaba Cloud.
<img src=/__resources/alicloud-kubeconfig_37fa38.png></p><blockquote><p>The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba Cloud fails.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-6725fdfc61db3ef032d903c3c4029514>2.1.1.2 - Deployment</h1><h1 id=deployment-of-the-alicloud-provider-extension>Deployment of the AliCloud provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AliCloud provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AliCloud provider extension <a href=https://github.com/gardener/gardener-extension-provider-alicloud>repository</a>.</p><h2 id=gardener-extension-admission-alicloud>gardener-extension-admission-alicloud</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-alicloud</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-07e59d41316d9bf3e092041d57e8dcfc>2.1.1.3 - Local Setup</h1><h3 id=admission-alicloud>admission-alicloud</h3><p><code>admission-alicloud</code> is an admission webhook server which is responsible for the validation of the cloud provider (Alicloud in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-alicloud.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-alicloud.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-alicloud</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bb31b8fd625e8e321607849b37068ed6>2.1.1.4 - Usage As End User</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-end-user>Using the Alicloud provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Alicloud and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Alicloud cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=alicloud-provider-credentials>Alicloud Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Alicloud infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Alicloud project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Alicloud project.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  accessKeySecret: base64(access-key-secret)
</span></span></code></pre></div><p>The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the Alicloud project are an <a href=https://www.alibabacloud.com/help/doc-detail/29009.htm>AccessKey Pair</a> associated with a <a href=https://www.alibabacloud.com/help/doc-detail/28627.htm>Resource Access Management (RAM) User</a>.
A RAM user is a special account that can be used by services and applications to interact with Alicloud Cloud Platform APIs.
Applications can use AccessKey pair to authorize themselves to a set of APIs and perform actions within the permissions granted to the RAM user.</p><p>Make sure to <a href=https://www.alibabacloud.com/help/doc-detail/93720.htm>create a Resource Access Management User</a>, and <a href=https://partners-intl.aliyun.com/help/doc-detail/116401.htm>create an AccessKey Pair</a> that shall be used for the Shoot cluster.</p><h3 id=permissions>Permissions</h3><p>Please make sure the provided credentials have the correct privileges. You can use the following Alicloud RAM policy document and attach it to the RAM user backed by the credentials you provided.</p><details><summary>Click to expand the Alicloud RAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;Statement&#34;: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;vpc:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ecs:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;slb:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:GetRole&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:CreateRole&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:CreateServiceLinkedRole&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ros:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;Version&#34;: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># id: my-vpc</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># gardenerManagedNATGateway: true</span>
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: eu-central-1a
</span></span><span style=display:flex><span>    workers: 10.250.1.0/24
</span></span><span style=display:flex><span>  <span style=color:green># natGateway:</span>
</span></span><span style=display:flex><span>    <span style=color:green># eipAllocationID: eip-ufxsdg122elmszcg</span>
</span></span></code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li>When <code>networks.vpc.id</code> is present, in addition, you can also choose to set <code>networks.vpc.gardenerManagedNATGateway</code>. It is by default <code>false</code>. When it is set to <code>true</code>,
Gardener will create an Enhanced NATGateway in the VPC and associate it with a VSwitch created in the first zone in the <code>networks.zones</code>.</li><li>Please note that when <code>networks.vpc.id</code> is present, and <code>networks.vpc.gardenerManagedNATGateway</code> is <code>false</code> or not set, you have to <strong>manually</strong> create an Enhance NATGateway
and associate it with a VSwitch that you <strong>manually</strong> created. In this case, make sure the worker CIDRs in <code>networks.zones</code> do not overlap with the one you created.
If a NATGateway is created manually and a shoot is created in the same VPC with <code>networks.vpc.gardenerManagedNATGateway</code> set <code>true</code>, you need to manually adjust the route rule accordingly.
You may refer to <a href=https://www.alibabacloud.com/help/en/doc-detail/121139.html>here</a>.</li></ul><p>The <code>networks.zones</code> section describes which subnets you want to create in availability zones.
For every zone, the Alicloud extension creates one subnet:</p><ul><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the Alicloud extension will also create a NAT gateway (only if a new VPC is created), a key pair, elastic IPs, VSwitches, a SNAT table entry, and security groups.</p><p>By default, the Alicloud extension will create a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>networks.zones[].natGateway.eipAllocationID</code> field allows you to specify the Elastic IP Allocation ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while Alicloud applies this change, because the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the Alicloud extension.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Alicloud-specific control plane components.
Today, the Alicloud extension deploys the <code>cloud-controller-manager</code> and the CSI controllers.</p><p>An example <code>ControlPlaneConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>csi:
</span></span><span style=display:flex><span>  enableADController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>csi.enableADController</code> is used as the value of environment <a href=https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/cd0788a0a440926d504d8f8fb7f6e738fe96f3ae/pkg/disk/nodeserver.go#L80>DISK_AD_CONTROLLER</a>, which is used for AliCloud csi-disk-plugin. This field is optional. When a new shoot is creatd, this field is automatically set true. For an existing shoot created in previous versions, it remains unchanged. If there are persistent volumes created before year 2021, please be cautious to set this field <em>true</em> because they may fail to mount to nodes.</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Alicloud extension does not support a specific <code>WorkerConfig</code>. However, it supports additional data volumes (plus encryption) per machine.
By default (if not stated otherwise), all the disks are unencrypted.
For each data volume, you have to specify a name.
It also supports encrypted system disk.
However, only <a href="https://www.alibabacloud.com/help/doc-detail/172789.htm?spm=a2c63.l28256.b99.244.5da67453bNBrCt">Customized image</a> is currently supported to be used as a basic image for encrypted system disk.
Please be noted that the change of system disk encryption flag will cause reconciliation of a shoot, and it will result in nodes rolling update within the worker group.</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        size: 25Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: alicloud
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-alicloud
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: ecs.sn2ne.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-two-availability-zones>Example <code>Shoot</code> manifest (two availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for two availability zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: alicloud
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-alicloud
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/26
</span></span><span style=display:flex><span>        - name: eu-central-1b
</span></span><span style=display:flex><span>          workers: 10.250.0.64/26
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: ecs.sn2ne.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 4
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        <span style=color:green># NOTE: Below comment is for the case when encrypted field of an existing shoot is updated from false to true. </span>
</span></span><span style=display:flex><span>        <span style=color:green># It will cause affected nodes to be rolling updated. Users must trigger a MAINTAIN operation of the shoot. </span>
</span></span><span style=display:flex><span>        <span style=color:green># Otherwise, the shoot will fail to reconcile.</span>
</span></span><span style=display:flex><span>        <span style=color:green># You could do it either via Dashboard or annotating the shoot with gardener.cloud/operation=maintain</span>
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>      - eu-central-1b
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-alicloud@v1.33</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-alicloud@v1.36</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-alicloud@v1.37</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb56b2718d72c1dc655334ee1654389a>2.1.1.5 - Usage As Operator</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-operator>Using the Alicloud provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension. In addition, this document also describes how to enable the use of customized machine images for Alicloud.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code> looks like for Alicloud by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Alicloud shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Alicloud environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Alicloud extension knows the AMI for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2023.4.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: eu-central-1
</span></span><span style=display:flex><span>      id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: alicloud
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2023.4.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: ecs.sn2ne.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: cloud_efficiency
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: cloud_ssd
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: eu-central-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: eu-central-1a
</span></span><span style=display:flex><span>    - name: eu-central-1b
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2023.4.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-central-1
</span></span><span style=display:flex><span>          id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</span></span></code></pre></div><h2 id=enable-customized-machine-images-for-the-alicloud-extension>Enable customized machine images for the Alicloud extension</h2><p>Customized machine images can be created for an Alicloud account and shared with other Alicloud accounts.
The same customized machine image has different image ID in different regions on Alicloud.
If you need to enable <code>encrypted system disk</code>, you must provide customized machine images.
Administrators/Operators need to explicitly declare them per imageID per region as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: customized_coreos
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - imageID: &lt;image_id_in_eu_central_1&gt;
</span></span><span style=display:flex><span>    region: eu-central-1
</span></span><span style=display:flex><span>  - imageID: &lt;image_id_in_cn_shanghai&gt;
</span></span><span style=display:flex><span>    region: cn-shanghai
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  version: 2191.4.1
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>End-users have to have the permission to use the customized image from its creator Alicloud account. To enable end-users to use customized images, the images are shared from Alicloud account of Seed operator with end-users&rsquo; Alicloud accounts. Administrators/Operators need to explicitly provide Seed operator&rsquo;s Alicloud account access credentials (base64 encoded) as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>machineImageOwnerSecret:
</span></span><span style=display:flex><span>  name: machine-image-owner
</span></span><span style=display:flex><span>  accessKeyID: &lt;base64_encoded_access_key_id&gt;
</span></span><span style=display:flex><span>  accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
</span></span></code></pre></div><p>As a result, a Secret named <code>machine-image-owner</code> by default will be created in namespace of Alicloud provider extension.</p><p>Operators should also maintain custom image IDs which are to be shared with end-users as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>toBeSharedImageIDs:
</span></span><span style=display:flex><span>- &lt;image_id_1&gt;
</span></span><span style=display:flex><span>- &lt;image_id_2&gt;
</span></span><span style=display:flex><span>- &lt;image_id_3&gt;
</span></span></code></pre></div><h3 id=example-controllerdeployment-manifest-for-enabling-customized-machine-images>Example <code>ControllerDeployment</code> manifest for enabling customized machine images</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-provider-alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: helm
</span></span><span style=display:flex><span>   providerConfig:
</span></span><span style=display:flex><span>    chart: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      H4sIFAAAAAAA/yk...
</span></span><span style=display:flex><span>    values:
</span></span><span style=display:flex><span>      config:
</span></span><span style=display:flex><span>        machineImageOwnerSecret:
</span></span><span style=display:flex><span>          accessKeyID: &lt;base64_encoded_access_key_id&gt;
</span></span><span style=display:flex><span>          accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
</span></span><span style=display:flex><span>        toBeSharedImageIDs:
</span></span><span style=display:flex><span>        - &lt;image_id_1&gt;
</span></span><span style=display:flex><span>        - &lt;image_id_2&gt;
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>        machineImages:
</span></span><span style=display:flex><span>        - name: customized_coreos
</span></span><span style=display:flex><span>          regions:
</span></span><span style=display:flex><span>          - imageID: &lt;image_id_in_eu_central_1&gt;
</span></span><span style=display:flex><span>            region: eu-central-1
</span></span><span style=display:flex><span>          - imageID: &lt;image_id_in_cn_shanghai&gt;
</span></span><span style=display:flex><span>            region: cn-shanghai
</span></span><span style=display:flex><span>          ...
</span></span><span style=display:flex><span>          version: 2191.4.1
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>        csi:
</span></span><span style=display:flex><span>          enableADController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      resources:
</span></span><span style=display:flex><span>        limits:
</span></span><span style=display:flex><span>          cpu: 500m
</span></span><span style=display:flex><span>          memory: 1Gi
</span></span><span style=display:flex><span>        requests:
</span></span><span style=display:flex><span>          memory: 128Mi
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>alicloud</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Alicloud <a href=https://www.alibabacloud.com/help/doc-detail/31817.htm>Object Storage Service</a>.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>).</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Alicloud Object Storage Service.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    region: cn-shanghai
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: alicloud
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>An example of the referenced secret containing the credentials for the Alicloud Object Storage Service can be found in the <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-alicloud-object-storage-service>Permissions for Alicloud Object Storage Service</h4><p>Please make sure the RAM user associated with the provided AccessKey pair has the following permission.</p><ul><li>AliyunOSSFullAccess</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2fdc592b220f6647eac2d46161fce5c7>2.1.2 - Provider AWS</h1><div class=lead>Gardener extension controller for the AWS cloud provider</div><h1 id=gardener-extension-for-aws-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for AWS provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-aws><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-aws alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the AWS provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20AWS/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20AWS/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20AWS/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20AWS/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20AWS/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20AWS/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>AWS Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt;= v1.15.0</code></td><td><code>>v1.10.0</code></td><td>Please update the provider version to <code>> v1.15.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. Shoots with CSI enabled (Kubernetes version >= 1.18) miss a mount to the directory <code>/etc/ssl</code> in the Shoot API Server. This can lead to not trusting external Root CAs when the API Server makes requests via webhooks or OIDC.</td></tr></tbody></table><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-aws/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ca61efb158d0f27eb40903f178bea1db>2.1.2.1 - Create a Kubernetes Cluster on AWS with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://aws.amazon.com/>AWS account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_ad03bc.png></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_3b1f20.png> and select <em>AWS</em>.</p><img src=/__resources/create-secret-aws_79dc1a.png></li><li><p>To copy the policy for AWS from the Gardener dashboard, click on the help icon <img src=/__resources/help-icon_01486c.png> for AWS secrets, and choose copy <img src=/__resources/copy-icon_0f5ab8.png>.</p><img src=/__resources/gardener-copy-policy_a52965.png></li><li><p><a href=https://console.aws.amazon.com/iam/home?#/policies>Create a new policy</a> in AWS:</p><ol><li><p>Choose <em>Create policy</em>.</p><img src=/__resources/amazon-create-policy_5ef114.png></li><li><p>Paste the policy that you copied from the Gardener dashboard to this custom policy.</p><img src=/__resources/amazon-create-policy-json_7d6327.png></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li><li><p>Fill in the name and description, then choose <em>Create policy</em>.</p><img src=/__resources/amazon-review-policy_6fba71.png></li></ol></li><li><p><a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user</a> in AWS:</p><ol><li><p>Type in a username and select the access key credential type.</p><img src=/__resources/add-user_775731.png></li><li><p>Choose <em>Attach an existing policy</em>.</p></li><li><p>Select <em>GardenerAccess</em> from the policy list.</p></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li></ol><img src=/__resources/attach-policy_a6a81f.png>
<img src=/__resources/finish-user_a9e956.png><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Note: After the user is created, <code>Access key ID</code> and <code>Secret access key</code> are generated and displayed. Remember to save them. The <code>Access key ID</code> is used later to create secrets for Gardener.</div><img src=/__resources/save-keys_f23816.png></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus-icon_3b1f20.png>. Select <em>AWS</em> from the drop down menu to add a new AWS secret.</p></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on AWS.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-aws-secret_ed47ad.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_a4c7f9.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_353d7b.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>AWS</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_7a45a2.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_522005.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_752d59.png></div><div class=td-content style=page-break-before:always><h1 id=pg-06631c97e1303d2dca2d0507f0a44b1b>2.1.2.2 - Deployment</h1><h1 id=deployment-of-the-aws-provider-extension>Deployment of the AWS provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AWS provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AWS provider extension <a href=https://github.com/gardener/gardener-extension-provider-aws>repository</a>.</p><h2 id=gardener-extension-admission-aws>gardener-extension-admission-aws</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-aws</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2a8b19ff0eb4b808c7bdd23b8033043b>2.1.2.3 - Local Setup</h1><h3 id=admission-aws>admission-aws</h3><p><code>admission-aws</code> is an admission webhook server which is responsible for the validation of the cloud provider (AWS in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-aws.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-aws.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-aws</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0bff1d7b2879983d062a0220314509f4>2.1.2.4 - Usage As End User</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-end-user>Using the AWS provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for AWS and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an AWS cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your AWS account.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span></code></pre></div><p>The <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>AWS documentation</a> explains the necessary steps to enable programmatic access, i.e. create <strong>access key ID</strong> and <strong>access key</strong>, for the user of your choice.</p><p>⚠️ For security reasons, we recommend creating a <strong>dedicated user with programmatic access only</strong>. Please avoid re-using a IAM user which has access to the AWS console (human user).</p><p>⚠️ Depending on your AWS API usage it can be problematic to reuse the same AWS Account for different Shoot clusters in the same region due to rate limits. Please consider spreading your Shoots over multiple AWS Accounts if you are hitting those limits.</p><h3 id=permissions>Permissions</h3><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;autoscaling:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;ec2:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;elasticloadbalancing:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListPolicyVersions&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListAttachedRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListInstanceProfilesForRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateServiceLinkedRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AddRoleToInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AttachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DetachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:RemoveRoleFromInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PutRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PassRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:UpdateAssumeRolePolicy&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>enableECRAccess: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># id: vpc-123456</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># gatewayEndpoints:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - s3</span>
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: eu-west-1a
</span></span><span style=display:flex><span>    internal: 10.250.112.0/22
</span></span><span style=display:flex><span>    public: 10.250.96.0/22
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  <span style=color:green># elasticIPAllocationID: eipalloc-123456</span>
</span></span><span style=display:flex><span>ignoreTags:
</span></span><span style=display:flex><span>  keys: <span style=color:green># individual ignored tag keys</span>
</span></span><span style=display:flex><span>  - SomeCustomKey
</span></span><span style=display:flex><span>  - AnotherCustomKey
</span></span><span style=display:flex><span>  keyPrefixes: <span style=color:green># ignored tag key prefixes</span>
</span></span><span style=display:flex><span>  - user.specific/prefix/
</span></span></code></pre></div><p>The <code>enableECRAccess</code> flag specifies whether the AWS IAM role policy attached to all worker nodes of the cluster shall contain permissions to access the Elastic Container Registry of the respective AWS account.
If the flag is not provided it is defaulted to <code>true</code>.
Please note that if the <code>iamInstanceProfile</code> is set for a worker pool in the <code>WorkerConfig</code> (see below) then <code>enableECRAccess</code> does not have any effect.
It only applies for those worker pools whose <code>iamInstanceProfile</code> is not set.</p><details><summary>Click to expand the default AWS IAM policy document used for the instance profiles!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ec2:DescribeInstances&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green>// Only if `.enableECRAccess` is `true`.
</span></span></span><span style=display:flex><span><span style=color:green></span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetAuthorizationToken&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchCheckLayerAvailability&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetDownloadUrlForLayer&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetRepositoryPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:DescribeRepositories&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:ListImages&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchGetImage&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).
Please make sure that the VPC has attached an internet gateway - the AWS controller won&rsquo;t create one automatically for existing VPCs. To make sure the nodes are able to join and operate in your cluster properly, please make sure that your VPC has enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS Support</a>, explicitly the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> must be set to <code>true</code>.</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li><code>networks.vpc.gatewayEndpoints</code> is optional. If specified then each item is used as service name in a corresponding Gateway VPC Endpoint.</li></ul><p>The <code>networks.zones</code> section contains configuration for resources you want to create or use in availability zones.
For every zone, the AWS extension creates three subnets:</p><ul><li>The <code>internal</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internal-load-balancers.html>internal AWS load balancers</a>.</li><li>The <code>public</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html>public AWS load balancers</a>.</li><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Also, the AWS extension creates a dedicated NAT gateway for each zone.
By default, it also creates a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>elasticIPAllocationID</code> field allows you to specify the ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while AWS applies this change.
The reason is that the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the AWS extension.</p><p>You can configure <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html>Gateway VPC Endpoints</a> by adding items in the optional list <code>networks.vpc.gatewayEndpoints</code>. Each item in the list is used as a service name and a corresponding endpoint is created for it. All created endpoints point to the service within the cluster&rsquo;s region. For example, consider this (partial) shoot config:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          gatewayEndpoints:
</span></span><span style=display:flex><span>          - s3
</span></span></code></pre></div><p>The service name of the S3 Gateway VPC Endpoint in this example is <code>com.amazonaws.eu-central-1.s3</code>.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the AWS extension will also create DHCP options and an internet gateway (only if a new VPC is created), routing tables, security groups, elastic IPs, NAT gateways, EC2 key pairs, IAM roles, and IAM instance profiles.</p><p>The <code>ignoreTags</code> section allows to configure which resource tags on AWS resources managed by Gardener should be ignored during
infrastructure reconciliation. By default, all tags that are added outside of Gardener&rsquo;s
reconciliation will be removed during the next reconciliation. This field allows users and automation to add
custom tags on AWS resources created and managed by Gardener without loosing them on the next reconciliation.
Tags can ignored either by specifying exact key values (<code>ignoreTags.keys</code>) or key prefixes (<code>ignoreTags.keyPrefixes</code>).
In both cases it is forbidden to ignore the <code>Name</code> tag or any tag starting with <code>kubernetes.io</code> or <code>gardener.cloud</code>.<br>Please note though, that the tags are only ignored on resources created on behalf of the <code>Infrastructure</code> CR (i.e. VPC,
subnets, security groups, keypair, etc.), while tags on machines, volumes, etc. are not in the scope of this controller.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the AWS-specific control plane components.
Today, the only component deployed by the AWS extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  useCustomRouteController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>storage:
</span></span><span style=display:flex><span>  managedDefaultClass: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p>The <code>cloudControllerManager.useCustomRouteController</code> controls if the <a href=https://github.com/gardener/aws-custom-route-controller>custom routes controller</a> should be enabled.
If enabled, it will add routes to the pod CIDRs for all nodes in the route tables for all zones.</p><p>The <code>storage.managedDefaultClass</code> controls if the <code>default</code> storage / volume snapshot classes are marked as default by Gardener. Set it to <code>false</code> to <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/>mark another storage / volume snapshot class as default</a> without Gardener overwriting this change. If unset, this field defaults to <code>true</code>.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The AWS extension supports encryption for volumes plus support for additional data volumes per machine.
For each data volume, you have to specify a name.
By default (if not stated otherwise), all the disks (root & data volumes) are encrypted.
Please make sure that your <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html>instance-type supports encryption</a>.
If your instance-type doesn&rsquo;t support encryption, you will have to disable encryption (which is enabled by default) by setting <code>volume.encrpyted</code> to <code>false</code> (refer below shown YAML snippet).</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 25Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span></code></pre></div><blockquote><p>Note: The AWS extension does not support EBS volume (root & data volumes) encryption with <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a>. Support for <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a> is out of scope for now. Only <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk>AWS managed CMK</a> is supported.</p></blockquote><p>Additionally, it is possible to provide further AWS-specific values for configuring the worker pools.
It can be provided in <code>.spec.provider.workers[].providerConfig</code> and is evaluated by the AWS worker controller when it reconciles the shoot machines.</p><p>An example <code>WorkerConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>volume:
</span></span><span style=display:flex><span>  iops: 10000
</span></span><span style=display:flex><span>  throughput: 200 
</span></span><span style=display:flex><span>dataVolumes:
</span></span><span style=display:flex><span>- name: kubelet-dir
</span></span><span style=display:flex><span>  iops: 12345
</span></span><span style=display:flex><span>  throughput: 150
</span></span><span style=display:flex><span>  snapshotID: snap-1234
</span></span><span style=display:flex><span>iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>  name: my-profile
</span></span><span style=display:flex><span><span style=color:green># arn: my-instance-profile-arn</span>
</span></span><span style=display:flex><span>nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    cpu: 2
</span></span><span style=display:flex><span>    gpu: 0
</span></span><span style=display:flex><span>    memory: 50Gi
</span></span></code></pre></div><p>The <code>.volume.iops</code> is the number of I/O operations per second (IOPS) that the volume supports.
For <code>io1</code> and <code>gp3</code> volume type, this represents the number of IOPS that are provisioned for the volume.
For <code>gp2</code> volume type, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, IOPS range and bursting, see Amazon EBS Volume Types (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>) in the Amazon Elastic Compute Cloud User Guide.<br>Constraint: IOPS should be a positive value. Validation of IOPS (i.e. whether it is allowed and is in the specified range for a particular volume type) is done on aws side.</p><p>The <code>volume.throughput</code> is the throughput that the volume supports, in <code>MiB/s</code>. As of <code>16th Aug 2022</code>, this parameter is valid only for <code>gp3</code> volume types and will return an error from the provider side if specified for other volume types. Its current range of throughput is from <code>125MiB/s</code> to <code>1000 MiB/s</code>. To know more about throughput and its range, see the official AWS documentation <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>here</a>.</p><p>The <code>.dataVolumes</code> can optionally contain configurations for the data volumes stated in the <code>Shoot</code> specification in the <code>.spec.provider.workers[].dataVolumes</code> list.
The <code>.name</code> must match to the name of the data volume in the shoot.
It is also possible to provide a snapshot ID. It allows to <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html>restore the data volume from an existing snapshot</a>.</p><p>The <code>iamInstanceProfile</code> section allows to specify the IAM instance profile name xor ARN that should be used for this worker pool.
If not specified, a dedicated IAM instance profile created by the infrastructure controller is used (see above).</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          internal: 10.250.112.0/22
</span></span><span style=display:flex><span>          public: 10.250.96.0/22
</span></span><span style=display:flex><span>          workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>    <span style=color:green># The following provider config is valid if the volume type is `io1`.</span>
</span></span><span style=display:flex><span>    <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   kind: WorkerConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   volume:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     iops: 10000</span>
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-three-availability-zones>Example <code>Shoot</code> manifest (three availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for three availability zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/26
</span></span><span style=display:flex><span>          public: 10.250.96.0/26
</span></span><span style=display:flex><span>          internal: 10.250.112.0/26
</span></span><span style=display:flex><span>        - name: eu-central-1b
</span></span><span style=display:flex><span>          workers: 10.250.0.64/26
</span></span><span style=display:flex><span>          public: 10.250.96.64/26
</span></span><span style=display:flex><span>          internal: 10.250.112.64/26
</span></span><span style=display:flex><span>        - name: eu-central-1c
</span></span><span style=display:flex><span>          workers: 10.250.0.128/26
</span></span><span style=display:flex><span>          public: 10.250.96.128/26
</span></span><span style=display:flex><span>          internal: 10.250.112.128/26
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 3
</span></span><span style=display:flex><span>      maximum: 9
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>      - eu-central-1b
</span></span><span style=display:flex><span>      - eu-central-1c
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every AWS shoot cluster will be deployed with the AWS EBS CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>ebs.csi.aws.com</code> provisioner.</p><h3 id=node-specific-volume-limits>Node-specific Volume Limits</h3><p>The Kubernetes scheduler allows configurable limit for the number of volumes that can be attached to a node. See <a href=https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits>https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits</a>.</p><p>CSI drivers usually have a different procedure for configuring this custom limit. By default, the EBS CSI driver parses the machine type name and then decides the volume limit. However, this is only a rough approximation and not good enough in most cases. Specifying the volume attach limit via command line flag (<code>--volume-attach-limit</code>) is currently the alternative until a more sophisticated solution presents itself (dynamically discovering the maximum number of attachable volume per EC2 machine type, see also <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347</a>). The AWS extension allows the <code>--volume-attach-limit</code> flag of the EBS CSI driver to be configurable via <code>aws.provider.extensions.gardener.cloud/volume-attach-limit</code> annotation on the <code>Shoot</code> resource. If the annotation is added to an existing <code>Shoot</code>, then reconciliation needs to be triggered manually (see <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>Immediate reconciliation</a>), as in general adding annotation to resource is not a change that leads to <code>.metadata.generation</code> increase in general.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-aws@v1.34</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-aws@v1.36</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1816fb764f847c52625eabfbeefd9827>2.1.2.5 - Usage As Operator</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-operator>Using the AWS provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
Similarly, the <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured.
Additionally, it allows to configure settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains what is necessary to configure for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>In this section we are describing how the configuration for <code>CloudProfile</code>s looks like for AWS and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating AWS shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the AWS environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the AWS extension knows the AMI for every version you want to offer.
For each AMI an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: eu-central-1
</span></span><span style=display:flex><span>      ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>      <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: m5.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: gp2
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: io1
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: eu-central-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: eu-central-1a
</span></span><span style=display:flex><span>    - name: eu-central-1b
</span></span><span style=display:flex><span>    - name: eu-central-1c
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-central-1
</span></span><span style=display:flex><span>          ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>          <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to manage backup infrastructure, i.e., you can specify configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups.
As you can see, the location/region where the backups will be stored can be different to the region where the seed cluster is running.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-credentials
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: aws
</span></span><span style=display:flex><span>    region: eu-central-1
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>Please look up <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a> as well.</p><h4 id=permissions-for-aws-iam-user>Permissions for AWS IAM user</h4><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;s3:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details></div><div class=td-content style=page-break-before:always><h1 id=pg-327ad4a2eb9d221f7e8f6cc268e1fee3>2.1.3 - Provider Azure</h1><div class=lead>Gardener extension controller for the Azure cloud provider</div><h1 id=gardener-extension-for-azure-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Azure provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-azure><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-azure alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Azure provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Azure/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Azure/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20Azure/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Azure/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Azure/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Azure/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-azure/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-258578c2b118f9a8b0365b3097d29ae2>2.1.3.1 - Azure Permissions</h1><h1 id=azure-permissions>Azure Permissions</h1><p>The following document describes the required Azure actions manage a Shoot cluster on Azure split by the different Azure provider/services.</p><p>Be aware some actions are just required if particilar deployment sceanrios or features e.g. bring your own vNet, use Azure-file, let the Shoot act as Seed etc. should be used.</p><h2 id=microsoftcompute><code>Microsoft.Compute</code></h2><pre tabindex=0><code># Required if a non zonal cluster based on Availability Set should be used.
Microsoft.Compute/availabilitySets/delete
Microsoft.Compute/availabilitySets/read
Microsoft.Compute/availabilitySets/write

# Required to let Kubernetes manage Azure disks.
Microsoft.Compute/disks/delete
Microsoft.Compute/disks/read
Microsoft.Compute/disks/write

# Required for to fetch meta information about disk and virtual machines sizes.
Microsoft.Compute/locations/diskOperations/read
Microsoft.Compute/locations/operations/read
Microsoft.Compute/locations/vmSizes/read

# Required if csi snapshot capabilities should be used and/or the Shoot should act as a Seed.
Microsoft.Compute/snapshots/delete
Microsoft.Compute/snapshots/read
Microsoft.Compute/snapshots/write

# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Compute/virtualMachines/delete
Microsoft.Compute/virtualMachines/read
Microsoft.Compute/virtualMachines/start/action
Microsoft.Compute/virtualMachines/write

# Required if a non zonal cluster based on VMSS Flex (VMO) should be used.
Microsoft.Compute/virtualMachineScaleSets/delete
Microsoft.Compute/virtualMachineScaleSets/read
Microsoft.Compute/virtualMachineScaleSets/write
</code></pre><h2 id=microsoftmanagedidentity><code>Microsoft.ManagedIdentity</code></h2><pre tabindex=0><code># Required if a user provided Azure managed identity should attached to the cluster nodes.
Microsoft.ManagedIdentity/userAssignedIdentities/assign/action
Microsoft.ManagedIdentity/userAssignedIdentities/read
</code></pre><h2 id=microsoftmarketplaceordering><code>Microsoft.MarketplaceOrdering</code></h2><pre tabindex=0><code># Required if nodes/machines should be created with images hosted on the Azure Marketplace.
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/read
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/write
</code></pre><h2 id=microsoftnetwork><code>Microsoft.Network</code></h2><pre tabindex=0><code># Required to let Kubernetes manage services of type &#39;LoadBalancer&#39;.
Microsoft.Network/loadBalancers/backendAddressPools/join/action
Microsoft.Network/loadBalancers/delete
Microsoft.Network/loadBalancers/read
Microsoft.Network/loadBalancers/write

# Required in case the Shoot should use NatGateway(s).
Microsoft.Network/natGateways/delete
Microsoft.Network/natGateways/join/action
Microsoft.Network/natGateways/read
Microsoft.Network/natGateways/write

# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Network/networkInterfaces/delete
Microsoft.Network/networkInterfaces/ipconfigurations/join/action
Microsoft.Network/networkInterfaces/ipconfigurations/read
Microsoft.Network/networkInterfaces/join/action
Microsoft.Network/networkInterfaces/read
Microsoft.Network/networkInterfaces/write

# Required to let Gardener maintain the basic infrastructure of the Shoot cluster and maintaing LoadBalancer services.
Microsoft.Network/networkSecurityGroups/delete
Microsoft.Network/networkSecurityGroups/join/action
Microsoft.Network/networkSecurityGroups/read
Microsoft.Network/networkSecurityGroups/write

# Required for managing LoadBalancers and NatGateways.
Microsoft.Network/publicIPAddresses/delete
Microsoft.Network/publicIPAddresses/join/action
Microsoft.Network/publicIPAddresses/read
Microsoft.Network/publicIPAddresses/write

# Required for managing the basic infrastructure of a cluster and maintaing LoadBalancer services.
Microsoft.Network/routeTables/delete
Microsoft.Network/routeTables/join/action
Microsoft.Network/routeTables/read
Microsoft.Network/routeTables/routes/delete
Microsoft.Network/routeTables/routes/read
Microsoft.Network/routeTables/routes/write
Microsoft.Network/routeTables/write

# Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
# Only a subset is required for the bring your own vNet scenario.
Microsoft.Network/virtualNetworks/delete # not required for bring your own vnet
Microsoft.Network/virtualNetworks/read
Microsoft.Network/virtualNetworks/subnets/delete
Microsoft.Network/virtualNetworks/subnets/join/action
Microsoft.Network/virtualNetworks/subnets/read
Microsoft.Network/virtualNetworks/subnets/write
Microsoft.Network/virtualNetworks/write # not required for bring your own vnet
</code></pre><h2 id=microsoftresources><code>Microsoft.Resources</code></h2><pre tabindex=0><code># Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
Microsoft.Resources/subscriptions/resourceGroups/delete
Microsoft.Resources/subscriptions/resourceGroups/read
Microsoft.Resources/subscriptions/resourceGroups/write
</code></pre><h2 id=microsoftstorage><code>Microsoft.Storage</code></h2><pre tabindex=0><code># Required if Azure File should be used and/or if the Shoot should act as Seed.
Microsoft.Storage/operations/read
Microsoft.Storage/storageAccounts/blobServices/containers/delete
Microsoft.Storage/storageAccounts/blobServices/containers/read
Microsoft.Storage/storageAccounts/blobServices/containers/write
Microsoft.Storage/storageAccounts/blobServices/read
Microsoft.Storage/storageAccounts/delete
Microsoft.Storage/storageAccounts/listkeys/action
Microsoft.Storage/storageAccounts/read
Microsoft.Storage/storageAccounts/write
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-e2063a2a48c39e655f1debf3ae7b7667>2.1.3.2 - Create a Kubernetes Cluster on Azure with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Azure.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://azure.microsoft.com/en-us/>Azure account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li><li>You have an Azure Service Principal assigned to your subscription.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_524827.png></li><li><p>Get the properties of your Azure AD tenant, Subscription and Service Principal.</p><p>Before you can provision and access a Kubernetes cluster on Azure, you need to add the Azure service principal, AD tenant and subscription credentials in Gardener.
Gardener needs the credentials to provision and operate the Azure infrastructure for your Kubernetes cluster.</p><p><strong>Ensure that the Azure service principal has the actions defined within the <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/azure-permissions/>Azure Permissions</a> within your Subscription assigned.
If no fine-grained permission/actions are required, then simply the built-in <code>Contributor</code> role can be assigned.</strong></p><ul><li><p>Tenant ID</p><p>To find your <code>TenantID</code>, follow this <a href=https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-how-to-find-tenant>guide</a>.</p></li><li><p>SubscriptionID</p><p>To find your <code>SubscriptionID</code>, search for and select <em>Subscriptions</em>.
<img src=/__resources/azure-select-subscription_e138b6.png></p><p>After that, copy the <code>SubscriptionID</code> from your subscription of choice.
<img src=/__resources/azure-choose-subscription_d79d28.png></p></li><li><p>Service Principal (SPN)</p><p>A service principal consist of a <code>ClientID</code> (also called <code>ApplicationID</code>) and a Client Secret. For more information, see <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals>Application and service principal objects in Azure Active Directory</a>. You need to obtain the:</p><ul><li><p>Client ID</p><p>Access the <a href=https://portal.azure.com>Azure Portal</a> and navigate to the <em>Active Directory</em> service.
Within the service navigate to <em>App registrations</em> and select your service principal. Copy the <code>ClientID</code> you see there.</p></li><li><p>Client Secret</p><p>Secrets for the Azure Account/Service Principal can be generated/rotated via the Azure Portal.
After copying your <code>ClientID</code>, in the <em>Detail</em> view of your Service Principal navigate to <em>Certificates & secrets</em>. In the section, you can generate a new secret.</p></li></ul></li></ul></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_df44c3.png> and select <em>Azure</em>.</p><img src=/__resources/create-secret-azure_cd3c81.png></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>TenantID</code>, <code>SubscriptionID</code> and the Service Principal credentials (<code>ClientID</code> and <code>ClientSecret</code>).</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-azure-secret_d9b7cf.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_6863bb.png></li><li><p>Register resource providers for your subscription.</p><ol><li>Go to your Azure dashboard</li><li>Navigate to <em>Subscriptions</em> -> &lt;your_subscription></li><li>Pick resource providers from the sidebar</li><li>Register microsoft.Network</li><li>Register microsoft.Compute</li></ol></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_88ec0e.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>Azure</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_55c4a1.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_19a7da.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_9889da.png></div><div class=td-content style=page-break-before:always><h1 id=pg-badcb60646a7f42794a59288364229e8>2.1.3.3 - Deployment</h1><h1 id=deployment-of-the-azure-provider-extension>Deployment of the Azure provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the Azure provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the Azure provider extension <a href=https://github.com/gardener/gardener-extension-provider-azure>repository</a>.</p><h2 id=gardener-extension-admission-azure>gardener-extension-admission-azure</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-azure</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-244fdac6e5820831afb7c9945e0b580c>2.1.3.4 - Local Setup</h1><h3 id=admission-azure>admission-azure</h3><p><code>admission-azure</code> is an admission webhook server which is responsible for the validation of the cloud provider (Azure in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-azure.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-azure.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-azure</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-46c4e745ca368d82e869f0ac23fd4e7d>2.1.3.5 - Migrate Loadbalancer</h1><h1 id=migrate-azure-shoot-load-balancer-from-basic-to-standard-sku>Migrate Azure Shoot Load Balancer from basic to standard SKU</h1><p>This guide descibes how to migrate the Load Balancer of an Azure Shoot cluster from the basic SKU to the standard SKU.<br><strong>Be aware:</strong> You need to delete and recreate all services of type Load Balancer, which means that the public ip addresses of your service endpoints will change.<br>Please do this only if the Stakeholder really needs to migrate this Shoot to use standard Load Balancers. All new Shoot clusters will automatically use Azure Standard Load Balancers.</p><ol><li>Disable temporarily Gardeners reconciliation.<br>The Gardener Controller Manager need to be configured to allow ignoring Shoot clusters.
This can be configured in its the <code>ControllerManagerConfiguration</code> via the field <code>.controllers.shoot.respectSyncPeriodOverwrite="true"</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Garden cluster.</span>
</span></span><span style=display:flex><span>kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore=<span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; scale deployment gardener-resource-manager --replicas=0
</span></span></code></pre></div><ol start=2><li>Backup all Kubernetes services of type Load Balancer.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Shoot cluster.</span>
</span></span><span style=display:flex><span><span style=color:green># Determine all Load Balancer services.</span>
</span></span><span style=display:flex><span>kubectl get service --all-namespaces | grep LoadBalancer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Backup each Load Balancer service.</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;---&#34;</span> &gt;&gt; service-backup.yaml &amp;&amp; kubectl -n &lt;namespace&gt; get service &lt;service-name&gt; -o yaml &gt;&gt; service-backup.yaml
</span></span></code></pre></div><ol start=3><li>Delete all Load Balancer services.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Shoot cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;namespace&gt; delete service &lt;service-name&gt;
</span></span></code></pre></div><ol start=4><li>Wait until until Load Balancer is deleted.
Wait until all services of type Load Balancer are deleted and the Azure Load Balancer resource is also deleted.
Check via the Azure Portal if the Load Balancer within the Shoot Resource Group has been deleted.
This should happen automatically after all Kubernetes Load Balancer service are gone within a few minutes.</li></ol><p>Alternatively the Azure cli can be used to check the Load Balancer in the Shoot Resource Group.
The credentials to configure the cli are available on the Seed cluster in the Shoot namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span><span style=color:green># Fetch the credentials from cloudprovider secret.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; get secret cloudprovider -o yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Configure the Azure cli, with the base64 decoded values of the cloudprovider secret.</span>
</span></span><span style=display:flex><span>az login --service-principal --username &lt;clientID&gt; --password &lt;clientSecret&gt; --tenant &lt;tenantID&gt;
</span></span><span style=display:flex><span>az account set -s &lt;subscriptionID&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Fetch the constantly the Shoot Load Balancer in the Shoot Resource Group. Wait until the resource is gone.</span>
</span></span><span style=display:flex><span>watch <span style=color:#a31515>&#39;az network lb show -g shoot--&lt;project-name&gt;--&lt;shoot-name&gt; -n shoot--&lt;project-name&gt;--&lt;shoot-name&gt;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Logout.</span>
</span></span><span style=display:flex><span>az logout
</span></span></code></pre></div><ol start=5><li>Modify the <code>cloud-povider-config</code> configmap in the Seed namespace of the Shoot.<br>The key <code>cloudprovider.conf</code> contains the Kubernetes cloud-provider configuration.
The value is a multiline string. Please change the value of the field <code>loadBalancerSku</code> from <code>basic</code> to <code>standard</code>.
Iff the field does not exists then append <code>loadBalancerSku: \"standard\"\n</code> to the value/string.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; edit cm cloud-provider-config
</span></span></code></pre></div><ol start=6><li>Enable Gardeners reconcilation and trigger a reconciliation.</li></ol><pre tabindex=0><code># In the Garden cluster
# Enable reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore-

# Trigger reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/operation=&#34;reconcile&#34;
</code></pre><p>Wait until the cluster has been reconciled.</p><ol start=6><li>Recreate the services from the backup file.<br>Probably you need to remove some fields from the service defintions e.g. <code>.spec.clusterIP</code>, <code>.metadata.uid</code> or <code>.status</code> etc.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f service-backup.yaml
</span></span></code></pre></div><ol start=7><li>If successful remove backup file.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Delete the backup file.</span>
</span></span><span style=display:flex><span>rm -f service-backup.yaml
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aaec8948c232e5161bb808b6485480de>2.1.3.6 - Usage As End User</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-end-user>Using the Azure provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Azure and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Azure cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=azure-provider-credentials>Azure Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Azure infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Azure subscription.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Azure subscription.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>Create an <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>Azure Application and Service Principle</a> and obtain its credentials.</p><p>Please ensure that the Azure application (spn) has the IAM actions defined <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/azure-permissions/>here</a> assigned.
If no fine-grained permissions/actions required then simply assign the <a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a> role.</p><p>The example below demonstrates how the secret containing the client credentials of the Azure Application has to look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  clientID: base64(client-id)
</span></span><span style=display:flex><span>  clientSecret: base64(client-secret)
</span></span><span style=display:flex><span>  subscriptionID: base64(subscription-id)
</span></span><span style=display:flex><span>  tenantID: base64(tenant-id)
</span></span></code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Principal for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over Service Principals from different Azure subscriptions if you are hitting those limits.</p><h3 id=managed-service-principals>Managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide managed service principals.
This eliminates the need for users to provide an own service principal for a Shoot.</p><p>To make use of a managed service principal, the Azure secret of a Shoot cluster must contain only a <code>subscriptionID</code> and a <code>tenantID</code> field, but no <code>clientID</code> and <code>clientSecret</code>.
Removing those fields from the secret of an existing Shoot will also let it adopt the managed service principal.</p><p>Based on the <code>tenantID</code> field, the Gardener extension will try to assign the managed service principal to the Shoot.
If no managed service principal can be assigned then the next operation on the Shoot will fail.</p><p>⚠️ The managed service principal need to be assigned to the users Azure subscription with proper permissions before using it.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:green># name: my-vnet</span>
</span></span><span style=display:flex><span>    <span style=color:green># resourceGroup: my-vnet-resource-group</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    <span style=color:green># ddosProtectionPlanID: /subscriptions/test/resourceGroups/test/providers/Microsoft.Network/ddosProtectionPlans/test-ddos-protection-plan</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  <span style=color:green># natGateway:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   enabled: false</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   idleConnectionTimeoutMinutes: 4</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   zone: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   ipAddresses:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   - name: my-public-ip-name</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     resourceGroup: my-public-ip-resource-group</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     zone: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green># serviceEndpoints:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - Microsoft.Test</span>
</span></span><span style=display:flex><span>  <span style=color:green># zones:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - name: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cidr: &#34;10.250.0.0/24</span>
</span></span><span style=display:flex><span>  <span style=color:green># - name: 2</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cidr: &#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   natGateway:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     enabled: false</span>
</span></span><span style=display:flex><span>zoned: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span><span style=color:green># resourceGroup:</span>
</span></span><span style=display:flex><span><span style=color:green>#   name: mygroup</span>
</span></span><span style=display:flex><span><span style=color:green>#identity:</span>
</span></span><span style=display:flex><span><span style=color:green>#  name: my-identity-name</span>
</span></span><span style=display:flex><span><span style=color:green>#  resourceGroup: my-identity-resource-group</span>
</span></span><span style=display:flex><span><span style=color:green>#  acrAccess: true</span>
</span></span></code></pre></div><p>Currently, it&rsquo;s not yet possible to deploy into existing resource groups, but in the future it will.
The <code>.resourceGroup.name</code> field will allow specifying the name of an already existing resource group that the shoot cluster and all infrastructure resources will be deployed to.</p><p>Via the <code>.zoned</code> boolean you can tell whether you want to use Azure availability zones or not.
If you don&rsquo;t use zones then an availability set will be created and only basic load balancers will be used.
Zoned clusters use standard load balancers.</p><p>The <code>networks.vnet</code> section describes whether you want to create the shoot cluster in an already existing VNet or whether to create a new one:</p><ul><li>If <code>networks.vnet.name</code> and <code>networks.vnet.resourceGroup</code> are given then you have to specify the VNet name and VNet resource group name of the existing VNet that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vnet.cidr</code> is given then you have to specify the VNet CIDR of a new VNet that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vnet.name</code> and <code>neworks.vnet.resourceGroup</code> or <code>networks.vnet.cidr</code> must be present, but not both at the same time.</li><li>The <code>networks.vnet.ddosProtectionPlanID</code> field can be used to specify the id of a ddos protection plan which should be assigned to the VNet. This will only work for a VNet managed by Gardener. For externally managed VNets the ddos protection plan must be assigned by other means.</li><li>If a vnet name is given and cilium shoot clusters are created without a network overlay within one vnet make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that vnet.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.
The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet.
You can freely choose this CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>In the <code>networks.serviceEndpoints[]</code> list you can specify the list of Azure service endpoints which shall be associated with the worker subnet. All available service endpoints and their technical names can be found in the (Azure Service Endpoint documentation](<a href=https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview>https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview</a>).</p><p>The <code>networks.natGateway</code> section contains configuration for the Azure NatGateway which can be attached to the worker subnet of a Shoot cluster. Here are some key information about the usage of the NatGateway for a Shoot cluster:</p><ul><li>NatGateway usage is optional and can be enabled or disabled via <code>.networks.natGateway.enabled</code>.</li><li>If the NatGateway is not used then the egress connections initiated within the Shoot cluster will be nated via the LoadBalancer of the clusters (default Azure behaviour, see <a href=https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios>here</a>).</li><li>NatGateway is only available for zonal clusters <code>.zoned=true</code>.</li><li>The NatGateway is currently <strong>not</strong> zone redundantly deployed. That mean the NatGateway of a Shoot cluster will always be in just one zone. This zone can be optionally selected via <code>.networks.natGateway.zone</code>.</li><li><strong>Caution:</strong> Modifying the <code>.networks.natGateway.zone</code> setting requires a recreation of the NatGateway and the managed public ip (automatically used if no own public ip is specified, see below). That mean you will most likely get a different public ip for egress connections.</li><li>It is possible to bring own zonal public ip(s) via <code>networks.natGateway.ipAddresses</code>. Those public ip(s) need to be in the same zone as the NatGateway (see <code>networks.natGateway.zone</code>) and be of SKU <code>standard</code>. For each public ip the <code>name</code>, the <code>resourceGroup</code> and the <code>zone</code> need to be specified.</li><li>The field <code>networks.natGateway.idleConnectionTimeoutMinutes</code> allows the configuration of NAT Gateway&rsquo;s idle connection timeout property. The idle timeout value can be adjusted from 4 minutes, up to 120 minutes. Omitting this property will set the idle timeout to its default value according to <a href=https://docs.microsoft.com/en-us/azure/virtual-network/nat-gateway-resource#timers>NAT Gateway&rsquo;s documentation</a>.</li></ul><p>In the <code>identity</code> section you can specify an <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview#how-does-the-managed-identities-for-azure-resources-work>Azure user-assigned managed identity</a> which should be attached to all cluster worker machines. With <code>identity.name</code> you can specify the name of the identity and with <code>identity.resourceGroup</code> you can specify the resource group which contains the identity resource on Azure. The identity need to be created by the user upfront (manually, other tooling, &mldr;). Gardener/Azure Extension will only use the referenced one and won&rsquo;t create an identity. Furthermore the identity have to be in the same subscription as the Shoot cluster. Via the <code>identity.acrAccess</code> you can configure the worker machines to use the passed identity for pulling from an <a href=https://docs.microsoft.com/en-us/azure/container-registry/container-registry-intro>Azure Container Registry (ACR)</a>.
<strong>Caution:</strong> Adding, exchanging or removing the identity will require a rolling update of all worker machines in the Shoot cluster.</p><p>Apart from the VNet and the worker subnet the Azure extension will also create a dedicated resource group, route tables, security groups, and an availability set (if not using zoned clusters).</p><h3 id=infrastructureconfig-with-dedicated-subnets-per-zone>InfrastructureConfig with dedicated subnets per zone</h3><p>Another deployment option <strong>for zonal clusters only</strong>, is to create and configure a separate subnet per availability zone. This network layout is recommended to users that require fine-grained control over their network setup. One prevalent usecase is to create a zone-redundant NAT Gateway deployment by taking advantage of the ability to deploy separate NAT Gateways for each subnet.</p><p>To use this configuration the following requirements must be met:</p><ul><li>the <code>zoned</code> field must be set to <code>true</code>.</li><li>the <code>networks.vnet</code> section must not be empty and must contain a valid configuration. For existing clusters that were not using the <code>networks.vnet</code> section, it is enough if <code>networks.vnet.cidr</code> field is set to the current <code>networks.worker</code> value.</li></ul><p>For each of the target zones a subnet CIDR range must be specified. The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet. In addition, the CIDR ranges must not overlap with the ranges of the other subnets.</p><p><em>ServiceEndpoints</em> and <em>NatGateways</em> can be configured per subnet. Respectively, when <code>networks.zones</code> is specified, the fields <code>networks.workers</code>, <code>networks.serviceEndpoints</code> and <code>networks.natGateway</code> cannot be set. All the configuration for the subnets must be done inside the respective zone&rsquo;s configuration.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: 1
</span></span><span style=display:flex><span>    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>  - name: 2
</span></span><span style=display:flex><span>    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>    natGateway:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><h3 id=migrating-to-zonal-shoots-with-dedicated-subnets-per-zone>Migrating to zonal shoots with dedicated subnets per zone</h3><p>For existing zonal clusters it is possible to migrate to a network layout with dedicated subnets per zone. The migration works by creating additional network resources as specified in the configuration and progressively roll part of your existing nodes to use the new resources. To achieve the controlled rollout of your nodes, parts of the existing infrastructure must be preserved which is why the following constraint is imposed:</p><p>One of your specified zones must have the exact same CIDR range as the current <code>network.workers</code> field. Here is an example of such migration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>      - name: 3
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
</span></span><span style=display:flex><span><span style=color:green># optionally add other zones </span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: 2  </span>
</span></span><span style=display:flex><span>    <span style=color:green>#   cidr: 10.250.32.0/19</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   natGateway:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     enabled: true</span>
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Another more advanced example with user-provided public IP addresses for the NAT Gateway and how it can be migrated:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    natGateway:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zone: 1
</span></span><span style=display:flex><span>      ipAddresses:
</span></span><span style=display:flex><span>        - name: pip1
</span></span><span style=display:flex><span>          resourceGroup: group
</span></span><span style=display:flex><span>          zone: 1
</span></span><span style=display:flex><span>        - name: pip2
</span></span><span style=display:flex><span>          resourceGroup: group
</span></span><span style=display:flex><span>          zone: 1
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>      - name: 1
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
</span></span><span style=display:flex><span>        natGateway:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>          ipAddresses:
</span></span><span style=display:flex><span>            - name: pip1
</span></span><span style=display:flex><span>              resourceGroup: group
</span></span><span style=display:flex><span>              zone: 1
</span></span><span style=display:flex><span>            - name: pip2
</span></span><span style=display:flex><span>              resourceGroup: group
</span></span><span style=display:flex><span>              zone: 1
</span></span><span style=display:flex><span><span style=color:green># optionally add other zones </span>
</span></span><span style=display:flex><span><span style=color:green>#     - name: 2  </span>
</span></span><span style=display:flex><span><span style=color:green>#       cidr: 10.250.32.0/19</span>
</span></span><span style=display:flex><span><span style=color:green>#       natGateway:</span>
</span></span><span style=display:flex><span><span style=color:green>#         enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#         ipAddresses:</span>
</span></span><span style=display:flex><span><span style=color:green>#           - name: pip3</span>
</span></span><span style=display:flex><span><span style=color:green>#             resourceGroup: group</span>
</span></span></code></pre></div><p>You can apply such change to your shoot by issuing a <code>kubectl patch</code> command to replace your current <code>.spec.provider.infrastructureConfig</code> section:</p><pre tabindex=0><code>$ cat new-infra.json

[
  {
    &#34;op&#34;: &#34;replace&#34;,
    &#34;path&#34;: &#34;/spec/provider/infrastructureConfig&#34;,
    &#34;value&#34;: {
      &#34;apiVersion&#34;: &#34;azure.provider.extensions.gardener.cloud/v1alpha1&#34;,
      &#34;kind&#34;: &#34;InfrastructureConfig&#34;,
      &#34;networks&#34;: {
        &#34;vnet&#34;: {
          &#34;cidr&#34;: &#34;&lt;your-vnet-cidr&gt;&#34;
        },
        &#34;zones&#34;: [
          {
            &#34;name&#34;: 1,
            &#34;cidr&#34;: &#34;10.250.0.0/24&#34;,
            &#34;natGateway&#34;: {
              &#34;enabled&#34;: true
            }
          },
          {
            &#34;name&#34;: 1,
            &#34;cidr&#34;: &#34;10.250.1.0/24&#34;,
            &#34;natGateway&#34;: {
              &#34;enabled&#34;: true
            }
          },
        ]
      },
      &#34;zoned&#34;: true
    }
  }
]

kubectl patch --type=&#34;json&#34; --patch-file new-infra.json shoot &lt;my-shoot&gt;
</code></pre><p>⚠️ The migration to shoots with dedicated subnets per zone is a one-way process. Reverting the shoot to the previous configuration is not supported.</p><p>⚠️ During the migration a subset of the nodes will be rolled to the new subnets.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Azure-specific control plane components.
Today, the only component deployed by the Azure extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Azure extension supports encryption for volumes plus support for additional data volumes per machine.
Please note that you cannot specify the <code>encrypted</code> flag for Azure disks as they are encrypted by default/out-of-the-box.
For each data volume, you have to specify a name.
The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>        size: 25Gi
</span></span></code></pre></div><p>Additionally, it supports for other Azure-specific values and could be configured under <code>.spec.provider.workers[].providerConfig</code></p><p>An example <code>WorkerConfig</code> for the Azure extension looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    cpu: 2
</span></span><span style=display:flex><span>    gpu: 1
</span></span><span style=display:flex><span>    memory: 50Gi
</span></span></code></pre></div><p>The <code>.nodeTemplate</code> is used to specify resource information of the machine during runtime. This then helps in Scale-from-Zero.
Some points to note for this field:
- Currently only cpu, gpu and memory are configurable.
- a change in the value lead to a rolling update of the machine in the workerpool
- all the resources needs to be specified</p><h2 id=example-shoot-manifest-non-zoned>Example <code>Shoot</code> manifest (non-zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a non-zoned cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span><span style=color:green>#      providerConfig:</span>
</span></span><span style=display:flex><span><span style=color:green>#        apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:green>#        kind: WorkerConfig</span>
</span></span><span style=display:flex><span><span style=color:green>#        nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span><span style=color:green>#          capacity:</span>
</span></span><span style=display:flex><span><span style=color:green>#            cpu: 2</span>
</span></span><span style=display:flex><span><span style=color:green>#            gpu: 1</span>
</span></span><span style=display:flex><span><span style=color:green>#            memory: 50Gi</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-zoned>Example <code>Shoot</code> manifest (zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-zoned-with-nat-gateways-per-zone>Example <code>Shoot</code> manifest (zoned with NAT Gateways per zone)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster using NAT Gateways per zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: 1
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/24
</span></span><span style=display:flex><span>          serviceEndpoints:
</span></span><span style=display:flex><span>          - Microsoft.Storage
</span></span><span style=display:flex><span>          - Microsoft.Sql
</span></span><span style=display:flex><span>          natGateway:
</span></span><span style=display:flex><span>            enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>            idleConnectionTimeoutMinutes: 4
</span></span><span style=display:flex><span>        - name: 2
</span></span><span style=display:flex><span>          cidr: 10.250.1.0/24
</span></span><span style=display:flex><span>          serviceEndpoints:
</span></span><span style=display:flex><span>          - Microsoft.Storage
</span></span><span style=display:flex><span>          - Microsoft.Sql
</span></span><span style=display:flex><span>          natGateway:
</span></span><span style=display:flex><span>            enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every Azure shoot cluster that has at least Kubernetes v1.21 will be deployed with the Azure Disk CSI driver and the Azure File CSI driver.
Both are compatible with the legacy in-tree volume provisioners that were deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>disk.csi.azure.com</code> or <code>file.csi.azure.com</code> provisioner, respectively.
Shoot clusters with Kubernetes v1.20 or less will use the in-tree <code>kubernetes.io/azure-disk</code> and <code>kubernetes.io/azure-file</code> volume provisioners in the kube-controller-manager and the kubelet.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-azure@v1.25</code>.
Note that this feature is only usable for <code>Shoot</code>s whose <code>.spec.kubernetes.version</code> is greater or equal than the CSI migration version (<code>1.21</code>).</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-azure@v1.28</code>.</p><h2 id=miscellaneous>Miscellaneous</h2><h3 id=azure-accelerated-networking>Azure Accelerated Networking</h3><p>All worker machines of the cluster will be automatically configured to use <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a> if the prerequisites are fulfilled.
The prerequisites are that the cluster must be zoned, and the used machine type and operating system image version are compatible for Accelerated Networking.
<code>Availability Set</code> based shoot clusters will not be enabled for accelerated networking even if the machine type and operating system support it, this is necessary because all machines from the availability set must be scheduled on special hardware, more daitls can be found <a href=https://github.com/MicrosoftDocs/azure-docs/issues/10536>here</a>.
Supported machine types are listed in the CloudProfile in <code>.spec.providerConfig.machineTypes[].acceleratedNetworking</code> and the supported operating system image versions are defined in <code>.spec.providerConfig.machineImages[].versions[].acceleratedNetworking</code>.</p><h3 id=preview-shoot-clusters-with-vmss-flexible-orchestration-vmss-flexvmo>Preview: Shoot clusters with VMSS Flexible Orchestration (VMSS Flex/VMO)</h3><p>The machines of an Azure cluster can be created while being attached to an <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#scale-sets-with-flexible-orchestration>Azure Virtual Machine ScaleSet with flexible orchestraion</a>.
The Virtual Machine ScaleSet with flexible orchestration feature is currently in preview and not yet general available on Azure.
Subscriptions need to <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#register-for-flexible-orchestration-mode>join the preview</a> to make use of the feature.</p><p>Azure VMSS Flex is intended to replace Azure AvailabilitySet for non-zoned Azure Shoot clusters in the mid-term (once the feature goes GA) as VMSS Flex come with less disadvantages like no blocking machine operations or compability with <code>Standard</code> SKU loadbalancer etc.</p><p>To configure an Azure Shoot cluster which make use of VMSS Flex you need to do the following:</p><ul><li>The <code>InfrastructureConfig</code> of the Shoot configuration need to contain <code>.zoned=false</code></li><li>Shoot resource need to have the following annotation assigned: <code>alpha.azure.provider.extensions.gardener.cloud/vmo=true</code></li></ul><p>Some key facts about VMSS Flex based clusters:</p><ul><li>Unlike regular non-zonal Azure Shoot clusters, which have a primary AvailabilitySet which is shared between all machines in all worker pools of a Shoot cluster, a VMSS Flex based cluster has an own VMSS for each workerpool</li><li>In case the configuration of the VMSS will change (e.g. amount of fault domains in a region change; configured in the CloudProfile) all machines of the worker pool need to be rolled</li><li>It is not possible to migrate an existing primary AvailabilitySet based Shoot cluster to VMSS Flex based Shoot cluster and vice versa</li><li>VMSS Flex based clusters are using <code>Standard</code> SKU LoadBalancers instead of <code>Basic</code> SKU LoadBalancers for AvailabilitySet based Shoot clusters</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-73b88a2c7d57533dd8df2fa1ec7d8e11>2.1.3.7 - Usage As Operator</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-an-operator>Using the Azure provider extension with Gardener as an operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for the Azure provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for Azure by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Azure shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Azure environment (image <code>urn</code>, <code>id</code>, <code>communityGalleryImageID</code> or <code>sharedGalleryImageID</code>).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> to an available VM image in your subscription.
The VM image can be either from the <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps?filters=virtual-machine-images">Azure Marketplace</a> and will then get identified via a <code>urn</code>, it can be a custom VM image from a shared image gallery and is then identified <code>sharedGalleryImageID</code>, or it can be from a community image gallery and is then identified by its <code>communityGalleryImageID</code>. You can use <code>id</code> field also to specifiy the image location in the azure compute gallery (in which case it would have a different kind of path) but it is not recommended as it sometimes faces problems in cross subscription image sharing.
For each machine image version an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>countUpdateDomains:
</span></span><span style=display:flex><span>- region: westeurope
</span></span><span style=display:flex><span>  count: 5
</span></span><span style=display:flex><span>countFaultDomains:
</span></span><span style=display:flex><span>- region: westeurope
</span></span><span style=display:flex><span>  count: 3
</span></span><span style=display:flex><span>machineTypes:
</span></span><span style=display:flex><span>- name: Standard_D3_v2
</span></span><span style=display:flex><span>  acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>- name: Standard_X
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># architecture: amd64 # optional</span>
</span></span><span style=display:flex><span>    acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>- name: myimage
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 1.0.0
</span></span><span style=display:flex><span>    id: <span style=color:#a31515>&#34;/subscriptions/&lt;subscription ID where the gallery is located&gt;/resourceGroups/myGalleryRG/providers/Microsoft.Compute/galleries/myGallery/images/myImageDefinition/versions/1.0.0&#34;</span>
</span></span><span style=display:flex><span>- name: GardenLinuxCommunityImage
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 1.0.0
</span></span><span style=display:flex><span>    communityGalleryImageID: <span style=color:#a31515>&#34;/CommunityGalleries/gardenlinux-567905d8-921f-4a85-b423-1fbf4e249d90/Images/gardenlinux/Versions/576.1.1&#34;</span>
</span></span><span style=display:flex><span>- name: SharedGalleryImageName
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>    - version: 1.0.0
</span></span><span style=display:flex><span>      sharedGalleryImageID: <span style=color:#a31515>&#34;/SharedGalleries/sharedGalleryName/Images/sharedGalleryImageName/Versions/sharedGalleryImageVersionName&#34;</span>
</span></span></code></pre></div><p>The cloud profile configuration contains information about the update via <code>.countUpdateDomains[]</code> and failure domain via <code>.countFaultDomains[]</code> counts in the Azure regions you want to offer.</p><p>The <code>.machineTypes[]</code> list contain provider specific information to the machine types e.g. if the machine type support <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a>, see <code>.machineTypes[].acceleratedNetworking</code>.</p><p>Additionally, it contains the real machine image identifiers in the Azure environment. You can provide either URN for Azure Market Place images or id of <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/linux/shared-image-galleries>Shared Image Gallery</a> images.
When Shared Image Gallery is used, you have to ensure that the image is available in the desired regions and the end-user subscriptions have access to the image or to the whole gallery.
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Azure extension knows the machine image identifiers for every version you want to offer.
Furthermore, you can specify for each image version via <code>.machineImages[].versions[].acceleratedNetworking</code> if Azure Accelerated Networking is supported.</p><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>The possible values for <code>.spec.volumeTypes[].name</code> on Azure are <code>Standard_LRS</code>, <code>StandardSSD_LRS</code> and <code>Premium_LRS</code>. There is another volume type called <code>UltraSSD_LRS</code> but this type is not supported to use as os disk. If an end user select a volume type whose name is not equal to one of the valid values then the machine will be created with the default volume type which belong to the selected machine type. Therefore it is recommended to configure only the valid values for the <code>.spec.volumeType[].name</code> in the <code>CloudProfile</code>.</p><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: azure
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: Standard_D3_v2
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 14Gi
</span></span><span style=display:flex><span>  - name: Standard_D4_v3
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 16Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: Standard_LRS
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: StandardSSD_LRS
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  - name: Premium_LRS
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: westeurope
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineTypes:
</span></span><span style=display:flex><span>    - name: Standard_D3_v2
</span></span><span style=display:flex><span>      acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    - name: Standard_D4_v3
</span></span><span style=display:flex><span>    countUpdateDomains:
</span></span><span style=display:flex><span>    - region: westeurope
</span></span><span style=display:flex><span>      count: 5
</span></span><span style=display:flex><span>    countFaultDomains:
</span></span><span style=display:flex><span>    - region: westeurope
</span></span><span style=display:flex><span>      count: 3
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2303.3.0
</span></span><span style=display:flex><span>        urn: CoreOS:CoreOS:Stable:2303.3.0
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span><span style=display:flex><span>        acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>azure</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Azure Blob storage.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the Seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Azure Blob storage.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    region: westeurope
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: azure
</span></span><span style=display:flex><span>    region: westeurope <span style=color:green># default region</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The referenced secret has to contain the provider credentials of the Azure subscription.
Please take a look <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>here</a> on how to create an Azure Application, Service Principle and how to obtain credentials.
The example below demonstrates how the secret has to look like.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  clientID: base64(client-id)
</span></span><span style=display:flex><span>  clientSecret: base64(client-secret)
</span></span><span style=display:flex><span>  subscriptionID: base64(subscription-id)
</span></span><span style=display:flex><span>  tenantID: base64(tenant-id)
</span></span></code></pre></div><h4 id=permissions-for-azure-blob-storage>Permissions for Azure Blob storage</h4><p>Please make sure the Azure application has the following IAM roles.</p><ul><li><a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a></li></ul><h2 id=miscellaneous>Miscellaneous</h2><h3 id=gardener-managed-service-principals>Gardener managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide a list of managed service principals (technical users) that can be used for Azure Shoots.
This eliminates the need for users to provide own service principals for their clusters.</p><p>The user would need to grant the managed service principal access to their subscription with proper permissions.</p><p>As service principals are managed in an Azure Active Directory for each supported Active Directory, an own service principal needs to be provided.</p><p>In case the user provides an own service principal in the Shoot secret, this one will be used instead of the managed one provided by the operator.</p><p>Each managed service principal will be maintained in a <code>Secret</code> like that:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: service-principal-my-tenant
</span></span><span style=display:flex><span>  namespace: extension-provider-azure
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    azure.provider.extensions.gardener.cloud/purpose: tenant-service-principal-secret
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  tenantID: base64(my-tenant)
</span></span><span style=display:flex><span>  clientID: base64(my-service-princiapl-id)
</span></span><span style=display:flex><span>  clientSecret: base64(my-service-princiapl-secret)
</span></span><span style=display:flex><span>type: Opaque
</span></span></code></pre></div><p>The user needs to provide in its Shoot secret a <code>tenantID</code> and <code>subscriptionID</code>.</p><p>The managed service principal will be assigned based on the <code>tenantID</code>.
In case there is a managed service principal secret with a matching <code>tenantID</code>, this one will be used for the Shoot.
If there is no matching managed service principal secret then the next Shoot operation will fail.</p><p>One of the benefits of having managed service principals is that the operator controls the lifecycle of the service principal and can rotate its secrets.</p><p>After the service principal secret has been rotated and the corresponding secret is updated, all Shoot clusters using it need to be reconciled or the last operation to be retried.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b182a904236beecf338898551c7afb28>2.1.4 - Provider Equinix Metal</h1><div class=lead>Gardener extension controller for the Equinix Metal cloud provider</div><h1 id=gardener-extension-for-equinix-metal-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Equinix Metal provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-equinix-metal><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-equinix-metal alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Equinix Metal provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.24</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.23</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.21</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.20</td><td>untested</td><td>N/A</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e44d3a514e91e1f451fbd09c9ff8af29>2.1.4.1 - Usage As End User</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-end-user>Using the Equinix Metal provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an Equinix Metal cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your Equinix Metal project.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  apiToken: base64(api-token)
</span></span><span style=display:flex><span>  projectID: base64(project-id)
</span></span></code></pre></div><p>Please look up <a href=https://metal.equinix.com/developers/api/>https://metal.equinix.com/developers/api/</a> as well.</p><p>With <code>Secret</code> created, create a <code>SecretBinding</code> resource referencing it. It may look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: SecretBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>secretRef:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>quotas: []
</span></span></code></pre></div><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>Currently, there is no infrastructure configuration possible for the Equinix Metal environment.</p><p>An example <code>InfrastructureConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span></code></pre></div><p>The Equinix Metal extension will only create a key pair.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Equinix Metal-specific control plane components.
Today, the Equinix Metal extension deploys the <code>cloud-controller-manager</code> and the CSI controllers, however, it doesn&rsquo;t offer any configuration options at the moment.</p><p>An example <code>ControlPlaneConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span></code></pre></div><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Equinix Metal extension supports specifying IDs for reserved devices that should be used for the machines of a specific worker pool.</p><p>An example <code>WorkerConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>reservationIDs:
</span></span><span style=display:flex><span>- my-reserved-device-1
</span></span><span style=display:flex><span>- my-reserved-device-2
</span></span><span style=display:flex><span>reservedDevicesOnly: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The <code>.reservationIDs[]</code> list contains the list of IDs of the reserved devices.
The <code>.reservedDevicesOnly</code> field indicates whether only reserved devices from the provided list of reservation IDs should be used when new machines are created.
It always will attempt to create a device from one of the reservation IDs.
If none is available, the behaviour depends on the setting:</p><ul><li><code>true</code>: return an error</li><li><code>false</code>: request a regular on-demand device</li></ul><p>The default value is <code>false</code>.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-shoot
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: equinix-metal
</span></span><span style=display:flex><span>  region: ny <span style=color:green># Corresponds to a metro</span>
</span></span><span style=display:flex><span>  secretBindingName: my-secret
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: equinixmetal
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-pool1
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: t1.small
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: storage_1
</span></span><span style=display:flex><span>      zones: <span style=color:green># Optional list of facilities, all of which MUST be in the metro; if not provided, then random facilities within the metro will be chosen for each machine.</span>
</span></span><span style=display:flex><span>      - ewr1
</span></span><span style=display:flex><span>      - ny5
</span></span><span style=display:flex><span>    - name: reserved-pool
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: t1.small
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: WorkerConfig
</span></span><span style=display:flex><span>        reservationIDs:
</span></span><span style=display:flex><span>        - reserved-device1
</span></span><span style=display:flex><span>        - reserved-device2
</span></span><span style=display:flex><span>        reservedDevicesOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: storage_1
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>⚠️ Note that if you specify multiple facilities in the <code>.spec.provider.workers[].zones[]</code> list then new machines are randomly created in one of the provided facilities.
Particularly, it is not ensured that all facilities are used or that all machines are equally or unequally distributed.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-equinix-metal@v2.2</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-equinix-metal@v2.3</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-equinix-metal@v2.4</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d4a62928e787b14269281e239057e96>2.1.4.2 - Usage As Operator</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-operator>Using the Equinix Metal provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating Equinix Metal shoot clusters.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: equinix-metal
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: equinixmetal
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.2
</span></span><span style=display:flex><span>    - version: 1.23.7
</span></span><span style=display:flex><span>    - version: 1.22.15
</span></span><span style=display:flex><span>      <span style=color:green>#expirationDate: &#34;2023-03-15T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: flatcar
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 0.0.0-stable
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: t1.small
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions: <span style=color:green># List of offered metros</span>
</span></span><span style=display:flex><span>  - name: ny
</span></span><span style=display:flex><span>    zones: <span style=color:green># List of offered facilities within the respective metro</span>
</span></span><span style=display:flex><span>    - name: ewr1
</span></span><span style=display:flex><span>    - name: ny5
</span></span><span style=display:flex><span>    - name: ny7
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: flatcar
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 0.0.0-stable
</span></span><span style=display:flex><span>        id: flatcar_stable
</span></span></code></pre></div><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the Equinix Metal environment (IDs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Equinix Metal extension knows the ID for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: flatcar
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 0.0.0-stable
</span></span><span style=display:flex><span>    id: flatcar_stable
</span></span></code></pre></div><blockquote><p>NOTE: <code>CloudProfileConfig</code> is not a Custom Resource, so you cannot create it directly.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-3958d75f5afea52ba6f339a00c82ee93>2.1.5 - Provider GCP</h1><div class=lead>Gardener extension controller for the GCP cloud provider</div><h1 id=gardener-extension-for-gcp-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for GCP provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-gcp><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-gcp alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the GCP provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20GCE/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20GCE/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20GCE/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20GCE/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20GCE/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20GCE/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a903239d744e07311330ca1f26fba6e6>2.1.5.1 - Create a Кubernetes Cluster on GCP with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on GCP.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created a <a href=https://console.cloud.google.com/>GCP account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_077542.png></li><li><p>Check which roles are required by Gardener.</p><ol><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_9d16d6.png> and select <em>GCP</em>.</p><img src=/__resources/create-secret-gcp_9b8911.png></li><li><p>Click on the help button <img src=/__resources/help-icon_1fcfd3.png>.</p><img src=/__resources/gardener-gcp-secret-1_3a2741.png>
<img src=/__resources/gardener-gcp-secret-2_fcc008.png></li></ol></li><li><p>Create a service account with the correct roles in GCP:</p><ol><li><p><a href=https://console.cloud.google.com/iam-admin/serviceaccounts>Create a new service account in GCP</a>.</p><img src=/__resources/gcp-create-service-account-0_0a398e.png></li><li><p>Enter the name and description of your service account.</p></li><li><p>Assign the roles required by Gardener.</p></li><li><p>Choose <em>Done</em>.</p><img src=/__resources/gcp-create-service-account-1_c55fe5.png></li></ol></li><li><p>Create a key for your service:</p><ol><li><p>Locate your service account, then choose <em>Actions</em> and <em>Manage keys</em>.</p><img src=/__resources/gcp-create-key-0_b97786.png></li><li><p>Choose <em>Add Key</em>, then <em>Create new key</em>.</p><img src=/__resources/gcp-create-key-1_83ef08.png></li><li><p>Save the private key of the service account in JSON format.</p><img src=/__resources/gcp-create-key-2_a661a1.png>
<img src=/__resources/gcp-create-key-3_ad2cf4.png></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Save the key of the user, it’s used later to create secrets for Gardener.</div></li><li><p>Enable the <a href=https://console.developers.google.com/apis/library/compute.googleapis.com>Google Compute API</a> by following <a href=https://cloud.google.com/endpoints/docs/openapi/enable-api>these steps</a>.</p><blockquote><p>When you are finished, you should see the following page:</p></blockquote><img src=/__resources/gcp-compute-engine-api_e4a22b.png></li><li><p>Enable the <a href=https://console.developers.google.com/apis/library/iam.googleapis.com>Google IAM API</a> by following <a href=https://cloud.google.com/endpoints/docs/openapi/enable-api>these steps</a>.</p><blockquote><p>When you are finished, you should see the following page:</p></blockquote><img src=/__resources/gcp-iam-api_8faa7c.png></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus-icon_9d16d6.png>. Select <em>GCP</em> from the drop down menu to add a new GCP secret.</p></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Select your <em>Cloud Profile</em>.</li><li>Copy and paste the contents of the <em>.JSON</em> file you saved when you created the secret key on GCP.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-gcp-secret_84ec33.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_2be4fd.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_581df0.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>GCP</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_66c946.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_309fda.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_3d13a9.png></div><div class=td-content style=page-break-before:always><h1 id=pg-e4646c279fbcc1d9ba0ae7b2f8d3e9ea>2.1.5.2 - Deployment</h1><h1 id=deployment-of-the-gcp-provider-extension>Deployment of the GCP provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the GCP provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the GCP provider extension <a href=https://github.com/gardener/gardener-extension-provider-gcp>repository</a>.</p><h2 id=gardener-extension-admission-gcp>gardener-extension-admission-gcp</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-gcp</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-26955d31b03a63ff367474277c84b6a2>2.1.5.3 - Local Setup</h1><h3 id=admission-gcp>admission-gcp</h3><p><code>admission-gcp</code> is an admission webhook server which is responsible for the validation of the cloud provider (GCP in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-gcp.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-gcp.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-gcp</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1e2221f3be40cdc59da201d0e7877323>2.1.5.4 - Usage As End User</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-end-user>Using the GCP provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for GCP and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create a GCP cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=gcp-provider-credentials>GCP Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using GCP infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired GCP project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the GCP project.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the GCP project are a <a href=https://cloud.google.com/iam/docs/service-accounts#service_account_keys>Service Account Key</a> to authenticate as a <a href=https://cloud.google.com/compute/docs/access/service-accounts>GCP Service Account</a>.
A service account is a special account that can be used by services and applications to interact with Google Cloud Platform APIs.
Applications can use service account credentials to authorize themselves to a set of APIs and perform actions within the permissions granted to the service account.</p><p>Make sure to <a href=https://cloud.google.com/service-usage/docs/enable-disable>enable the Google Identity and Access Management (IAM) API</a>.
<a href=https://cloud.google.com/iam/docs/creating-managing-service-accounts>Create a Service Account</a> that shall be used for the Shoot cluster.
<a href=https://cloud.google.com/iam/docs/granting-changing-revoking-access>Grant at least the following IAM roles</a> to the Service Account.</p><ul><li>Service Account Admin</li><li>Service Account Token Creator</li><li>Service Account User</li><li>Compute Admin</li></ul><p>Create a <a href=https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys>JSON Service Account key</a> for the Service Account.
Provide it in the <code>Secret</code> (base64 encoded for field <code>serviceaccount.json</code>), that is being referenced by the <code>SecretBinding</code> in the Shoot cluster configuration.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-gcp
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  serviceaccount.json: base64(serviceaccount-json)
</span></span></code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Account Key for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple Service Accounts on different GCP projects if you are hitting those limits, see <a href=https://cloud.google.com/compute/docs/api-rate-limits>https://cloud.google.com/compute/docs/api-rate-limits</a>.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span><span style=color:green># vpc:</span>
</span></span><span style=display:flex><span><span style=color:green>#   name: my-vpc</span>
</span></span><span style=display:flex><span><span style=color:green>#   cloudRouter:</span>
</span></span><span style=display:flex><span><span style=color:green>#     name: my-cloudrouter</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/16
</span></span><span style=display:flex><span><span style=color:green># internal: 10.251.0.0/16</span>
</span></span><span style=display:flex><span><span style=color:green># cloudNAT:</span>
</span></span><span style=display:flex><span><span style=color:green>#   minPortsPerVM: 2048</span>
</span></span><span style=display:flex><span><span style=color:green>#   natIPNames:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: manualnat1</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: manualnat2</span>
</span></span><span style=display:flex><span><span style=color:green># flowLogs:</span>
</span></span><span style=display:flex><span><span style=color:green>#   aggregationInterval: INTERVAL_5_SEC</span>
</span></span><span style=display:flex><span><span style=color:green>#   flowSampling: 0.2</span>
</span></span><span style=display:flex><span><span style=color:green>#   metadata: INCLUDE_ALL_METADATA</span>
</span></span></code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li><p>If <code>networks.vpc.name</code> is given then you have to specify the VPC name of the existing VPC that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh VPC for the shoot then just omit the <code>networks.vpc</code> field.</p></li><li><p>If a VPC name is not given then we will create the cloud router + NAT gateway to ensure that worker nodes don&rsquo;t get external IPs.</p></li><li><p>If a VPC name is given then a cloud router name must also be given, failure to do so would result in validation errors
and possibly clusters without egress connectivity.</p></li><li><p>If a VPC name is given and calico shoot clusters are created without a network overlay within one VPC make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that VPC.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>The <code>networks.internal</code> section is optional and can describe a CIDR for a subnet that is used for <a href=https://cloud.google.com/load-balancing/docs/internal/>internal load balancers</a>,</p><p>The <code>networks.cloudNAT.minPortsPerVM</code> is optional and is used to define the <a href=https://cloud.google.com/nat/docs/overview#number_of_nat_ports_and_connections>minimum number of ports allocated to a VM for the CloudNAT</a></p><p>The <code>networks.cloudNAT.natIPNames</code> is optional and is used to specify the names of the manual ip addresses which should be used by the nat gateway</p><p>The specified CIDR ranges must be contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>The <code>networks.flowLogs</code> section describes the configuration for the VPC flow logs. In order to enable the VPC flow logs at least one of the following parameters needs to be specified in the flow log section:</p><ul><li><p><code>networks.flowLogs.aggregationInterval</code> an optional parameter describing the aggregation interval for collecting flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#aggregation_interval>aggregation_interval reference</a>.</p></li><li><p><code>networks.flowLogs.flowSampling</code> an optional parameter describing the sampling rate of VPC flow logs within the subnetwork where 1.0 means all collected logs are reported and 0.0 means no logs are reported. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#flow_sampling>flow_sampling reference</a>.</p></li><li><p><code>networks.flowLogs.metadata</code> an optional parameter describing whether metadata fields should be added to the reported VPC flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#metadata>metadata reference</a>.</p></li></ul><p>Apart from the VPC and the subnets the GCP extension will also create a dedicated service account for this shoot, and firewall rules.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the GCP-specific control plane components.
Today, the only component deployed by the GCP extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>zone: europe-west1-b
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>zone</code> field tells the cloud-controller-manager in which zone it should mainly operate.
You can still create clusters in multiple availability zones, however, the cloud-controller-manager requires one &ldquo;main&rdquo; zone.
⚠️ You always have to specify this field!</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig>WorkerConfig</h2><p>The worker configuration contains:</p><ul><li><p>Local SSD interface for the additional volumes attached to GCP worker machines.</p><p>If you attach the disk with <code>SCRATCH</code> type, either an <code>NVMe</code> interface or a <code>SCSI</code> interface must be specified.
It is only meaningful to provide this volume interface if only <code>SCRATCH</code> data volumes are used.</p></li><li><p>Service Account with their specified scopes, authorized for this worker.</p><p>Service accounts created in advance that generate access tokens that can be accessed through the metadata server and used to authenticate applications on the instance.</p></li><li><p>GPU with its type and count per node. This will attach that GPU to all the machines in the worker grp</p><p><strong>Note</strong>:</p><ul><li>A rolling upgrade of the worker group would be triggered in case the <code>acceleratorType</code> or <code>count</code> is updated.</li><li>Some machineTypes like <a href=https://cloud.google.com/blog/products/compute/announcing-google-cloud-a2-vm-family-based-on-nvidia-a100-gpu>a2 family</a> come with already attached gpu of <code>a100</code> type and pre-defined count. If your workerPool consists of those machineTypes, please <strong>do not</strong> specify any GPU configuration.</li><li>Sufficient quota of gpu is needed in the GCP project. This includes quota to support autoscaling if enabled.</li><li>GPU-attached machines can&rsquo;t be live migrated during host maintenance events. Find out how to handle that in your application <a href=https://cloud.google.com/compute/docs/gpus/gpu-host-maintenance>here</a></li><li>GPU count specified here is considered for forming node template during scale-from-zero in Cluster Autoscaler</li></ul><p>An example <code>WorkerConfig</code> for the GCP looks as follows:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>volume:
</span></span><span style=display:flex><span>  interface: NVME
</span></span><span style=display:flex><span>serviceAccount:
</span></span><span style=display:flex><span>  email: foo@bar.com
</span></span><span style=display:flex><span>  scopes:
</span></span><span style=display:flex><span>  - https://www.googleapis.com/auth/cloud-platform
</span></span><span style=display:flex><span>gpu:
</span></span><span style=display:flex><span>  acceleratorType: nvidia-tesla-t4
</span></span><span style=display:flex><span>  count: 1
</span></span></code></pre></div><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-gcp
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: gcp
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretBindingName: core-gcp
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/16
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      zone: europe-west1-b
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: n1-standard-4
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: pd-standard
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-west1-b
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every GCP shoot cluster will be deployed with the GCP PD CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>pd.csi.storage.gke.io</code> provisioner.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-gcp@v1.21</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-gcp@v1.23</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d02efe54eb56f07360e0d78ae62e3a23>2.1.5.5 - Usage As Operator</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-operator>Using the GCP provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for GCP by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of GCP shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the GCP environment (image URLs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the GCP extension knows the image URL for every version you want to offer.
For each machine image version an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</span></span><span style=display:flex><span>    <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>If you want to allow that shoots can create VMs with local SSDs volumes then you have to specify the type of the disk with <code>SCRATCH</code> in the <code>.spec.volumeTypes[]</code> list.
Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gcp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: gcp
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: n1-standard-4
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 15Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: pd-standard
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: pd-ssd
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  - name: SCRATCH
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - region: europe-west1
</span></span><span style=display:flex><span>    names:
</span></span><span style=display:flex><span>    - europe-west1-b
</span></span><span style=display:flex><span>    - europe-west1-c
</span></span><span style=display:flex><span>    - europe-west1-d
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>gcp</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Google Cloud Storage buckets.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Google Cloud Storage buckets.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    region: europe-west1
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: gcp
</span></span><span style=display:flex><span>    region: europe-west1 <span style=color:green># default region</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>An example of the referenced secret containing the credentials for the GCP Cloud storage can be found in the <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-gcp-cloud-storage>Permissions for GCP Cloud Storage</h4><p>Please make sure the service account associated with the provided credentials has the following IAM roles.</p><ul><li><a href=https://cloud.google.com/storage/docs/access-control/iam-roles>Storage Admin</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b46674e0cabcf6e848e4d419e8c17465>2.1.6 - Provider Openstack</h1><div class=lead>Gardener extension controller for the OpenStack cloud provider</div><h1 id=gardener-extension-for-openstack-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for OpenStack provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-openstack><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-openstack alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the OpenStack provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20OpenStack/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20OpenStack/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.23%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.23%20OpenStack/tests_status?style=svg" alt="Gardener v1.23 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20OpenStack/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20OpenStack/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20OpenStack/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>OpenStack Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt; v1.12.0</code></td><td><code>> v1.10.0</code></td><td>Please update the provider version to <code>>= v1.12.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. This is to prevent duplicate volume mounts to <code>/usr/share/ca-certificates</code> in the Shoot API Server.</td></tr></tbody></table><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-openstack/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ec2c3c7ff8df2101c099ac6ffc355b52>2.1.6.1 - Deployment</h1><h1 id=deployment-of-the-openstack-provider-extension>Deployment of the OpenStack provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the OpenStack provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the OpenStack provider extension <a href=https://github.com/gardener/gardener-extension-provider-openstack>repository</a>.</p><h2 id=gardener-extension-admission-openstack>gardener-extension-admission-openstack</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-openstack</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-af315a6fe4fa0e8ce5057c406a991397>2.1.6.2 - Local Setup</h1><h3 id=admission-openstack>admission-openstack</h3><p><code>admission-openstack</code> is an admission webhook server which is responsible for the validation of the cloud provider (OpenStack in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-openstack.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-openstack.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-openstack</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9185f7aa7b9d6eabd12b95706054e195>2.1.6.3 - Usage As End User</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-end-user>Using the OpenStack provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an OpenStack cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your OpenStack tenant.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-openstack
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  domainName: base64(domain-name)
</span></span><span style=display:flex><span>  tenantName: base64(tenant-name)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:green># either use username/password</span>
</span></span><span style=display:flex><span>  username: base64(user-name)
</span></span><span style=display:flex><span>  password: base64(password)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># or application credentials</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialID: base64(app-credential-id)</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialName: base64(app-credential-name) # optional</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialSecret: base64(app-credential-secret)</span>
</span></span></code></pre></div><p>Please look up <a href=https://docs.openstack.org/keystone/pike/admin/identity-concepts.html>https://docs.openstack.org/keystone/pike/admin/identity-concepts.html</a> as well.</p><p>For authentication with username/password see <a href=https://docs.openstack.org/keystone/latest/user/supported_clients.html>Keystone username/password</a></p><p>Alternatively, for authentication with application credentials see <a href=https://docs.openstack.org/keystone/latest/user/application_credentials.html>Keystone Application Credentials</a>.</p><p>⚠️ Depending on your API usage it can be problematic to reuse the same provider credentials for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple credentials from different tenants if you are hitting those limits.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>floatingPoolName: MY-FLOATING-POOL
</span></span><span style=display:flex><span><span style=color:green># floatingPoolSubnetName: my-floating-pool-subnet-name</span>
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span><span style=color:green># id: 12345678-abcd-efef-08af-0123456789ab</span>
</span></span><span style=display:flex><span><span style=color:green># router:</span>
</span></span><span style=display:flex><span><span style=color:green>#   id: 1234</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/19
</span></span></code></pre></div><p>The <code>floatingPoolName</code> is the name of the floating pool you want to use for your shoot.
If you don&rsquo;t know which floating pools are available look it up in the respective <code>CloudProfile</code>.</p><p>With <code>floatingPoolSubnetName</code> you can explicitly define to which subnet in the floating pool network (defined via <code>floatingPoolName</code>) the router should be attached to.</p><p><code>networks.id</code> is an optional field. If it is given, you can specify the uuid of an existing private Neutron network (created manually, by other tooling, &mldr;) that should be reused. A new subnet for the Shoot will be created in it.</p><p>If a <code>networks.id</code> is given and calico shoot clusters are created without a network overlay within one network make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that network.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</p><p>The <code>networks.router</code> section describes whether you want to create the shoot cluster in an already existing router or whether to create a new one:</p><ul><li><p>If <code>networks.router.id</code> is given then you have to specify the router id of the existing router that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh router for the shoot then just omit the <code>networks.router</code> field.</p></li><li><p>In any case, the shoot cluster will be created in a <strong>new</strong> subnet.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Apart from the router and the worker subnet the OpenStack extension will also create a network, router interfaces, security groups, and a key pair.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the OpenStack-specific control plane components.
Today, the only component deployed by the OpenStack extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>loadBalancerProvider: haproxy
</span></span><span style=display:flex><span>loadBalancerClasses:
</span></span><span style=display:flex><span>- name: lbclass-1
</span></span><span style=display:flex><span>  purpose: default
</span></span><span style=display:flex><span>  floatingNetworkID: fips-1-id
</span></span><span style=display:flex><span>  floatingSubnetName: internet-*
</span></span><span style=display:flex><span>- name: lbclass-2
</span></span><span style=display:flex><span>  floatingNetworkID: fips-1-id
</span></span><span style=display:flex><span>  floatingSubnetTags: internal,private
</span></span><span style=display:flex><span>- name: lbclass-3
</span></span><span style=display:flex><span>  purpose: private
</span></span><span style=display:flex><span>  subnetID: internal-id
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>loadBalancerProvider</code> is the provider name you want to use for load balancers in your shoot.
If you don&rsquo;t know which types are available look it up in the respective <code>CloudProfile</code>.</p><p>The <code>loadBalancerClasses</code> field contains an optional list of load balancer classes which will be available in the cluster. Each entry can have the following fields:</p><ul><li><code>name</code> to select the load balancer class via the kubernetes <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#switching-between-floating-subnets-by-using-preconfigured-classes>service annotations</a> <code>loadbalancer.openstack.org/class=name</code></li><li><code>purpose</code> with values <code>default</code> or <code>private</code><ul><li>The configuration of the <code>default</code> load balancer class will be used as default for all other kubernetes loadbalancer services without a class annotation</li><li>The configuration of the <code>private</code> load balancer class will be also set to the global loadbalancer configuration of the cluster, but will be overridden by the <code>default</code> purpose</li></ul></li><li><code>floatingNetworkID</code> can be specified to receive an ip from an floating/external network, additionally the subnet in this network can be selected via<ul><li><code>floatingSubnetName</code> can be either a full subnet name or a regex/glob to match subnet name</li><li><code>floatingSubnetTags</code> a comma seperated list of subnet tags</li><li><code>floatingSubnetID</code> the id of a specific subnet</li></ul></li><li><code>subnetID</code> can be specified by to receive an ip from an internal subnet (will not have an effect in combination with floating/external network configuration)</li></ul><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommended to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>Each worker group in a shoot may contain provider-specific configurations and options. These are contained in the <code>providerConfig</code> section of a worker group and can be configured using a <code>WorkerConfig</code> object.
An example of a <code>WorkerConfig</code> looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>serverGroup:
</span></span><span style=display:flex><span>  policy: soft-anti-affinity
</span></span></code></pre></div><p>When you specify the <code>serverGroup</code> section in your worker group configuration, a new server group will be created with the configured policy for each worker group that enabled this setting and all machines managed by this worker group will be assigned as members of the created server group.</p><p>For users to have access to the server group feature, it must be enabled on the <code>CloudProfile</code> by your operator.
Existing clusters can take advantage of this feature by updating the server group configuration of their respective worker groups. Worker groups that are already configured with server groups can update their setting to change the policy used, or remove it altogether at any time.</p><p>Users must be aware that <strong>any change to the server group settings will result in a rolling deployment of new nodes for the affected worker group</strong>.</p><p>Please note the following restrictions when deploying workers with server groups:</p><ul><li>The <code>serverGroup</code> section is optional, but if it is included in the worker configuration, it must contain a valid policy value.</li><li>The available <code>policy</code> values that can be used, are defined in the provider specific section of <code>CloudProfile</code> by your operator.</li><li>Certain policy values may induce further constraints. Using the <code>affinity</code> policy is only allowed when the worker group utilizes a single zone.</li></ul><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-openstack
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: openstack
</span></span><span style=display:flex><span>  region: europe-1
</span></span><span style=display:flex><span>  secretBindingName: core-openstack
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: openstack
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      floatingPoolName: MY-FLOATING-POOL
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      loadBalancerProvider: haproxy
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: medium_4_8
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every OpenStack shoot cluster will be deployed with the OpenStack Cinder CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>cinder.csi.openstack.org</code> provisioner.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-openstack@v1.23</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-openstack@v1.26</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-62d6c9c1665ce648d6bce255102f039f>2.1.6.4 - Usage As Operator</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-operator>Using the OpenStack provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating OpenStack shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the OpenStack environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the OpenStack extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>Also, you have to specify the keystone URL in the <code>keystoneURL</code> field to your environment.</p><p>Additionally, you can influence the HTTP request timeout when talking to the OpenStack API in the <code>requestTimeout</code> field.
This may help when you have for example a long list of load balancers in your environment.</p><p>In case your OpenStack system uses <a href=https://docs.openstack.org/octavia/latest/>Octavia</a> for network load balancing then you have to set the <code>useOctavia</code> field to <code>true</code> such that the cloud-controller-manager for OpenStack gets correctly configured (it defaults to <code>false</code>).</p><p>Some hypervisors (especially those which are VMware-based) don&rsquo;t automatically send a new volume size to a Linux kernel when a volume is resized and in-use.
For those hypervisors you can enable the storage plugin interacting with Cinder to telling the SCSI block device to refresh its information to provide information about it&rsquo;s updated size to the kernel. You might need to enable this behavior depending on the underlying hypervisor of your OpenStack installation. The <code>rescanBlockStorageOnResize</code> field controls this. Please note that it only applies for Kubernetes versions where CSI is used.</p><p>Some openstack configurations do not allow to attach more volumes than a specific amount to a single node.
To tell the k8s scheduler to not over schedule volumes on a node, you can set <code>nodeVolumeAttachLimit</code> which defaults to 256.
Some openstack configurations have different names for volume and compute availability zones, which might cause pods to go into pending state as there are no nodes available in the detected volume AZ. To ignore the volume AZ when scheduling pods, you can set <code>ignoreVolumeAZ</code> to <code>true</code>, which is only supported for shoot kubernetes version 1.20.x and newer (it defaults to <code>false</code>).
See <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md#block-storage>CSI Cinder driver</a>.</p><p>The cloud profile config also contains constraints for floating pools and load balancer providers that can be used in shoots.</p><p>If your OpenStack system supports server groups, the <code>serverGroupPolicies</code> property will enable your end-users to create shoots with workers where the nodes are managed by Nova&rsquo;s server groups.
Specifying <code>serverGroupPolicies</code> is optional and can be omitted. If enabled, the end-user can choose whether or not to use this feature for a shoot&rsquo;s workers. Gardener will handle the creation of the server group and node assignment.</p><p>To enable this feature, an operator should:</p><ul><li>specify the allowed policy values (e.g. <code>affintity</code>, <code>anti-affinity</code>) in this section. Only the policies in the allow-list will be available for end-users.</li><li>make sure that your OpenStack project has enough server group capacity. Otherwise, shoot creation will fail.</li></ul><p>If your OpenStack system has multiple <code>volume-types</code>, the <code>storageClasses</code> property enables the creation of kubernetes <code>storageClasses</code> for shoots.
Set <code>storageClasses[].parameters.type</code> to map it with an openstack <code>volume-type</code>. Specifying <code>storageClasses</code> is optional and can be omitted.</p><p>An example <code>CloudProfileConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    image: coreos-2135.6.0
</span></span><span style=display:flex><span><span style=color:green># keystoneURL: https://url-to-keystone/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># keystoneURLs:</span>
</span></span><span style=display:flex><span><span style=color:green># - region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   url: https://europe.example.com/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># - region: asia</span>
</span></span><span style=display:flex><span><span style=color:green>#   url: https://asia.example.com/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># dnsServers:</span>
</span></span><span style=display:flex><span><span style=color:green># - 10.10.10.11</span>
</span></span><span style=display:flex><span><span style=color:green># - 10.10.10.12</span>
</span></span><span style=display:flex><span><span style=color:green># requestTimeout: 60s</span>
</span></span><span style=display:flex><span><span style=color:green># useOctavia: true</span>
</span></span><span style=display:flex><span><span style=color:green># useSNAT: true</span>
</span></span><span style=display:flex><span><span style=color:green># rescanBlockStorageOnResize: true</span>
</span></span><span style=display:flex><span><span style=color:green># ignoreVolumeAZ: true</span>
</span></span><span style=display:flex><span><span style=color:green># nodeVolumeAttachLimit: 30</span>
</span></span><span style=display:flex><span><span style=color:green># serverGroupPolicies:</span>
</span></span><span style=display:flex><span><span style=color:green># - soft-anti-affinity</span>
</span></span><span style=display:flex><span><span style=color:green># - anti-affinity</span>
</span></span><span style=display:flex><span><span style=color:green># resolvConfOptions:</span>
</span></span><span style=display:flex><span><span style=color:green># - rotate</span>
</span></span><span style=display:flex><span><span style=color:green># - timeout:1</span>
</span></span><span style=display:flex><span><span style=color:green># storageClasses:</span>
</span></span><span style=display:flex><span><span style=color:green># - name: example-sc</span>
</span></span><span style=display:flex><span><span style=color:green>#   default: false</span>
</span></span><span style=display:flex><span><span style=color:green>#   provisioner: cinder.csi.openstack.org</span>
</span></span><span style=display:flex><span><span style=color:green>#   volumeBindingMode: WaitForFirstConsumer</span>
</span></span><span style=display:flex><span><span style=color:green>#   parameters:</span>
</span></span><span style=display:flex><span><span style=color:green>#     type: storage_premium_perf0</span>
</span></span><span style=display:flex><span>constraints:
</span></span><span style=display:flex><span>  floatingPools:
</span></span><span style=display:flex><span>  - name: fp-pool-1
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-*&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-eu-demo&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   domain: demo</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-eu-dev&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   domain: dev</span>
</span></span><span style=display:flex><span><span style=color:green>#   nonConstraining: true</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span>  loadBalancerProviders:
</span></span><span style=display:flex><span>  - name: haproxy
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green># - name: f5</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: asia</span>
</span></span></code></pre></div><p>Please note that it is possible to configure a region mapping for keystone URLs, floating pools, and load balancer providers.
Additionally, floating pools can be constrainted to a keystone domain by specifying the <code>domain</code> field.
Floating pool names may also contains simple wildcard expressions, like <code>*</code> or <code>fp-pool-*</code> or <code>*-fp-pool</code>. Please note that the <code>*</code> must be either single or at the beginning or at the end. Consequently, <code>fp-*-pool</code> is not possible/allowed.
The default behavior is that, if found, the regional (and/or domain restricted) entry is taken.
If no entry for the given region exists then the fallback value is the most matching entry (w.r.t. wildcard matching) in the list without a <code>region</code> field (or the <code>keystoneURL</code> value for the keystone URLs).
If an additional floating pool should be selectable for a region and/or domain, you can mark it as non constraining
with setting the optional field <code>nonConstraining</code> to <code>true</code>.</p><p>The <code>loadBalancerClasses</code> field is an optional list of load balancer classes which can be when the corresponding floating pool network is choosen. The load balancer classes can be configured in the same way as in the <code>ControlPlaneConfig</code> in the <code>Shoot</code> resource, therefore see <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/usage-as-end-user/#ControlPlaneConfig>here</a> for more details.</p><p>Some OpenStack environments don&rsquo;t need these regional mappings, hence, the <code>region</code> and <code>keystoneURLs</code> fields are optional.
If your OpenStack environment only has regional values and it doesn&rsquo;t make sense to provide a (non-regional) fallback then simply
omit <code>keystoneURL</code> and always specify <code>region</code>.</p><p>If Gardener creates and manages the router of a shoot cluster, it is additionally possible to specify that the <a href=https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs/resources/networking_router_v2#enable_snat>enable_snat</a> field is set to <code>true</code> via <code>useSNAT: true</code> in the <code>CloudProfileConfig</code>.</p><p>On some OpenStack enviroments, there may be the need to set options in the file <code>/etc/resolv.conf</code> on worker nodes.
If the field <code>resolvConfOptions</code> is set, a systemd service will be installed which copies <code>/run/systemd/resolve/resolv.conf</code>
on every change to <code>/etc/resolv.conf</code> and appends the given options.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: openstack
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: openstack
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: medium_4_8
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    storage:
</span></span><span style=display:flex><span>      class: standard
</span></span><span style=display:flex><span>      type: default
</span></span><span style=display:flex><span>      size: 40Gi
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: europe-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: europe-1a
</span></span><span style=display:flex><span>    - name: europe-1b
</span></span><span style=display:flex><span>    - name: europe-1c
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        image: coreos-2135.6.0
</span></span><span style=display:flex><span>    keystoneURL: https://url-to-keystone/v3/
</span></span><span style=display:flex><span>    constraints:
</span></span><span style=display:flex><span>      floatingPools:
</span></span><span style=display:flex><span>      - name: fp-pool-1
</span></span><span style=display:flex><span>      loadBalancerProviders:
</span></span><span style=display:flex><span>      - name: haproxy
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5b5f3488fb6707d456d4d70245b0f4ab>2.1.7 - Provider vSphere</h1><div class=lead>Gardener extension controller for the vSphere cloud provider</div><h1 id=gardener-extension-for-vsphere-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for vSphere provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-vsphere><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-vsphere alt="Go Report Card"></a></p><h2 id=overview>Overview</h2><p>The Gardener Extension for vSphere is a <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> provider implementation that allows Gardener to leverage vSphere clusters for machine provisioning.</p><p>vSphere is an undeniable class leader for commercially supported virtual machine orchestration. The Gardener extension for vSphere provider compliments this leadership by allowing Gardener to create Kubernetes nodes within vSphere.</p><p>Like other Gardener provider extensions, the vSphere provider pairs with a provider-specific Machine Controller Manager providing node services to Kubernetes clusters. This extension provides complimentary APIs to Gardener. A Gardener-hosted Kubernetes
cluster does not know anything about it&rsquo;s environment (such as bare metal vs. public cloud or within a hyperscaler vs. standalone), only that the MCM abstraction can manage requests such as cluster autoscaling.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-vsphere/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and the architecture details in the GEP-1 proposal.</p><h2 id=use-cases>Use Cases</h2><p>The primary use case for this extension is organizations who wish to deploy a substantial Gardener landscape and use vSphere for data center fleet management. We intentionally sidestep prescribing any particular extension as this is
an intimately local determination and the benefits of different solutions are more than adequately debated in industry literature.</p><p>While we may inadvertently duplicate some documentation in the mainline Gardener documentation, it is only to reduce tedium as new evaluators and developers come up-to-speed with the concepts relevant to successful deployment.
We refer directly to the mainline Gardener documentation for the most up-to-date information.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.25</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.24</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.23</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.22</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.21</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.20</td><td>untested</td><td>not yet available</td></tr></tbody></table><p>Older versions of the extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere/releases/tag/v0.16.0>(<code>v0.16.0</code> and earlier)</a> are supported prior to current releases.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=deployment-patterns>Deployment patterns</h2><p>As with any production software, deployment of Gardener and this extension should be considered in the context of both lifecycle and automation. Orgs should aspire to have apply</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-vsphere/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2843258f8c2907b5d063aa027786599e>2.1.7.1 - Docs</h1><p>#Documentation</p><p>##Table of Contents</p><h3 id=installation>Installation</h3><ul><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/prepare-vsphere/>prepare-vsphere.md</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/deployment/>deployment.md</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/tanzu-vsphere/>tanzu-vsphere.md</a></li></ul><h3 id=usage>Usage</h3><ul><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/usage-as-end-user/>usage-as-end-user.md</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/usage-as-operator/>usage-as-operator.md</a></li></ul><h3 id=development>Development</h3><ul><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/local-setup/>local-setup.md</a></li></ul></div><div class=td-content><h1 id=pg-35a78d43d0bac7f0de61739ce0338d7f>2.1.7.1.1 - Deployment</h1><h1 id=deployment-of-the-vsphere-provider-extension>Deployment of the vSphere provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the vSphere provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the vSphere provider extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere>repository</a>.</p><h2 id=gardener-extension-validator-vsphere>gardener-extension-validator-vsphere</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-validator-vsphere</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e2908e6901d240206d5754a946ddf138>2.1.7.1.2 - Local Setup</h1><h1 id=deployment>Deployment</h1><h3 id=admission-vsphere>admission-vsphere</h3><p><code>admission-vsphere</code> is an admission webhook server which is responsible for the validation of the cloud provider (vSphere in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-vsphere.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-vsphere.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-vsphere</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-636e168a1b7671fa4ceef9e39ea82a55>2.1.7.1.3 - Prepare Vsphere</h1><h1 id=vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</h1><ul><li><a href=#vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</a><ul><li><a href=#vsphere-preparation>vSphere Preparation</a><ul><li><a href=#user-and-role-creation>User and Role Creation</a><ul><li><a href=#vcentervsphere>vCenter/vSphere</a></li><li><a href=#nsx-t>NSX-T</a></li></ul></li><li><a href=#create-folders>Create Folders</a></li><li><a href=#upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</a></li><li><a href=#prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</a><ul><li><a href=#create-resource-pools>Create Resource Pool(s)</a></li><li><a href=#tag-regions-and-zones>Tag Regions and Zones</a></li><li><a href=#storage-policies>Storage policies</a><ul><li><a href=#tag-zone-storages>Tag Zone Storages</a></li><li><a href=#create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</a></li></ul></li></ul></li></ul></li><li><a href=#nsx-t-preparation>NSX-T Preparation</a><ul><li><a href=#create-ip-pools>Create IP pools</a><ul><li><a href=#sizing-the-ip-pools>Sizing the IP pools</a></li></ul></li><li><a href=#check-edge-cluster-sizing>Check edge cluster sizing</a></li></ul></li><li><a href=#get-vds-uuids>Get VDS UUIDs</a></li></ul></li></ul><p>Several preparational steps are necessary for VMware vSphere and NSX-T, before this extension can be used
to create Gardener shoot clusters.</p><p>The main version target of this extension is vSphere 7.x together with NSX-T 3.x.
The recommended environment is a system setup with VMware Cloud Foundation (VCF) 4.1.
Older versions like vSphere 6.7U3 with NSX-T 2.5 or 3.0 should still work, but are not tested extensively.</p><h2 id=vsphere-preparation>vSphere Preparation</h2><h3 id=user-and-role-creation>User and Role Creation</h3><p>This extension needs credentials for both the vSphere/vCenter and the NSX-T endpoints.
This section guides through the creation of appropriate roles and users.</p><h4 id=vcentervsphere>vCenter/vSphere</h4><p>The vCenter/vSphere user used for this provider should have been assigned to a role including these permissions
(use vCenter/vSphere Client / Menu Administration / Access Control / Role to define a role and assign it to the user
with <code>Global Permissions</code>)</p><ul><li>Datastore<ul><li>Allocate space</li><li>Browse datastore</li><li>Low level file operations</li><li>Remove file</li><li>Update virtual machine files</li><li>Update virtual machine metadata</li></ul></li><li>Global<ul><li>Cancel task</li><li>Manage custom attributes</li><li>Set custom attribute</li></ul></li><li>Network<ul><li>Assign network</li></ul></li><li>Resource<ul><li>Assign virtual machine to resource pool</li></ul></li><li>Tasks<ul><li>Create task</li><li>Update task</li></ul></li><li>vApp<ul><li>Add virtual machine</li><li>Assign resource pool</li><li>Assign vApp</li><li>Clone</li><li>Power off</li><li>Power on</li><li>View OVF environment</li><li>vApp application configuration</li><li>vApp instance configuration</li><li>vApp managedBy configuration</li><li>vApp resource configuration</li></ul></li><li>Virtual machine<ul><li>Change Configuration<ul><li>Acquire disk lease</li><li>Add existing disk</li><li>Add new disk</li><li>Add or remove device</li><li>Advanced configuration</li><li>Change CPU count</li><li>Change Memory</li><li>Change Settings</li><li>Change Swapfile placement</li><li>Change resource</li><li>Configure Host USB device</li><li>Configure Raw device</li><li>Configure managedBy</li><li>Display connection settings</li><li>Extend virtual disk</li><li>Modify device settings</li><li>Query Fault Tolerance compatibility</li><li>Query unowned files</li><li>Reload from path</li><li>Remove disk</li><li>Rename</li><li>Reset guest information</li><li>Set annotation</li><li>Toggle disk change tracking</li><li>Toggle fork parent</li><li>Upgrade virtual machine compatibility</li></ul></li><li>Edit Inventory<ul><li>Create from existing</li><li>Create new</li><li>Move</li><li>Register</li><li>Remove</li><li>Unregister</li></ul></li><li>Guest operations<ul><li>Guest operation alias modification</li><li>Guest operation alias query</li><li>Guest operation modifications</li><li>Guest operation program execution</li><li>Guest operation queries</li></ul></li><li>Interaction<ul><li>Power off</li><li>Power on</li><li>Reset</li></ul></li><li>Provisioning<ul><li>Allow disk access</li><li>Allow file access</li><li>Allow read-only disk access</li><li>Allow virtual machine files upload</li><li>Clone template</li><li>Clone virtual machine</li><li>Customize guest</li><li>Deploy template</li><li>Mark as virtual machine</li><li>Modify customization specification</li><li>Promote disks</li><li>Read customization specifications</li></ul></li></ul></li></ul><h4 id=nsx-t>NSX-T</h4><p>The NSX-T API is accessed from the infrastructure controller of the vsphere-provider for setting up the network infrastructure resources and the cloud-controller-manager for managing load balancers. Currently, the NSX-T user must have the <code>Enterprise Admin</code> role.</p><h3 id=create-folders>Create Folders</h3><p>Two folders need to be created:
- a folder which will contain the VMs of the shoots (cloud profile <code>spec.providerConfig.folder</code>)
- a folder containing templates (used by cloud profile <code>spec.providerConfig.machineImages[*].versions[*].path</code>)</p><p>In vSphere client:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>VMs and Templates</em></li><li>Select the vSphere Datacenter of the work load vCenter in the browser</li><li>From the context menu select <em>New Folder</em> > <em>New VM and Template Folder</em>, set folder name to e.g. &ldquo;gardener&rdquo;</li><li>From the context menu of the new folder <em>gardener</em> select <em>New Folder</em>, set folder name to &ldquo;templates&rdquo;</li></ol><h3 id=upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</h3><p>Upload <a href=https://github.com/gardenlinux/gardenlinux/releases>gardenlinux OVA</a> or
<a href=https://flatcar-linux.org/releases#stable-release>flatcar OVA</a> templates.</p><ol><li>From the context menu of the folder <code>gardener/templates</code> choose <em>Deploy OVF Template&mldr;</em></li><li>Adjust name if needed</li><li>Select any compute cluster as compute resource</li><li>Select a storage (e.g. VSAN)</li><li>Select any network (not important)</li><li>No need to customize the template</li><li>After deployment is finished select from the context menu of the new deployed VM <em>Template</em> > <em>Convert To Template</em></li></ol><h3 id=prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</h3><p>This step has to be done regardless of whether you actually have more than a single region and zone or not!
Two labels need to be defined in the cloud profile (section <code>spec.providerConfig.failureDomainLabels</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    failureDomainLabels:
</span></span><span style=display:flex><span>      region: k8s-region
</span></span><span style=display:flex><span>      zone: k8s-zone
</span></span></code></pre></div><p>A Kubernetes zone can either be a vCenter or one of its datacenters</p><p>Zones must be sub-resources of it. If the region is a complete vCenter, the zone must specify datacenter and either compute cluster or resource pool.
Otherwise, i.e. tf the region is a datacenter, the zone must specify either compute cluster or resource pool.</p><p>In the following steps it is assumed:
- the region is specified by a datacenter
- the zone is specified by a compute cluster or one of its resource pools</p><h4 id=create-resource-pools>Create Resource Pool(s)</h4><p>Create a resource pool for every zone:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>From the context menu of the compute cluster select <em>New Resource Pool&mldr;</em> and provide the name of the zone. CPU and Memory settings are optional.</li></ol><h4 id=tag-regions-and-zones>Tag Regions and Zones</h4><p>Each zone must be tagged with the category defined by the label defined in the cloud profile (<code>spec.providerConfig.failureDomainLabels.region</code>).
Assuming that the region is a datacenter and the region label is <code>k8s-region</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the region&rsquo;s datacenter in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-region</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the region.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the region tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><p>Assuming that the zones are specified by resource pools and the zone label is <code>k8s-zone</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the zone&rsquo;s Compute Cluster in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h4 id=storage-policies>Storage policies</h4><p>Each zone can have a separate storage. In this case a storage policy is needed to be compatible with all the zone storages.</p><h5 id=tag-zone-storages>Tag Zone Storages</h5><p>For each zone tag the storage with the corresponding <code>k8s-zone</code> tag for the zone.</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Storage</em></li><li>Select the zone&rsquo;s storage in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h5 id=create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</h5><ol><li><p>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Policies and Profiles</em></p></li><li><p>In the <em>Policies and Profiles</em> list select <em>VM Storage Policies</em></p></li><li><p>Create or clone an existing storage policy</p><p>a) set name, e.g. &ldquo;<region-name> Storage Policy&rdquo; (will be needed for the cloud profile later in <code>spec.providerConfig.defaultClassStoragePolicyName</code>)</p><p>b) On the page <em>Policy structure</em> check only the checkbox <em>Enable tag based placement rules</em></p><p>c) On the page <em>Tage based placement</em> press the <em>ADD TAG RULE</em> button.</p><p>d) For <em>Rule 1</em> select</p><pre tabindex=0><code>*Tag category* =  *k8s-zone*
*Usage option* = *Use storage tagged with*
*Tags* = *all zone tags*.
</code></pre><p>e) Validate the compatible storages on the page <em>Storage compatibility</em></p><p>f) Press <em>FINISH</em> on the <em>Review and finish</em> page</p></li><li><p><strong>IMPORTANT</strong>: Repeat steps 1-3 and create a second StoragePolicy by the name of <code>garden-etcd-fast-main</code>. This will be used by Gardener to provision shoot&rsquo;s etcd PVCs.</p></li></ol><h2 id=nsx-t-preparation>NSX-T Preparation</h2><p>A shared NSX-T is needed for all zones of a region.
External IP address ranges are needed for SNAT and load balancers.
Besides the edge cluster must be sized large enough to deal with the load balancers of all shoots.</p><h3 id=create-ip-pools>Create IP pools</h3><p>Two IP pools are needed for external IP addresses.</p><ol><li>IP pool for <strong>SNAT</strong>
The IP pool name needs to be specified in the cloud profile at <code>spec.providerConfig.regions[*].snatIPPool</code>. Each shoot cluster needs one SNAT IP address for outgoing traffic.</li><li>IP pool(s) for the <strong>load balancers</strong>
The IP pool name(s) need to be specified in the cloud profile at <code>spec.providerConfig.contraints.loadBalancerConfig.classes[*].ipPoolName</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</li></ol><p>To create them, follow these steps in the NSX-T Manager UI in the web browser:</p><ol><li>From the <em>toolbar</em> at the top of the page choose <em>Networking</em></li><li>From the left side list choose <em>IP Address Pools</em> below the <em>IP Management</em></li><li>Press the <em>ADD IP ADRESS POOL</em> button</li><li>Enter <em>Name</em></li><li>Enter at least one subnet by clicking on <em>Sets</em></li><li>Press the <em>Save</em> button</li></ol><h4 id=sizing-the-ip-pools>Sizing the IP pools</h4><p>Each shoot cluster needs one IP address for SNAT and at least two IP addresses for load balancers VIPs (kube-apiservcer and Gardener shoot-seed VPN). A third IP address may be needed for ingress.
Depending on the payload of a shoot cluster, there may be additional services of type <code>LoadBalancer</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</p><h3 id=check-edge-cluster-sizing>Check edge cluster sizing</h3><p>For load balancer related configurations limitations of NSX-T, please see the web pages <a href="https://configmax.vmware.com/guest?vmwareproduct=NSX-T%20Data%20Center&release=NSX-T%20Data%20Center%203.1.0&categories=20-0">VMWare Configuration Maximums</a>. The link shows the limitations for NSX-T 3.1, if you have another version, please select the version from the left panel under <em>Select Version</em> and press the <em>VIEW LIMITS</em> button to update the view.</p><p>By default, settings, each shoot cluster has an own T1 gateway and an own LB service (instance) of &ldquo;T-shirt&rdquo; size <code>SMALL</code>.</p><p>Examples for limitations on NSX-T 3.1 using <em>Large Edge Node</em> and <em>SMALL</em> load balancers instances:</p><ol><li><p>There is a limit of 40 small LB instances per egde cluster (for HA 40 per pair of edge nodes)</p><p>=> maximum number of shoot clusters = 40 * (number of edge nodes) / 2</p></li><li><p>For <code>SMALL</code> load balancers, there is a maximum of 20 virtual servers. A virtual server is needed for every port of a service of type <code>LoadBalancer</code></p><p>=> maximum number of services/ports pairs = 20 * (number of edge nodes) / 2</p><p>The load balancer &ldquo;T-shirt&rdquo; size can be set on cloud profile level (<code>spec.providerConfig.contraints.loadBalancerConfig.size</code>) or in the shoot manifest (<code>spec.provider.controlPlaneConfig.loadBalancerSize</code>)</p></li><li><p>The number of pool members is limited to 7,500. For every K8s service port, every worker node is a pool member.</p><p>=> If every shoot cluster has an average number of 15 worker nodes, there can be 500 service/port pairs over all shoot clusters per pair of edge nodes</p></li></ol><h2 id=get-vds-uuids>Get VDS UUIDs</h2><p>This step is only needed, if there are several VDS (virtual distributed switches) for each zone.</p><p>In this case, their UUIDs need to be fetched and set in the cloud profile at <code>spec.providerConfig.regions[*].zones[*].switchUuid</code>.</p><p>Unfortunately, they are not displayed in the vSphere Client.</p><p>Here the command line tool <code>govc</code> is used to look them
up.</p><ol><li>Run <code>govc find / -type DistributedVirtualSwitch</code> to get the full path of all vds/dvs</li><li>For each switch run <code>govc dvs.portgroup.info &lt;switch-path> | grep DvsUuid</code></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-fdbdddf9cd1476016152695500fac711>2.1.7.1.4 - Tanzu Vsphere</h1><h2 id=create-tanzu-cluster>Create Tanzu Cluster</h2><p>For gardener a Tanzu Kubernetes „guest” cluster is used. Look here for the vSphere documentation <a href=https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2597788E-2FA4-420E-B9BA-9423F8F7FD9F.html>Provisioning Tanzu Kubernetes Clusters</a></p><h3 id=virtual-machine-classes>Virtual Machine Classes</h3><p>For gardener the minimum Virtual Machine Classes must set to <code>best-effort-large</code>.</p><h3 id=network-settings>Network Settings</h3><p>For the deployment it is possible to provision the cluster with a minimal amount of configuration parameter. It is recommended to set the parameter <code>Default Pod CIDR</code>, <code>Default Services CIDR</code> with values which fit to your enviroment.</p><h3 id=storage-class-settings>Storage Class settings</h3><p>The <code>storageClass</code> Parameter should be defined to avoid problems during deployment.</p><p>Example:</p><pre tabindex=0><code>```yaml
apiVersion: run.tanzu.vmware.com/v1alpha1      #TKG API endpoint
kind: TanzuKubernetesCluster                   #required parameter
metadata:
name: tkg-cluster-1                          #cluster name, user defined
namespace: ns1                               #supervisor namespace
spec:
distribution:
    version: v1.24				 #resolved kubernetes version
topology:
    controlPlane:
    count: 1                                 #number of control plane nodes
    class: best-effort-small                 #vmclass for control plane nodes
    storageClass: vsan-default-storage-policy         #storageclass for control plane
    workers:
    count: 3                                 #number of worker nodes
    class: best-effort-large                 #vmclass for worker nodes
    storageClass: vsan-default-storage-policy         #storageclass for worker nodes
settings:
    network:
    cni:
        name: calico
    services:
        cidrBlocks: [&#34;198.51.100.0/12&#34;]        #Cannot overlap with Supervisor Cluster
    pods:
        cidrBlocks: [&#34;192.0.2.0/16&#34;]           #Cannot overlap with Supervisor Cluster
```
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-cdac9defb035df31c6dfc2cee8cbc281>2.1.7.1.5 - Usage As End User</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-end-user>Using the vSphere provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an vSphere cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your vSphere tenant.
It contains two authentication sets. One for the vSphere host and another for the NSX-T host, which is needed to set up the network infrastructure.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-vsphere
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  vspherePassword: base64(vsphere-password)
</span></span><span style=display:flex><span>  vsphereUsername: base64(vSphere-UserName)
</span></span><span style=display:flex><span>  vsphereInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
</span></span><span style=display:flex><span>  nsxtPassword: base64(NSX-T-password)
</span></span><span style=display:flex><span>  nsxtUserName: base64(NSX-T-UserName)
</span></span><span style=display:flex><span>  nsxtInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
</span></span></code></pre></div><p>Here <code>base64(...)</code> are only a placeholders for the Base64 encoded values.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration is used for advanced scenarios only.
Nodes on all zones are using IP addresses from the common nodes network as the network is managed by NSX-T.
The infrastructure controller will create several network objects using NSX-T. A network segment is used as the subnet
for the VMs (nodes), a tier-1 gateway, a DHCP server, and a SNAT for the nodes.</p><p>An example <code>InfrastructureConfig</code> for the vSphere extension looks as follows.
You only need to specify it, if you either want to use an existing Tier-1 gateway and load balancer service pair
or if you want to overwrite the automatic selection of the NSX-T version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  <span style=color:green>#overwriteNSXTInfraVersion: &#39;1&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#networks:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d</span>
</span></span></code></pre></div><h3 id=advanced-configuration-settings>Advanced configuration settings</h3><h4 id=section-networks>Section networks</h4><p>By default, the infrastructure controller creates a separate Tier-1 gateway for each shoot cluster
and the cloud controller manager (<code>vsphere-cloud-provider</code>) creates a load balancer service.</p><p>If an existing Tier-1 gateway should be used, you can specify its &lsquo;path&rsquo;. In this case, there
must also be a load balancer service defined for this tier-1 gateway and its &lsquo;path&rsquo; needs to be specified, too.
In the NSX-T manager UI, the path of the tier-1 gateway can be found at <code>Networking / Tier-1 Gateways</code>.
Then select <code>Copy path to clipboard</code> from the context menu of the tier-1 gateway
(click on the three vertical dots on the left of the row). Do the same with the
corresponding load balancer at <code>Networking / Load balancing / Tab Load Balancers</code>
For security reasons the referenced Tier-1 gateway in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb
</span></span><span style=display:flex><span>    loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d
</span></span></code></pre></div><p>Please ensure, that the worker nodes cidr (shoot manifest <code>spec.networking.nodes</code>) do not overlap with
other existing segments of the selected tier-1 gateway.</p><h4 id=option-overwritensxtinfraversion>Option overwriteNSXTInfraVersion</h4><p>The option <code>overwriteNSXTInfraVersion</code> can be used to change the network objects created during the initial infrastructure creation.
By default the infra-version is automatically selected according to the NSX-T version. The infra-version <code>'1'</code> is used
for NSX-T 2.5, and infra-version <code>'2'</code> for NSX-T versions >= 3.0. The difference is creation of the the logical DHCP server.
For NSX-T 2.5, only the DHCP server of the &ldquo;Advanced API&rdquo; is usable. For NSX-T >= 3.0 the new DHCP server is default,
but for special purposes infra-version <code>'1'</code> is also allowed.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the vSphere-specific control plane components.
Today, the only component deployed by the vSphere extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>loadBalancerClasses:
</span></span><span style=display:flex><span>  - name: mypubliclbclass
</span></span><span style=display:flex><span>  - name: myprivatelbclass
</span></span><span style=display:flex><span>    ipPoolName: pool42 <span style=color:green># optional overwrite</span>
</span></span><span style=display:flex><span>loadBalancerSize: SMALL
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>loadBalancerClasses</code> optionally defines the load balancer classes to be used.
The specified names must be defined in the constraints section of the cloud profile.
If the list contains a load balancer named &ldquo;default&rdquo;, it is used as the default load balancer.
Otherwise the first one is also the default.
If no classes are specified the default load balancer class is used as defined in the cloud profile constraints section.
If the ipPoolName is overwritten, the corresponding IP pool object in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>The <code>loadBalancerSize</code> is optional and overwrites the default value specified in the cloud profile config.
It must be one of the values <code>SMALL</code>, <code>MEDIUM</code>, or <code>LARGE</code>. <code>SMALL</code> can manage 10 service ports,
<code>MEDIUM</code> 100, and <code>LARGE</code> 1000.</p><p>The <code>cloudControllerManager.featureGates</code> contains an optional map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-vsphere
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: vsphere
</span></span><span style=display:flex><span>  region: europe-1
</span></span><span style=display:flex><span>  secretBindingName: core-vsphere
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: vsphere
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>    <span style=color:green>#infrastructureConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  kind: InfrastructureConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  overwriteNSXTInfraVersion: &#39;1&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    <span style=color:green>#  loadBalancerClasses:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  - name: mylbclass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: std-04
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-vsphere@v0.12</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-vsphere@v0.13</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-vsphere@v0.14</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0e203669ce71e2ac15660f0895763756>2.1.7.1.6 - Usage As Operator</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-operator>Using the vSphere provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating vSphere shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image paths in the vSphere environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the vSphere extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>The <code>dhcpOptions</code> list allows to specify DHCP options. See <a href=https://www.iana.org/assignments/bootp-dhcp-parameters/bootp-dhcp-parameters.xhtml>BOOTP Vendor Extensions and DHCP Options</a>
for valid codes (tags) and details about values. The code <code>15</code> (domain name) is only allowed for
when using NSX-T 2.5. For NSX-T >= 3.0 use <code>119</code> (search domain).</p><p>The <code>dockerDaemonOptions</code> allow to adjust the docker daemon configuration.</p><ul><li>with <code>dockerDaemonOptions.httpProxyConf</code> the content of the proxy configuration file can be set.
See <a href=https://docs.docker.com/config/daemon/systemd/#httphttps-proxy>Docker HTTP/HTTPS proxy</a> for more details</li><li>with <code>dockerDaemonOptions.insecureRegistries</code> insecure registries can be specified. This
should only be used for development or evaluation purposes.</li></ul><p>Also, you have to specify several name of NSX-T objects in the constraints.</p><p>An example <code>CloudProfileConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>namePrefix: my_gardener
</span></span><span style=display:flex><span>defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
</span></span><span style=display:flex><span>folder: my-vsphere-vm-folder
</span></span><span style=display:flex><span>regions:
</span></span><span style=display:flex><span>- name: region1
</span></span><span style=display:flex><span>  vsphereHost: my.vsphere.host
</span></span><span style=display:flex><span>  vsphereInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  nsxtHost: my.vsphere.host
</span></span><span style=display:flex><span>  nsxtInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
</span></span><span style=display:flex><span>  logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
</span></span><span style=display:flex><span>  edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
</span></span><span style=display:flex><span>  snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
</span></span><span style=display:flex><span>  datacenter: my-vsphere-dc
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: zone1
</span></span><span style=display:flex><span>    computeCluster: my-vsphere-computecluster1
</span></span><span style=display:flex><span>    <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    datastore: my-vsphere-datastore1
</span></span><span style=display:flex><span>    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>  - name: zone2
</span></span><span style=display:flex><span>    computeCluster: my-vsphere-computecluster2
</span></span><span style=display:flex><span>    <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    datastore: my-vsphere-datastore2
</span></span><span style=display:flex><span>    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>constraints:
</span></span><span style=display:flex><span>  loadBalancerConfig:
</span></span><span style=display:flex><span>    size: MEDIUM
</span></span><span style=display:flex><span>    classes:
</span></span><span style=display:flex><span>    - name: default
</span></span><span style=display:flex><span>      ipPoolName: gardener_lb_vip
</span></span><span style=display:flex><span><span style=color:green># optional DHCP options like 119 (search domain), 42 (NTP), 15 (domain name (only NSX-T 2.5))</span>
</span></span><span style=display:flex><span><span style=color:green>#dhcpOptions:</span>
</span></span><span style=display:flex><span><span style=color:green>#- code: 15</span>
</span></span><span style=display:flex><span><span style=color:green>#  values:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - foo.bar.com</span>
</span></span><span style=display:flex><span><span style=color:green>#- code: 42</span>
</span></span><span style=display:flex><span><span style=color:green>#  values:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 136.243.202.118</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 80.240.29.124</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 78.46.53.8</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 162.159.200.123</span>
</span></span><span style=display:flex><span>dnsServers:
</span></span><span style=display:flex><span>- 10.10.10.11
</span></span><span style=display:flex><span>- 10.10.10.12
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: flatcar
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 3139.2.3
</span></span><span style=display:flex><span>    path: gardener/templates/flatcar-3139.2.3
</span></span><span style=display:flex><span>    guestId: other4xLinux64Guest
</span></span><span style=display:flex><span><span style=color:green>#dockerDaemonOptions:</span>
</span></span><span style=display:flex><span><span style=color:green>#  httpProxyConf: |</span>
</span></span><span style=display:flex><span><span style=color:green>#    [Service]</span>
</span></span><span style=display:flex><span><span style=color:green>#    Environment=&#34;HTTPS_PROXY=https://proxy.example.com:443&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#  insecureRegistries:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - myregistrydomain.com:5000</span>
</span></span><span style=display:flex><span><span style=color:green>#  - blabla.mycompany.local</span>
</span></span></code></pre></div><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vsphere
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: vsphere
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    namePrefix: my_gardener
</span></span><span style=display:flex><span>    defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
</span></span><span style=display:flex><span>    folder: my-vsphere-vm-folder
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: region1
</span></span><span style=display:flex><span>      vsphereHost: my.vsphere.host
</span></span><span style=display:flex><span>      vsphereInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      nsxtHost: my.vsphere.host
</span></span><span style=display:flex><span>      nsxtInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
</span></span><span style=display:flex><span>      logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
</span></span><span style=display:flex><span>      edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
</span></span><span style=display:flex><span>      snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
</span></span><span style=display:flex><span>      datacenter: my-vsphere-dc
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - name: zone1
</span></span><span style=display:flex><span>        computeCluster: my-vsphere-computecluster1
</span></span><span style=display:flex><span>        <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        datastore: my-vsphere-datastore1
</span></span><span style=display:flex><span>        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>      - name: zone2
</span></span><span style=display:flex><span>        computeCluster: my-vsphere-computecluster2
</span></span><span style=display:flex><span>        <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        datastore: my-vsphere-datastore2
</span></span><span style=display:flex><span>        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>    constraints:
</span></span><span style=display:flex><span>      loadBalancerConfig:
</span></span><span style=display:flex><span>        size: MEDIUM
</span></span><span style=display:flex><span>        classes:
</span></span><span style=display:flex><span>        - name: default
</span></span><span style=display:flex><span>          ipPoolName: gardener_lb_vip
</span></span><span style=display:flex><span>    dnsServers:
</span></span><span style=display:flex><span>    - 10.10.10.11
</span></span><span style=display:flex><span>    - 10.10.10.12
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 3139.2.3
</span></span><span style=display:flex><span>        path: gardener/templates/flatcar-3139.2.3
</span></span><span style=display:flex><span>        guestId: other4xLinux64Guest
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.23.4
</span></span><span style=display:flex><span>    - version: 1.24.0
</span></span><span style=display:flex><span>    - version: 1.24.1
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: flatcar
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 3139.2.3
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: std-02
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: std-04
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 16Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: std-08
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;8&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 32Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: region1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: zone1
</span></span><span style=display:flex><span>    - name: zone2
</span></span></code></pre></div><h2 id=which-versions-of-kubernetesvsphere-are-supported>Which versions of Kubernetes/vSphere are supported</h2><p>This extension targets Kubernetes >= <code>v1.20</code> and vSphere <code>6.7 U3</code> or later.</p><ul><li>vSphere CSI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.16</code>
(see <a href=https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/index.html>VMware vSphere Container Storage Plug-in</a> for more details)</li><li>vSpere CPI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.11</code>
(see <a href=https://github.com/kubernetes/cloud-provider-vsphere/blob/master/docs/book/cloud_provider_interface.md#which-versions-of-kubernetesvsphere-support-it>cloud-provider-vsphere CPI - Cloud Provider Interface</a>)</li></ul><h2 id=supported-vm-images>Supported VM images</h2><p>Currently, only Gardenlinux and Flatcar (CoreOS fork) are supported.
Virtual Machine Hardware must be version 15 or higher, but images are upgraded
automatically if their hardware has an older version.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cf44505b1a6fee28c6d8a49feb963c88>2.2 - Operating System Extensions</h1><div class=lead>Gardener extension controllers for the supported operating systems</div></div><div class=td-content><h1 id=pg-cd6184375a7182a45366dc72ad6e13d1>2.2.1 - CoreOS/FlatCar OS</h1><div class=lead>Gardener extension controller for the CoreOS/FlatCar Container Linux operating system</div><h1 id=gardener-extension-for-coreos-container-linuxhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for CoreOS Container Linux</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-coreos-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-coreos-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-coreos><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-coreos alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><p>This controller operates on the <code>OperatingSystemConfig</code> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It supports <a href=https://coreos.com/os/docs/latest/>CoreOS Container Linux</a> and <a href=https://www.flatcar-linux.org/>Flatcar Container Linux</a> (&ldquo;a friendly fork of CoreOS Container Linux&rdquo;).</p><p>The controller manages those objects that are requesting <a href=https://coreos.com/os/docs/latest/>CoreOS Container Linux</a> configuration (<code>.spec.type=coreos</code>) or <a href=https://www.flatcar-linux.org/>Flatcar Container Linux</a> configuration (<code>.spec.type=flatcar</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: coreos
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-coreos/blob/master/example/40-operatingsystemconfig.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/coreos-cloudinit -from-file=&lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-coreos/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-coreos/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f0372020becfbe84c4e4f4935adc4ba5>2.2.1.1 - Usage As End User</h1><h1 id=using-the-coreos-extension-with-gardener-as-end-user>Using the CoreOS extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-coreos-workers>AWS VPC settings for CoreOS workers</h2><p>Gardener allows you to create CoreOS based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise CoreOS based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true (necessary for collecting <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/>node metrics</a>)</li><li><code>enableDnsSupport</code>: true</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0aede9ce025bb447b055d2593567eef6>2.2.2 - Garden Linux OS</h1><div class=lead>Gardener extension controller for the Garden Linux operating system</div><h1 id=gardener-extension-for-garden-linux-oshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Garden Linux OS</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-gardenlinux-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-gardenlinux-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-gardenlinux><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-gardenlinux alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://gardenlinux.io/>Garden Linux OS</a> configuration (<code>.spec.type=gardenlinux</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: gardenlinux
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/blob/master/example/40-operatingsystemconfig.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/blob/master/example/controller-registration.yaml>here</a>.</p><p>This controller is based on revision <a href=https://github.com/gardener/gardener-extension-os-ubuntu-alicloud/commit/b5ba8164ed4c52872b1d4bd5ee3eaa4ed58da966>b5ba8164</a> of <a href=https://github.com/gardener/gardener-extension-os-ubuntu-alicloud>gardener-extension-os-ubuntu-alicloud</a>. Its implementation is using the <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e032feff371332908baf100fa0bb6350>2.2.3 - SUSE CHost OS</h1><div class=lead>Gardener extension controller for the SUSE Container Host operating system (CHost)</div><h1 id=gardener-extension-for-suse-chosthttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for SUSE CHost</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-suse-chost-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-suse-chost-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-suse-chost><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-suse-chost alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting SUSE Container Host configuration, i.e. <code>suse-chost</code> type:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: suse-chost
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-suse-chost/blob/master/example/40-operatingsystemconfig-chost.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>It is also capable of supporting the <a href=https://www.scalemp.com/products/vsmp-memoryone/>vSMP MemoryOne</a> operating system with the <code>memoryone-chost</code> type. Please find more information <a href=/docs/extensions/os-extensions/gardener-extension-os-suse-chost/docs/usage-as-end-user/#support-for-vsmp-memoryone>here</a>.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-suse-chost/blob/master/example/controller-registration.yaml>here</a>.</p><p>This controller is implemented using the <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-suse-chost/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4c716cf75a12831ab8c55b04e48e8330>2.2.3.1 - Usage As End User</h1><h1 id=using-the-suse-chost-extension-with-gardener-as-end-user>Using the SuSE CHost extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-suse-chost-workers>AWS VPC settings for SuSE CHost workers</h2><p>Gardener allows you to create SuSE CHost based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise SuSE CHost based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true</li><li><code>enableDnsSupport</code>: true</li></ul><h2 id=support-for-vsmp-memoryone>Support for vSMP MemoryOne</h2><p>This extension controller is also capable of generating user-data for the <a href=https://www.scalemp.com/products/vsmp-memoryone/>vSMP MemoryOne</a> operating system in conjunction with SuSE CHost.
It reacts on the <code>memoryone-chost</code> extension type.
Additionally, it allows certain customizations with the following configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfiguration
</span></span><span style=display:flex><span>memoryTopology: <span style=color:#a31515>&#34;3&#34;</span>
</span></span><span style=display:flex><span>systemMemory: <span style=color:#a31515>&#34;7x&#34;</span>
</span></span></code></pre></div><ul><li>The <code>memoryTopology</code> field controls the <code>mem_topology</code> setting. If it&rsquo;s not provided then it will default to <code>2</code>.</li><li>The <code>systemMemory</code> field controls the <code>system_memory</code> setting. If it&rsquo;s not provided then it defaults to <code>6x</code>.</li></ul><p>Please note that it was only e2e-tested on AWS.
Additionally, you need a snapshot ID of a SuSE CHost/CHost volume (see below how to create it).</p><p>An exemplary worker pool configuration inside a <code>Shoot</code> resource using for the vSMP MemoryOne operating system would look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vsmp-memoryone
</span></span><span style=display:flex><span>  namespace: garden-foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  workers:
</span></span><span style=display:flex><span>  - name: cpu-worker3
</span></span><span style=display:flex><span>    minimum: 1
</span></span><span style=display:flex><span>    maximum: 1
</span></span><span style=display:flex><span>    maxSurge: 1
</span></span><span style=display:flex><span>    maxUnavailable: 0
</span></span><span style=display:flex><span>    machine:
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: memoryone-chost
</span></span><span style=display:flex><span>        version: 9.5.195
</span></span><span style=display:flex><span>        providerConfig:
</span></span><span style=display:flex><span>          apiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>          kind: OperatingSystemConfiguration
</span></span><span style=display:flex><span>          memoryTopology: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>          systemMemory: <span style=color:#a31515>&#34;6x&#34;</span>
</span></span><span style=display:flex><span>      type: c5d.metal
</span></span><span style=display:flex><span>    volume:
</span></span><span style=display:flex><span>      size: 20Gi
</span></span><span style=display:flex><span>      type: gp2
</span></span><span style=display:flex><span>    dataVolumes:
</span></span><span style=display:flex><span>    - name: chost
</span></span><span style=display:flex><span>      size: 50Gi
</span></span><span style=display:flex><span>      type: gp2
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: WorkerConfig
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: chost
</span></span><span style=display:flex><span>        snapshotID: snap-123456
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-central-1b
</span></span></code></pre></div><p>Please note that vSMP MemoryOne only works for EC2 bare-metal instance types such as <code>M5d</code>, <code>R5</code>, <code>C5</code>, <code>C5d</code>, etc. - please consult <a href=https://aws.amazon.com/ec2/instance-types/>the EC2 instance types overview page</a> and the documentation of vSMP MemoryOne to find out whether the instance type in question is eligible.</p><h3 id=generating-an-aws-snapshot-id-for-the-chostchost-operating-system>Generating an AWS snapshot ID for the CHost/CHost operating system</h3><p>The following script will help to generate the snapshot ID on AWS.
It runs in the region that is selected in your <code>$HOME/.aws/config</code> file.
Consequently, if you want to generate the snapshot in multiple regions, you have to run in multiple times after configuring the respective region using <code>aws configure</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ami=<span style=color:#a31515>&#34;ami-1234&#34;</span> <span style=color:green>#Replace the ami with the intended one. </span>
</span></span><span style=display:flex><span>name=<span style=color:#a31515>`</span>aws ec2 describe-images --image-ids $ami  --query=<span style=color:#a31515>&#34;Images[].Name&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>cur=<span style=color:#a31515>`</span>aws ec2 describe-snapshots --filter=<span style=color:#a31515>&#34;Name=description,Values=snap-</span>$name<span style=color:#a31515>&#34;</span> --query=<span style=color:#a31515>&#34;Snapshots[].Description&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> [ -n <span style=color:#a31515>&#34;</span>$cur<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>  echo <span style=color:#a31515>&#34;AMI </span>$name<span style=color:#a31515> exists as snapshot </span>$cur<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span><span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;AMI </span>$name<span style=color:#a31515> ... creating private snapshot&#34;</span>
</span></span><span style=display:flex><span>inst=<span style=color:#a31515>`</span>aws ec2 run-instances --instance-type t3.nano --image-id $ami --query <span style=color:#a31515>&#39;Instances[0].InstanceId&#39;</span> --output=text --subnet-id subnet-1234 --tag-specifications <span style=color:#a31515>&#39;ResourceType=instance,Tags=[{Key=scalemp-test,Value=scalemp-test}]&#39;</span><span style=color:#a31515>`</span> <span style=color:green>#Replace the subnet-id with the intended one.</span>
</span></span><span style=display:flex><span>aws ec2 wait instance-running --instance-ids $inst 
</span></span><span style=display:flex><span>vol=<span style=color:#a31515>`</span>aws ec2 describe-instances --instance-ids $inst --query <span style=color:#a31515>&#34;Reservations[].Instances[].BlockDeviceMappings[0].Ebs.VolumeId&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>snap=<span style=color:#a31515>`</span>aws ec2 create-snapshot --description <span style=color:#a31515>&#34;snap-</span>$name<span style=color:#a31515>&#34;</span> --volume-id $vol --query=<span style=color:#a31515>&#39;SnapshotId&#39;</span> --tag-specifications <span style=color:#a31515>&#34;ResourceType=snapshot,Tags=[{Key=Name,Value=\&#34;</span>$name<span style=color:#a31515>\&#34;}]&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>aws ec2 wait snapshot-completed --snapshot-ids $snap
</span></span><span style=display:flex><span>aws ec2 terminate-instances --instance-id $inst &gt; /dev/null
</span></span><span style=display:flex><span>echo $snap
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2d1349b5e5eed4ce62ec06792f100731>2.2.4 - Ubuntu OS</h1><div class=lead>Gardener extension controller for the Ubuntu operating system</div><h1 id=gardener-extension-for-ubuntu-oshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Ubuntu OS</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-ubuntu-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-ubuntu-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-ubuntu><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-ubuntu alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://www.ubuntu.com/>Ubuntu OS</a> configuration (<code>.spec.type=ubuntu</code>). An experimental support for Ubuntu Pro is added (<code>.spec.type=ubuntu-pro</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: ubuntu
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-ubuntu/blob/master/example/40-operatingsystemconfig.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-ubuntu/blob/master/example/controller-registration.yaml>here</a>.</p><p>This controller is implemented using the <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-ubuntu/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a57bbc2de67841c5dcff59fcea4417ba>2.2.4.1 - Usage As End User</h1><h1 id=using-the-ubuntu-extension-with-gardener-as-end-user>Using the Ubuntu extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-ubuntu-workers>AWS VPC settings for Ubuntu workers</h2><p>Gardener allows you to create Ubuntu based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise Ubuntu based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true</li><li><code>enableDnsSupport</code>: true</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-55b1ec84af5ed73ab7f671bfe8da200e>2.3 - Network Extensions</h1><div class=lead>Gardener extension controllers for the supported container network interfaces</div></div><div class=td-content><h1 id=pg-306b134fc579fc34549bdcef99158c9b>2.3.1 - Calico CNI</h1><div class=lead>Gardener extension controller for the Calico CNI network plugin</div><h1 id=gardener-extension-for-calico-networkinghttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Calico Networking</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-networking-calico-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-networking-calico-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-networking-calico><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-networking-calico alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking-extensibility.md#gardener-network-extension><code>Network</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://www.projectcalico.org/>Calico Networking</a> configuration (<code>.spec.type=calico</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: calico-network
</span></span><span style=display:flex><span>  namespace: shoot--core--test-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: calico
</span></span><span style=display:flex><span>  clusterCIDR: 192.168.0.0/24
</span></span><span style=display:flex><span>  serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span>    overlay:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-networking-calico/blob/master/example/20-network.yaml>a concrete example</a> in the <code>example</code> folder. All the <code>Calico</code> specific configuration
should be configured in the <code>providerConfig</code> section. If additional configuration is required, it should be added to
the <code>networking-calico</code> chart in <code>controllers/networking-calico/charts/internal/calico/values.yaml</code> and corresponding code
parts should be adapted (for example in <code>controllers/networking-calico/pkg/charts/utils.go</code>).</p><p>Once the network resource is applied, the <code>networking-calico</code> controller would then create all the necessary <code>managed-resources</code> which should be picked
up by the <a href=https://github.com/gardener/gardener-resource-manager>gardener-resource-manager</a> which will then apply all the
network extensions resources to the shoot cluster.</p><p>Finally after successful reconciliation an output similar to the one below should be expected.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  status:
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      description: Successfully reconciled network
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;...&#34;</span>
</span></span><span style=display:flex><span>      progress: 100
</span></span><span style=display:flex><span>      state: Succeeded
</span></span><span style=display:flex><span>      type: Reconcile
</span></span><span style=display:flex><span>    observedGeneration: 1
</span></span><span style=display:flex><span>    providerStatus:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkStatus
</span></span></code></pre></div><h2 id=compatibility>Compatibility</h2><p>The following table lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>Calico Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>>= v1.30.0</code></td><td><code>&lt; v1.63.0</code></td><td>Please first update Gardener components to <code>>= v1.63.0</code>.</td><td>Without the mentioned minimum Gardener version, Calico <code>Pod</code>s are not only scheduled to dedicated system component nodes in the shoot cluster.</td></tr></tbody></table><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the <code>kubeconfig</code> pointed to the cluster you want to connect to.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-networking-calico/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6a7fc305607605cf28032e069053cc1c>2.3.1.1 - Deployment</h1><h1 id=deployment-of-the-networking-calico-extension>Deployment of the networking Calico extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step deployment guide for the networking Calico extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the networking Calico extension <a href=https://github.com/gardener/gardener-extension-networking-calico>repository</a>.</p><h2 id=gardener-extension-admission-calico>gardener-extension-admission-calico</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of Virtual Garden</a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster>Virtual Garden is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><h5 id=automounted-service-account-token>Automounted Service Account Token</h5><p>The easiest way to deploy the <code>gardener-extension-admission-calico</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><h5 id=service-account-token-volume-projection>Service Account Token Volume Projection</h5><p>Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster>Virtual Garden is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><h5 id=service-account>Service Account</h5><p>The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=client-certificate>Client Certificate</h5><p>Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=projected-service-account-token>Projected Service Account Token</h5><p>This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-343f6194cbd44bce333f5019b2a83934>2.3.1.2 - Shoot Overlay Network</h1><h1 id=enable--disable-overlay-network-for-shoots-with-calico>Enable / disable overlay network for shoots with Calico</h1><p>Gardener can be used with or without the overlay network.</p><p>Starting versions:</p><ul><li><a href=https://github.com/gardener/gardener-extension-provider-gcp/releases/tag/v1.25.0>provider-gcp@v1.25.0</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-alicloud/tag/v1.43.0>provider-alicloud@v1.43.0</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/releases/tag/v1.38.2>provider-aws@v1.38.2</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-openstack/releases/tag/v1.30.0>provider-openstack@v1.30.0</a></li></ul><p>The default configuration of shoot clusters is without overlay network.</p><h2 id=understanding-overlay-network>Understanding overlay network</h2><p>The Overlay networking permits the routing of packets between multiples pods located on multiple nodes, even if the pod and the node network are not the same.</p><p>This is done through the encapsulation of pod packets in the node network so that the routing can be done as usual. We use <code>ipip</code> encapsulation with calico in case the overlay network is enabled. This (simply put) sends an IP packet as workload in another IP packet.</p><p><img src=/__resources/Overlay-Network.drawio_f88c6f.png alt></p><p>In order to simplify the troubleshooting of problems and reducing the latency of packets travelling between nodes, the overlay network is disabled by default for extension default as stated above for all new clusters.</p><p><img src=/__resources/No-Overlay-Network.drawio_f5b3bd.png alt></p><p>This means that the routing is done directly through the VPC routing table. Basically, when a new node it created, it is assigned a slice (usually a /24) of the pod network. All future pods in that node are going to be in this slice. Then, the cloud-controller-manager updated the cloud provider router to add the new route (all packets with in the network slice as destination should go to that node).</p><p>This has the advantage of:</p><ul><li>Doing less work for the node as encapsulation takes some CPU cycles.</li><li>The maximum transmission unit (MTU) is slightly bigger resulting in slightly better performance, i.e. potentially more workload bytes per packet.</li><li>More direct and simpler setup, which makes the problems much easier to troubleshoot.</li></ul><h2 id=enabling-the-overlay-network>Enabling the overlay network</h2><p>In certain cases, the overlay network might be preferable if, for example, the customer wants to create multiple clusters in the same VPC without ensuring there&rsquo;s no overlap between the pod networks.</p><p><strong>In that case (multiple shoot in the same VPC), if the pod&rsquo;s network is not configured properly, there is a very strong chance that some pod IP address might overlap, which is going to cause all sorts of funny problems.</strong> So, if someone asks you how to avoid that, they need to make sure that the podCIDR for each shoot <strong>do not overlap with each other</strong>.</p><p>To enable the overlay network, add the following to the shoot&rsquo;s YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=disabling-the-overlay-network>Disabling the overlay network</h2><p>Inversely, here is how to disable the overlay network:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=how-to-know-if-a-cluster-is-using-overlay-or-not>How to know if a cluster is using overlay or not?</h2><p>You can look at any of the old nodes. If there are tunl0 devices at least at some point in time the overlay network was used.
Another way is to look into the Network object in the shoot&rsquo;s control plane namespace on the seed (see example above).</p><h2 id=do-we-have-some-documentation-somewhere-on-how-to-do-the-migration>Do we have some documentation somewhere on how to do the migration?</h2><p>No, not yet. The migration from no overlay to overlay is fairly simply by just setting the configuration as specified above. The other way is more complicated as the Network configuration needs to be changed AND the local routes need to be cleaned.
Unfortunately, the change will be rolled out slowly (one calico-node at a time). Hence, it implies some network outages during the migration.</p><h2 id=aws-implementation>AWS implementation</h2><p>On AWS, it is not possible to use the cloud-controller-manager for managing the routes as it does not support multiple route tables, which Gardener creates. Therefore, a custom controller is created to manage the routes.</p><p>Also, the removal of the overlay network is only possible with Kubernetes >= 1.22. This is due to the machine-controller-manager only setting the source/destination flag accordingly for these Kubernetes versions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-698188e9db6f94e648cef00c2404ef33>2.3.1.3 - Usage As End User</h1><h1 id=using-the-networking-calico-extension-with-gardener-as-end-user>Using the Networking Calico extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a <code>networking</code> field that is meant to contain network-specific configuration.</p><p>In this document we are describing how this configuration looks like for Calico and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create a cluster.</p><h2 id=calico-typha>Calico Typha</h2><p>Calico Typha is an optional component of Project Calico designed to offload the Kubernetes API server. The Typha daemon sits between the datastore (such as the Kubernetes API server which is the one used by Gardener managed Kubernetes) and many instances of Felix. Typha’s main purpose is to increase scale by reducing each node’s impact on the datastore. You can opt-out Typha via <code>.spec.networking.providerConfig.typha.enabled=false</code> of your Shoot manifest. By default the Typha is enabled.</p><h2 id=ebpf-dataplane>EBPF Dataplane</h2><p>Calico can be run in ebpf dataplane mode. This has several benefits, calico scales to higher troughput, uses less cpu per GBit and has native support for kubernetes services (without needing kube-proxy).
To switch to a pure ebpf dataplane it is recommended to run without an overlay network. The following configuration can be used to run without an overlay and without kube-proxy.</p><p>An example ebpf dataplane <code>NetworkingConfig</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>ebpfDataplane:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>overlay:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>To disable kube-proxy set the enabled field to false in the shoot manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: ebpf-shoot
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeProxy:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><h2 id=example-networkingconfig-manifest>Example <code>NetworkingConfig</code> manifest</h2><p>An example <code>NetworkingConfig</code> for the Calico extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>ipam:
</span></span><span style=display:flex><span>  type: host-local
</span></span><span style=display:flex><span>  cidr: usePodCIDR
</span></span><span style=display:flex><span>vethMTU: 1440
</span></span><span style=display:flex><span>typha:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>overlay:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest with calico networking configratations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      ipam:
</span></span><span style=display:flex><span>        type: host-local
</span></span><span style=display:flex><span>      vethMTU: 1440
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      typha:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-58100f8e5ed774c092d58df15d7c4cbf>2.3.1.4 - Usage As Operator</h1><h1 id=using-the-calico-networking-extension-with-gardener-as-operator>Using the Calico networking extension with Gardener as operator</h1><p>This document explains configuration options supported by the networking-calico extension.</p><h3 id=run-calico-node-in-non-privileged-and-non-root-mode>Run calico-node in non-privileged and non-root mode</h3><p><strong>Feature State</strong>: <code>Alpha</code></p><h5 id=motivation>Motivation</h5><p>Running containers in privileged mode is not recommended as privileged containers run with all <a href=https://man7.org/linux/man-pages/man7/capabilities.7.html>linux capabilities</a> enabled and can access the host&rsquo;s resources. Running containers in privileged mode opens number of security threats such as breakout to underlying host OS.</p><h5 id=support-for-non-privileged-and-non-root-mode>Support for non-privileged and non-root mode</h5><p>The Calico project has a preliminary support for running the calico-node component in non-privileged mode (see <a href=https://projectcalico.docs.tigera.io/security/non-privileged>this guide</a>). Similar to <a href=https://github.com/tigera/operator>Tigera Calico operator</a> the networking-calico extension can also run calico-node in non-privileged and non-root mode. This feature is controller via feature gate named <code>NonPrivilegedCalicoNode</code>. The feature gates are configured in the <a href=https://github.com/gardener/gardener-extension-networking-calico/blob/master/example/00-componentconfig.yaml>ControllerConfiguration</a> of networking-calico. The corresponding ControllerDeployment configuration that enables the <code>NonPrivilegedCalicoNode</code> would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: networking-calico
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    chart: &lt;omitted&gt;
</span></span><span style=display:flex><span>    config:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        NonPrivilegedCalicoNode: <span style=color:#00f>false</span>
</span></span></code></pre></div><h5 id=limitations>Limitations</h5><ul><li>The support for the non-privileged mode in the Calico project is not ready for productive usage. The <a href=https://projectcalico.docs.tigera.io/security/non-privileged>upstream documentation</a> states that in non-privileged mode the support for features added after Calico v3.21 is not guaranteed.</li><li>Calico in non-privileged mode does not support eBPF dataplane. That&rsquo;s why when eBPF dataplane is enabled, calico-node has to run in privileged mode (even when the <code>NonPrivilegedCalicoNode</code> feature gate is enabled).</li><li>(At the time of writing this guide) there is the following issue <a href=https://github.com/projectcalico/calico/issues/5348>projectcalico/calico#5348</a> that is not addressed.</li><li>(At the time of writing this guide) the upstream adoptions seems to be low. The Calico charts and manifest in <a href=https://github.com/projectcalico/calico>projectcalico/calico</a> run calico-node in privileged mode.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4892da3b15a7be1985453914cbefd13a>2.3.2 - Cilium CNI</h1><div class=lead>Gardener extension controller for the Cilium CNI network plugin</div><h1 id=gardener-extension-for-cilium-networkinghttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for cilium Networking</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-networking-cilium-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-networking-cilium-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-networking-cilium><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-networking-cilium alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking-extensibility.md#gardener-network-extension><code>Network</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://cilium.io/>cilium Networking</a> configuration (<code>.spec.type=cilium</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cilium-network
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: cilium
</span></span><span style=display:flex><span>  podCIDR: 10.244.0.0/16
</span></span><span style=display:flex><span>  serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span><span style=color:green>#    hubble:</span>
</span></span><span style=display:flex><span><span style=color:green>#      enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#    store: kubernetes</span>
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-networking-cilium/blob/master/example/20-network.yaml>a concrete example</a> in the <code>example</code> folder. All the <code>cilium</code> specific configuration
should be configured in the <code>providerConfig</code> section. If additional configuration is required, it should be added to
the <code>networking-cilium</code> chart in <code>controllers/networking-cilium/charts/internal/cilium/values.yaml</code> and corresponding code
parts should be adapted (for example in <code>controllers/networking-cilium/pkg/charts/utils.go</code>).</p><p>Once the network resource is applied, the <code>networking-cilium</code> controller would then create all the necessary <code>managed-resources</code> which should be picked
up by the <a href=https://github.com/gardener/gardener-resource-manager>gardener-resource-manager</a> which will then apply all the
network extensions resources to the shoot cluster.</p><p>Finally after successful reconciliation an output similar to the one below should be expected.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  status:
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      description: Successfully reconciled network
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;...&#34;</span>
</span></span><span style=display:flex><span>      progress: 100
</span></span><span style=display:flex><span>      state: Succeeded
</span></span><span style=display:flex><span>      type: Reconcile
</span></span><span style=display:flex><span>    observedGeneration: 1
</span></span></code></pre></div><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the <code>kubeconfig</code> pointed to the cluster you want to connect to.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-networking-cilium/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://docs.cilium.io/>Docs for <code>cilium</code> user</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5aaf52271cfc68c80f23ccf2888af4c2>2.3.2.1 - Usage As End User</h1><h1 id=using-the-networking-cilium-extension-with-gardener-as-end-user>Using the Networking Cilium extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a <code>networking</code> field that is meant to contain network-specific configuration.</p><p>In this document we are describing how this configuration looks like for Cilium and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create a cluster.</p><h2 id=cilium-hubble>Cilium Hubble</h2><p>Hubble is a fully distributed networking and security observability platform build on top of Cilium and BPF. It is optional and is deployed to the cluster when enabled in the <code>NetworkConfig</code>.
If the dashboard is not externally exposed</p><pre tabindex=0><code>kubectl port-forward -n kube-system deployment/hubble-ui 8081
</code></pre><p>can be used to acess it locally.</p><h2 id=example-networkingconfig-manifest>Example <code>NetworkingConfig</code> manifest</h2><p>An example <code>NetworkingConfig</code> for the Cilium extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>hubble:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green>#debug: false</span>
</span></span><span style=display:flex><span><span style=color:green>#psp: true</span>
</span></span><span style=display:flex><span><span style=color:green>#tunnel: vxlan</span>
</span></span><span style=display:flex><span><span style=color:green>#store: kubernetes</span>
</span></span></code></pre></div><h2 id=networkingconfig-options><code>NetworkingConfig</code> options</h2><p>The <code>hubble.enabled</code> field describes whether hubble should be deployed into the cluster or not (default).</p><p>The <code>debug</code> field describes whether you want to run cilium in debug mode or not (default), change this value to <code>true</code> to use debug mode.</p><p>The <code>psp</code> field describes whether <code>cilium-operator</code> and <code>cilium-agent</code> shall be deployed with pod security policies or not (default).</p><p>The <code>tunnel</code> field describes the encapsulation mode for communication between nodes. Possible values are <code>vxlan</code> (default), <code>geneve</code> or <code>disabled</code>.</p><p>The <code>bpfSocketLBHostnsOnly.enabled</code> field describes wheter socket LB will be skipped for services when inside a pod namespace (default), in favor of service LB at the pod interface. Socket LB is still used when in the host namespace. This feature is required when using cilium with a service mesh like istio or linkerd.</p><p>The <code>egressGateway.enabled</code> field describes wheter egress gateways are enabled or not (default). To use this feature kube-proxy must be disabled. This can be done with the following configuration in the shoot.yaml file:</p><pre tabindex=0><code>spec:
  kubernetes:
    kubeProxy:
      enabled: false
</code></pre><p>The <code>snatToUpstreamDNS.enabled</code> field describes wheter the traffic to the upstream dns server should be masqueraded or not (default). This is needed on some infrastructures where traffic to the dns server with the pod CIDR range is blocked.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest with cilium networking configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws-cilium
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: cilium
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      hubble:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>If you would like to see a provider specific shoot example, please check out the documentation of the well-known extensions. A list of them can be found <a href=https://github.com/gardener/gardener/tree/master/extensions#infrastructure-provider>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-43f4dc9f68e5519a75d0b2b944be89d1>2.4 - Container Runtime Extensions</h1><div class=lead>Gardener extensions for the supported container runtime interfaces</div></div><div class=td-content><h1 id=pg-60e80e50f871d6bf3e0bd35d36ca22c2>2.4.1 - GVisor container runtime</h1><div class=lead>Gardener extension controller for the gVisor container runtime sandbox</div><h1 id=gardener-extension-for-the-gvisor-container-runtime-sandboxhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for the gVisor Container Runtime Sandbox</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-runtime-gvisor-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-runtime-gvisor-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-runtime-gvisor><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-runtime-gvisor alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-runtime-gvisor/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/10-shoot-additional-container-runtimes.md>GEP-10 (Additional Container Runtimes)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cffa3bdf900ab3321268d1bbbd4743ff>2.5 - Others</h1><div class=lead>Other Gardener extensions</div></div><div class=td-content><h1 id=pg-ed74ceaa562b93985c3d5ce5042a7a22>2.5.1 - Certificate services</h1><div class=lead>Gardener extension controller for certificate services for shoot clusters</div><h1 id=gardener-extension-for-certificate-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for certificate services</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-shoot-cert-service-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-shoot-cert-service-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-shoot-cert-service><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-shoot-cert-service alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><h2 id=configuration>Configuration</h2><p>Example configuration for this extension controller:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: shoot-cert-service.extensions.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Configuration
</span></span><span style=display:flex><span>issuerName: gardener
</span></span><span style=display:flex><span>restrictIssuer: <span style=color:#00f>true</span> <span style=color:green># restrict issuer to any sub-domain of shoot.spec.dns.domain (default)</span>
</span></span><span style=display:flex><span>acme:
</span></span><span style=display:flex><span>  email: john.doe@example.com
</span></span><span style=display:flex><span>  server: https://acme-v02.api.letsencrypt.org/directory
</span></span><span style=display:flex><span><span style=color:green># privateKey: | # Optional key for Let&#39;s Encrypt account.</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----BEGIN BEGIN RSA PRIVATE KEY-----</span>
</span></span><span style=display:flex><span><span style=color:green>#   ...</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----END RSA PRIVATE KEY-----</span>
</span></span></code></pre></div><h2 id=extension-resources>Extension-Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: <span style=color:#a31515>&#34;extension-certificate-service&#34;</span>
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-cert-service
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create an instance of <a href=https://github.com/gardener/cert-management>Cert-Management</a> as well as an <code>Issuer</code> with the ACME information provided in the <a href=#Configuration>configuration</a> above. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary <code>RBAC</code> resources for the seed as well as for the shoot.</p><p>Please note, this extension controller relies on the <a href=https://github.com/gardener/gardener-resource-manager>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters, i.e. it never deploys them directly.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-cert-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e3d472d5c54cb7691330603df319378d>2.5.1.1 - Manage certificates with Gardener for default domain</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-default-domain>Manage certificates with Gardener for default domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please keep reading this article.</li><li>You want to use a certificate for a custom domain. If this is your case, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/>Manage certificates with Gardener for public domain</a></li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li></ul><p>Since you are using the default DNS name, all DNS configuration should already be done and ready.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will automatically be created.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    <span style=color:green># You may add more domains separated by commas (e.g. &#34;service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud&#34;)</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;service.shoot.project.default-domain.gardener.cloud&#34;</span> 
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer</span>
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span></code></pre></div><p>If you&rsquo;re interested in the current progress of your request, you&rsquo;re advised to consult the description, more specifically the <code>status</code> attribute in case the issuance failed.</p><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.ingress.shoot.project.default-domain.gardener.cloud&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=more-information>More information</h2><p>For more information and more examples about using the certificate extension, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/>Manage certificates with Gardener for public domain</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d8df150994f505e48131ac2122a9c747>2.5.1.2 - Manage certificates with Gardener for public domain</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-public-domain>Manage certificates with Gardener for public domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_default_domain_cert/>Manage certificates with Gardener for default domain</a></li><li>You want to use a certificate for a custom domain. If this is your case, please keep reading this article.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li><li>Your custom domain is under a <a href=https://www.iana.org/domains/root/db>public top level domain</a> (e.g. <code>.com</code>)</li><li>Your custom zone is resolvable with a public resolver via the internet (e.g. <code>8.8.8.8</code>)</li><li>You have a custom DNS provider configured and working (see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_providers/>&ldquo;DNS Providers&rdquo;</a>)</li></ul><p>As part of the <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> <a href=https://tools.ietf.org/html/rfc8555>ACME</a> challenge validation process, Gardener sets a DNS TXT entry and Let&rsquo;s Encrypt checks if it can both resolve and authenticate it. Therefore, it&rsquo;s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8)</span>
</span></span><span style=display:flex><span>dig cert-example.example.com @8.8.8.8 A
</span></span></code></pre></div><h3 id=dns-provider>DNS provider</h3><p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for <code>host.example.com</code> your DNS provider must be capable of managing subdomains of <code>host.example.com</code>.</p><p>DNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_providers/>DNS provider</a> documentation.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>Certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will created automatically.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Optional but recommended, this is going to create the DNS entry at the same time</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/commonname: &#34;*.example.com&#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/dnsnames: &#34;&#34; # optional, if not specified the names from spec.tls[].hosts are used</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34; # optional, same as spec.followCNAME in certificates    </span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Replace the <code>hosts</code> and <code>rules[].host</code> value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an <code>istio</code> Ingress controller and forwards traffic to a <code>service1</code> on port 80).</p><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: example.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># Optional</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonname: <span style=color:#a31515>&#34;*.example.example.com&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34; # optional, same as spec.followCNAME in certificates    </span>
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: amazing.example.com
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># If delegated domain for DNS01 challenge should be used. This has only an effect if a CNAME record is set for</span>
</span></span><span style=display:flex><span>  <span style=color:green># &#39;_acme-challenge.amazing.example.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># For example: If a CNAME record exists &#39;_acme-challenge.amazing.example.com&#39; =&gt; &#39;_acme-challenge.writable.domain.com&#39;,</span>
</span></span><span style=display:flex><span>  <span style=color:green># the DNS challenge will be written to &#39;_acme-challenge.writable.domain.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#followCNAME: true</span>
</span></span></code></pre></div><h2 id=supported-attributes>Supported attributes</h2><p>Here is a list of all supported annotations regarding the certificate extension:</p><table><thead><tr><th>Path</th><th>Annotation</th><th>Value</th><th>Required</th><th>Description</th></tr></thead><tbody><tr><td>N/A</td><td><code>cert.gardener.cloud/purpose:</code></td><td><code>managed</code></td><td>Yes when using annotations</td><td>Flag for Gardener that this specific Ingress or Service requires a certificate</td></tr><tr><td><code>spec.commonName</code></td><td><code>cert.gardener.cloud/commonname:</code></td><td>E.g. &ldquo;*.demo.example.com&rdquo; or<br>&ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td><code>spec.dnsName</code></td><td><code>cert.gardener.cloud/dnsnames:</code></td><td>E.g. &ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.</td></tr><tr><td><code>spec.secretRef.name</code></td><td><code>cert.gardener.cloud/secretname:</code></td><td><code>any-name</code></td><td>Yes for certificate and Service</td><td>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued.</td></tr><tr><td><code>spec.issuerRef.name</code></td><td><code>cert.gardener.cloud/issuer:</code></td><td>E.g. <code>gardener</code></td><td>No</td><td>Specifies the issuer you want to use. Only necessary if you request certificates for <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td>N/A</td><td><code>cert.gardener.cloud/revoked:</code></td><td><code>true</code> otherwise always false</td><td>No</td><td>Use only to revoke a certificate, see <a href=#references>reference</a> for more details</td></tr><tr><td><code>spec.followCNAME</code></td><td><code>cert.gardener.cloud/follow-cname</code></td><td>E.g. <code>true</code></td><td>No</td><td>Specifies that the usage of a delegated domain for DNS challenges is allowed. Details see <a href=https://github.com/gardener/cert-management#follow-cname>Follow CNAME</a>.</td></tr></tbody></table><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.example.com&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=using-a-custom-issuer>Using a custom Issuer</h2><p>Most Gardener deployment with the certification extension enabled have a preconfigured <code>garden</code> issuer. It is also usually configured to use Let&rsquo;s Encrypt as the certificate provider.</p><p>If you need a custom issuer for a specific cluster, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/custom_shoot_issuer/>Using a custom Issuer</a></p><h2 id=quotas>Quotas</h2><p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.</p><p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
<code>requestsPerDayQuota</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: your-email@example.com
</span></span><span style=display:flex><span>          name: custom-issuer <span style=color:green># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span>
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          requestsPerDayQuota: 10
</span></span></code></pre></div><h2 id=dns-propagation>DNS Propagation</h2><p>As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain&rsquo;s certificate you are requesting. This works by creating a DNS TXT record in your DNS provider under <code>_acme-challenge.example.example.com</code> containing a token to compare with. The TXT record is only visible during the domain validation. Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes. This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with <code>kubectl describe ingress example</code> for troubleshooting.</p><h2 id=character-restrictions>Character Restrictions</h2><p>Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).</p><p>For example, the following request is invalid:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-invalid
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><p>But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  dnsNames:
</span></span><span style=display:flex><span>  - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><h2 id=references>References</h2><ul><li><a href=https://github.com/gardener/cert-management>Gardener cert-management</a></li><li><a href=https://github.com/gardener/gardener-extension-shoot-dns-service>Managing DNS with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-043866020dc42e1f2f748bc6386a91d7>2.5.1.3 - Using a custom Issuer</h1><div class=lead>How to define a custom issuer forma shoot cluster</div><h1 id=using-a-custom-issuer>Using a custom Issuer</h1><p>Another possibility to request certificates for custom domains is a dedicated issuer.</p><blockquote><p>Note: This is only needed if the default issuer provided by Gardener is restricted to shoot related domains or you are using domain names not visible to public DNS servers. <strong>Which means that your senario most likely doesn&rsquo;t require your to add an issuer</strong>.</p></blockquote><p>The custom issuers are specified normally in the shoot manifest. If the <code>shootIssuers</code> feature is enabled, it can alternatively be defined in the shoot cluster.</p><h2 id=custom-issuer-in-the-shoot-manifest>Custom issuer in the shoot manifest</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: your-email@example.com
</span></span><span style=display:flex><span>          name: custom-issuer <span style=color:green># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span>
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          privateKeySecretName: my-privatekey <span style=color:green># referenced resource, the private key must be stored in the secret at `data.privateKey` (optionally, only needed as alternative to auto registration) </span>
</span></span><span style=display:flex><span>          <span style=color:green>#precheckNameservers: # to provide special set of nameservers to be used for prechecking DNSChallenges for an issuer</span>
</span></span><span style=display:flex><span>          <span style=color:green>#- dns1.private.company-net:53</span>
</span></span><span style=display:flex><span>          <span style=color:green>#- dns2.private.company-net:53&#34; </span>
</span></span><span style=display:flex><span>      <span style=color:green>#shootIssuers:</span>
</span></span><span style=display:flex><span>        <span style=color:green># if true, allows to specify issuers in the shoot cluster</span>
</span></span><span style=display:flex><span>        <span style=color:green>#enabled: true </span>
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - name: my-privatekey
</span></span><span style=display:flex><span>    resourceRef:
</span></span><span style=display:flex><span>      apiVersion: v1
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: custom-issuer-privatekey <span style=color:green># name of secret in Gardener project</span>
</span></span></code></pre></div><p>If you are using an ACME provider for private domains, you may need to change the nameservers used for
checking the availability of the DNS challenge&rsquo;s TXT record before the certificate is requested from the ACME provider.
By default, only public DNS servers may be used for this purpose.
At least one of the <code>precheckNameservers</code> must be able to resolve the private domain names.</p><h2 id=using-an-issuer-in-the-shoot-cluster>Using an issuer in the shoot cluster</h2><p><em>Prerequiste</em>: The <code>shootIssuers</code> feature has to be enabled.
It is either enabled globally in the <code>ControllerDeployment</code> or in the shoot manifest
with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      shootIssuers:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span> <span style=color:green># if true, allows to specify issuers in the shoot cluster</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Example for specifying an <code>Issuer</code> resource and its <code>Secret</code> directly in any
namespace of the shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Issuer
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-issuer
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  acme:
</span></span><span style=display:flex><span>    domains:
</span></span><span style=display:flex><span>      include:
</span></span><span style=display:flex><span>      - my.own.domain.com
</span></span><span style=display:flex><span>    email: some.user@my.own.domain.com
</span></span><span style=display:flex><span>    privateKeySecretRef:
</span></span><span style=display:flex><span>      name: my-own-issuer-secret
</span></span><span style=display:flex><span>      namespace: my-namespace
</span></span><span style=display:flex><span>    server: https://acme-v02.api.letsencrypt.org/directory
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-issuer-secret
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  privateKey: ... <span style=color:green># replace &#39;...&#39; with valus encoded as base64</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c482a0726f279fe3ad5f53ba778b84f3>2.5.1.4 - Gardener yourself a Shoot with Istio, custom Domains, and Certificates</h1><p>As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply</p><ul><li>create a Gardener managed Kubernetes Cluster (Shoot) via kubectl,</li><li>install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon),</li><li>attach your own custom domain to be managed by Gardener,</li><li>combine everything with certificates from Let&rsquo;s Encrypt.</li></ul><p>Here are some pre-pointers that you will need to go deeper:</p><ul><li><a href=/docs/guides/administer_shoots/create-delete-shoot/>CRUD Gardener Shoot</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/README.md>DNS Management</a></li><li><a href=https://github.com/gardener/cert-management/blob/master/README.md>Certificate Management</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_names/>Tutorial Domain Names</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/>Tutorial Certificates</a></li></ul><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tip</h4>If you try my instructions and fail, then read the alternative title of this tutorial as "Shoot yourself in the foot with Gardener, custom Domains, Istio and Certificates".</div><h2 id=first-things-first>First Things First</h2><p>Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as <code>shoot-operator-gcp</code>.</p><p>From the Access widget (leave the default settings) download your personalized <code>kubeconfig</code> into <code>~/.kube/kubeconfig-garden-myproject</code>. Follow the instructions to setup <code>kubelogin</code>:</p><p><img src=/__resources/access_5deea7.png alt=access></p><p>For convinience, let us set an alias command with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>alias kgarden=<span style=color:#a31515>&#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml&#34;</span>
</span></span></code></pre></div><p><code>kgarden</code> now gives you all botanical powers and connects you directly with your Gardener.</p><p>You should now be able to run <code>kgarden get shoots</code>, automatically get an oidc token, and list already running clusters/shoots.</p><h2 id=prepare-your-custom-domain>Prepare your Custom Domain</h2><p>I am going to use <a href=https://www.cloudflare.com/>Cloud Flare</a> as programmatic DNS of my custom domain <code>mydomain.io</code>. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support <a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>delegating subdomains</a>.</p><p>I needed to follow these <a href=https://github.com/gardener/external-dns-management/blob/master/docs/cloudflare/README.md>instructions</a> and created the following secret:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cloudflare-mydomain-io
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  CLOUDFLARE_API_TOKEN: useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==
</span></span></code></pre></div><p>Apply this secret into your project with <code>kgarden create -f cloudflare-mydomain-io.yaml</code>.</p><p>Our <a href=https://github.com/gardener/external-dns-management/>External DNS Manager</a> also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.</p><h2 id=prepare-gardener-extensions>Prepare Gardener Extensions</h2><p>I now need to prepare the Gardener extensions <code>shoot-dns-service</code> and <code>shoot-cert-service</code> and set the parameters accordingly.</p><div class="alert alert-info" role=alert>Please note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.</div><p>The following snipplet allows Gardener to manage my entire custom domain, whereas with the <code>include:</code> attribute I restrict all dynamic entries under the subdomain <code>gsicdc.mydomain.io</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>      - domains:
</span></span><span style=display:flex><span>          include:
</span></span><span style=display:flex><span>            - gsicdc.mydomain.io
</span></span><span style=display:flex><span>        primary: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>        secretName: cloudflare-mydomain-io
</span></span><span style=display:flex><span>        type: cloudflare-dns
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span></code></pre></div><p>The next snipplet allows Gardener to manage certificates automatically from <em><a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a></em> on <code>mydomain.io</code> for me:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-cert-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        issuers:
</span></span><span style=display:flex><span>          - email: me@mail.com
</span></span><span style=display:flex><span>            name: mydomain
</span></span><span style=display:flex><span>            server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          - email: me@mail.com
</span></span><span style=display:flex><span>            name: mydomain-staging
</span></span><span style=display:flex><span>            server: <span style=color:#a31515>&#39;https://acme-staging-v02.api.letsencrypt.org/directory&#39;</span>
</span></span></code></pre></div><div class="alert alert-info" role=alert>Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.</div><p>References for <a href=https://letsencrypt.org>Let&rsquo;s Encrypt</a>:</p><ul><li><a href=https://letsencrypt.org/docs/rate-limits/>Rate limit</a></li><li><a href=https://letsencrypt.org/docs/staging-environment/>Staging environment</a></li><li><a href=https://letsencrypt.org/docs/challenge-types/>Challenge Types</a></li><li><a href=https://community.letsencrypt.org/t/acme-v2-production-environment-wildcards/55578>Wildcard Certificates</a></li></ul><h2 id=create-the-gardener-shoot-cluster>Create the Gardener Shoot Cluster</h2><p>Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with <code>shoot-operator-gcp</code> and I combined the above snipplets into the yaml file:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gsicdc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - domains:
</span></span><span style=display:flex><span>        include:
</span></span><span style=display:flex><span>          - gsicdc.mydomain.io
</span></span><span style=display:flex><span>      primary: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      secretName: cloudflare-mydomain-io
</span></span><span style=display:flex><span>      type: cloudflare-dns
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-dns-service
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: me@mail.com
</span></span><span style=display:flex><span>          name: mydomain
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>        - email: me@mail.com
</span></span><span style=display:flex><span>          name: mydomain-staging
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-staging-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>  cloudProfileName: gcp
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.24.8
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      zone: europe-west1-d
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - machine:
</span></span><span style=display:flex><span>        image:
</span></span><span style=display:flex><span>          name: gardenlinux
</span></span><span style=display:flex><span>          version: 576.9.0
</span></span><span style=display:flex><span>        type: n1-standard-2
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      name: my-workerpool
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: pd-standard
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-west1-d
</span></span><span style=display:flex><span>  purpose: testing
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretBindingName: shoot-operator-gcp
</span></span></code></pre></div><p>Create your cluster and wait for it to be ready (about 5 to 7min).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kgarden create -f gsicdc.yaml
</span></span><span style=display:flex><span>shoot.core.gardener.cloud/gsicdc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kgarden get shoot gsicdc --watch
</span></span><span style=display:flex><span>NAME     CLOUDPROFILE   VERSION   SEED   DOMAIN                                        HIBERNATION   OPERATION    PROGRESS   APISERVER     CONTROL       NODES     SYSTEM    AGE
</span></span><span style=display:flex><span>gsicdc   gcp            1.24.8    gcp    gsicdc.myproject.shoot.devgarden.cloud   Awake         Processing   38         Progressing   Progressing   Unknown   Unknown   83s
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>gsicdc   gcp            1.24.8    gcp    gsicdc.myproject.shoot.devgarden.cloud   Awake         Succeeded    100        True          True          True          False         6m7s
</span></span></code></pre></div><p>Get access to your freshly baked cluster and set your <code>KUBECONFIG</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d &gt;kubeconfig-gsicdc.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ export KUBECONFIG=<span style=color:#00f>$(</span>pwd<span style=color:#00f>)</span>/kubeconfig-gsicdc.yaml
</span></span><span style=display:flex><span>$ kubectl get all
</span></span><span style=display:flex><span>NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
</span></span><span style=display:flex><span>service/kubernetes   ClusterIP   100.64.0.1   &lt;none&gt;        443/TCP   89m
</span></span></code></pre></div><h2 id=install-istio>Install Istio</h2><p>Please follow the Istio installation <a href=https://istio.io/docs/setup/getting-started/>instructions</a> and download <code>istioctl</code>. If you are on a Mac, I recommend</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ brew install istioctl
</span></span></code></pre></div><p>I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object <code>istio-ingressgateway</code> in the <code>istio-system</code> namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/issuer: mydomain-staging
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: wildcard-tls
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;*.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;120&#34;</span>
</span></span></code></pre></div><p>With these annotations three things now happen automagically:</p><ol><li>The <a href=https://github.com/gardener/external-dns-management/blob/master/README.md>External DNS Manager</a>, provided to you as a service (<code>dns.gardener.cloud/class: garden</code>), picks up the request and creates the wildcard DNS entry <code>*.gsicdc.mydomain.io</code> with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with <code>dig lovemygardener.gsicdc.mydomain.io</code> within seconds.</li><li>The <a href=https://github.com/gardener/cert-management/blob/master/README.md>Certificate Management</a> picks up the request as well and initates a DNS01 protocol exchange with Let&rsquo;s Encrypt; using the staging environment referred to with the issuer behind <code>mydomain-staging</code>.</li><li>After aproximately 70sec (give and take) you will receive the wildcard certificate in the <code>wildcard-tls</code> secret in the namespace <code>istio-system</code>.</li></ol><div class="alert alert-info" role=alert>Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.</div><p>Here is the istio-install script:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ export domainname=<span style=color:#a31515>&#34;*.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>$ export issuer=<span style=color:#a31515>&#34;mydomain-staging&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cat <span style=color:#a31515>&lt;&lt;EOF | istioctl install -y -f -
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: install.istio.io/v1alpha1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: IstioOperator
</span></span></span><span style=display:flex><span><span style=color:#a31515>spec:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  profile: default
</span></span></span><span style=display:flex><span><span style=color:#a31515>  components:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    ingressGateways:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: istio-ingressgateway
</span></span></span><span style=display:flex><span><span style=color:#a31515>      enabled: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>      k8s:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        serviceAnnotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          cert.gardener.cloud/issuer: &#34;${issuer}&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>          cert.gardener.cloud/secretname: wildcard-tls
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/class: garden
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/dnsnames: &#34;${domainname}&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/ttl: &#34;120&#34; 
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>Verify that setup is working and that DNS and certificates have been created/delivered:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl -n istio-system describe service istio-ingressgateway
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason                Age                From                     Message
</span></span><span style=display:flex><span>  ----    ------                ----               ----                     -------
</span></span><span style=display:flex><span>  Normal  EnsuringLoadBalancer  58s                service-controller       Ensuring load balancer
</span></span><span style=display:flex><span>  Normal  reconcile             58s                cert-controller-manager  created certificate object istio-system/istio-ingressgateway-service-pwqdm
</span></span><span style=display:flex><span>  Normal  cert-annotation       58s                cert-controller-manager  wildcard-tls: cert request is pending
</span></span><span style=display:flex><span>  Normal  cert-annotation       54s                cert-controller-manager  wildcard-tls: certificate pending: certificate requested, preparing/waiting <span style=color:#00f>for</span> successful DNS01 challenge
</span></span><span style=display:flex><span>  Normal  cert-annotation       28s                cert-controller-manager  wildcard-tls: certificate ready
</span></span><span style=display:flex><span>  Normal  EnsuredLoadBalancer   26s                service-controller       Ensured load balancer
</span></span><span style=display:flex><span>  Normal  reconcile             26s                dns-controller-manager   created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb
</span></span><span style=display:flex><span>  Normal  dns-annotation        26s                dns-controller-manager   *.gsicdc.mydomain.io: dns entry is pending
</span></span><span style=display:flex><span>  Normal  dns-annotation        21s (x3 over 21s)  dns-controller-manager   *.gsicdc.mydomain.io: dns entry active
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ dig lovemygardener.gsicdc.mydomain.io
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; lovemygardener.gsicdc.mydomain.io
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>;; ANSWER SECTION:
</span></span><span style=display:flex><span>lovemygardener.gsicdc.mydomain.io. 120 IN A	35.195.120.62
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span></code></pre></div><p>There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.</p><h2 id=handy-tools-to-install>Handy tools to install</h2><p>Another set of fine tools to use are <a href=https://get-kapp.io/>kapp</a> (formerly known as k14s), <a href=https://k9scli.io/>k9s</a> and <a href=https://httpie.org/>HTTPie</a>. While we are at it, let&rsquo;s install them all. If you are on a Mac, I recommend:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew tap vmware-tanzu/carvel
</span></span><span style=display:flex><span>brew install ytt kbld kapp kwt imgpkg vendir
</span></span><span style=display:flex><span>brew install derailed/k9s/k9s
</span></span><span style=display:flex><span>brew install httpie
</span></span></code></pre></div><h2 id=ingress-to-your-service>Ingress to your service</h2><div class="alert alert-info" role=alert>Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!</div><p>Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch <a href="https://www.youtube.com/watch?v=cduG0FrjdJA">Evolving the Kubernetes Ingress APIs to GA and Beyond</a> for a good introduction. In this example, I did not want to use the Kubernetes <code>Ingress</code> compatibility option of Istio. Instead, I used <code>VirtualService</code> and <code>Gateway</code> from the Istio&rsquo;s API group <code>networking.istio.io/v1beta1</code> directly, and enabled istio-injection generically for the namespace.</p><p>I use <a href=https://httpbin.org/>httpbin</a> as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Namespace
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: production
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    istio-injection: enabled
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: httpbin
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - name: http
</span></span><span style=display:flex><span>    port: 8000
</span></span><span style=display:flex><span>    targetPort: 80
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: httpbin
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: httpbin
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: httpbin
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: docker.io/kennethreitz/httpbin
</span></span><span style=display:flex><span>        imagePullPolicy: IfNotPresent
</span></span><span style=display:flex><span>        name: httpbin
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 80
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1beta1
</span></span><span style=display:flex><span>kind: Gateway
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin-gw
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    istio: ingressgateway <span style=color:green>#! use istio default ingress gateway</span>
</span></span><span style=display:flex><span>  servers:
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 80
</span></span><span style=display:flex><span>      name: http
</span></span><span style=display:flex><span>      protocol: HTTP
</span></span><span style=display:flex><span>    tls:
</span></span><span style=display:flex><span>      httpsRedirect: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 443
</span></span><span style=display:flex><span>      name: https
</span></span><span style=display:flex><span>      protocol: HTTPS
</span></span><span style=display:flex><span>    tls:
</span></span><span style=display:flex><span>      mode: SIMPLE
</span></span><span style=display:flex><span>      credentialName: wildcard-tls
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1beta1
</span></span><span style=display:flex><span>kind: VirtualService
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin-vs
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>  - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>  gateways:
</span></span><span style=display:flex><span>  - httpbin-gw
</span></span><span style=display:flex><span>  http:
</span></span><span style=display:flex><span>  - match:
</span></span><span style=display:flex><span>    - uri:
</span></span><span style=display:flex><span>        regex: /.*
</span></span><span style=display:flex><span>    route:
</span></span><span style=display:flex><span>    - destination:
</span></span><span style=display:flex><span>        port:
</span></span><span style=display:flex><span>          number: 8000
</span></span><span style=display:flex><span>        host: httpbin
</span></span><span style=display:flex><span>---
</span></span></code></pre></div><p>Let us now deploy the whole package of Kubernetes primitives using <code>kapp</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kapp deploy -a httpbin -f httpbin-kapp.yaml
</span></span><span style=display:flex><span>Target cluster <span style=color:#a31515>&#39;https://api.gsicdc.myproject.shoot.devgarden.cloud&#39;</span> (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Changes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Namespace   Name        Kind            Conds.  Age  Op      Wait to    Rs  Ri
</span></span><span style=display:flex><span>(cluster)   production  Namespace       -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>production  httpbin     Deployment      -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin     Service         -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin-gw  Gateway         -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin-vs  VirtualService  -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Op:      5 create, 0 delete, 0 update, 0 noop
</span></span><span style=display:flex><span>Wait to: 5 reconcile, 0 delete, 0 noop
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Continue? [yN]: y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>5:36:31PM: ---- applying 1 changes [0/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production
</span></span><span style=display:flex><span>5:37:00PM: ---- applying complete [5/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>5:37:00PM: ---- waiting complete [5/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Succeeded
</span></span></code></pre></div><p>Let&rsquo;s finaly test the service (Of course you can use the browser as well):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ http httpbin.gsicdc.mydomain.io
</span></span><span style=display:flex><span>HTTP/1.1 301 Moved Permanently
</span></span><span style=display:flex><span>content-length: 0
</span></span><span style=display:flex><span>date: Wed, 13 May 2020 21:29:13 GMT
</span></span><span style=display:flex><span>location: https://httpbin.gsicdc.mydomain.io/
</span></span><span style=display:flex><span>server: istio-envoy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ curl -k https://httpbin.gsicdc.mydomain.io/ip
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;origin&#34;</span>: <span style=color:#a31515>&#34;10.250.0.2&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Quod erat demonstrandum.
The proof of exchanging the issuer is now left to the reader.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tip</h4>Remember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed "curl -k" or "http --verify no".</div><p>Hint: use the interactive k9s tool.
<img src=/__resources/k9s_5b92ae.png alt=k9s></p><h2 id=cleanup>Cleanup</h2><p>Remove the cloud native application:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kapp ls
</span></span><span style=display:flex><span>Apps in namespace <span style=color:#a31515>&#39;default&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name     Namespaces            Lcs   Lca
</span></span><span style=display:flex><span>httpbin  (cluster),production  true  17m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kapp delete -a httpbin
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Continue? [yN]: y
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>11:47:47PM: ---- waiting complete [8/8 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Succeeded
</span></span></code></pre></div><p>Remove Istio:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ istioctl x uninstall --purge
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io <span style=color:#a31515>&#34;prometheus-istio-system&#34;</span> deleted
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io <span style=color:#a31515>&#34;prometheus-istio-system&#34;</span> deleted
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Delete your Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite
</span></span><span style=display:flex><span>kgarden delete shoot gsicdc --wait=false
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5414c77c31206a3d115894d74244106d>2.5.1.5 - Setup</h1><h1 id=gardener-certificate-management>Gardener Certificate Management</h1><h2 id=introduction>Introduction</h2><p>Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.</p><h2 id=extension-installation>Extension Installation</h2><p>The <code>Shoot-Cert-Service</code> extension can be deployed and configured via Gardener&rsquo;s native resource <a href=/docs/gardener/extensions/controllerregistration/>ControllerRegistration</a>.</p><h3 id=prerequisites>Prerequisites</h3><p>To let the <code>Shoot-Cert-Service</code> operate properly, you need to have:</p><ul><li>a <a href=https://github.com/gardener/external-dns-management>DNS service</a> in your seed</li><li>contact details and optionally a private key for a pre-existing <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> account</li></ul><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>Shoot-Cert-Service</code> can be found here: <a href=https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml>https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml</a></p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploy the <code>Shoot-Cert-Service</code> to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let&rsquo;s Encrypt accounts can be used and shared with shoot clusters (See &ldquo;One Account or Many?&rdquo; of the <a href=https://letsencrypt.org/docs/integration-guide/>Integration Guide</a>).</p><blockquote><p>Please keep the Let&rsquo;s Encrypt <a href=https://letsencrypt.org/docs/rate-limits/>Rate Limits</a> in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    certificateConfig:
</span></span><span style=display:flex><span>      defaultIssuer:
</span></span><span style=display:flex><span>        acme:
</span></span><span style=display:flex><span>            email: foo@example.com
</span></span><span style=display:flex><span>            privateKey: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>            -----BEGIN RSA PRIVATE KEY-----
</span></span></span><span style=display:flex><span><span style=color:#a31515>            ...
</span></span></span><span style=display:flex><span><span style=color:#a31515>            -----END RSA PRIVATE KEY-----
</span></span></span><span style=display:flex><span><span style=color:#a31515>            server: https://acme-v02.api.letsencrypt.org/directory</span>            
</span></span><span style=display:flex><span>        name: default-issuer
</span></span><span style=display:flex><span><span style=color:green>#       restricted: true # restrict default issuer to any sub-domain of shoot.spec.dns.domain</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     defaultRequestsPerDayQuota: 50</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     precheckNameservers: 8.8.8.8,8.8.4.4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     caCertificates: | # optional custom CA certificates when using private ACME provider</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#     ...</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#     ...</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      shootIssuers:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span> <span style=color:green># if true, allows to specify issuers in the shoot clusters</span>
</span></span></code></pre></div><h4 id=enablement>Enablement</h4><p>If the <code>Shoot-Cert-Service</code> should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the <code>ControllerRegistration</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - globallyEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Extension
</span></span><span style=display:flex><span>    type: shoot-cert-service
</span></span></code></pre></div><p>Alternatively, you&rsquo;re given the option to only enable the service for certain shoots:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-4773e80fa0a0952ed75e41786df93e8c>2.5.2 - DNS services</h1><div class=lead>Gardener extension controller for DNS services for shoot clusters</div><h1 id=gardener-extension-for-dns-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for DNS services</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-shoot-dns-service-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-shoot-dns-service-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-shoot-dns-service><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-shoot-dns-service alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><h2 id=extension-resources>Extension-Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: <span style=color:#a31515>&#34;extension-dns-service&#34;</span>
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-dns-service
</span></span></code></pre></div><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-894b148165d987aaec9d1c03dacfabfc>2.5.2.1 - Deployment</h1><h1 id=deployment-of-the-shoot-dns-service-extension>Deployment of the shoot DNS service extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step deployment guide for the shoot DNS service extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the shoot DNS service extension <a href=https://github.com/gardener/gardener-extension-shoot-dns-service>repository</a>.</p><h2 id=gardener-extension-admission-shoot-dns-service>gardener-extension-admission-shoot-dns-service</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of Virtual Garden</a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster>Virtual Garden is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><h5 id=automounted-service-account-token>Automounted Service Account Token</h5><p>The easiest way to deploy the <code>gardener-extension-admission-shoot-dns-service</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><h5 id=service-account-token-volume-projection>Service Account Token Volume Projection</h5><p>Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster>Virtual Garden is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><h5 id=service-account>Service Account</h5><p>The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=client-certificate>Client Certificate</h5><p>Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=projected-service-account-token>Projected Service Account Token</h5><p>This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-239831692757463bc316d892218e20f9>2.5.2.2 - DNS Names</h1><h1 id=request-dns-names-in-shoot-clusters>Request DNS Names in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to request DNS records via the following resource types:</p><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Ingress</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/README.md#the-model>DNSEntry</a></li></ul><p>It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-dns-service</code> extension. This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In some Gardener setups the <code>shoot-dns-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=before-you-start>Before you start</h2><p>You should :</p><ul><li>Have created a shoot cluster</li><li>Have created and correctly configured a DNS Provider (Please consult <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_providers/>this page</a> for more information)</li><li>Have a basic understanding of DNS (see link under <a href=#references>References</a>)</li></ul><p>There are 2 types of DNS that you can use within Kubernetes :</p><ul><li>internal (usually managed by coreDNS)</li><li>external (managed by a public DNS provider).</li></ul><p>This page, and the extension, exclusively works for external DNS handling.</p><p>Gardener allows 2 way of managing your external DNS:</p><ul><li>Manually, which means you are in charge of creating / maintaining your Kubernetes related DNS entries</li><li>Via the Gardener DNS extension</li></ul><h2 id=gardener-dns-extension>Gardener DNS extension</h2><p>The managed external DNS records feature of the Gardener clusters makes all this easier. You do not need DNS service provider specific knowledge, and in fact you do not need to leave your cluster at all to achieve that. You simply annotate the Ingress / Service that needs its DNS records managed and it will be automatically created / managed by Gardener.</p><p>Managed external DNS records are supported with the following DNS provider types:</p><ul><li>aws-route53</li><li>azure-dns</li><li>azure-private-dns</li><li>google-clouddns</li><li>openstack-designate</li><li>alicloud-dns</li><li>cloudflare-dns</li></ul><h3 id=request-dns-records-for-ingress-resources>Request DNS records for Ingress resources</h3><p>To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class <code>garden</code> and an annotation denoting the desired DNS names.</p><p>Example for an annotated Ingress resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Ingress.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com <span style=color:green># Use &#34;*&#34; to collects domains names from .spec.rules[].host</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># If you are delegating the certificate management to Gardener, uncomment the following line</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/purpose: managed</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: special.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span><span style=display:flex><span>  <span style=color:green># Uncomment the following part if you are delegating the certificate management to Gardener</span>
</span></span><span style=display:flex><span>  <span style=color:green>#tls:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  - hosts:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#      - special.example.com</span>
</span></span><span style=display:flex><span>  <span style=color:green>#    secretName: my-cert-secret-name</span>
</span></span></code></pre></div><p>For an Ingress, the DNS names are already declared in the specification. Nevertheless the <em>dnsnames</em> annotation must be present. Here a subset of the DNS names of the ingress can be specified. If DNS names for all names are desired, the value <code>all</code> can be used.</p><p>Keep in mind that ingress resources are ignored unless an ingress controller is set up. Gardener does not provide an ingress controller by default. For more details, see <a href=https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>Ingress Controllers</a> and <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a> in the Kubernetes documentation.</p><h3 id=request-dns-records-for-service-type-loadbalancer>Request DNS records for service type LoadBalancer</h3><p>Example for an annotated Service (it must have the type <code>LoadBalancer</code>) resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-svc
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Service.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: amazing-app
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - protocol: TCP
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h4 id=creating-a-dnsentry-resource-explicitly>Creating a DNSEntry resource explicitly</h4><p>It is also possible to create a DNS entry via the Kubernetes resource called <code>DNSEntry</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage this DNS entry.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: special-dnsentry
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dnsName: special.example.com
</span></span><span style=display:flex><span>  ttl: 600
</span></span><span style=display:flex><span>  targets:
</span></span><span style=display:flex><span>  - 1.2.3.4
</span></span></code></pre></div><p>If one of the accepted DNS names is a direct subname of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><p>You can check the status of the <code>DNSEntry</code> with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get dnsentry
</span></span><span style=display:flex><span>NAME          DNS                                                            TYPE          PROVIDER      STATUS    AGE
</span></span><span style=display:flex><span>mydnsentry    special.example.com     aws-route53   default/aws   Ready     24s
</span></span></code></pre></div><p>As soon as the status of the entry is <code>Ready</code>, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, <strong>it may take up to 24 hours for the new entry to be propagated over all internet</strong>.</p><p>More examples can be found <a href=https://github.com/gardener/external-dns-management/blob/master/examples/>here</a></p><h3 id=request-dns-records-for-serviceingress-resources-using-a-dnsannotation-resource>Request DNS records for Service/Ingress resources using a DNSAnnotation resource</h3><p>In rare cases it may not be possible to add annotations to a <code>Service</code> or <code>Ingress</code> resource object.</p><p>E.g.: the helm chart used to deploy the resource may not be adaptable for some reasons or some automation is used, which always restores the original content of the resource object by dropping any additional annotations.</p><p>In these cases, it is recommended to use an additional <code>DNSAnnotation</code> resource in order to have more flexibility that <code>DNSentry resources</code>. The <code>DNSAnnotation</code> resource makes the DNS shoot service behave as if annotations have been added to the referenced resource.</p><p>For the Ingress example shown above, you can create a <code>DNSAnnotation</code> resource alternatively to provide the annotations.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSAnnotation
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: test-ingress-annotation
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resourceRef:
</span></span><span style=display:flex><span>    kind: Ingress
</span></span><span style=display:flex><span>    apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>    name: test-ingress
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden    
</span></span></code></pre></div><p>Note that the DNSAnnotation resource itself needs the <code>dns.gardener.cloud/class=garden</code> annotation. This also only works for annotations known to the DNS shoot service (see <a href=#accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</a>).</p><p>For more details, see also <a href=https://github.com/gardener/external-dns-management#dnsannotation-objects>DNSAnnotation objects</a></p><h3 id=accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</h3><p>Here are all of the accepted annotation related to the DNS extension:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>- dns.gardener.cloud/dnsnames <span style=color:green># Mandatory, accepts a comma-separated list of DNS names if multiple names are required</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/class <span style=color:green># Mandatory, DNS extension class name (usually &#34;garden&#34;)</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/ttl <span style=color:green># Recommended, Time-To-Live of the DNS record</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/cname-lookup-interval <span style=color:green># Optional, lookup interval for CNAMEs that must be resolved to IP (in seconds)</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/realms <span style=color:green># Optional, for restricting provider access for shoot DNS entries</span>
</span></span></code></pre></div><p>If one of the accepted DNS names is a direct subdomain of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore, this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><h2 id=troubleshooting>Troubleshooting</h2><h3 id=general-dns-tools>General DNS tools</h3><p>To check the DNS resolution, use the <code>nslookup</code> or <code>dig</code> command.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ nslookup special.your-domain.com
</span></span></code></pre></div><p>or with dig</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ dig +short special.example.com
</span></span><span style=display:flex><span>Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dig @8.8.8.8 +short special.example.com
</span></span></code></pre></div><h3 id=dns-record-events>DNS record events</h3><p>The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don&rsquo;t own.</p><p>Events for a successfully created DNS record:</p><pre tabindex=0><code>$ kubectl describe service my-service

Events:
  Type    Reason          Age                From                    Message
  ----    ------          ----               ----                    -------
  Normal  dns-annotation  19s                dns-controller-manager  special.example.com: dns entry is pending
  Normal  dns-annotation  19s (x3 over 19s)  dns-controller-manager  special.example.com: dns entry pending: waiting for dns reconciliation
  Normal  dns-annotation  9s (x3 over 10s)   dns-controller-manager  special.example.com: dns entry active
</code></pre><p>Please note, events vanish after their retention period (usually <code>1h</code>).</p><h3 id=dnsentry-status>DNSEntry status</h3><p><code>DNSEntry</code> resources offer a <code>.status</code> sub-resource which can be used to check the current state of the object.</p><p>Status of a erroneous <code>DNSEntry</code>.</p><pre tabindex=0><code>  status:
    message: No responsible provider found
    observedGeneration: 3
    provider: remote
    state: Error
</code></pre><h2 id=references>References</h2><ul><li><a href=https://www.cloudflare.com/en-ca/learning/dns/what-is-dns>Understanding DNS</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/>Kubernetes Internal DNS</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/pkg/apis/dns/v1alpha1/dnsentry.go>DNSEntry API (Golang)</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/>Managing Certificates with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c899528b6cabcab3ac3befed9c9602e1>2.5.2.3 - DNS Providers</h1><h1 id=dns-providers>DNS Providers</h1><h2 id=introduction>Introduction</h2><p>Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_names/>here</a>) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.</p><h2 id=shoot-provider>Shoot provider</h2><p>By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster&rsquo;s <code>kube-apiserver</code> public DNS record (DNS address in your Kubeconfig).</p><pre tabindex=0><code>kind: Shoot
...
dns:
  domain: shoot.project.default-domain.gardener.cloud
</code></pre><p>You are permitted to request any sub-domain of <code>.dns.domain</code> that is not already taken (e.g. <code>api.shoot.project.default-domain.gardener.cloud</code>, <code>*.ingress.shoot.project.default-domain.gardener.cloud</code>) with this provider.</p><h2 id=additional-providers>Additional providers</h2><p>If you need to request DNS records for domains not managed by the <a href=#Shoot-provider>default provider</a>, additional providers can
be configured in the shoot specification.
Alternatively, if it is enabled, it can be added as <code>DNSProvider</code> resources to the shoot cluster.</p><h3 id=additional-providers-in-the-shoot-specification>Additional providers in the shoot specification</h3><p>To add a providers in the shoot spec, you need set them in the <code>spec.dns.providers</code> list.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    domain: shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - secretName: my-aws-account
</span></span><span style=display:flex><span>      type: aws-route53
</span></span><span style=display:flex><span>    - secretName: my-gcp-account
</span></span><span style=display:flex><span>      type: google-clouddns
</span></span></code></pre></div><blockquote><p>Please consult the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.DNSProvider>API-Reference</a> to get a complete list of supported fields and configuration options.</p></blockquote><p>Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The <strong>External-DNS-Management</strong> project provides corresponding examples (<a href=https://github.com/gardener/external-dns-management/tree/master/examples>20-secret-&lt;provider-name>-credentials.yaml</a>) for known providers.</p><h3 id=additional-providers-as-resources-in-the-shoot-cluster>Additional providers as resources in the shoot cluster</h3><p>If it is not enabled globally, you have to enable the feature in the shoot manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.dns.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: DNSConfig
</span></span><span style=display:flex><span>        dnsProviderReplication:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To add a provider directly in the shoot cluster, provide a <code>DNSProvider</code> in any namespace together
with <code>Secret</code> containing the credentials.</p><p>For example if the domain is hosted with AWS Route 53 (provider type <code>aws-route53</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSProvider
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: my-own-domain
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws-route53
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-own-domain-credentials
</span></span><span style=display:flex><span>  domains:
</span></span><span style=display:flex><span>    include:
</span></span><span style=display:flex><span>    - my.own.domain.com
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-domain-credentials
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># replace &#39;...&#39; with values encoded as base64</span>
</span></span><span style=display:flex><span>  AWS_ACCESS_KEY_ID: ...
</span></span><span style=display:flex><span>  AWS_SECRET_ACCESS_KEY: ...
</span></span></code></pre></div><p>The <strong>External-DNS-Management</strong> project provides examples with more details for <code>DNSProviders</code> (30-provider-&lt;provider-name>.yaml)
and credential <code>Secrets</code> (20-secret-&lt;provider-name>.yaml) at <a href=https://github.com/gardener/external-dns-management/tree/master/examples>https://github.com/gardener/external-dns-management//examples</a>
for all supported provider types.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cbae37fec9d116b4ad59b1b4a7edbe19>2.5.2.4 - Setup</h1><h1 id=gardener-dns-management-for-shoots>Gardener DNS Management for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box.
To support this the gardener must be installed with the <code>shoot-dns-service</code>
extension.
This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS
names for shoot clusters. So, far only the external DNS domain of a shoot
(already used for the kubernetes api server and ingress DNS names) can be used
for managed DNS names.</p><h2 id=configuration>Configuration</h2><p>To generally enable the DNS management for shoot objects the
<code>shoot-dns-service</code> extension must be registered by providing an
appropriate <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available
for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots, the registration must set the <em>globallyEnabled</em> flag to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-dns-service
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=deployment-of-dns-controller-manager>Deployment of DNS controller manager</h3><p>If you are using Gardener version >= <code>1.54</code>, please make sure to deploy the DNS controller manager by
adding the <code>dnsControllerManager</code> section to the <code>providerConfig.values</code> section.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-dns-service
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: ...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    image:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    dnsControllerManager:
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        repository: eu.gcr.io/gardener-project/dns-controller-manager
</span></span><span style=display:flex><span>        tag: v0.13.3
</span></span><span style=display:flex><span>      configuration:
</span></span><span style=display:flex><span>        cacheTtl: 300
</span></span><span style=display:flex><span>        controllers: dnscontrollers,dnssources
</span></span><span style=display:flex><span>        dnsPoolResyncPeriod: 30m
</span></span><span style=display:flex><span>        <span style=color:green>#poolSize: 20</span>
</span></span><span style=display:flex><span>        <span style=color:green>#providersPoolResyncPeriod: 24h</span>
</span></span><span style=display:flex><span>        serverPortHttp: 8080
</span></span><span style=display:flex><span>      createCRDs: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      deploy: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      replicaCount: 1
</span></span><span style=display:flex><span>      <span style=color:green>#resources:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  limits:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    memory: 1Gi</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  requests:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    cpu: 50m</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    memory: 500Mi</span>
</span></span><span style=display:flex><span>    dnsProviderManagement:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=providing-base-domains-usable-for-a-shoot>Providing Base Domains usable for a Shoot</h3><p>So, far only the external DNS domain of a shoot already used
for the kubernetes api server and ingress DNS names can be used for managed
DNS names. This is either the shoot domain as subdomain of the default domain
configured for the gardener installation, or a dedicated domain with dedicated
access credentials configured for a dedicated shoot via the shoot manifest.</p><p>Alternatively, you can specify <code>DNSProviders</code> and its credentials
<code>Secret</code> directly in the shoot, if this feature is enabled.
By default, <code>DNSProvider</code> replication is disabled, but it can be enabled globally in the <code>ControllerDeployment</code>
or for a shoot cluster in the shoot manifest (details see further below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-dns-service
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: ...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    image:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    dnsProviderReplication:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>See <a href=https://github.com/gardener/external-dns-management/tree/master/examples>example files (20-* and 30-*)</a>
for details for the various provider types.</p><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot DNS feature is not globally enabled by default (depends on the
extension registration on the garden cluster), it must be enabled per shoot.</p><p>To enable the feature for a shoot, the shoot manifest must explicitly add the
<code>shoot-dns-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h4 id=enabledisable-dns-provider-replication-for-a-shoot>Enable/disable DNS provider replication for a shoot</h4><p>The DNSProvider` replication feature enablement can be overwritten in the
shoot manifest, e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.dns.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: DNSConfig
</span></span><span style=display:flex><span>        dnsProviderReplication:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-26c3d61f82c9dd9b0643835430f275f6>2.5.3 - Egress filtering</h1><div class=lead>Gardener extension controller for egress filtering for shoot clusters</div><h1 id=gardener-extension-for-networking-filterhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Networking Filter</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-networking-filter</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=extension-resources>Extension Resources</h2><p>Currently there is nothing to specify in the extension spec.</p><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-networking-filter
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create a daemonset <code>egress-filter-applier</code> on the shoot containing either
a <a href=https://github.com/gardener/egress-filter-refresher/tree/master/blackholer>blackholer</a> or <a href=https://github.com/gardener/egress-filter-refresher/tree/master/firewaller>firewaller</a> container.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21e535267e8fae14e7830d803b477da8>2.5.3.1 - Installation</h1><h1 id=gardener-networking-policy-filter-for-shoots>Gardener Networking Policy Filter for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows shoot clusters to filter egress traffic on node level. To support this the Gardener must be installed with the <code>shoot-networking-filter</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the networking filter for shoot objects the <code>shoot-networking-filter</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-networking-filter
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>shoot-networking-filter</code> can be found here: <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml</a></p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploys the <code>shoot-networking-filter</code> to seed clusters. It offers some configuration options, mainly to set up a static filter list or provide the configuration for downloading the filter list from a service endpoint.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    egressFilter:
</span></span><span style=display:flex><span>      blackholingEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      filterListProviderType: static
</span></span><span style=display:flex><span>      staticFilterList:
</span></span><span style=display:flex><span>        - network: 1.2.3.4/31
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>        - network: 5.6.7.8/32
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>        - network: ::2/128
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:green>#filterListProviderType: download</span>
</span></span><span style=display:flex><span>      <span style=color:green>#downloaderConfig:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  endpoint: https://my.filter.list.server/lists/policy</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  oauth2Endpoint: https://my.auth.server/oauth2/token</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  refreshPeriod: 1h</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:green>## if the downloader needs an OAuth2 access token, client credentials can be provided with oauth2Secret</span>
</span></span><span style=display:flex><span>      <span style=color:green>#oauth2Secret:</span>
</span></span><span style=display:flex><span>      <span style=color:green># clientID: 1-2-3-4</span>
</span></span><span style=display:flex><span>      <span style=color:green># clientSecret: secret!!</span>
</span></span><span style=display:flex><span>      <span style=color:green>## either clientSecret of client certificate is required</span>
</span></span><span style=display:flex><span>      <span style=color:green># client.crt.pem: |</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   ...</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>      <span style=color:green># client.key.pem: |</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----BEGIN PRIVATE KEY-----</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   ...</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----END PRIVATE KEY-----</span>
</span></span></code></pre></div><h3 id=enablement-for-a-shoot>Enablement for a Shoot</h3><p>If the shoot networking filter is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-networking-filter</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-9a2a854d8b67658dc92d25a947779f42>2.5.3.2 - Shoot Networking Filter</h1><h1 id=register-shoot-networking-filter-extension-in-shoot-clusters>Register Shoot Networking Filter Extension in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to enable the networking filter. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-networking-filter</code> extension. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-networking-filter</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=opt-out>Opt-out</h2><p>If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-4872a011abb88ad2166ddf01e06b0631>2.5.4 - OpenID Connect services</h1><div class=lead>Gardener extension controller for OpenID Connect services for shoot clusters</div><h1 id=gardener-extension-for-openid-connect-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for openid connect services</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-oidc-service</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>OIDC Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>>= v0.15.0</code></td><td><code>>= 1.60.0 &lt; v1.63.0</code></td><td>Please update Gardener components to <code>>= v1.63.0</code></td><td>A typical side-effect of running this combination is an unexpected scale-down of the OIDC webhook from <code>2 -> 1</code>.</td></tr></tbody></table><hr><h2 id=extension-resources>Extension Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-oidc-service
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-oidc-service
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create an instance of <a href=https://github.com/gardener/oidc-webhook-authenticator>OIDC Webhook Authenticator</a>. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary <code>RBAC</code> resources for the seed as well as for the shoot.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c46226090aca8bf7d4db23eab9831327>2.5.4.1 - Installation</h1><h1 id=gardener-oidc-service-for-shoots>Gardener OIDC Service for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to dynamically register OpenID Connect providers. To support this the Gardener must be installed with the <code>shoot-oidc-service</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the OIDC service for shoot objects the <code>shoot-oidc-service</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-oidc-service
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot OIDC service is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-oidc-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot OIDC service is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e960d1fa36d9bdde684e1a9bf6224898>2.5.4.2 - Openidconnects</h1><h1 id=register-openid-connect-provider-in-shoot-clusters>Register OpenID Connect provider in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to dynamically register OpenID Connect providers. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-oidc-service</code> extension. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-oidc-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=openid-connect-provider>OpenID Connect provider</h2><p>In order to register an OpenID Connect provider an <code>openidconnect</code> resource should be deployed in the shoot cluster.</p><p>It is <strong>strongly</strong> recommended to <strong>NOT</strong> disable prefixing since it may result in unwanted impersonations. The rule of thumb is to always use meaningful and unique prefixes for both <code>username</code> and <code>groups</code>. A good way to ensure this is to use the name of the <code>openidconnect</code> resource as shown in the example below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: authentication.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OpenIDConnect
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  <span style=color:green># issuerURL is the URL the provider signs ID Tokens as.</span>
</span></span><span style=display:flex><span>  <span style=color:green># This will be the &#34;iss&#34; field of all tokens produced by the provider and is used for configuration discovery.</span>
</span></span><span style=display:flex><span>  issuerURL: https://abc-oidc-provider.example
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># clientID is the audience for which the JWT must be issued for, the &#34;aud&#34; field.</span>
</span></span><span style=display:flex><span>  clientID: my-shoot-cluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># usernameClaim is the JWT field to use as the user&#39;s username.</span>
</span></span><span style=display:flex><span>  usernameClaim: sub
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># usernamePrefix, if specified, causes claims mapping to username to be prefix with the provided value.</span>
</span></span><span style=display:flex><span>  <span style=color:green># A value &#34;oidc:&#34; would result in usernames like &#34;oidc:john&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If not provided, the prefix defaults to &#34;( .metadata.name )/&#34;. The value &#34;-&#34; can be used to disable all prefixing.</span>
</span></span><span style=display:flex><span>  usernamePrefix: <span style=color:#a31515>&#34;abc:&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsClaim, if specified, causes the OIDCAuthenticator to try to populate the user&#39;s groups with an ID Token field.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If the groupsClaim field is present in an ID Token the value must be a string or list of strings.</span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsClaim: groups</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsPrefix, if specified, causes claims mapping to group names to be prefixed with the value.</span>
</span></span><span style=display:flex><span>  <span style=color:green># A value &#34;oidc:&#34; would result in groups like &#34;oidc:engineering&#34; and &#34;oidc:marketing&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If not provided, the prefix defaults to &#34;( .metadata.name )/&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The value &#34;-&#34; can be used to disable all prefixing.</span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsPrefix: &#34;abc:&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># caBundle is a PEM encoded CA bundle which will be used to validate the OpenID server&#39;s certificate. If unspecified, system&#39;s trusted certificates are used.</span>
</span></span><span style=display:flex><span>  <span style=color:green># caBundle: &lt;base64 encoded bundle&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># supportedSigningAlgs sets the accepted set of JOSE signing algorithms that can be used by the provider to sign tokens.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The default value is RS256.</span>
</span></span><span style=display:flex><span>  <span style=color:green># supportedSigningAlgs:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - RS256</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># maxTokenExpirationSeconds if specified, sets a limit in seconds to the maximum validity duration of a token.</span>
</span></span><span style=display:flex><span>  <span style=color:green># Tokens issued with validity greater that this value will not be verified.</span>
</span></span><span style=display:flex><span>  <span style=color:green># Setting this will require that the tokens have the &#34;iat&#34; and &#34;exp&#34; claims.</span>
</span></span><span style=display:flex><span>  <span style=color:green># maxTokenExpirationSeconds: 3600</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># jwks if specified, provides an option to specify JWKS keys offline.</span>
</span></span><span style=display:flex><span>  <span style=color:green># jwks:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   keys is a base64 encoded JSON webkey Set. If specified, the OIDCAuthenticator skips the request to the issuer&#39;s jwks_uri endpoint to retrieve the keys.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   keys: &lt;base64 encoded jwks&gt;</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6ad45a4c07647ef1437167d1474c3cfe>3 - Dashboard</h1><div class=lead>The web UI for managing your projects and clusters</div><h1 id=gardener-dashboard>Gardener Dashboard</h1><p><img src=/__resources/logo_gardener_dashboard_75c046.png alt></p><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/dashboard-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/dashboard-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://kubernetes.slack.com/messages/gardener><img src="https://img.shields.io/badge/slack-gardener-brightgreen.svg?logo=slack" alt="Slack channel #gardener"></a></p><h2 id=demo>Demo</h2><img src=/__resources/dashboard-demo_f287c1.gif alt="Gardener Demo"><h2 id=documentation>Documentation</h2><p><a href=https://github.com/gardener/dashboard/blob/master/docs/README.md>Gardener Dashboard Documentation</a></p><h2 id=people>People</h2><p>The following SAP developers contributed to this project until this
initial contribution was published as open source.</p><table><thead><tr><th>contributor</th><th style=text-align:right>commits (%)</th><th style=text-align:right>+lines</th><th style=text-align:right>-lines</th><th>first commit</th><th>last commit</th></tr></thead><tbody><tr><td>Holger Koser</td><td style=text-align:right>313 (42%)</td><td style=text-align:right>57878</td><td style=text-align:right>18562</td><td>2017-07-13</td><td>2018-01-23</td></tr><tr><td>Andreas Herz</td><td style=text-align:right>307 (41%)</td><td style=text-align:right>13666</td><td style=text-align:right>11099</td><td>2017-07-14</td><td>2017-10-27</td></tr><tr><td>Peter Sutter</td><td style=text-align:right>99 (13%)</td><td style=text-align:right>4838</td><td style=text-align:right>3967</td><td>2017-11-07</td><td>2018-01-23</td></tr><tr><td>Gross, Lukas</td><td style=text-align:right>31 (4%)</td><td style=text-align:right>400</td><td style=text-align:right>267</td><td>2018-01-10</td><td>2018-01-23</td></tr></tbody></table><p>It is derived from the historical, internal <em>gardener-ui</em> repository
at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.</p><h2 id=license>License</h2><p><a href=https://github.com/gardener/dashboard/blob/master/LICENSES/Apache-2.0.txt>Apache License 2.0</a></p><p>Copyright 2020 The Gardener Authors</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8b422ac48ca14791153d56297cd4263f>3.1 - Architecture</h1><h1 id=dashboard-architecture-overview>Dashboard Architecture Overview</h1><h2 id=overview>Overview</h2><p>The dashboard <code>frontend</code> is a Single Page Application (SPA) built with <a href=https://vuejs.org/>Vue.js</a>. The dashboard <code>backend</code> is web server build with <a href=http://expressjs.com>Express</a> and <a href=https://nodejs.org/>Node.js</a>. The <code>backend</code> serves the bundled <code>frontend</code> as static content. The dashboard uses <a href=https://socket.io/>Socket.IO</a> to enable real-time, bidirectional and event-based communication between the <code>frontend</code> and the <code>backend</code>. For the communication from the <code>backend</code> to different <code>kube-apiservers</code> the http/2 network protocol is used. Authentication at the <code>apiserver</code> of the garden cluster is done via JWT tokens. These can either be an ID Token issued by an OpenID Connect Provider or the token of a Kubernetes Service Account.</p><img src=/__resources/architecture-1_09ad4f.png><h2 id=frontend>Frontend</h2><p>The dashboard <code>frontend</code> consists of many Vue.js single file components that manage their state via a <a href=https://vuex.vuejs.org/>centralized store</a>. The store defines mutations to modify the state synchronously. If several mutations have to be combined or the state in the <code>backend</code> has to be modified at the same time, the store provides asynchronous actions to do this job. The synchronization of the data with the <code>backend</code> is done by plugins that also use actions.</p><img src=/__resources/architecture-2_718ce1.png><h2 id=backend>Backend</h2><p>The <code>backend</code> is currently a monolithic Node.js application, but it performs several tasks that are actually independent.</p><ul><li>Static web server for the <code>frontend</code> single page application</li><li>Forward real time events of the <code>apiserver</code> to the <code>frontend</code></li><li>Provide an HTTP Api</li><li>Bootstrapping shoot and seed clusters to support web terminals</li><li>Initiate and manage the end user login flow in order to obtain an ID Token</li><li>Bidirectional integration with the github issue management</li></ul><img src=/__resources/architecture-3_c848c2.png><p>It is planed to split the <code>backend</code> into several independent containers to increase stability and performance.</p><h2 id=authentication>Authentication</h2><p>The following diagram shows the authorization code flow in the gardener dashboard. When the user clicks the login button he is redirected to the authorization endpoint of the openid connect provider. In the case of <a href=https://dexidp.io/>Dex IDP</a>, authentication is delegated to the connected IDP. After successful login, the OIDC provider redirects back to the dashboard <code>backend</code> with a one time authorization code. With this code the dashboard <code>backend</code> can now request an ID token for the logged in user. The ID token is encrypted and stored as a secure httpOnly session cookie.</p><img src=/__resources/architecture-4_371bde.png></div><div class=td-content style=page-break-before:always><h1 id=pg-4d21f515d35e131d7c72df0ccecc36d6>3.2 - Concepts</h1></div><div class=td-content><h1 id=pg-c7dc017ae3b409ab334a5c29414fd5a6>3.2.1 - Webterminals</h1><h1 id=webterminals>Webterminals</h1><img width=180 alt=gardener-terminal-ascii src=https://user-images.githubusercontent.com/5526658/66032047-ecfacc80-e504-11e9-9864-57d4f0bbaf5d.png><h2 id=architecture-overview>Architecture Overview</h2><img src=/__resources/webterminals-1_a34bff.png><h2 id=motivation>Motivation</h2><p>We want to give garden operators and &ldquo;regular&rdquo; users of the Gardener dashboard an easy way to have a preconfigured shell directly in the browser.</p><p>This has several advantages:</p><ul><li>no need to set up any tools locally</li><li>no need to download / store kubeconfigs locally</li><li>Each terminal session will have its own &ldquo;access&rdquo; service account created. This makes it easier to see &ldquo;who&rdquo; did &ldquo;what&rdquo; when using the web terminals.</li><li>The &ldquo;access&rdquo; service account is deleted when the terminal session expires</li><li>Easy &ldquo;privileged&rdquo; access to a node (privileged container, hostPID, and hostNetwork enabled, mounted host root fs) in case of troubleshooting node. If allowed by PSP.</li></ul><h2 id=how-its-done---tldr>How it&rsquo;s done - TL;DR</h2><p>On the host cluster, we schedule a pod to which the dashboard frontend client attaches to (similar to <code>kubectl attach</code>). Usually the <a href=https://github.com/gardener/ops-toolbelt/><code>ops-toolbelt</code></a> image is used, containing all relevant tools like <code>kubectl</code>. The Pod has a kubeconfig secret mounted with the necessary privileges for the target cluster - usually <code>cluster-admin</code>.</p><h2 id=target-types>Target types</h2><p>There are currently three targets, where a user can open a terminal session to:</p><ul><li>The (virtual) garden cluster - Currently operator only</li><li>The shoot cluster</li><li>The control plane of the shoot cluster - operator only</li></ul><h2 id=host>Host</h2><p>There are different factors on where the host cluster (and namespace) is chosen by the dashboard:</p><ul><li>Depending on, the selected target and the role of the user (operator or &ldquo;regular&rdquo; user) the host is chosen.</li><li>For performance / low latency reasons, we want to place the &ldquo;terminal&rdquo; pods as near as possible to the target kube-apiserver.</li></ul><p>For example, the user wants to have a terminal for a shoot cluster. The kube-apiserver of the shoot is running in the seed-shoot-ns on the seed.</p><ul><li>If the user is an operator, we place the &ldquo;terminal&rdquo; pod directly in the seed-shoot-ns on the seed.</li><li>However, if the user is a &ldquo;regular&rdquo; user, we don’t want to have &ldquo;untrusted&rdquo; workload scheduled on the seeds, that&rsquo;s why the &ldquo;terminal&rdquo; pod is scheduled on the shoot itself, in a temporary namespace that is deleted afterwards.</li></ul><h2 id=lifecycle-of-a-web-terminal-session>Lifecycle of a Web Terminal Session</h2><h3 id=1-browser--dashboard-frontend---open-terminal>1. Browser / Dashboard Frontend - Open Terminal</h3><p>User chooses the target and clicks in the browser on <code>Open terminal</code> button. A POST request is made to the dashboard backend to request a new terminal session.</p><h3 id=2-dashboard-backend---create-terminal-resource>2. Dashboard Backend - Create Terminal Resource</h3><p>According to the privileges of the user (operator / enduser) and the selected target, the dashboard backend creates a <code>terminal</code> resource <strong>on behalf of the user</strong> in the (virtual) garden and responds with a handle to the terminal session.</p><h3 id=3-browser--dashboard-frontend>3. Browser / Dashboard Frontend</h3><p>The frontend makes another POST request to the dashboard backend to fetch the terminal session. The Backend waits until the <code>terminal</code> resource is in a &ldquo;ready&rdquo; state (timeout 10s) before sending a response to the frontend. More to that later.</p><h3 id=4-terminal-resource>4. Terminal Resource</h3><p>The <code>terminal</code> resource, among other things, holds the information of the desired host and target cluster. The credentials to these clusters are declared as references (secretRef / serviceAccountRef). The <code>terminal</code> resource itself doesn’t contain sensitive information.</p><h3 id=5-admission>5. Admission</h3><p>A validating webhook is in place to ensure that the user, that created the <code>terminal</code> resource, has the <strong>permission to read the referenced credentials</strong>. There is also a mutating webhook in place. Both admission configurations have <strong><code>failurePolicy: Fail</code></strong>.</p><h3 id=6-terminal-controller-manager---apply-resources-on-host--target-cluster>6. Terminal-Controller-Manager - Apply Resources on Host & Target Cluster</h3><p><em>Sidenote: The terminal-controller-manager has no knowledge about the gardener, its shoots, and seeds. In that sense it can be considered as independent from the gardener.</em></p><p>The <a href=https://github.com/gardener/terminal-controller-manager>terminal-controller-manager</a> watches <code>terminal</code> resources and ensures the desired state on the host and target cluster. The terminal-controller-manager needs the permission to read all secrets / service accounts in the virtual garden.
As additional safety net, the <strong>terminal-controller-manager</strong> ensures that the <code>terminal</code> resource was not created before the admission configurations were created.</p><p>The terminal-controller-manager then creates the necessary resources in the host and target cluster.</p><ul><li>Target Cluster:<ul><li>&ldquo;Access&rdquo; service account + (cluster)rolebinding usually to <code>cluster-admin</code> cluster role<ul><li>used from within the &ldquo;terminal&rdquo; pod</li></ul></li></ul></li><li>Host Cluster:<ul><li>&ldquo;Attach&rdquo; service Account + rolebinding to &ldquo;attach&rdquo; cluster role (privilege to attach and get pod)<ul><li>will be used by the browser to attach to the pod</li></ul></li><li>Kubeconfig secret, containing the &ldquo;access&rdquo; token from the target cluster</li><li>The &ldquo;terminal&rdquo; pod itself, having the kubeconfig secret mounted</li></ul></li></ul><h3 id=7-dashboard-backend---responds-to-frontend>7. Dashboard Backend - Responds to Frontend</h3><p>As mentioned in step 3, the dashboard backend waits until the <code>terminal</code> resource is &ldquo;ready&rdquo;. It then reads the &ldquo;attach&rdquo; token from the host cluster <strong>on behalf of the user</strong>.
It responds with:</p><ul><li>attach token</li><li>hostname of the host cluster&rsquo;s api server</li><li>name of the pod and namespace</li></ul><h3 id=8-browser--dashboard-frontend---attach-to-pod>8. Browser / Dashboard Frontend - Attach to Pod</h3><p>Dashboard frontend attaches to the pod located on the host cluster by opening a WebSocket connection using the provided parameter and credentials.
As long as the terminal window is open, the dashboard regularly annotates the <code>terminal</code> resource (heartbeat) to keep it alive.</p><h3 id=9-terminal-controller-manager---cleanup>9. Terminal-Controller-Manager - Cleanup</h3><p>When there is no heartbeat on the <code>terminal</code> resource for a certain amount of time (default is <code>5m</code>) the created resources in the host and target cluster are cleaned up again and the <code>terminal</code> resource will be deleted.</p><h2 id=browser-trusted-certificates-for-kube-apiservers>Browser Trusted Certificates for Kube-Apiservers</h2><h3 id=motivation-1>Motivation</h3><p>The dashboard frontend opens up a secure WebSocket connection to the kube-apiserver. The certificate presented by the kube-apiserver must be browser trusted, otherwise the connection can&rsquo;t be established (rejected by browser policy).
Most kube-apiservers have self-signed certificates from a custom Root CA.</p><h3 id=bootstrapping>Bootstrapping</h3><h4 id=preferred-solution>Preferred Solution</h4><p>There is an <a href=https://github.com/gardener/gardener/issues/1413>issue</a> on the gardener component, to have browser trusted certificates for shoot kube-apiservers using SNI and certmanager.
However, this would solve the issue for shoots and shooted-seeds, but not for soil and plant kube-apiservers and potentially others.</p><h4 id=current-solution>Current Solution</h4><p>We had to &ldquo;workaround&rdquo; it by creating ingress resources for the kube-apiservers and letting the certmanager (or the new <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>shoot cert service</a>) request browser trusted certificates.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fded572106ea032f6f7259ad6d91a5a9>3.3 - Deployment</h1></div><div class=td-content><h1 id=pg-2b3c0da553737ad4f6d8ce03143568fa>3.3.1 - Access Restrictions</h1><h1 id=access-restrictions>Access Restrictions</h1><p>The dashboard can be configured with access restrictions.</p><img src=/__resources/access-restrictions-1_071de9.png><p>Access restrictions are shown for regions that have a matching label in the <code>CloudProfile</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: pangaea-north-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: pangaea-north-1a
</span></span><span style=display:flex><span>    - name: pangaea-north-1b
</span></span><span style=display:flex><span>    - name: pangaea-north-1c
</span></span><span style=display:flex><span>    labels:
</span></span><span style=display:flex><span>      seed.gardener.cloud/eu-access: <span style=color:#a31515>&#34;true&#34;</span>
</span></span></code></pre></div><ul><li>If the user selects the access restriction, <code>spec.seedSelector.matchLabels[key]</code> will be set.</li><li>When selecting an option, <code>metadata.annotations[optionKey]</code> will be set.</li></ul><p>The value that is set depends on the configuration. See <em>2.</em> under <em>Configuration</em> section below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    support.gardener.cloud/eu-access-for-cluster-addons: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    support.gardener.cloud/eu-access-for-cluster-nodes: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  seedSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      seed.gardener.cloud/eu-access: <span style=color:#a31515>&#34;true&#34;</span>
</span></span></code></pre></div><p>In order for the shoot (with enabled access restriction) to be scheduled on a seed, the seed needs to have the label set. E.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    seed.gardener.cloud/eu-access: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><img src=/__resources/access-restrictions-2_2e2c49.png><p><strong>Configuration</strong>
As gardener administrator:</p><ol><li>you can control the visibility of the chips with the <code>accessRestriction.items[].display.visibleIf</code> and <code>accessRestriction.items[].options[].display.visibleIf</code> property. E.g. in this example the access restriction chip is shown if the value is true and the option is shown if the value is false.</li><li>you can control the value of the input field (switch / checkbox) with the <code>accessRestriction.items[].input.inverted</code> and <code>accessRestriction.items[].options[].input.inverted</code> property. Setting the <code>inverted</code> property to <code>true</code> will invert the value. That means that when selecting the input field the value will be<code>'false'</code> instead of <code>'true'</code>.</li><li>you can configure the text that is displayed when no access restriction options are available by setting <code>accessRestriction.noItemsText</code>
example <code>values.yaml</code>:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>accessRestriction:
</span></span><span style=display:flex><span>  noItemsText: <span style=color:#00f>No</span> access restriction options available for region {region} and cloud profile {cloudProfile}
</span></span><span style=display:flex><span>  items:
</span></span><span style=display:flex><span>  - key: seed.gardener.cloud/eu-access
</span></span><span style=display:flex><span>    display:
</span></span><span style=display:flex><span>      visibleIf: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      <span style=color:green># title: foo # optional title, if not defined key will be used</span>
</span></span><span style=display:flex><span>      <span style=color:green># description: bar # optional description displayed in a tooltip</span>
</span></span><span style=display:flex><span>    input:
</span></span><span style=display:flex><span>      title: EU Access
</span></span><span style=display:flex><span>      description: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>        </span>        This service is offered to you with our regular SLAs and 24x7 support for the control plane of the cluster. 24x7 support for cluster add-ons and nodes is only available if you meet the following conditions:
</span></span><span style=display:flex><span>    options:
</span></span><span style=display:flex><span>    - key: support.gardener.cloud/eu-access-for-cluster-addons
</span></span><span style=display:flex><span>      display:
</span></span><span style=display:flex><span>        visibleIf: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>        <span style=color:green># title: bar # optional title, if not defined key will be used</span>
</span></span><span style=display:flex><span>        <span style=color:green># description: baz # optional description displayed in a tooltip</span>
</span></span><span style=display:flex><span>      input:
</span></span><span style=display:flex><span>        title: <span style=color:#00f>No</span> personal data is used as name or in the content of Gardener or Kubernetes resources (e.g. Gardener project name or Kubernetes namespace, configMap or secret in Gardener or Kubernetes)
</span></span><span style=display:flex><span>        description: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          </span>          If you can&#39;t comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all cluster add-ons such as DNS and certificates, Calico overlay network and network policies, kube-proxy and services, and everything else that would require direct inspection of your cluster through its API server
</span></span><span style=display:flex><span>        inverted: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    - key: support.gardener.cloud/eu-access-for-cluster-nodes
</span></span><span style=display:flex><span>      display:
</span></span><span style=display:flex><span>        visibleIf: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      input:
</span></span><span style=display:flex><span>        title: <span style=color:#00f>No</span> personal data is stored in any Kubernetes volume except for container file system, emptyDirs, and persistentVolumes (in particular, not on hostPath volumes)
</span></span><span style=display:flex><span>        description: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          </span>          If you can&#39;t comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all node-related components such as Docker and Kubelet, the operating system, and everything else that would require direct inspection of your nodes through a privileged pod or SSH
</span></span><span style=display:flex><span>        inverted: <span style=color:#00f>true</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-cf4e25b6fc85473b93fdbd5f490b779f>3.3.2 - Theming</h1><h1 id=theming>Theming</h1><h2 id=motivation>Motivation</h2><p>Gardener landscape administrators should have the possibility to change the appearance of the Gardener Dashboard via configuration without the need to touch the code.</p><h2 id=colors>Colors</h2><p>Gardener Dashboard has been built with Vuetify. We use Vuetify&rsquo;s built-in <a href=https://vuetifyjs.com/en/features/theme/>theming support</a> to centrally configure colors that are used throughout the web application.
Colors can be configured for both light and dark themes. Configuration is done via the helm chart, see the respective theme section there. Colors can be specified as HTML color code (e.g. #FF0000 for red) or by referencing a color from Vuetify&rsquo;s Material Design <a href=https://vuetifyjs.com/en/styles/colors/#javascript-color-pack>Color Pack</a>.</p><p>The following colors can be configured:</p><table><thead><tr><th>name</th><th>usage</th></tr></thead><tbody><tr><td><code>primary</code></td><td>icons, chips, buttons, popovers, etc.</td></tr><tr><td><code>anchor</code></td><td>links</td></tr><tr><td><code>main-background</code></td><td>main navigation, login page</td></tr><tr><td><code>main-navigation-title</code></td><td>text color on main navigation</td></tr><tr><td><code>toolbar-background</code></td><td>background color for toolbars in cards, dialogs, etc.</td></tr><tr><td><code>toolbar-title</code></td><td>text color for toolbars in cards, dialogs, etc.</td></tr><tr><td><code>action-button</code></td><td>buttons in tables and cards, e.g. cluster details page</td></tr><tr><td><code>info</code></td><td>Snotify info popups</td></tr><tr><td><code>warning</code></td><td>Snotify warning popups, warning texts</td></tr><tr><td><code>error</code></td><td>Snotify error popups, error texts</td></tr></tbody></table><p>If you use the helm chart, you can configure those with <code>frontendConfig.themes.light</code> for the light theme and <code>frontendConfig.themes.dark</code> for the dark theme.</p><h3 id=example>Example</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>frontend:
</span></span><span style=display:flex><span>  themes:
</span></span><span style=display:flex><span>    light:
</span></span><span style=display:flex><span>      primary: <span style=color:#a31515>&#39;#0b8062&#39;</span>
</span></span><span style=display:flex><span>      anchor: <span style=color:#a31515>&#39;#0b8062&#39;</span>
</span></span><span style=display:flex><span>      main-background: <span style=color:#a31515>&#39;grey.darken3&#39;</span>
</span></span><span style=display:flex><span>      main-navigation-title: <span style=color:#a31515>&#39;shades.white&#39;</span>
</span></span><span style=display:flex><span>      toolbar-background: <span style=color:#a31515>&#39;#0b8062&#39;</span>
</span></span><span style=display:flex><span>      toolbar-title: <span style=color:#a31515>&#39;shades.white&#39;</span>
</span></span><span style=display:flex><span>      action-button: <span style=color:#a31515>&#39;grey.darken4&#39;</span>
</span></span></code></pre></div><h2 id=logos-and-icons>Logos and Icons</h2><p>It is also possible to exchange the Dashboard logo and icons. You can replace the <a href=https://github.com/gardener/dashboard/tree/master/frontend/public/static/assets>assets</a> folder when using the <a href=https://github.com/gardener/dashboard/blob/master/charts/gardener-dashboard>helm chart</a> in the <code>frontendConfig.assets</code> map.</p><p>Attention: You need to set values for all files as mapping the volume will overwrite all files. It is not possible to exchange single files.</p><p>The files have to be encoded as base64 for the chart - to generate the encoded files for the <code>values.yaml</code> of the helm chart, you can use the following shorthand with <code>bash</code> or <code>zsh</code> on Linux systems. If you use macOS, install coreutils with brew (<code>brew install coreutils</code>) or remove the <code>-w0</code> parameter.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#a31515>  ###
</span></span></span><span style=display:flex><span><span style=color:#a31515>  ### COPY EVERYTHING BELOW THIS LINE
</span></span></span><span style=display:flex><span><span style=color:#a31515>  ###
</span></span></span><span style=display:flex><span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  assets:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    favicon-16x16.png: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      $(cat frontend/public/static/assets/favicon-16x16.png | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>    favicon-32x32.png: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      $(cat frontend/public/static/assets/favicon-32x32.png | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>    favicon-96x96.png: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      $(cat frontend/public/static/assets/favicon-96x96.png | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>    favicon.ico: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      $(cat frontend/public/static/assets/favicon.ico | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>    logo.svg: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>      $(cat frontend/public/static/assets/logo.svg | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>Then, swap in the base64 encoded version of your files where needed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6f9920e1d343cae23d179cb8c8598b26>3.4 - Development</h1></div><div class=td-content><h1 id=pg-cefe386e9eb1b2d605f2fbd40762eac2>3.4.1 - Local Setup</h1><h1 id=local-development>Local development</h1><p float=left><img width=90 src=/__resources/logo_gardener_dashboard_727121.png>
<img width=200 src=https://raw.githubusercontent.com/yarnpkg/assets/master/yarn-kitten-full.png></p><h2 id=purpose>Purpose</h2><p>Develop new feature and fix bug on the Gardener Dashboard.</p><h2 id=requirements>Requirements</h2><ul><li>Yarn. For the required version, refer to <code>.engines.yarn</code> in <a href=https://github.com/gardener/dashboard/blob/master/package.json>package.json</a>.</li><li>Node.js. For the required version, refer to <code>.engines.node</code> in <a href=https://github.com/gardener/dashboard/blob/master/package.json>package.json</a>.</li></ul><h2 id=steps>Steps</h2><h3 id=1-clone-repository>1. Clone repository</h3><p>Clone the <a href=https://github.com/gardener/dashboard.git>gardener/dashboard</a> repository</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>git clone git@github.com:gardener/dashboard.git
</span></span></code></pre></div><h3 id=2-install-dependencies>2. Install dependencies</h3><p>Run <code>yarn</code> at the repository root to install all dependencies.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cd dashboard
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>yarn
</span></span></code></pre></div><h3 id=3-configuration>3. Configuration</h3><p>Place the Gardener Dashboard configuration under <code>${HOME}/.gardener/config.yaml</code> or alternatively set the path to the configuration file using the <code>GARDENER_CONFIG</code> environment variable.</p><p>A local configuration example could look like follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>port: 3030
</span></span><span style=display:flex><span>logLevel: debug
</span></span><span style=display:flex><span>logFormat: text
</span></span><span style=display:flex><span>apiServerUrl: https://my-local-cluster <span style=color:green># garden cluster kube-apiserver url - kubectl config view --minify -ojsonpath=&#39;{.clusters[].cluster.server}&#39;</span>
</span></span><span style=display:flex><span>sessionSecret: c2VjcmV0                <span style=color:green># symmetric key used for encryption</span>
</span></span><span style=display:flex><span>frontend:
</span></span><span style=display:flex><span>  dashboardUrl:
</span></span><span style=display:flex><span>    pathname: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/
</span></span><span style=display:flex><span>  defaultHibernationSchedule:
</span></span><span style=display:flex><span>    evaluation:
</span></span><span style=display:flex><span>    - start: 00 17 * * 1,2,3,4,5
</span></span><span style=display:flex><span>    development:
</span></span><span style=display:flex><span>    - start: 00 17 * * 1,2,3,4,5
</span></span><span style=display:flex><span>      end: 00 08 * * 1,2,3,4,5
</span></span><span style=display:flex><span>    production: ~
</span></span></code></pre></div><h3 id=5-run-it-locally>5. Run it locally</h3><p>The Gardener Dashboard <a href=https://github.com/gardener/dashboard/tree/master/backend><code>backend</code></a> server requires a kubeconfig for the Garden cluster. You can set it e.g. by using the <code>KUBECONFIG</code> environment variable.</p><p>If you want to run the Garden cluster locally, follow the <a href=/docs/gardener/development/getting_started_locally/>getting started locally</a> documentation.</p><p>Concurrently run the <code>backend</code> server (port <code>3030</code>) and the <a href=https://github.com/gardener/dashboard/tree/master/frontend><code>frontend</code></a> server (port <code>8080</code>) with hot reload enabled.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cd backend
</span></span><span style=display:flex><span>export KUBECONFIG=/path/to/garden/cluster/kubeconfig.yaml
</span></span><span style=display:flex><span>yarn serve
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cd frontend
</span></span><span style=display:flex><span>yarn serve
</span></span></code></pre></div><p>You can now access the UI on http://localhost:8080/</p><h2 id=build>Build</h2><p>Build docker image locally.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make build
</span></span></code></pre></div><h2 id=push>Push</h2><p>Push docker image to Google Container Registry.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make push
</span></span></code></pre></div><p>This command expects a valid gcloud configuration named <code>gardener</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>gcloud config configurations describe gardener
</span></span><span style=display:flex><span>is_active: true
</span></span><span style=display:flex><span>name: gardener
</span></span><span style=display:flex><span>properties:
</span></span><span style=display:flex><span>  core:
</span></span><span style=display:flex><span>    account: john.doe@example.org
</span></span><span style=display:flex><span>    project: johndoe-1008
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-08e980a2b66d32056398b13e93494854>3.4.2 - Testing</h1><h1 id=testing>Testing</h1><h2 id=jest>Jest</h2><p>We use Jest JavaScript Testing Framework</p><img width=200 src=https://jestjs.io/img/jest.svg><ul><li>Jest can collect code coverage information​</li><li>Jest support snapshot testing out of the box​</li><li>All in One solution. Replaces Mocha, Chai, Sinon and Istanbul​</li><li>It works with Vue.js and Node.js projects​</li></ul><p>To execute all tests, simply run</p><pre tabindex=0><code>yarn workspaces foreach --all run test
</code></pre><p>or to include test coverage generation</p><pre tabindex=0><code>yarn workspaces foreach --all run test-coverage
</code></pre><p>You can also run tests for frontend, backend and charts directly inside the respective folder via</p><pre tabindex=0><code>yarn test
</code></pre><h2 id=lint>Lint</h2><p>We use ESLint for static code analyzing.</p><img width=200 src=https://d33wubrfki0l68.cloudfront.net/204482ca413433c80cd14fe369e2181dd97a2a40/092e2/assets/img/logo.svg><p>To execute, run</p><pre tabindex=0><code>yarn workspaces foreach --all run lint
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-c5261d8eb8b1bd669e38eac19fb095c8>3.5 - Usage</h1></div><div class=td-content><h1 id=pg-cb9d88908acbc18ce280d84ae90569c4>3.5.1 - Accessing the Gardener API</h1><h1 id=accessing-the-gardener-api-through-the-dashboard>Accessing the Gardener API Through the Dashboard</h1><h2 id=overview>Overview</h2><p>The cluster operations that are performed manually in the dashboard or via <code>kubectl</code> can be automated using the <a href=/docs/gardener/api-reference/><strong>Gardener API</strong></a> and a <strong>Service Account</strong> authorized to perform them.</p><h2 id=create-a-service-account>Create a Service Account</h2><h3 id=prerequisites>Prerequisites</h3><ul><li>You are logged on to the Gardener Dashboard</li><li>You have <a href=/docs/dashboard/usage/working-with-projects/>created a project</a></li></ul><h3 id=steps>Steps</h3><ol><li><p>Select your project and choose <em>MEMBERS</em> from the menu on the left.</p></li><li><p>Locate the section <em>Service Accounts</em> and choose <em>+</em>.</p><p><img src=/__resources/01-add-service-account_553867.png alt="Add service account"></p></li><li><p>Enter the service account details.</p><p><img src=/__resources/02-enter-service-account-details_e9e737.png alt="Enter service account details"></p><p>The following <em>Roles</em> are available:</p></li></ol><table><thead><tr><th style=text-align:left>Role</th><th style=text-align:left>Granted Permissions</th></tr></thead><tbody><tr><td style=text-align:left><em>Owner</em></td><td style=text-align:left>Combines the <em>Admin</em>, <em>UAM</em> and <em>Service Account Manager</em> roles. There can only be one owner per project. You can change the owner on the project administration page.</td></tr><tr><td style=text-align:left><em>Admin</em></td><td style=text-align:left>Allows to manage resources inside the project (e.g. secrets, shoots, configmaps and similar) and to manage permissions for service accounts. Note that the <em>Admin</em> role has read-only access to service accounts.</td></tr><tr><td style=text-align:left><em>Viewer</em></td><td style=text-align:left>Provides read access to project details and shoots. Has access to shoots but is not able to create new ones. Cannot read cloud provider secrets.</td></tr><tr><td style=text-align:left><em>UAM</em></td><td style=text-align:left>Allows to add/modify/remove human users, service accounts or groups to/from the project member list. In case an external UAM system is connected via a service account, only this account should get the <em>UAM</em> role.</td></tr><tr><td style=text-align:left><em><a href=/docs/gardener/usage/service-account-manager/>Service Account Manager</a></em></td><td style=text-align:left>Allows to manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the <em>Admin</em> role. For security reasons this role should not be assigned to service accounts. In particular it should be ensured that the service account is not able to refresh service account tokens forever.</td></tr></tbody></table><ol start=4><li>Choose <em>CREATE</em>.</li></ol><h2 id=use-the-service-account>Use the Service Account</h2><p>To use the service account, download or copy its <code>kubeconfig</code>. With it you can connect to the API endpoint of your Gardener project.</p><p><img src=/__resources/03-download-service-account-kubeconfig_08a0f5.png alt="Download service account kubeconfig"></p><blockquote><p>Note: The downloaded <code>kubeconfig</code> contains the service account credentials. Treat with care.</p></blockquote><h2 id=delete-the-service-account>Delete the Service Account</h2><p>Choose <em>Delete Service Account</em> to delete it.</p><p><img src=/__resources/04-delete-service-account_ecfc83.png alt="Delete service account"></p><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/usage/service-account-manager/>Service Account Manager</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-aa5ce6ab58af085fb770047f13702ed1>3.5.2 - Connect Kubectl</h1><h1 id=connect-kubectl>Connect kubectl</h1><p>In Kubernetes, the configuration for access to your cluster is a format known as <code>kubeconfig</code> that is stored as a file. It contains details such as cluster API server addresses and access credentials or a command to get the access credential from a <code>kubectl</code> credential plugin. In general, treat a <code>kubeconfig</code> as sensitive data. Tools like <code>kubectl</code> use the <code>kubeconfig</code> to connect and authenticate to a cluster and perform operations on it.
Learn more about <a href=https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig</a> and <a href=https://kubernetes.io/docs/reference/kubectl/>kubectl</a> on <a href=https://kubernetes.io>kubernetes.io</a>.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You are logged on to the Gardener Dashboard.</li><li>You have created a cluster and its status is operational.</li></ul><p>On this page:</p><ul><li><a href=#downloading-kubeconfig-for-a-cluster>Downloading kubeconfig for a cluster</a></li><li><a href=#connecting-to-the-cluster>Connecting to the cluster</a></li><li><a href=#exporting-kubeconfig-environment-variable>Exporting KUBECONFIG environment variable</a></li></ul><br><h3 id=downloading-kubeconfig-for-a-cluster>Downloading kubeconfig for a cluster</h3><ol><li><p>Select your project from the dropdown on the left, then choose <strong>CLUSTERS</strong> and locate your cluster in the list. Choose the <em>key</em> icon to bring up a dialog with the access options.</p><img width=1000 src=/__resources/01-select-cluster_524ec7.png><p>In the <strong>Kubeconfig - Gardenlogin</strong> section the options are to <em>show gardenlogin info</em>, <em>download</em>, <em>copy</em> or <em>view</em> the <code>kubeconfig</code> for the cluster.
The same options are available also in the <strong>Access</strong> section in the cluster details screen. To find it, choose a cluster from the list.
Enabling the static token kubeconfig is not recommended and you should consider to disable it for your cluster, if not already done. Instead use the <code>gardenlogin</code> <code>kubeconfig</code>.</p><img width=1000 src=/__resources/01-access-1_742623.png></li><li><p>Choose the download icon to download the <code>kubeconfig</code> as file on your local system.</p><img width=400 src=/__resources/02-download_48ab9b.png></li><li><p>If <code>gardenlogin</code> is not installed or configured, click on the <em>show gardenlogin info</em> action to follow the installation and configuration hints.</p><img width=700 src=/__resources/03-gardenlogin-info_6d0e81.png></li><li><p>You might also need to install <a href=https://github.com/int128/kubelogin#setup>kubelogin</a></p></li></ol><h3 id=connecting-to-the-cluster>Connecting to the cluster</h3><p>In the following command, change <code>&lt;path-to-gardenlogin-kubeconfig></code> with the actual path to the file where you stored the <code>kubeconfig</code> downloaded in the previous step 2.</p><pre tabindex=0><code>$ kubectl --kubeconfig=&lt;path-to-gardenlogin-kubeconfig&gt; get namespaces
</code></pre><p>The command connects to the cluster and list its namespaces.</p><h3 id=exporting-kubeconfig-environment-variable>Exporting KUBECONFIG environment variable</h3><p>Since many <code>kubectl</code> commands will be used, it’s a good idea to take advantage of every opportunity to shorten the expressions. The <code>kubectl</code> tool has a fallback strategy for looking up a kubeconfig to work with. For example, it looks for the <code>KUBECONFIG</code> environment variable with value that is the path to the <code>kubeconfig</code> file meant to be used. Export the variable:</p><pre tabindex=0><code>$ export KUBECONFIG=&lt;path-to-gardenlogin-kubeconfig&gt;
</code></pre><p>In the previous snippet make sure to change the <code>&lt;path-to-gardenlogin-kubeconfig></code> with the path to the kubeconfig for the cluster that you want to connect to on your system.</p><br><h2 id=whats-next>What&rsquo;s next?</h2><ul><li><a href=/docs/dashboard/usage/using-terminal/>Using Dashboard Terminal</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-649b4b9d61dd468416ed2f342ec25614>3.5.3 - Custom Fields</h1><h1 id=custom-shoot-fields>Custom Shoot Fields</h1><p>The Dashboard supports custom shoot fields, that can be defined per project by specifying <code>metadata.annotations["dashboard.gardener.cloud/shootCustomFields"]</code>.
The fields can be configured to be displayed on the cluster list and cluster details page.
Custom fields do not show up on the <code>ALL_PROJECTS</code> page.</p><h2 id=project-administration-page>Project administration page:</h2><p>Each custom field configuration is shown with it&rsquo;s own chip.</p><img width=800 src=/__resources/custom-fields-1_791812.png><p>Click on the chip to show more details for the custom field configuration.</p><img width=800 src=/__resources/custom-fields-2_ed16ee.png><p>Custom fields can be shown on the cluster list, if <code>showColumn</code> is enabled. See <a href=#configuration>configuration</a> below for more details. In this example, a custom field for the Shoot status was configured.</p><img width=1400 src=/__resources/custom-fields-3_f57b82.png><p>Custom fields can be shown in a dedicated card (<code>Custom Fields</code>) on the cluster details page, if <code>showDetails</code> is enabled. See <a href=#configuration>configuration</a> below for more details.</p><img width=800 src=/__resources/custom-fields-4_e9dfc0.png><h2 id=configuration>Configuration</h2><table><thead><tr><th>Property</th><th>Type</th><th>Default</th><th>Required</th><th>Description</th></tr></thead><tbody><tr><td>name</td><td>String</td><td></td><td>✔️</td><td>Name of the custom field</td></tr><tr><td>path</td><td>String</td><td></td><td>✔️</td><td>Path in shoot resource, of which the value must be of primitive type (no object / array). Use <a href=https://lodash.com/docs/4.17.15#get>lodash get</a> path syntax, e.g. <code>metadata.labels["shoot.gardener.cloud/status"]</code> or <code>spec.networking.type</code></td></tr><tr><td>icon</td><td>String</td><td></td><td></td><td>MDI icon for field on the cluster details page. See <a href=https://materialdesignicons.com/>https://materialdesignicons.com/</a> for available icons. Must be in the format: <code>mdi-&lt;icon-name></code>.</td></tr><tr><td>tooltip</td><td>String</td><td></td><td></td><td>Tooltip for the custom field that appears when hovering with the mouse over the value</td></tr><tr><td>defaultValue</td><td>String/Number</td><td></td><td></td><td>Default value, in case there is no value for the given <code>path</code></td></tr><tr><td>showColumn</td><td>Bool</td><td>true</td><td></td><td>Field shall appear as column in the cluster list</td></tr><tr><td>columnSelectedByDefault</td><td>Bool</td><td>true</td><td></td><td>Indicates if field shall be selected by default on the cluster list (not hidden by default)</td></tr><tr><td>weight</td><td>Number</td><td>0</td><td></td><td>Defines the order of the column. The standard columns start with weight 100 and continue in 100 increments (200, 300, ..)</td></tr><tr><td>sortable</td><td>Bool</td><td>true</td><td></td><td>Indicates if column is sortable on the cluster list</td></tr><tr><td>searchable</td><td>Bool</td><td>true</td><td></td><td>Indicates if column is searchable on the cluster list</td></tr><tr><td>showDetails</td><td>Bool</td><td>true</td><td></td><td>Indicates if field shall appear in a dedicated card (<code>Custom Fields</code>) on the cluster details page</td></tr></tbody></table><p>As there is currently no way to configure the custom shoot fields for a project in the gardener dashboard, you have to use <code>kubectl</code> to update the <code>project</code> resource. See /docs/dashboard/usage/project-operations/#download-kubeconfig-for-a-user on how to get a <code>kubeconfig</code> for the <code>garden</code> cluster in order to edit the <code>project</code>.</p><h3 id=example>Example</h3><p>The following is an example project yaml:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Project
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dashboard.gardener.cloud/shootCustomFields: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      {
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;shootStatus&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;name&#34;: &#34;Shoot Status&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;path&#34;: &#34;metadata.labels[\&#34;shoot.gardener.cloud/status\&#34;]&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;icon&#34;: &#34;mdi-heart-pulse&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;tooltip&#34;: &#34;Indicates the health status of the cluster&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;defaultValue&#34;: &#34;unknown&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;showColumn&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;columnSelectedByDefault&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;weight&#34;: 950,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;searchable&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;sortable&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;showDetails&#34;: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>        },
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;networking&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;name&#34;: &#34;Networking Type&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;path&#34;: &#34;spec.networking.type&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;icon&#34;: &#34;mdi-table-network&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;showColumn&#34;: false
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      }</span>      
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-414147fe38ba5c54e30044e956fefca9>3.5.4 - Project Operations</h1><h1 id=project-operations>Project Operations</h1><p>This section demonstrates how to use the standard Kubernetes tool for cluster operation <code>kubectl</code> for common cluster operations with emphasis on Gardener resources. For more information on <code>kubectl</code>, see <a href=https://kubernetes.io/docs/reference/kubectl/overview/>kubectl</a> on <em>kubernetes.io</em>.</p><ul><li><a href=#project-operations>Project Operations</a><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#downloading-kubeconfig-for-remote-project-operations>Downloading <code>kubeconfig</code> for remote project operations</a><ul><li><a href=#download-kubeconfig-for-a-user>Download <code>kubeconfig</code> for a user</a></li><li><a href=#download-kubeconfig-for-a-service-account>Download <code>kubeconfig</code> for a Service Account</a></li></ul></li><li><a href=#list-gardener-api-resources>List Gardener API resources</a></li><li><a href=#check-your-permissions>Check your permissions</a></li><li><a href=#working-with-projects>Working with projects</a></li><li><a href=#working-with-clusters>Working with clusters</a><ul><li><a href=#list-project-clusters>List project clusters</a></li><li><a href=#create-a-new-cluster>Create a new cluster</a></li><li><a href=#delete-cluster>Delete cluster</a></li><li><a href=#get-kubeconfig-for-a-cluster>Get <code>kubeconfig</code> for a cluster</a></li></ul></li><li><a href=#related-links>Related Links</a></li></ul></li></ul><h2 id=prerequisites>Prerequisites</h2><ul><li>You’re logged on to the Gardener Dashboard.</li><li>You’ve created a cluster and its status is operational.</li></ul><p>It&rsquo;s recommended that you get acquainted with the resources in the <a href=https://github.com/gardener/gardener/tree/master/docs/api-reference>Gardener API</a>.</p><h2 id=downloading-kubeconfig-for-remote-project-operations>Downloading <code>kubeconfig</code> for remote project operations</h2><p>The <code>kubeconfig</code> for project operations is different from the one for cluster operations. It has a larger scope and allows a different set of operations that are applicable for a project administrator role, such as lifecycle control on clusters and managing project members.</p><p>Depending on your goal, you create a service account suitable for automation and download its <code>kubeconfig</code>, or you can get a user-specific <code>kubeconfig</code>. The difference is the identity on behalf of which the operations are performed.</p><h3 id=download-kubeconfig-for-a-user>Download <code>kubeconfig</code> for a user</h3><p>Kubernetes doesn’t offer an own resource type for human users that access the API server. Instead, you either have to manage unique user strings, or use an OpenID-Connect (OIDC) compatible Identity Provider (IDP) to do the job.</p><p>Once the latter is set up, each Gardener user can use the <code>kubelogin</code> plugin for <code>kubectl</code> to authenticate against the API server:</p><ol><li><p>Set up <code>kubelogin</code> if you don&rsquo;t have it yet. More information: <a href=https://github.com/int128/kubelogin#setup>kubelogin setup</a>.</p></li><li><p>Open the menu at the top right of the screen, then choose <strong>MY ACCOUNT</strong>.</p><p><img src=/__resources/Show-account-details_d990b0.png alt="Show account details"></p></li><li><p>On the <strong>Access</strong> card, choose the arrow to see all options for the personalized command-line interface access.</p><p><img src=/__resources/Show-details-of-OICD-login_61f8f3.png alt="Show details of OICD login"></p><blockquote><p>The personal bearer token that is also offered here only provides access for a limited amount of time for one time operations, for example, in <code>curl</code> commands. The <code>kubeconfig</code> provided for the personalized access is used by <code>kubelogin</code> to grant access to the Gardener API for the user permanently by using a refresh token.</p></blockquote></li><li><p>Check that the right <strong>Project</strong> is chosen and keep the settings otherwise. Download the <code>kubeconfig</code> file and add its path to the <code>KUBECONFIG</code> environment variable.</p></li></ol><p>You can now execute <code>kubectl</code> commands on the garden cluster using the identity of your user.</p><h3 id=download-kubeconfig-for-a-service-account>Download <code>kubeconfig</code> for a Service Account</h3><ol><li><p>Go to a service account and choose <strong>Download</strong>.</p><p><img src=/__resources/Download-service-account-kubeconfig_a1aacd.png alt="Download service account kubeconfig"></p></li><li><p>Add the downloaded <code>kubeconfig</code> to your configuration.</p></li></ol><p>You can now execute <code>kubectl</code> commands on the garden cluster using the technical service account.</p><h2 id=list-gardener-api-resources>List Gardener API resources</h2><ol><li><p>Using a <code>kubeconfig</code> for project operations, you can list the Gardner API resources using the following command:</p><pre tabindex=0><code>kubectl api-resources | grep garden
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>backupbuckets                     bbc             core.gardener.cloud            false        BackupBucket
backupentries                     bec             core.gardener.cloud            true         BackupEntry
cloudprofiles                     cprofile,cpfl   core.gardener.cloud            false        CloudProfile
controllerinstallations           ctrlinst        core.gardener.cloud            false        ControllerInstallation
controllerregistrations           ctrlreg         core.gardener.cloud            false        ControllerRegistration
plants                            pl              core.gardener.cloud            true         Plant
projects                                          core.gardener.cloud            false        Project
quotas                            squota          core.gardener.cloud            true         Quota
secretbindings                    sb              core.gardener.cloud            true         SecretBinding
seeds                                             core.gardener.cloud            false        Seed
shoots                                            core.gardener.cloud            true         Shoot
shootstates                                       core.gardener.cloud            true         ShootState
terminals                                         dashboard.gardener.cloud       true         Terminal
clusteropenidconnectpresets       coidcps         settings.gardener.cloud        false        ClusterOpenIDConnectPreset
openidconnectpresets              oidcps          settings.gardener.cloud        true         OpenIDConnectPreset
</code></pre></li><li><p>Enter the following command to view the Gardener API versions:</p><pre tabindex=0><code>kubectl api-versions | grep garden
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>core.gardener.cloud/v1alpha1
core.gardener.cloud/v1beta1
dashboard.gardener.cloud/v1alpha1
settings.gardener.cloud/v1alpha1
</code></pre></li></ol><h2 id=check-your-permissions>Check your permissions</h2><ol><li><p>The operations on project resources are limited by the role of the identity that tries to perform them. To get an overview over your permissions, use the following command:</p><pre tabindex=0><code>kubectl auth can-i --list | grep garden
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>plants.core.gardener.cloud                      []                       []                 [create delete deletecollection get list patch update watch]
quotas.core.gardener.cloud                      []                       []                 [create delete deletecollection get list patch update watch]
secretbindings.core.gardener.cloud              []                       []                 [create delete deletecollection get list patch update watch]
shoots.core.gardener.cloud                      []                       []                 [create delete deletecollection get list patch update watch]
terminals.dashboard.gardener.cloud              []                       []                 [create delete deletecollection get list patch update watch]
openidconnectpresets.settings.gardener.cloud    []                       []                 [create delete deletecollection get list patch update watch]
cloudprofiles.core.gardener.cloud               []                       []                 [get list watch]
projects.core.gardener.cloud                    []                       [flowering]             [get patch update delete]
namespaces                                      []                       [garden-flowering]      [get]
</code></pre></li><li><p>Try to execute an operation that you aren’t allowed, for example:</p><pre tabindex=0><code>kubectl get projects
</code></pre><p>You receive an error message like this:</p><pre tabindex=0><code>Error from server (Forbidden): projects.core.gardener.cloud is forbidden: User &#34;system:serviceaccount:garden-flowering:robot&#34; cannot list resource &#34;projects&#34; in API group &#34;core.gardener.cloud&#34; at the cluster scope
</code></pre></li></ol><h2 id=working-with-projects>Working with projects</h2><ol><li><p>You can get the details for a project, where you (or the service account) is a member.</p><pre tabindex=0><code>kubectl get project flowering
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>NAME        NAMESPACE          STATUS   OWNER                    CREATOR                         AGE
flowering   garden-flowering   Ready    [PROJECT-ADMIN]@domain   [PROJECT-ADMIN]@domain system   45m
</code></pre><blockquote><p>For more information, see <a href=/docs/gardener/api-reference/core/#project>Project</a> in the API reference.</p></blockquote></li><li><p>To query the names of the members of a project, use the following command:</p><pre tabindex=0><code>kubectl get project docu -o jsonpath=&#39;{.spec.members[*].name }&#39;
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>[PROJECT-ADMIN]@domain system:serviceaccount:garden-flowering:robot
</code></pre><blockquote><p>For more information, see <a href=/docs/gardener/api-reference/core/#projectmember>members</a> in the API reference.</p></blockquote></li></ol><h2 id=working-with-clusters>Working with clusters</h2><p>The Gardener domain object for a managed cluster is called <a href=/docs/gardener/api-reference/core/#shoot>Shoot</a>.</p><h3 id=list-project-clusters>List project clusters</h3><p>To query the clusters in a project:</p><pre tabindex=0><code>kubectl get shoots
</code></pre><p>The output looks like this:</p><pre tabindex=0><code>NAME       CLOUDPROFILE   VERSION   SEED      DOMAIN                                 HIBERNATION   OPERATION   PROGRESS   APISERVER   CONTROL   NODES   SYSTEM   AGE
geranium   aws            1.18.3    aws-eu1   geranium.flowering.shoot.&lt;truncated&gt;   Awake         Succeeded   100        True        True      True    True     74m
</code></pre><h3 id=create-a-new-cluster>Create a new cluster</h3><p>To create a new cluster using the command line, you need a YAML definition of the <code>Shoot</code> resource.</p><ol><li><p>To get started, copy the following YAML definition to a new file, for example, <code>daffodil.yaml</code> (or copy file <a href=https://github.com/gardener/dashboard/blob/master/docs/usage/shoot.yaml>shoot.yaml</a> to <code>daffodil.yaml</code>) and adapt it to your needs.</p><pre tabindex=0><code>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: daffodil
  namespace: garden-flowering
spec:
  secretBindingName: trial-secretbinding-gcp
  cloudProfileName: gcp
  region: europe-west1
  purpose: evaluation
  provider:
    type: gcp
    infrastructureConfig:
      kind: InfrastructureConfig
      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
      networks:
        workers: 10.250.0.0/16
    controlPlaneConfig:
      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
      zone: europe-west1-c
      kind: ControlPlaneConfig
    workers:
    - name: cpu-worker
      maximum: 2
      minimum: 1
      maxSurge: 1
      maxUnavailable: 0
      machine:
        type: n1-standard-2
        image:
          name: coreos
          version: 2303.3.0
      volume:
        type: pd-standard
        size: 50Gi
      zones:
        - europe-west1-c
  networking:
    type: calico
    pods: 100.96.0.0/11
    nodes: 10.250.0.0/16
    services: 100.64.0.0/13
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
    autoUpdate:
      kubernetesVersion: true
      machineImageVersion: true
  hibernation:
    enabled: true
    schedules:
      - start: &#39;00 17 * * 1,2,3,4,5&#39;
        location: Europe/Kiev
  kubernetes:
    allowPrivilegedContainers: true
    kubeControllerManager:
      nodeCIDRMaskSize: 24
    kubeProxy:
      mode: IPTables
    version: 1.18.3
  addons:
    nginxIngress:
      enabled: false
    kubernetesDashboard:
      enabled: false
</code></pre></li><li><p>In your new YAML definition file, replace the value of field <code>metadata.namespace</code> with your namespace following the convention <code>garden-[YOUR-PROJECTNAME]</code>.</p></li><li><p>Create a cluster using this manifest (with flag <code>--wait=false</code> the command returns immediately, otherwise it doesn&rsquo;t return until the process is finished):</p><pre tabindex=0><code>kubectl apply -f daffodil.yaml --wait=false
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>shoot.core.gardener.cloud/daffodil created
</code></pre></li><li><p>It takes 5–10 minutes until the cluster is created. To watch the progress, get all shoots and use the <code>-w</code> flag.</p><pre tabindex=0><code>kubectl get shoots -w
</code></pre></li></ol><p>For a more extended example, see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Gardener example shoot manifest</a>.</p><h3 id=delete-cluster>Delete cluster</h3><p>To delete a shoot cluster, you must first annotate the shoot resource to confirm the operation with <code>confirmation.gardener.cloud/deletion: "true"</code>:</p><ol><li><p>Add the annotation to your manifest (<code>daffodil.yaml</code> in the previous example):</p><pre tabindex=0><code>apiVersion: core.gardener.cloud/v1beta1
  kind: Shoot
  metadata:
    name: daffodil
    namespace: garden-flowering
    annotations:
      confirmation.gardener.cloud/deletion: &#34;true&#34;
  spec:
    addons:
...
</code></pre></li><li><p>Apply your changes of <code>daffodil.yaml</code>.</p><pre tabindex=0><code>kubectl apply -f daffodil.yaml
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>shoot.core.gardener.cloud/daffodil configured
</code></pre></li><li><p>Trigger the deletion.</p><pre tabindex=0><code>kubectl delete shoot daffodil --wait=false
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>shoot.core.gardener.cloud &#34;daffodil&#34; deleted
</code></pre></li><li><p>It takes 5–10 minutes to delete the cluster. To watch the progress, get all shoots and use the <code>-w</code> flag.</p><pre tabindex=0><code>kubectl get shoots -w
</code></pre></li></ol><h3 id=get-kubeconfig-for-a-cluster>Get <code>kubeconfig</code> for a cluster</h3><p>To get the kubeconfig for a cluster:</p><pre tabindex=0><code>kubectl get secrets daffodil.kubeconfig -o jsonpath=&#39;{.data.kubeconfig}&#39; | base64 -d
</code></pre><p>The response looks like this:</p><pre tabindex=0><code>---
apiVersion: v1
kind: Config
current-context: shoot--flowering--daffodil
clusters:
- name: shoot--flowering--daffodil
  cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDR &lt;truncated&gt;
    server: https://api.daffodil.flowering.shoot.&lt;truncated&gt;
contexts:
- name: shoot--flowering--daffodil
  context:
    cluster: shoot--flowering--daffodil
    user: shoot--flowering--daffodil-token
users:
- name: shoot--flowering--daffodil-token
  user:
    token: HbjYIMuR9hmyb9 &lt;truncated&gt;
</code></pre><p>The name of the Secret containing the kubeconfig is in the form <code>&lt;cluster-name>.kubeconfig</code>, that is, in this example: <code>daffodil.kubeconfig</code></p><h2 id=related-links>Related Links</h2><p><a href=/docs/dashboard/usage/gardener-api/>Accessing the Gardener API Through the Dashboard</a></p><p><a href=/docs/tutorials/oidc-login/>Authenticating with an Identity Provider</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-36f3fb8c71197ddfe931001c3368d443>3.5.5 - Terminal Shortcuts</h1><h1 id=terminal-shortcuts>Terminal Shortcuts</h1><p>As user and/or gardener administrator you can configure terminal shortcuts, which are preconfigured terminals for frequently used views.</p><p>You can launch the terminal shortcuts directly on the shoot details screen.
<img src=/__resources/terminal-shortcuts-1_2cfc41.png></p><p>You can view the definition of a terminal terminal shortcut by clicking on they eye icon
<img src=/__resources/terminal-shortcuts-2_87763a.png></p><p>What also has improved is, that when creating a new terminal you can directly alter the configuration.
<img src=/__resources/terminal-shortcuts-3_6aec85.png></p><p>With expanded configuration
<img src=/__resources/terminal-shortcuts-4_daae3a.png></p><p>On the <code>Create Terminal Session</code> dialog you can choose one or multiple terminal shortcuts.
<img src=/__resources/terminal-shortcuts-5_df45cf.png></p><p>Project specific terminal shortcuts created (by a member of the project) have a project icon badge and are listed as <code>Unverified</code>.
<img src=/__resources/terminal-shortcuts-6_8a6dff.png></p><p>A warning message is displayed before a project specific terminal shortcut is ran informing the user about the risks.
<img src=/__resources/terminal-shortcuts-7_d2d536.png></p><p><strong>How to create a project specific terminal shortcut</strong></p><p><em>Disclaimer:</em> &ldquo;Project specific terminal shortcuts&rdquo; is experimental feature and may change in future releases (we plan to introduce a dedicated custom resource).</p><p>You need to create a secret with the name <code>terminal.shortcuts</code> within your project namespace, containing your terminal shortcut configurations. Under <code>data.shortcuts</code> you add a list of terminal shortcuts (base64 encoded).
Example <code>terminal.shortcuts</code> secret:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: terminal.shortcuts
</span></span><span style=display:flex><span>  namespace: garden-myproject
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  shortcuts: LS0tCi0gdGl0bGU6IE5ldHdvcmtEZWxheVRlc3RzCiAgZGVzY3JpcHRpb246IFNob3cgbmV0d29ya21hY2hpbmVyeS5pbydzIE5ldHdvcmtEZWxheVRlc3RzCiAgdGFyZ2V0OiBzaG9vdAogIGNvbnRhaW5lcjoKICAgIGltYWdlOiBxdWF5LmlvL2RlcmFpbGVkL2s5czpsYXRlc3QKICAgIGFyZ3M6CiAgICAtIC0taGVhZGxlc3MKICAgIC0gLS1jb21tYW5kPW5ldHdvcmtkZWxheXRlc3QKICBzaG9vdFNlbGVjdG9yOgogICAgbWF0Y2hMYWJlbHM6CiAgICAgIGZvbzogYmFyCi0gdGl0bGU6IFNjYW4gQ2x1c3RlcgogIGRlc2NyaXB0aW9uOiBTY2FucyBsaXZlIEt1YmVybmV0ZXMgY2x1c3RlciBhbmQgcmVwb3J0cyBwb3RlbnRpYWwgaXNzdWVzIHdpdGggZGVwbG95ZWQgcmVzb3VyY2VzIGFuZCBjb25maWd1cmF0aW9ucwogIHRhcmdldDogc2hvb3QKICBjb250YWluZXI6CiAgICBpbWFnZTogcXVheS5pby9kZXJhaWxlZC9rOXM6bGF0ZXN0CiAgICBhcmdzOgogICAgLSAtLWhlYWRsZXNzCiAgICAtIC0tY29tbWFuZD1wb3BleWU=
</span></span></code></pre></div><p><strong>How to configure the dashboard with terminal shortcuts</strong>
Example <code>values.yaml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>frontend:
</span></span><span style=display:flex><span>  features:
</span></span><span style=display:flex><span>    terminalEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    projectTerminalShortcutsEnabled: <span style=color:#00f>true</span> <span style=color:green># members can create a `terminal.shortcuts` secret containing the project specific terminal shortcuts</span>
</span></span><span style=display:flex><span>  terminal:
</span></span><span style=display:flex><span>    shortcuts:
</span></span><span style=display:flex><span>    - title: <span style=color:#a31515>&#34;Control Plane Pods&#34;</span>
</span></span><span style=display:flex><span>      description: Using K9s to view the pods of the control plane for this cluster
</span></span><span style=display:flex><span>      target: cp
</span></span><span style=display:flex><span>      container:
</span></span><span style=display:flex><span>        image: quay.io/derailed/k9s:latest
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;--headless&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;--command=pods&#34;</span>
</span></span><span style=display:flex><span>    - title: <span style=color:#a31515>&#34;Cluster Overview&#34;</span>
</span></span><span style=display:flex><span>      description: This gives a quick overview about the status of your cluster using K9s pulse feature
</span></span><span style=display:flex><span>      target: shoot
</span></span><span style=display:flex><span>      container:
</span></span><span style=display:flex><span>        image: quay.io/derailed/k9s:latest
</span></span><span style=display:flex><span>        args:
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;--headless&#34;</span>
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;--command=pulses&#34;</span>
</span></span><span style=display:flex><span>    - title: <span style=color:#a31515>&#34;Nodes&#34;</span>
</span></span><span style=display:flex><span>      description: View the nodes for this cluster
</span></span><span style=display:flex><span>      target: shoot
</span></span><span style=display:flex><span>      container:
</span></span><span style=display:flex><span>        image: quay.io/derailed/k9s:latest
</span></span><span style=display:flex><span>        command:
</span></span><span style=display:flex><span>        - bin/sh
</span></span><span style=display:flex><span>        args:
</span></span><span style=display:flex><span>        - -c
</span></span><span style=display:flex><span>        - sleep 1 &amp;&amp; while true; do k9s --headless --command=nodes; done
</span></span><span style=display:flex><span><span style=color:green>#      shootSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#        matchLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#          foo: bar</span>
</span></span><span style=display:flex><span>[...]
</span></span><span style=display:flex><span>terminal: <span style=color:green># is generally required for the terminal feature</span>
</span></span><span style=display:flex><span>  container:
</span></span><span style=display:flex><span>    image: eu.gcr.io/gardener-project/gardener/ops-toolbelt:0.10.0
</span></span><span style=display:flex><span>  containerImageDescriptions:
</span></span><span style=display:flex><span>    - image: /.*/ops-toolbelt:.*/
</span></span><span style=display:flex><span>      description: Run `ghelp` to get information about installed tools and packages
</span></span><span style=display:flex><span>  gardenTerminalHost:
</span></span><span style=display:flex><span>    seedRef: my-soil
</span></span><span style=display:flex><span>  garden:
</span></span><span style=display:flex><span>    operatorCredentials:
</span></span><span style=display:flex><span>      serviceAccountRef:
</span></span><span style=display:flex><span>        name: dashboard-terminal-admin
</span></span><span style=display:flex><span>        namespace: garden
</span></span><span style=display:flex><span>  bootstrap:
</span></span><span style=display:flex><span>    disabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    shootDisabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    seedDisabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    gardenTerminalHostDisabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    apiServerIngress:
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>        kubernetes.io/ingress.class: nginx
</span></span><span style=display:flex><span>        nginx.ingress.kubernetes.io/backend-protocol: HTTPS
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-39c606a8880e63eb6d46f63b13995794>3.5.6 - Using Terminal</h1><h1 id=using-the-dashboard-terminal>Using the Dashboard Terminal</h1><p>The dashboard features an integrated web-based terminal to your clusters. It allows you to use <code>kubectl</code> without the need to supply <code>kubeconfig</code>. There are several ways to access it and they&rsquo;re described on this page.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You are logged on to the Gardener Dashboard.</li><li>You have created a cluster and its status is operational.</li><li>The landscape administrator has enabled the terminal feature</li><li>The cluster you want to connect to is reachable from the dashboard</li></ul><p>On this page:</p><ul><li><a href=#open-from-cluster-list>Open from cluster list</a></li><li><a href=#open-from-cluster-details-page>Open from cluster details page</a></li><li><a href=#terminal>Terminal</a></li></ul><br><h3 id=open-from-cluster-list>Open from cluster list</h3><ol><li><p>Choose your project from the menu on the left and choose CLUSTERS.</p></li><li><p>Locate a cluster for which you want to open a Terminal and choose the <em>key</em> icon.</p></li><li><p>In the dialog, choose the icon on the right of the Terminal label.</p><img style=max-width:40% src=/__resources/01-open-terminal_8f2712.png></li></ol><h3 id=open-from-cluster-details-page>Open from cluster details page</h3><ol><li><p>Choose your project from the menu on the left and choose CLUSTERS.</p></li><li><p>Locate a cluster for which you want to open a Terminal and choose to display its details.</p></li><li><p>In the Access section, choose the icon on the right of the Terminal label.</p><img style=max-width:40% src=/__resources/01-open-terminal_8f2712.png></li></ol><h3 id=terminal>Terminal</h3><p>Opening up the terminal in either of the ways discussed here results in the following screen:</p><img src=/__resources/02-terminal_ee605f.png><p>It provides a <code>bash</code> environment and range of useful tools and an installed and configured <code>kubectl</code> (with alias <code>k</code>) to use right away with your cluster.</p><p>Try to list the namespaces in the cluster.</p><pre tabindex=0><code>$ k get ns
</code></pre><p>You get a result like this:
<img src=/__resources/03-list-ns_eb34a9.png></p></div><div class=td-content style=page-break-before:always><h1 id=pg-47ee17f6d3aeb873ef3534aadf19ad15>3.5.7 - Working With Projects</h1><h1 id=working-with-projects>Working with Projects</h1><h2 id=overview>Overview</h2><p>Projects are used to group clusters, onboard IaaS resources utilized by them, and organize access control.
To work with clusters, first you need to create a project that they will belong to.</p><h2 id=creating-your-first-project>Creating Your First Project</h2><h3 id=prerequisites>Prerequisites</h3><ul><li>You have access to the Gardener Dashboard and have permissions to create projects</li></ul><h3 id=steps>Steps</h3><ol><li><p>Logon to the Gardener Dashboard and choose <strong>CREATE YOUR FIRST PROJECT</strong>.</p><img class=gs-image src=/__resources/00-create-project_0da539.png></li><li><p>Provide a project <strong>Name</strong>, and optionally a <strong>Description</strong> and a <strong>Purpose</strong>, and choose <strong>CREATE</strong>.</p><img class=gs-image style=max-width:65% src=/__resources/01-provide-project-details-public_9a4510.png></li></ol><blockquote><p>⚠️ You will <strong>not</strong> be able to change the project name later. The rest of the details will be editable.</p></blockquote><h3 id=result>Result</h3><p>After completing the steps above, you will arrive at a similar screen:
<img class=gs-image src=/__resources/02-create-project-done_e5b906.png></p><h2 id=creating-more-projects>Creating More Projects</h2><p>If you need to create more projects, expand the <code>Projects</code> list dropdown on the left. When expanded, it reveals a <strong>CREATE PROJECT</strong> button that brings up the same dialog as above.</p><img class=gs-image style=max-width:65% src=/__resources/03-create-project-1_47dedb.png><h2 id=deleting-your-project>Deleting Your Project</h2><p>When you need to delete your project, go to <strong>ADMINISTRATON</strong>, choose the trash bin icon and, confirm the operation.</p><img class=gs-image src=/__resources/04-delete-project_b5a975.png></div><div class=td-content style=page-break-before:always><h1 id=pg-bc2953b8ebe1baf73a7cde44eb04670e>4 - Gardenctl V2</h1><div class=lead>The command line interface to control your clusters</div><h1 id=gardenctl-v2>gardenctl-v2</h1><p><img src=https://github.com/gardener/gardenctl-v2/raw/DEFAULT_BRANCH/logo/logo_gardener_cli_large.png alt></p><p><a href=https://goreportcard.com/report/github.com/gardener/gardenctl-v2><img src=https://goreportcard.com/badge/github.com/gardener/gardenctl-v2 alt="Go Report Card"></a>
<a href=https://badge.fury.io/gh/gardener%2Fgardenctl-v2><img src=https://badge.fury.io/gh/gardener%2Fgardenctl-v2.svg alt=release></a>
<a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><h2 id=what-is-gardenctl>What is <code>gardenctl</code>?</h2><p>gardenctl is a command-line client for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters. Use this tool to configure access to clusters and configure cloud provider CLI tools. It also provides support for accessing cluster nodes via ssh.</p><h2 id=installation>Installation</h2><p>Install the latest release from <a href=https://brew.sh/>Homebrew</a>, <a href=https://chocolatey.org/packages/gardenctl-v2>Chocolatey</a> or <a href=https://github.com/gardener/gardenctl-v2/releases>GitHub Releases</a>.</p><h3 id=install-using-package-managers>Install using Package Managers</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Homebrew (macOS and Linux)</span>
</span></span><span style=display:flex><span>brew install gardener/tap/gardenctl-v2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Chocolatey (Windows)</span>
</span></span><span style=display:flex><span><span style=color:green># default location C:\ProgramData\chocolatey\bin\gardenctl-v2.exe</span>
</span></span><span style=display:flex><span>choco install gardenctl-v2
</span></span></code></pre></div><p>Attention <code>brew</code> users: <code>gardenctl-v2</code> uses the same binary name as the legacy <code>gardenctl</code> (<code>gardener/gardenctl</code>) CLI. If you have an existing installation you should remove it with <code>brew uninstall gardenctl</code> before attempting to install <code>gardenctl-v2</code>. Alternatively, you can choose to link the binary using a different name. If you try to install without removing or relinking the old installation, brew will run into an error and provide instructions how to resolve it.</p><h3 id=install-from-github-release>Install from Github Release</h3><p>If you install via GitHub releases, you need to</p><ul><li>put the <code>gardenctl</code> binary on your path</li><li>and <a href=https://github.com/gardener/gardenlogin#installation>install gardenlogin</a>.</li></ul><p>The other install methods do this for you.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Example for macOS</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># set operating system and architecture</span>
</span></span><span style=display:flex><span>os=darwin <span style=color:green># choose between darwin, linux, windows</span>
</span></span><span style=display:flex><span>arch=amd64 <span style=color:green># choose between amd64, arm64</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Get latest version. Alternatively set your desired version</span>
</span></span><span style=display:flex><span>version=<span style=color:#00f>$(</span>curl -s https://raw.githubusercontent.com/gardener/gardenctl-v2/master/LATEST<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Download gardenctl</span>
</span></span><span style=display:flex><span>curl -LO <span style=color:#a31515>&#34;https://github.com/gardener/gardenctl-v2/releases/download/</span><span style=color:#a31515>${</span>version<span style=color:#a31515>}</span><span style=color:#a31515>/gardenctl_v2_</span><span style=color:#a31515>${</span>os<span style=color:#a31515>}</span><span style=color:#a31515>_</span><span style=color:#a31515>${</span>arch<span style=color:#a31515>}</span><span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Make the gardenctl binary executable</span>
</span></span><span style=display:flex><span>chmod +x <span style=color:#a31515>&#34;./gardenctl_v2_</span><span style=color:#a31515>${</span>os<span style=color:#a31515>}</span><span style=color:#a31515>_</span><span style=color:#a31515>${</span>arch<span style=color:#a31515>}</span><span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Move the binary in to your PATH</span>
</span></span><span style=display:flex><span>sudo mv <span style=color:#a31515>&#34;./gardenctl_v2_</span><span style=color:#a31515>${</span>os<span style=color:#a31515>}</span><span style=color:#a31515>_</span><span style=color:#a31515>${</span>arch<span style=color:#a31515>}</span><span style=color:#a31515>&#34;</span> /usr/local/bin/gardenctl
</span></span></code></pre></div><h2 id=configuration>Configuration</h2><p><code>gardenctl</code> requires a configuration file. The default location is in <code>~/.garden/gardenctl-v2.yaml</code>.</p><p>You can modify this file directly using the <code>gardenctl config</code> command. It allows adding, modifying and deleting gardens.</p><p>Example <code>config</code> command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Adapt the path to your kubeconfig file for the garden cluster</span>
</span></span><span style=display:flex><span>export KUBECONFIG=~/relative/path/to/kubeconfig.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Method 1 : Fetch cluster-identity of garden cluster from the configmap</span>
</span></span><span style=display:flex><span>cluster_identity=<span style=color:#00f>$(</span>kubectl -n kube-system get configmap cluster-identity -ojsonpath={.data.cluster-identity}<span style=color:#00f>)</span>
</span></span><span style=display:flex><span><span style=color:green># OR</span>
</span></span><span style=display:flex><span><span style=color:green># Method 2 : If you don&#39;t have access to the kube-system namespace in the garden cluster, the garden cluster-identity can also be extracted from every shoot&#39;s yaml</span>
</span></span><span style=display:flex><span>project=<span style=color:#a31515>&#34;your-project-name&#34;</span> <span style=color:green># Change to your project name</span>
</span></span><span style=display:flex><span>shoot=<span style=color:#a31515>&#34;your-shoot-name&#34;</span> <span style=color:green># Change to any shoot&#39;s name in your project</span>
</span></span><span style=display:flex><span><span style=color:green># Simply copy/paste the following lines</span>
</span></span><span style=display:flex><span>ns=<span style=color:#00f>$(</span>kubectl get project $project -ojsonpath={.spec.namespace}<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>prefix=<span style=color:#a31515>&#34;shoot--</span>$project<span style=color:#a31515>--</span>$shoot<span style=color:#a31515>-&#34;</span><span style=color:#00f>$(</span>kubectl get shoot -n $ns $shoot -ojsonpath={.metadata.uid}<span style=color:#00f>)</span><span style=color:#a31515>&#34;-&#34;</span>
</span></span><span style=display:flex><span>identity_status=<span style=color:#00f>$(</span>kubectl get shoot -n $ns $shoot -ojsonpath={.status.clusterIdentity}<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>cluster_identity=<span style=color:#00f>$(</span>echo <span style=color:#a31515>${</span>identity_status#<span style=color:#a31515>&#34;</span>$prefix<span style=color:#a31515>&#34;</span><span style=color:#a31515>}</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Configure garden cluster</span>
</span></span><span style=display:flex><span>gardenctl config set-garden $cluster_identity --kubeconfig $KUBECONFIG
</span></span></code></pre></div><p>This command will create or update a garden with the provided identity and kubeconfig path of your garden cluster.</p><h3 id=example-config>Example Config</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardens:
</span></span><span style=display:flex><span>  - identity: landscape-dev <span style=color:green># Unique identity of the garden cluster. See cluster-identity ConfigMap in kube-system namespace of the garden cluster</span>
</span></span><span style=display:flex><span>    kubeconfig: ~/relative/path/to/kubeconfig.yaml
</span></span><span style=display:flex><span><span style=color:green># name: my-name # An alternative, unique garden name for targeting</span>
</span></span><span style=display:flex><span><span style=color:green># context: different-context # Overrides the current-context of the garden cluster kubeconfig</span>
</span></span><span style=display:flex><span><span style=color:green># patterns: ~ # List of regex patterns for pattern targeting</span>
</span></span></code></pre></div><p>Note: You need to have <a href=https://github.com/gardener/gardenlogin>gardenlogin</a> installed as <code>kubectl</code> plugin in order to use the <code>kubeconfig</code>s for <code>Shoot</code> clusters provided by <code>gardenctl</code>.</p><h3 id=config-path-overwrite>Config Path Overwrite</h3><ul><li>The <code>gardenctl</code> config path can be overwritten with the environment variable <code>GCTL_HOME</code>.</li><li>The <code>gardenctl</code> config name can be overwritten with the environment variable <code>GCTL_CONFIG_NAME</code>.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export GCTL_HOME=/alternate/garden/config/dir
</span></span><span style=display:flex><span>export GCTL_CONFIG_NAME=myconfig <span style=color:green># without extension!</span>
</span></span><span style=display:flex><span><span style=color:green># config is expected to be under /alternate/garden/config/dir/myconfig.yaml</span>
</span></span></code></pre></div><h3 id=shell-session>Shell Session</h3><p>The state of gardenctl is bound to a shell session and is not shared across windows, tabs or panes.
A shell session is defined by the environment variable <code>GCTL_SESSION_ID</code>. If this is not defined,
the value of the <code>TERM_SESSION_ID</code> environment variable is used instead. If both are not defined,
this leads to an error and gardenctl cannot be executed. The <code>target.yaml</code> and temporary
<code>kubeconfig.*.yaml</code> files are store in the following directory <code>${TMPDIR}/garden/${GCTL_SESSION_ID}</code>.</p><p>You can make sure that <code>GCTL_SESSION_ID</code> or <code>TERM_SESSION_ID</code> is always present by adding
the following code to your terminal profile <code>~/.profile</code>, <code>~/.bashrc</code> or comparable file.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash and zsh: [ -n <span style=color:#a31515>&#34;</span>$GCTL_SESSION_ID<span style=color:#a31515>&#34;</span> ] || [ -n <span style=color:#a31515>&#34;</span>$TERM_SESSION_ID<span style=color:#a31515>&#34;</span> ] || export GCTL_SESSION_ID=<span style=color:#00f>$(</span>uuidgen<span style=color:#00f>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>fish:         [ -n <span style=color:#a31515>&#34;</span>$GCTL_SESSION_ID<span style=color:#a31515>&#34;</span> ] || [ -n <span style=color:#a31515>&#34;</span>$TERM_SESSION_ID<span style=color:#a31515>&#34;</span> ] || set -gx GCTL_SESSION_ID (uuidgen)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ps data-lang=ps><span style=display:flex><span>powershell:   if <span style=color:#a31515>( !(Test-Path Env:GCTL_SESSION_ID) -and !(Test-Path Env:TERM_SESSION_ID) )</span> { $Env:GCTL_SESSION_ID = [guid]::NewGuid<span style=color:#a31515>()</span>.ToString<span style=color:#a31515>()</span> }
</span></span></code></pre></div><h3 id=completion>Completion</h3><p>Gardenctl supports completion that will help you working with the CLI and save you typing effort.
It will also help you find clusters by providing suggestions for gardener resources such as shoots or projects.
Completion is supported for <code>bash</code>, <code>zsh</code>, <code>fish</code> and <code>powershell</code>.
You will find more information on how to configure your shell completion for gardenctl by executing the help for
your shell completion command. Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gardenctl completion bash --help
</span></span></code></pre></div><h2 id=usage>Usage</h2><h3 id=targeting>Targeting</h3><p>You can set a target to use it in subsequent commands. You can also overwrite the target for each command individually.</p><p>Note that this will not affect your KUBECONFIG env variable. To update the KUBECONFIG env for your current target see <a href=#configure-kubeconfig-for-shoot-clusters>Configure KUBECONFIG</a> section</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># target control plane</span>
</span></span><span style=display:flex><span>gardenctl target --garden landscape-dev --project my-project --shoot my-shoot --control-plane
</span></span></code></pre></div><p>Find more information in the <a href=https://github.com/gardener/gardenctl-v2/blob/DEFAULT_BRANCH/docs/usage/targeting.md>documentation</a>.</p><h3 id=configure-kubeconfig-for-shoot-clusters>Configure KUBECONFIG for Shoot Clusters</h3><p>Generate a script that points KUBECONFIG to the targeted cluster for the specified shell. Use together with <code>eval</code> to configure your shell. Example for <code>bash</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>eval <span style=color:#00f>$(</span>gardenctl kubectl-env bash<span style=color:#00f>)</span>
</span></span></code></pre></div><h3 id=configure-cloud-provider-clis>Configure Cloud Provider CLIs</h3><p>Generate the cloud provider CLI configuration script for the specified shell. Use together with <code>eval</code> to configure your shell. Example for <code>bash</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>eval <span style=color:#00f>$(</span>gardenctl provider-env bash<span style=color:#00f>)</span>
</span></span></code></pre></div><h3 id=ssh>SSH</h3><p>Establish an SSH connection to a Shoot cluster&rsquo;s node.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gardenctl ssh my-node
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d2c6535126cca927d2a9c893abde92a0>5 - Guides</h1></div><div class=td-content><h1 id=pg-ed30d4942f1afb26454fab9f11a445e1>5.1 - Set Up Client Tools</h1></div><div class=td-content><h1 id=pg-33170d28a5e3a076ed0ed50fed19149e>5.1.1 - Automated Deployment</h1><div class=lead>Automated deployment with kubectl</div><h2 id=overview>Overview</h2><p>With kubectl, you can easily deploy an image from your local environment.</p><p>However, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don&rsquo;t want to store
the KUBECONFIG on that server?</p><p>You can use kubectl and connect to the API-server of your cluster.</p><h2 id=prerequisites>Prerequisites</h2><ol><li><p>Create a service account user:</p><pre tabindex=0><code>kubectl create serviceaccount deploy-user -n default
</code></pre></li><li><p>Bind a role to the newly created serviceuser:</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>In this example, the preconfigured role <code>edit</code> and the namespace <code>default</code> is being used, please adjust the role to a more strict scope! For more information, see <a href=https://kubernetes.io/docs/admin/authorization/rbac/>Using RBAC Authorization</a>.</div><pre tabindex=0><code>kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default
</code></pre></li><li><p>Get the URL of your API-server:</p><pre tabindex=0><code>APISERVER=$(kubectl config view | grep server | cut -f 2- -d &#34;:&#34; | tr -d &#34; &#34;)
</code></pre></li><li><p>Get the service account:</p><pre tabindex=0><code>SERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})
</code></pre></li><li><p>Generate a token for the serviceaccount:</p><pre tabindex=0><code>TOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)
</code></pre></li></ol><h2 id=usage>Usage</h2><p>You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)</p><pre tabindex=0><code>kubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-6cfaa64f7452ed600c92ebe35f6920a7>5.1.2 - Kubeconfig Context as bash Prompt</h1><div class=lead>Expose the active kubeconfig into bash</div><h2 id=overview>Overview</h2><p>Use the Kubernetes command-line tool, <strong>kubectl</strong>, to deploy and manage applications on Kubernetes.
Using kubectl, you can inspect cluster resources, as well as create, delete, and update components.</p><p><img src=/__resources/howto-kubeconfig-bash_5d5422.gif alt=port-forward></p><p>By default, the kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Let us suppose that you have two clusters, one for development work and one for scratch work.</p><p>How to handle this easily without copying the used configuration always to the right place?</p><h2 id=export-the-kubeconfig-enviroment-variable>Export the KUBECONFIG Enviroment Variable</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ export KUBECONFIG=&lt;PATH-TO-M&gt;-CONFIG&gt;/kubeconfig-dev.yaml
</span></span></code></pre></div><p>How to determine which cluster is used by the kubectl command?</p><h2 id=determine-active-cluster>Determine Active Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ kubectl cluster-info
</span></span><span style=display:flex><span>Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To further debug and diagnose cluster problems, use <span style=color:#a31515>&#39;kubectl cluster-info dump&#39;</span>.
</span></span><span style=display:flex><span>bash$ 
</span></span></code></pre></div><h2 id=display-cluster-in-the-bash---linux-and-alike>Display Cluster in the bash - Linux and Alike</h2><p>I found this tip on Stackoverflow and find it worth to be added here.
Edit your <code>~/.bash_profile</code> and add the following code snippet to show the current K8s
context in the shell&rsquo;s prompt:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>prompt_k8s(){
</span></span><span style=display:flex><span>  k8s_current_context=<span style=color:#00f>$(</span>kubectl config current-context 2&gt; /dev/null<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>if</span> [[ $? -eq 0 ]] ; <span style=color:#00f>then</span> echo -e <span style=color:#a31515>&#34;(</span><span style=color:#a31515>${</span>k8s_current_context<span style=color:#a31515>}</span><span style=color:#a31515>) &#34;</span>; <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>PS1+=<span style=color:#a31515>&#39;$(prompt_k8s)&#39;</span>
</span></span></code></pre></div><p>After this, your bash command prompt contains the active KUBECONFIG context and you always know
which cluster is active - <em>develop</em> or <em>production</em>.</p><p>e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml 
</span></span><span style=display:flex><span>bash (garden_dev)$ 
</span></span></code></pre></div><p>Note the <strong>(garden_dev)</strong> prefix in the bash command prompt.</p><p><strong>This helps immensely to avoid thoughtless mistakes.</strong></p><h2 id=display-cluster-in-the-powershell---windows>Display Cluster in the PowerShell - Windows</h2><p>Display current K8s cluster in the title of PowerShell window.</p><p>Create a <a href=https://superuser.com/a/1045659>profile</a> file for your shell under <code>%UserProfile%\Documents\Windows­PowerShell\Microsoft.PowerShell_profile.ps1</code></p><p>Copy following code to <code>Microsoft.PowerShell_profile.ps1</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span> <span style=color:#00f>function</span> prompt_k8s {
</span></span><span style=display:flex><span>     $k8s_current_context = (kubectl config current-context) | Out-String
</span></span><span style=display:flex><span>     <span style=color:#00f>if</span>($?) {
</span></span><span style=display:flex><span>         <span style=color:#00f>return</span> $k8s_current_context
</span></span><span style=display:flex><span>     }<span style=color:#00f>else</span> {
</span></span><span style=display:flex><span>         <span style=color:#00f>return</span> <span style=color:#a31515>&#34;No K8S contenxt found&#34;</span>
</span></span><span style=display:flex><span>     }
</span></span><span style=display:flex><span> }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> $host.ui.rawui.WindowTitle = prompt_k8s
</span></span></code></pre></div><p><img src=/__resources/howto-bash_kubeconfig_powershell_be60c7.png alt=port-forward></p><p>If you want to switch to different cluster, you can set <code>KUBECONFIG</code> to new value, and re-run the file <code>Microsoft.PowerShell_profile.ps1</code></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4811effaf016a84906632f2b2585ac68>5.1.3 - Organizing Access Using kubeconfig Files</h1><h2 id=overview>Overview</h2><p>The kubectl command-line tool uses <code>kubeconfig</code> files to find the information it needs to choose a cluster and
communicate with the API server of a cluster.</p><h2 id=problem>Problem</h2><p>If you&rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials
in case anything was leaked. However, this is not possible with the initial or master <code>kubeconfig</code> from your
cluster.</p><p><img src=/__resources/teaser_775456.svg alt=teaser></p><h2 id=pitfall>Pitfall</h2><p>Never distribute the <code>kubeconfig</code>, which you can download directly within the Gardener dashboard, for a productive cluster.</p><p><img src=/__resources/kubeconfig-initial_32df26.png alt=kubeconfig-dont></p><h2 id=create-a-custom-kubeconfig-file-for-each-user>Create a Custom kubeconfig File for Each User</h2><p>Create a separate <code>kubeconfig</code> for each user. One of the big advantages of this approach is that you can revoke them and control
the permissions better. A limitation to single namespaces is also possible here.</p><p>The script creates a new <code>ServiceAccount</code> with read privileges in the whole cluster (Secrets are excluded).
To run the script, <a href=https://deno.land/>Deno</a>, a secure TypeScript runtime, must be installed.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-TypeScript data-lang=TypeScript><span style=display:flex><span><span>#</span>!<span>/usr/bin/env -S deno run --allow-run</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>/*
</span></span></span><span style=display:flex><span><span style=color:green>* This script create Kubernetes ServiceAccount and other required resource and print KUBECONFIG to console.
</span></span></span><span style=display:flex><span><span style=color:green>* Depending on your requirements you might want change clusterRoleBindingTemplate() function
</span></span></span><span style=display:flex><span><span style=color:green>*
</span></span></span><span style=display:flex><span><span style=color:green>* In order to execute this script it&#39;s required to install Deno.js https://deno.land/ (TypeScript &amp; JavaScript runtime).
</span></span></span><span style=display:flex><span><span style=color:green>* It&#39;s single executable binary for the major OSs from the original author of the Node.js
</span></span></span><span style=display:flex><span><span style=color:green>* example: deno run --allow-run kubeconfig-for-custom-user.ts d00001
</span></span></span><span style=display:flex><span><span style=color:green>* example: deno run --allow-run kubeconfig-for-custom-user.ts d00001 --delete
</span></span></span><span style=display:flex><span><span style=color:green>*
</span></span></span><span style=display:flex><span><span style=color:green>* known issue: shebang does works under the Linux but not for Windows Linux Subsystem
</span></span></span><span style=display:flex><span><span style=color:green>*/</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> KUBECTL = <span style=color:#a31515>&#34;/usr/local/bin/kubectl&#34;</span> <span style=color:green>//or
</span></span></span><span style=display:flex><span><span style=color:green>// const KUBECTL = &#34;C:\\Program Files\\Docker\\Docker\\resources\\bin\\kubectl.exe&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> serviceAccName = Deno.args[0]
</span></span><span style=display:flex><span><span style=color:#00f>const</span> deleteIt = Deno.args[1]
</span></span><span style=display:flex><span><span style=color:#00f>if</span> (serviceAccName == <span style=color:#00f>undefined</span> || serviceAccName == <span style=color:#a31515>&#34;--delete&#34;</span> ) {
</span></span><span style=display:flex><span>    console.log(<span style=color:#a31515>&#34;please provide username as an argument, for example: deno run --allow-run kubeconfig-for-custom-user.ts USER_NAME [--delete]&#34;</span>)
</span></span><span style=display:flex><span>    Deno.exit(1)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> (deleteIt == <span style=color:#a31515>&#34;--delete&#34;</span>) {
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;serviceaccount&#34;</span>, serviceAccName])
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;secret&#34;</span>, <span style=color:#a31515>`</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret`</span>])
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;clusterrolebinding&#34;</span>, <span style=color:#a31515>`view-</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-global`</span>])
</span></span><span style=display:flex><span>    Deno.exit(0)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;serviceaccount&#34;</span>, serviceAccName, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>, <span style=color:#a31515>&#34;-f&#34;</span>, <span style=color:#a31515>&#34;-&#34;</span>], secretYamlTemplate())
</span></span><span style=display:flex><span><span style=color:#00f>let</span> secret = <span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;get&#34;</span>, <span style=color:#a31515>&#34;secret&#34;</span>, <span style=color:#a31515>`</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret`</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>])
</span></span><span style=display:flex><span><span style=color:#00f>let</span> caCRT = secret.data[<span style=color:#a31515>&#34;ca.crt&#34;</span>];
</span></span><span style=display:flex><span><span style=color:#00f>let</span> userToken = atob(secret.data[<span style=color:#a31515>&#34;token&#34;</span>]); <span style=color:green>//decode base64
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:#00f>let</span> kubeConfig = <span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;config&#34;</span>, <span style=color:#a31515>&#34;view&#34;</span>, <span style=color:#a31515>&#34;--minify&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>]);
</span></span><span style=display:flex><span><span style=color:#00f>let</span> clusterApi = kubeConfig.clusters[0].cluster.server
</span></span><span style=display:flex><span><span style=color:#00f>let</span> clusterName = kubeConfig.clusters[0].name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>, <span style=color:#a31515>&#34;-f&#34;</span>, <span style=color:#a31515>&#34;-&#34;</span>], clusterRoleBindingTemplate())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>console.log(kubeConfigTemplate(caCRT, userToken, clusterApi, clusterName, serviceAccName + <span style=color:#a31515>&#34;-&#34;</span> + clusterName))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>async</span> <span style=color:#00f>function</span> exec(args: <span style=color:#2b91af>string</span>[], stdInput?: <span style=color:#2b91af>string</span>): Promise&lt;Object&gt; {
</span></span><span style=display:flex><span>    console.log(<span style=color:#a31515>&#34;# &#34;</span>+args.join(<span style=color:#a31515>&#34; &#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#00f>let</span> opt: <span style=color:#2b91af>Deno.RunOptions</span> = {
</span></span><span style=display:flex><span>        cmd: <span style=color:#2b91af>args</span>,
</span></span><span style=display:flex><span>        stdout: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>        stderr: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>        stdin: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>    };
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> p = Deno.run(opt);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> (stdInput != <span style=color:#00f>undefined</span>) {
</span></span><span style=display:flex><span>        <span style=color:#00f>await</span> p.stdin.write(<span style=color:#00f>new</span> TextEncoder().encode(stdInput));
</span></span><span style=display:flex><span>        <span style=color:#00f>await</span> p.stdin.close();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> status = <span style=color:#00f>await</span> p.status()
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> output = <span style=color:#00f>await</span> p.output()
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> stderrOutput = <span style=color:#00f>await</span> p.stderrOutput()
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> (status.code === 0) {
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> JSON.parse(<span style=color:#00f>new</span> TextDecoder().decode(output))
</span></span><span style=display:flex><span>    } <span style=color:#00f>else</span> {
</span></span><span style=display:flex><span>        <span style=color:#00f>let</span> error = <span style=color:#00f>new</span> TextDecoder().decode(stderrOutput);
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> clusterRoleBindingTemplate() {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: rbac.authorization.k8s.io/v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: ClusterRoleBinding
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: view-</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-global
</span></span></span><span style=display:flex><span><span style=color:#a31515>subjects:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  namespace: default
</span></span></span><span style=display:flex><span><span style=color:#a31515>roleRef:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: view
</span></span></span><span style=display:flex><span><span style=color:#a31515>  apiGroup: rbac.authorization.k8s.io    
</span></span></span><span style=display:flex><span><span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> secretYamlTemplate() {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: Secret
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret
</span></span></span><span style=display:flex><span><span style=color:#a31515>  annotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    kubernetes.io/service-account.name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>type: kubernetes.io/service-account-token`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> kubeConfigTemplate(certificateAuthority: <span style=color:#2b91af>string</span>, token: <span style=color:#2b91af>string</span>, clusterApi: <span style=color:#2b91af>string</span>, clusterName: <span style=color:#2b91af>string</span>, username: <span style=color:#2b91af>string</span>) {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>## KUBECONFIG generated on </span><span style=color:#a31515>${</span><span style=color:#00f>new</span> Date()<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>clusters:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- cluster:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    certificate-authority-data: </span><span style=color:#a31515>${</span>certificateAuthority<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    server: </span><span style=color:#a31515>${</span>clusterApi<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>contexts:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- context:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    cluster: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    user: </span><span style=color:#a31515>${</span>username<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>current-context: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: Config
</span></span></span><span style=display:flex><span><span style=color:#a31515>preferences: {}
</span></span></span><span style=display:flex><span><span style=color:#a31515>users:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- name: </span><span style=color:#a31515>${</span>username<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  user:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    token: </span><span style=color:#a31515>${</span>token<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If <strong>edit</strong> or <strong>admin</strong> rights are to be assigned, the <code>ClusterRoleBinding</code> must be adapted in the <code>roleRef</code> section
with the roles listed below.</p><p>Furthermore, you can restrict this to a single namespace by not creating a <code>ClusterRoleBinding</code> but only a <code>RoleBinding</code>
within the desired namespace.</p><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>cluster-admin</td><td>system:masters group</td><td>Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding&rsquo;s namespace, including the namespace itself.</td></tr><tr><td>admin</td><td>None</td><td>Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.</td></tr><tr><td>edit</td><td>None</td><td>Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.</td></tr><tr><td>view</td><td>None</td><td>Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-acb3d3deb29a8ddc337a39814f4ddbcb>5.1.4 - Use a Helm Chart to Deploy an Application or Service</h1><h2 id=overview>Overview</h2><p>Basically, <a href=https://helm.sh/docs/topics/charts/>Helm Charts</a> can be installed as described e.g. in the Helm
<a href=https://helm.sh/docs/intro/quickstart/>QuickStart Guide</a>. However, our clusters come with
<a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>RBAC</a> enabled by default, hence Helm must be installed as follows:</p><h2 id=create-a-service-account>Create a Service Account</h2><p>Create a service account via the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt;EOF | kubectl create -f -
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515> name: helm
</span></span></span><span style=display:flex><span><span style=color:#a31515> namespace: kube-system
</span></span></span><span style=display:flex><span><span style=color:#a31515>---
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: rbac.authorization.k8s.io/v1beta1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: ClusterRoleBinding
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515> name: helm
</span></span></span><span style=display:flex><span><span style=color:#a31515>roleRef:
</span></span></span><span style=display:flex><span><span style=color:#a31515> apiGroup: rbac.authorization.k8s.io
</span></span></span><span style=display:flex><span><span style=color:#a31515> kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#a31515> name: cluster-admin
</span></span></span><span style=display:flex><span><span style=color:#a31515>subjects:
</span></span></span><span style=display:flex><span><span style=color:#a31515> - kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#a31515>   name: helm
</span></span></span><span style=display:flex><span><span style=color:#a31515>   namespace: kube-system
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><h2 id=initialize-helm>Initialize Helm</h2><p>Initialise Helm via <code>helm init --service-account helm</code>. You can now use <code>helm</code>.</p><h2 id=in-case-of-failure>In Case of Failure</h2><p>In case you have already executed <code>helm init</code>, but without the above service account, you will get the following error:</p><p><code>Error: User "system:serviceaccount:kube-system:default" cannot list configmaps in the namespace "kube-system". (get configmaps)</code>
(e.g. when you run <code>helm list</code>). You will now need to delete the Tiller deployment (Helm backend
implicitly deployed to the Kubernetes cluster when you call <code>helm init</code>) as well as the local Helm files (usually
<code>$HELM_HOME</code> is set to <code>~/.helm</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl delete deployment tiller-deploy --namespace=kube-system
</span></span><span style=display:flex><span>kubectl delete service tiller-deploy --namespace=kube-system 
</span></span><span style=display:flex><span>rm -rf ~/.helm/
</span></span></code></pre></div><p>Now follow the instructions above. For more details see this <a href=https://github.com/kubernetes/helm/issues/2687>Kubernetes Helm issue #2687</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f7165d236b09de8503d3a17a03402a0e>5.2 - Install Gardener</h1></div><div class=td-content><h1 id=pg-009818d3feade374ae615681087e6434>5.2.1 - Hardening the Gardener Community Setup</h1><h2 id=overview>Overview</h2><p>Gardener stakeholders in the Open Source community usually use the <a href=https://github.com/gardener/landscape-setup>Gardener Setup Scripts</a>, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. shoot clusters can play the following roles in a Gardener landscape:</p><ul><li>Seed cluster</li><li>Shoot cluster</li></ul><p>As Alban Crequy from Kinvolk has recommended in his recent Gardener blog <a href=/docs/guides/applications/insecure-configuration/>Auditing Kubernetes for Secure Setup</a>, the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.</p><h2 id=recommendations>Recommendations</h2><h3 id=mitigation-for-gardener-cve-2018-2475>Mitigation for Gardener CVE-2018-2475</h3><p>The following recommendations describe how you can harden your Gardener Community Setup by adding a seed cluster hardened with network policies.</p><ul><li>Use the Gardener Setup Scripts to create a garden cluster in a dedicated IaaS account</li><li>Create a shoot cluster in a different IaaS account</li><li>As a precaution, you should not deploy the Kubernetes dashboard on this shoot cluster</li><li>Register this newly created shoot cluster as a seed cluster in the Gardener</li><li>End user shoot clusters can then be created using this newly created seed cluster (which in turn is a shoot cluster).</li></ul><p>A tutorial on how to create a shooted seed cluster can be found at <a href=/docs/guides/install_gardener/setup-seed/>Setting up the Seed Cluster</a>.</p><p>The rationale behind this activity is that Calico network policies harden this seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.</p><p>When you have added a hardened seed cluster you are expected not be vulnerable to the Gardener <a href=https://groups.google.com/forum/#!topic/gardener/Pom2Y70cDpw>CVE-2018-2475</a> anymore.</p><h3 id=mitigation-for-kubernetes-cve-2018-1002105>Mitigation for Kubernetes CVE-2018-1002105</h3><p>In addition, when you follow the recommendations in the <a href=https://groups.google.com/forum/#!topic/gardener/2icxEz0RAK4>recent Gardener Security Announcement</a>, you are expected to not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.</p><h2 id=alternative-approach>Alternative Approach</h2><p>For this alternative approach, there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his <a href=/docs/guides/applications/insecure-configuration/>blog</a> directly in the Garden cluster and create shoot clusters from there in a different IaaS account.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d7002605ea6dad33b86146a592a09963>5.2.2 - Manually Adding a Node to an Existing Cluster</h1><div class=lead>How to add a node to an existing cluster without the support of Gardener</div><h2 id=overview>Overview</h2><p>Gardener has an excellent ability to <a href=/blog/2021/01.25-machine-controller-manager/>automatically scale machines</a> for the cluster. From the point of view of scalability, there is no need for manual intervention.</p><p>This tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported
by Gardener. For example, an end-user who wants some workload that requires <code>runnc</code> instead of <code>runc</code> as container runtime.</p><p><img src=/__resources/teaser_eb7b15.svg alt=teaser></p><h2 id=disclaimer>Disclaimer</h2><p>Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener.
Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be
responsible to replace it.</p><h2 id=steps>Steps</h2><ol><li>Create a new instance in the same VPC/network as the other machines in the cluster. You should be able to ssh into the machine. Save its private key and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.</li></ol><p>To ssh into a machine which is already in the cluster, use the steps defined in <a href=/docs/guides/monitoring_and_troubleshooting/shell-to-node/>Get a Shell to a Kubernetes Node</a>.</p><p>Attach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by the kubelet in the new machine so that it can contact the cloud provider to query the node&rsquo;s name.</p><ol><li>On the new machine, create the file <code>/var/lib/kubelet/kubeconfig-bootstrap</code> with the following content:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>current-context: kubelet-bootstrap@default
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA Certificate&gt;
</span></span><span style=display:flex><span>    server: &lt;Server&gt;
</span></span><span style=display:flex><span>  name: default
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: default
</span></span><span style=display:flex><span>    user: kubelet-bootstrap
</span></span><span style=display:flex><span>  name: kubelet-bootstrap@default
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: kubelet-bootstrap
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    as-user-extra: {}
</span></span><span style=display:flex><span>    token: &lt;Token&gt;
</span></span></code></pre></div><ol start=3><li>ssh into an existing node, and run these commands to get the values of the &lt;CA Certificate> and &lt;Server> to be replaced in the above file:</li></ol><ul><li>&lt;Server></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> /opt/bin/hyperkube kubectl <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   --kubeconfig /var/lib/kubelet/kubeconfig-real <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   config view <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   -o go-template=<span style=color:#a31515>&#39;{{index .clusters 0 &#34;cluster&#34; &#34;server&#34;}}&#39;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   --raw
</span></span></code></pre></div><ul><li>&lt;CA Certificate></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/opt/bin/hyperkube kubectl <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   --kubeconfig /var/lib/kubelet/kubeconfig-real <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   config view <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   -o go-template=<span style=color:#a31515>&#39;{{index .clusters 0 &#34;cluster&#34; &#34;certificate-authority-data&#34;}}&#39;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>   --raw
</span></span></code></pre></div><ol start=4><li><p>Obtain the bootstrap &lt;Token><br>The kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the <code>kube-system</code> namespace to authenticate this token, which is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its <code>.data.expiration</code> field. The name of this secret is of the format <code>bootstrap-token-*</code>. Gardener takes care of creating new bootstrap tokens and the corresponding secrets.
To get an unexpired token, find the secrets with the name format <code>bootstrap-token-*</code> in the <code>kube-system</code> namespace in the cluster, and pick the one with minimum age. Eg. <code>bootstrap-token-abcdef</code>.<br>Run these commands to get the token:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> tokenid=<span style=color:#00f>$(</span>kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=<span style=color:#a31515>&#39;{{index .data &#34;token-id&#34;}}&#39;</span> | base64 --decode<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> tokensecret=<span style=color:#00f>$(</span>kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=<span style=color:#a31515>&#39;{{index .data &#34;token-secret&#34;}}&#39;</span> | base64 --decode<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> echo $tokenid.$tokensecret
</span></span></code></pre></div><p>The value of $TOKEN will be <code>tokenid.tokensecret</code>. Replace $TOKEN in above file with this value</p></li><li><p>Copy contents of the files - <code>/var/lib/kubelet/config/kubelet</code>, <code>/var/lib/kubelet/ca.crt</code> and <code>/etc/systemd/system/kubelet.service</code> - from an existing node to the new node</p></li><li><p>Run the following command in the new node to start the kubelet:</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl enable kubelet &amp;&amp; systemctl start kubelet
</span></span></code></pre></div><p>The new node should be added to the existing cluster within a couple of minutes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b226b144fd3cd9bed81a4429318310db>5.2.3 - Setting Up a Seed Cluster</h1><div class=lead>How to configure a Kubernetes cluster as a Gardener seed</div><h2 id=overview>Overview</h2><p>The <a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a> is meant to provide an as-simple-as-possible Gardener installation. Therefore, it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don&rsquo;t have network policies, for example. For more information, see <a href=/docs/guides/install_gardener/secure-setup/>Hardening the Gardener Community Setup</a>.</p><p>To have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.</p><h2 id=setting-up-the-shoot>Setting Up the Shoot</h2><p>The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won&rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So, either your seed cluster is deployed with different CIDRs - not using the dashboard, but <code>kubectl apply</code> and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.</p><p>So, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest in the <a href=https://github.com/gardener/gardener/tree/master/example>gardener/gardener repository</a>. You could, for example, change the CIDRs to this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        internal:
</span></span><span style=display:flex><span>        - 10.254.112.0/22
</span></span><span style=display:flex><span>        nodes: 10.254.0.0/19
</span></span><span style=display:flex><span>        pods: 10.255.0.0/17
</span></span><span style=display:flex><span>        public:
</span></span><span style=display:flex><span>        - 10.254.96.0/22
</span></span><span style=display:flex><span>        services: 10.255.128.0/17
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.254.0.0/16
</span></span><span style=display:flex><span>        workers:
</span></span><span style=display:flex><span>        - 10.254.0.0/19
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>Also, make sure that your new seed cluster has enough resources for the expected number of shoots.</p><h2 id=registering-the-shoot-as-seed>Registering the Shoot as Seed</h2><p>The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the <a href=https://github.com/gardener/landscape-setup/tree/0.5.0/components/seed-config>seed-config component</a> of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the <code>state/seed-config/</code> directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.</p><h3 id=1-seed-namespace>1. Seed Namespace</h3><p>First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called <code>seed-test</code>.</p><h3 id=2-cloud-provider-secret>2. Cloud Provider Secret</h3><p>The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-seed-secret
</span></span><span style=display:flex><span>  namespace: seed-test
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    cloudprofile.garden.sapcloud.io/name: aws 
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: &lt;base64-encoded AWS access key&gt;
</span></span><span style=display:flex><span>  secretAccessKey: &lt;base64-encoded AWS secret key&gt;
</span></span><span style=display:flex><span>  kubeconfig: &lt;base64-encoded kubeconfig&gt;
</span></span></code></pre></div><p>Deploy the secret into your seed namespace. Apart from the kubeconfig, infrastructure credentials are also required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.</p><h3 id=3-secretbinding-for-cloud-provider-secret>3. Secretbinding for Cloud Provider Secret</h3><p>Create a secretbinding for your cloud provider secret:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: SecretBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-seed-secret
</span></span><span style=display:flex><span>  namespace: seed-test
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    cloudprofile.garden.sapcloud.io/name: aws
</span></span><span style=display:flex><span>secretRef:
</span></span><span style=display:flex><span>  name: test-seed-secret
</span></span><span style=display:flex><span><span style=color:green># namespace: only required if in different namespace than referenced secret</span>
</span></span><span style=display:flex><span>quotas: []
</span></span></code></pre></div><p>You can give it the same name as the referenced secret.</p><h3 id=4-cloudprofile>4. Cloudprofile</h3><p>The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don&rsquo;t want to change anything.</p><h3 id=5-seed>5. Seed</h3><p>Now the seed resource can be created. Choose a name, reference the cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws-secure
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: test-seed-secret
</span></span><span style=display:flex><span>    namespace: seed-test
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    ingressDomain: ingress.&lt;your cluster domain&gt;
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    nodes: 10.254.0.0/19
</span></span><span style=display:flex><span>    pods: 10.255.0.0/17
</span></span><span style=display:flex><span>    services: 10.255.128.0/17
</span></span></code></pre></div><h3 id=6-hide-the-original-seed>6. Hide the Original Seed</h3><p>In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, Gardener will try to distribute the shoots equally among both seeds.</p><p>To solve this problem, edit the original seed and set its <code>spec.visible</code> field to <code>false</code>. This will prevent Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-db9e41f7ef6a1a47f4ca49bdf49d64db>5.3 - Administer Client (Shoot) Clusters</h1></div><div class=td-content><h1 id=pg-2341371c4780b4857a5d435af93bb356>5.3.1 - Create / Delete a Shoot Cluster</h1><h2 id=create-a-shoot-cluster>Create a Shoot Cluster</h2><p>As you have already prepared an <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>example Shoot manifest</a> in the steps described in the development documentation, please open another Terminal pane/window with the <code>KUBECONFIG</code> environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>You should see that Gardener has immediately picked up your manifest and has started to deploy the Shoot cluster.</p><p>In order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: <code>shoot-johndoe-johndoe-1</code>, whereas the first <code>johndoe</code> is your namespace in the Garden cluster (also called &ldquo;project&rdquo;) and the <code>johndoe-1</code> suffix is the actual name of the Shoot cluster.</p><p>To connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the <code>kubecfg</code> secret in that namespace.</p><h2 id=delete-a-shoot-cluster>Delete a Shoot Cluster</h2><p>In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared <code>delete shoot</code> script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don&rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ./hack/usage/delete shoot johndoe-1 johndoe
</span></span></code></pre></div><p>(the <code>hack</code> bash script can be found at <a href=https://github.com/gardener/gardener/blob/master/hack/usage/delete>GitHub</a>)</p><h1 id=configure-a-shoot-cluster-alert-receiver>Configure a Shoot cluster alert receiver</h1><p>The receiver of the Shoot alerts can be configured from the <code>.spec.monitoring.alerting.emailReceivers</code> section in the Shoot specification. The value of the field has to be a list of valid mail addresses.</p><p>The alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the <code>Shoot</code> resource specifies <code>.spec.monitoring.alerting.emailReceivers</code> and if a <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>SMTP secret</a> exists.</p><p>If the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4bf829bf283322bf7b7a67cb455fb03>5.3.2 - Create a Shoot Cluster Into an Existing AWS VPC</h1><h2 id=overview>Overview</h2><p>Gardener can create a new VPC, or use an existing one for your shoot cluster. Depending on your needs, you may want to create shoot(s) into an already created VPC.
The tutorial describes how to create a shoot cluster into an existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.</p><h2 id=tldr>TL;DR</h2><p>If <code>.spec.provider.infrastructureConfig.networks.vpc.cidr</code> is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on shoot deletion.<br>If <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> is specified, Gardener will use the existing VPC and respectively won&rsquo;t delete it on shoot deletion.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>It&rsquo;s not recommended to create a shoot cluster into a VPC that is managed by Gardener (that is created for another shoot cluster). In this case the deletion of the initial shoot cluster will fail to delete the VPC because there will be resources attached to it.</p><p>Gardener won&rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.</p></div><h2 id=1-configure-the-aws-cli>1. Configure the AWS CLI</h2><p>The <code>aws configure</code> command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws configure
</span></span><span style=display:flex><span>AWS Access Key ID [None]: &lt;ACCESS_KEY_ID&gt;
</span></span><span style=display:flex><span>AWS Secret Access Key [None]: &lt;SECRET_ACCESS_KEY&gt;
</span></span><span style=display:flex><span>Default region name [None]: &lt;DEFAULT_REGION&gt;
</span></span><span style=display:flex><span>Default output format [None]: &lt;DEFAULT_OUTPUT_FORMAT&gt;
</span></span></code></pre></div><h2 id=2-create-a-vpc>2. Create a VPC</h2><p>Create the VPC by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 create-vpc --cidr-block &lt;cidr-block&gt;
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;Vpc&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;VpcId&#34;</span>: <span style=color:#a31515>&#34;vpc-ff7bbf86&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;InstanceTenancy&#34;</span>: <span style=color:#a31515>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlockAssociations&#34;</span>: [
</span></span><span style=display:flex><span>          {
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;AssociationId&#34;</span>: <span style=color:#a31515>&#34;vpc-cidr-assoc-6e42b505&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlockState&#34;</span>: {
</span></span><span style=display:flex><span>                  <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;associated&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Ipv6CidrBlockAssociationSet&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;pending&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;DhcpOptionsId&#34;</span>: <span style=color:#a31515>&#34;dopt-38f7a057&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;IsDefault&#34;</span>: false
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Gardener requires the VPC to have enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS support</a>, i.e the attributes <code>enableDnsSupport</code> and <code>enableDnsHostnames</code> must be set to <em>true</em>. <code>enableDnsSupport</code> attribute is enabled by default, <code>enableDnsHostnames</code> - not. Set the <code>enableDnsHostnames</code> attribute to <em>true</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames
</span></span></code></pre></div><h2 id=3-create-an-internet-gateway>3. Create an Internet Gateway</h2><p>Gardener also requires that an internet gateway is attached to the VPC. You can create one by using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 create-internet-gateway
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;InternetGateway&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;InternetGatewayId&#34;</span>: <span style=color:#a31515>&#34;igw-c0a643a9&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Attachments&#34;</span>: []
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>and attach it to the VPC using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86
</span></span></code></pre></div><h2 id=4-create-the-shoot>4. Create the Shoot</h2><p>Prepare your shoot manifest (you could check the <a href=https://github.com/gardener/gardener/tree/master/example>example manifests</a>). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: &lt;aws-region-of-vpc&gt;
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          id: vpc-ff7bbf86
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span></code></pre></div><p>Apply your shoot manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>Ensure that the shoot cluster is properly created:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE
</span></span><span style=display:flex><span>NAME           CLOUDPROFILE   VERSION   SEED   DOMAIN           OPERATION   PROGRESS   APISERVER   CONTROL   NODES   SYSTEM   AGE
</span></span><span style=display:flex><span>&lt;SHOOT_NAME&gt;   aws            1.15.0    aws    &lt;SHOOT_DOMAIN&gt;   Succeeded   100        True        True      True    True     20m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c59fbc42a6186b5b6f2f75de63a4efc7>5.3.3 - Shoot Cluster Maintenance</h1><div class=lead>Understanding and configuring Gardener&rsquo;s Day-2 operations for Shoot clusters.</div><h2 id=overview>Overview</h2><p>Day two operations for shoot clusters are related to:</p><ul><li>The Kubernetes version of the control plane and the worker nodes</li><li>The operating system version of the worker nodes</li></ul><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>When referring to an update of the &ldquo;operating system version&rdquo; in this document, the update of the machine image of the shoot cluster&rsquo;s worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.</div><p>The following table summarizes what options Gardener offers to maintain these versions:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Auto-Update</th><th style=text-align:left>Forceful Updates</th><th style=text-align:left>Manual Updates</th></tr></thead><tbody><tr><td style=text-align:left>Kubernetes version</td><td style=text-align:left>Patches only</td><td style=text-align:left>Patches and consecutive minor updates only</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>Operating system version</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td></tr></tbody></table><h2 id=allowed-target-versions-in-the-cloudprofile>Allowed Target Versions in the <code>CloudProfile</code></h2><p>Administrators maintain the allowed target versions that you can update to in the <code>CloudProfile</code> for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:</p><pre tabindex=0><code>kubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml
</code></pre><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th><th style=text-align:left>More Information</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.kubernetes.versions</code></td><td style=text-align:left>The supported Kubernetes version <code>major.minor.patch</code>.</td><td style=text-align:left><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md#patch-releases>Patch releases</a></td></tr><tr><td style=text-align:left><code>spec.machineImages</code></td><td style=text-align:left>The supported operating system versions for worker nodes</td><td style=text-align:left></td></tr></tbody></table><p>Both the Kubernetes version and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.</p><p>For more information, see <a href=http://semver.org/>Semantic Versioning</a>.</p><h3 id=impact-of-version-classifications-on-updates>Impact of Version Classifications on Updates</h3><p>Gardener allows to classify versions in the <code>CloudProfile</code> as <code>preview</code>, <code>supported</code>, <code>deprecated</code>, or <code>expired</code>. During maintenance operations, <code>preview</code> versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.</p><p>For more information, see <a href=/docs/gardener/usage/shoot_versions/#version-classifications>Version Classifications</a>.</p><h2 id=let-gardener-manage-your-updates>Let Gardener Manage Your Updates</h2><h3 id=the-maintenance-window>The Maintenance Window</h3><p>Gardener can manage updates for you automatically. It offers users to specify a <em>maintenance window</em> during which updates are scheduled:</p><ul><li>The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours.</li><li>If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.</li></ul><p>You can either specify the maintenance window in the shoot cluster specification (<code>.spec.maintenance.timeWindow</code>) or the start time of the maintenance window using the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>).</p><h3 id=auto-update-and-forceful-updates>Auto-Update and Forceful Updates</h3><p>To trigger updates during the maintenance window automatically, Gardener offers the following methods:</p><ul><li><p><em>Auto-update</em>:<br>Gardener starts an update during the next maintenance window whenever there’s a version available in the <code>CloudProfile</code> that is higher than the one of your shoot cluster specification, and that isn’t classified as <code>preview</code> version. For Kubernetes versions, auto-update only updates to higher patch levels.</p><p>You can either activate auto-update on the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>) or in the shoot cluster specification:</p><ul><li><code>.spec.maintenance.autoUpdate.kubernetesVersion: true</code></li><li><code>.spec.maintenance.autoUpdate.machineImageVersion: true</code></li></ul></li><li><p><em>Forceful updates</em>:<br>In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the <code>CloudProfile</code>. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the <code>CloudProfile</code> that isn’t classified as <code>preview</code> version. The highest version in <code>CloudProfile</code> can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.</p></li></ul><p>If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with <code>shoot.gardener.cloud/operation: maintain</code>. Gardener then checks immediately if there’s an auto-update or a forceful update needed.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Forceful version updates are executed even if the auto-update for the Kubernetes version(or the auto-update for the machine image version) is deactivated (set to <code>false</code>).</div><p>With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows for smoother transitions to new versions.</p><h3 id=kubernetes-update-paths>Kubernetes Update Paths</h3><p>The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:</p><table><thead><tr><th style=text-align:left>Update Type</th><th style=text-align:left>Example</th><th style=text-align:left>Update Method</th></tr></thead><tbody><tr><td style=text-align:left>Patches</td><td style=text-align:left><code>1.10.12</code> to <code>1.10.13</code></td><td style=text-align:left>auto-update or Forceful update</td></tr><tr><td style=text-align:left>Update to consecutive minor version</td><td style=text-align:left><code>1.10.12</code> to <code>1.11.10</code></td><td style=text-align:left>Forceful update</td></tr><tr><td style=text-align:left>Other</td><td style=text-align:left><code>1.10.12</code> to <code>1.12.0</code></td><td style=text-align:left>Manual update</td></tr></tbody></table><p>Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>The administrator who maintains the <code>CloudProfile</code> has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from <code>1.10.x</code> to <code>1.11.y</code>. If the minor version increases in bigger steps, for example, from <code>1.10.x</code> to <code>1.12.y</code>, then the shoot cluster updates will fail during the maintenance window.</div><h2 id=manual-updates>Manual Updates</h2><p>To update the Kubernetes version or the node operating system manually, change the <code>.spec.kubernetes.version</code> field or the <code>.spec.provider.workers.machine.image.version</code> field correspondingly.</p><p>Manual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically, as they can have breaking changes that could impact the cluster workload.</p><p>Manual updates are either executed immediately (default) or can be confined to the maintenance time window.<br>Choosing the latter option causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation to only predictably happen during a defined time window (available since <a href=https://github.com/gardener/gardener/releases/tag/v1.4.0>Gardener version 1.4</a>).</p><p>For more information, see <a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Update Roll Out</a>.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Before applying such an update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.</div><h2 id=examples>Examples</h2><p>In the examples for the <code>CloudProfile</code> and the shoot cluster specification, only the fields relevant for the example are shown.</p><h3 id=auto-update-of-kubernetes-version>Auto-Update of Kubernetes Version</h3><p>Let&rsquo;s assume that the Kubernetes versions <code>1.10.5</code> and <code>1.11.0</code> were added in the following <code>CloudProfile</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.11.0
</span></span><span style=display:flex><span>    - version: 1.10.5
</span></span><span style=display:flex><span>    - version: 1.10.0
</span></span></code></pre></div><p>Before this change, the shoot cluster specification looked like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.0
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0000
</span></span><span style=display:flex><span>      end: 230000+0000
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>As a consequence, the shoot cluster is updated to Kubernetes version <code>1.10.5</code> between 22:00-23:00 UTC. Your shoot cluster isn&rsquo;t updated automatically to <code>1.11.0</code>, even though it&rsquo;s the highest Kubernetes version in the <code>CloudProfile</code>, because Gardener only does automatic updates of the Kubernetes patch level.</p><h3 id=forceful-update-due-to-expired-kubernetes-version>Forceful Update Due to Expired Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.10.13
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers to a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.10.13</code> (independently of the value of <code>.spec.maintenance.autoUpdate.kubernetesVersion</code>).</p><h3 id=forceful-update-to-new-minor-kubernetes-version>Forceful Update to New Minor Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.11.09
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.11.10</code>, which is the highest patch version of minor target version <code>1.11</code> that follows the source version <code>1.10</code>.</p><h3 id=automatic-update-from-expired-machine-image-version>Automatic Update from Expired Machine Image Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2191.5.0
</span></span><span style=display:flex><span>    - version: 2191.4.1
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: name
</span></span><span style=display:flex><span>      maximum: 1
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: coreos
</span></span><span style=display:flex><span>        version: 2135.6.0
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a machine image version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the machine image version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the machine image version of the shoot cluster is updated to <code>2191.5.0</code> (independently of the value of <code>.spec.maintenance.autoUpdate.machineImageVersion</code>) as version <code>2135.6.0</code> is expired.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-87218383e3eaa4076a198b81967714ae>5.4 - Monitor and Troubleshoot</h1></div><div class=td-content><h1 id=pg-e97e6a2625fdc17259ec0a4ae54ea0db>5.4.1 - Get a Shell to a Gardener Shoot Worker Node</h1><div class=lead>Describes the methods for getting shell access to worker nodes</div><h2 id=overview>Overview</h2><p>To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node. This can be required if a node misbehaves or fails to join the cluster in the first place.</p><p>With access to the host, it is for instance possible to check the <code>kubelet</code> logs and interact with common tools such as <code>systemctl</code>and <code>journalctl</code>.</p><p>The first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster.
The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.</p><p>This guide only covers how to get access to the host, but does not cover troubleshooting methods.</p><ul><li><a href=#overview>Overview</a></li><li><a href=#get-a-shell-to-an-operational-cluster-node>Get a Shell to an Operational Cluster Node</a><ul><li><a href=#gardener-dashboard>Gardener Dashboard</a><ul><li><a href=#result>Result</a></li></ul></li><li><a href=#gardener-ops-toolbelt>Gardener Ops Toolbelt</a></li><li><a href=#custom-root-pod>Custom Root Pod</a></li></ul></li><li><a href=#ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH Access to a Node That Failed to Join the Cluster</a><ul><li><a href=#identifying-the-problematic-instance>Identifying the Problematic Instance</a></li><li><a href=#gardenctl-ssh>gardenctl ssh</a></li><li><a href=#ssh-with-a-manually-created-bastion-on-aws>SSH with a Manually Created Bastion on AWS</a><ul><li><a href=#create-the-bastion-security-group>Create the Bastion Security Group</a></li><li><a href=#create-the-bastion-instance>Create the Bastion Instance</a></li></ul></li><li><a href=#connecting-to-the-target-instance>Connecting to the Target Instance</a></li></ul></li><li><a href=#cleanup>Cleanup</a></li></ul><h2 id=get-a-shell-to-an-operational-cluster-node>Get a Shell to an Operational Cluster Node</h2><p>The following describes four different approaches to get a shell to an operational Shoot worker node.
As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod.
All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.</p><h3 id=gardener-dashboard>Gardener Dashboard</h3><p><strong>Prerequisite</strong>: the terminal feature is configured for the Gardener dashboard.</p><ol><li>Navigate to the cluster overview page and find the <code>Terminal</code> in the <code>Access</code> tile.</li></ol><img style=margin-left:0;width:80%;height:auto alt="Access Tile" src=/__resources/9fb6ca4ff9b7480f93debba833f48590_f160ec.png><br><p>Select the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and
access rights (only certain users have access to the Seed Control Plane).</p><ol start=2><li>To open the terminal configuration, interact with the top right-hand corner of the screen.</li></ol><img style=margin-left:0 alt="Terminal configuration" src=/__resources/db573582bfc544d294cbde8906a74e07_25a25d.png><br><ol start=3><li>Set the Terminal Runtime to &ldquo;Privileged&rdquo;. Also, specify the target node from the drop-down menu.</li></ol><img style=margin-left:0;width:50%;height:auto alt="Dashboard terminal pod configuration" src=/__resources/f7b10d48edf44c17ba838ff5c429e39d_a24f67.png><br><h4 id=result>Result</h4><p>The Dashboard then schedules a pod and opens a shell session to the node.</p><p>To get access to the common binaries installed on the host, prefix the command with <code>chroot /hostroot</code>.
Note that the path depends on where the root path is mounted in the container.
In the default image used by the Dashboard, it is under <code>/hostroot</code>.</p><img style=margin-left:0 alt="Dashboard terminal pod configuration" src=/__resources/3da659e9cc4744a2ad3e1c6a50d39c04_8b99f7.png><br><h3 id=gardener-ops-toolbelt>Gardener Ops Toolbelt</h3><p><strong>Prerequisite</strong>: <code>kubectl</code> is available.</p><p>The <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> can be used as a convenient way to deploy a root pod to a node.
The pod uses an image that is bundled with a bunch of useful <a href=https://github.com/gardener/ops-toolbelt/tree/master/dockerfile-configs>troubleshooting tools</a>.
This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the <a href=#gardener-dashboard>previous section</a>.</p><p>The easiest way to use the <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> is to execute
the <a href=https://github.com/gardener/ops-toolbelt/blob/master/hacks/ops-pod><code>ops-pod</code> script</a> in the <code>hacks</code> folder.
To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:</p><pre tabindex=0><code>$ &lt;path-to-ops-toolbelt-repo&gt;/hacks/ops-pod &lt;target-node&gt;
</code></pre><h3 id=custom-root-pod>Custom Root Pod</h3><p>Alternatively, a pod can be <a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>assigned</a> to a target node and a shell can
be opened via <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/>standard Kubernetes means</a>.
To enable root access to the node, the pod specification requires proper <code>securityContext</code> and <code>volume</code> properties.</p><p>For instance, you can use the following pod manifest, after changing <target-node-name>with the name of the node you want this pod attached to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: privileged-pod
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  nodeSelector:
</span></span><span style=display:flex><span>    kubernetes.io/hostname: &lt;target-node-name&gt;
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: busybox
</span></span><span style=display:flex><span>    image: busybox
</span></span><span style=display:flex><span>    stdin: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    securityContext:
</span></span><span style=display:flex><span>      privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - name: host-root-volume
</span></span><span style=display:flex><span>      mountPath: /host
</span></span><span style=display:flex><span>      readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: host-root-volume
</span></span><span style=display:flex><span>    hostPath:
</span></span><span style=display:flex><span>      path: /
</span></span><span style=display:flex><span>  hostNetwork: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  restartPolicy: Never
</span></span></code></pre></div><h2 id=ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH Access to a Node That Failed to Join the Cluster</h2><p>This section explores two options that can be used to get SSH access to a node that failed to join the cluster.
As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.</p><p>Additionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.</p><p>For this scenario, cloud providers typically have extensive documentation (e.g <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html>AWS</a> & <a href=https://cloud.google.com/compute/docs/instances/connecting-to-instance>GCP</a>
and in <a href=https://cloud.google.com/compute/docs/instances/connecting-advanced#vpn>some cases tooling support</a>).
However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes
the installation of a <a href=https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-agent.html>cloud provider specific agent</a> on the node.</p><p>Alternatively, <code>gardenctl</code> can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet.
Currently <code>gardenctl</code> supports AWS, GCP, Openstack, Azure and Alibaba Cloud.</p><h3 id=identifying-the-problematic-instance>Identifying the Problematic Instance</h3><p>First, the problematic instance has to be identified.
In Gardener, worker pools can be created in different cloud provider regions, zones, and accounts.</p><p>The instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem.
Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.</p><p>Gardener uses the <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> to create the Shoot worker nodes.
For each worker node, the Machine Controller Manager creates a <code>Machine</code> CRD in the Shoot namespace in the respective <code>Seed</code> cluster.
Usually the problematic instance can be identified, as the respective <code>Machine</code> CRD has status <code>pending</code>.</p><p>The instance / node name can be obtained from the <code>Machine</code> <code>.status</code> field:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .status.node
</code></pre><p>This is all the information needed to go ahead and use <code>gardenctl ssh</code> to get a shell to the node.
In addition, the used cloud provider, the specific identifier of the instance, and the instance region can be identified from the <code>Machine</code> CRD.</p><p>Get the identifier of the instance via:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640
</code></pre><p>The identifier shows that the instance belongs to the cloud provider <code>aws</code> with the ec2 instance-id <code>i-069733c435bdb4640</code> in region <code>eu-north-1</code>.</p><p>To get more information about the instance, check out the <code>MachineClass</code> (e.g <code>AWSMachineClass</code>) that is associated with each <code>Machine</code> CRD in the <code>Shoot</code> namespace of the <code>Seed</code> cluster.
The <code>AWSMachineClass</code> contains the machine image (<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html>ami</a>), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.</p><p>Of course, the information can also be used to get the instance with the cloud provider CLI / API.</p><h3 id=gardenctl-ssh>gardenctl ssh</h3><p>Using the node name of the problematic instance, we can use the <code>gardenctl ssh</code> command to get SSH access to the cloud provider
instance via an automatically set up <a href=https://en.wikipedia.org/wiki/Bastion_host>bastion host</a>.
<code>gardenctl</code> takes care of spinning up the <code>bastion</code> instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance.
After the SSH session has ended, <code>gardenctl</code> deletes the created cloud provider resources.</p><p>Use the following commands:</p><ol><li>First, target a Garden cluster containing all the Shoot definitions.</li></ol><pre tabindex=0><code>$ gardenctl target garden &lt;target-garden&gt;
</code></pre><ol start=2><li>Target an available Shoot by name.
This sets up the context, configures the <code>kubeconfig</code> file of the Shoot cluster and downloads the cloud provider credentials.
Subsequent commands will execute in this context.</li></ol><pre tabindex=0><code>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre><ol start=3><li>This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.</li></ol><pre tabindex=0><code>$ gardenctl ssh &lt;target-node&gt;
</code></pre><h3 id=ssh-with-a-manually-created-bastion-on-aws>SSH with a Manually Created Bastion on AWS</h3><p>In case you are not using <code>gardenctl</code> or want to control the bastion instance yourself, you can also manually set it up.
The steps described here are generally the same as <a href=https://github.com/gardener/gardenctl/blob/10a537942b94234914758c0f6d053dc1cf218ecd/pkg/cmd/ssh_aws.go#L53-L52>those used by <code>gardenctl</code> internally</a>.
Despite some cloud provider specifics, they can be generalized to the following list:</p><ul><li>Open port 22 on the target instance.</li><li>Create an instance / VM in a public subnet (the bastion instance needs to have a public IP address).</li><li>Set-up security groups and roles, and open port 22 for the bastion instance.</li></ul><p>The following diagram shows an overview of how the SSH access to the target instance works:</p><img style=margin-left:0 alt="SSH Bastion diagram" src=/__resources/913441003e5641bc90249bdc07d55656_8710e0.png><br><p>This guide demonstrates the setup of a bastion on AWS.</p><p><strong>Prerequisites:</strong></p><ul><li>The <code>AWS CLI</code> is set up.</li><li>Obtain target <code>instance-id</code> (see <a href=#identifying-the-problematic-instance>Identifying the Problematic Instance</a>).</li><li>Obtain the VPC ID the Shoot resources are created in. This can be found in the <code>Infrastructure</code> CRD in the <code>Shoot</code> namespace in the <code>Seed</code>.</li><li>Make sure that port 22 on the target instance is open (default for Gardener deployed instances).<ul><li>Extract security group via:</li></ul><pre tabindex=0><code>$ aws ec2 describe-instances --instance-ids &lt;instance-id&gt;
</code></pre><ul><li>Check for rule that allows inbound connections on port 22:</li></ul><pre tabindex=0><code>$ aws ec2 describe-security-groups --group-ids=&lt;security-group-id&gt;
</code></pre><ul><li>If not available, create the rule with the following comamnd:</li></ul><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></li></ul><h4 id=create-the-bastion-security-group>Create the Bastion Security Group</h4><ol><li>The common name of the security group is <code>&lt;shoot-name>-bsg</code>. Create the security group:</li></ol><pre tabindex=0><code>$ aws ec2 create-security-group --group-name &lt;bastion-security-group-name&gt;  --description ssh-access --vpc-id &lt;VPC-ID&gt;
</code></pre><ol start=2><li>Optionally, create identifying tags for the security group:</li></ol><pre tabindex=0><code>$ aws ec2 create-tags --resources &lt;bastion-security-group-id&gt; --tags Key=component,Value=&lt;tag&gt;
</code></pre><ol start=3><li>Create a permission in the bastion security group that allows ssh access on port 22:</li></ol><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;bastion-security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre><ol start=4><li>Create an IAM role for the bastion instance with the name <code>&lt;shoot-name>-bastions</code>:</li></ol><pre tabindex=0><code>$ aws iam create-role --role-name &lt;shoot-name&gt;-bastions
</code></pre><p>The content should be:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>&#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>&#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>        &#34;Action&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;ec2:DescribeRegions&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        &#34;Resource&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol start=5><li>Create the instance profile and name it <code>&lt;shoot-name>-bastions</code>:</li></ol><pre tabindex=0><code>$ aws iam create-instance-profile --instance-profile-name &lt;name&gt;
</code></pre><ol start=6><li>Add the created role to the instance profile:</li></ol><pre tabindex=0><code>$ aws iam add-role-to-instance-profile --instance-profile-name &lt;instance-profile-name&gt; --role-name &lt;role-name&gt;
</code></pre><h4 id=create-the-bastion-instance>Create the Bastion Instance</h4><p>Next, in order to be able to <code>ssh</code> into the bastion instance, the instance has to be set up with a user with a public ssh key.
Create a user <code>gardener</code> that has the same Gardener-generated public ssh key as the target instance.</p><ol><li>First, we need to get the public part of the <code>Shoot</code> ssh-key.
The ssh-key is stored in a secret in the the project namespace in the Garden cluster.
The name is: <code>&lt;shoot-name>-ssh-publickey</code>.
Get the key via:</li></ol><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34;
</code></pre><ol start=2><li>A script handed over as <code>user-data</code> to the bastion <code>ec2</code> instance, can be used to create the <code>gardener</code> user and add the ssh-key.
For your convenience, you can use the following script to generate the <code>user-data</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#00f>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#00f></span>saveUserDataFile () {
</span></span><span style=display:flex><span>  ssh_key=$1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat &gt; gardener-bastion-userdata.sh <span style=color:#a31515>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#a31515>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#a31515>id gardener || useradd gardener -mU
</span></span></span><span style=display:flex><span><span style=color:#a31515>mkdir -p /home/gardener/.ssh
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;$ssh_key&#34; &gt; /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>chown gardener:gardener /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;gardener ALL=(ALL) NOPASSWD:ALL&#34; &gt;/etc/sudoers.d/99-gardener-user
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> [ -p /dev/stdin ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>    read -r input
</span></span><span style=display:flex><span>    cat | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>else</span>
</span></span><span style=display:flex><span>    pbpaste | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>fi</span>
</span></span></code></pre></div><ol start=3><li>Use the script by handing-over the public ssh-key of the <code>Shoot</code> cluster:</li></ol><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34; | ./generate-userdata.sh
</code></pre><p>This generates a file called <code>gardener-bastion-userdata.sh</code> in the same directory containing the <code>user-data</code>.</p><ol start=4><li>The following information is needed to create the bastion instance:</li></ol><p><code>bastion-IAM-instance-profile-name</code>
- Use the created instance profile with the name <code>&lt;shoot-name>-bastions</code></p><p><code>image-id</code>
- It is possible to use the same image-id as the one used for the target instance (or any other image). Has cloud provider specific format (AWS: <code>ami</code>).</p><p><code>ssh-public-key-name</code></p><pre tabindex=0><code>- This is the ssh key pair already created in the Shoot&#39;s cloud provider account by Gardener during the `Infrastructure` CRD reconciliation.
- The name is usually: `&lt;shoot-name&gt;-ssh-publickey`
</code></pre><p><code>subnet-id</code>
- Choose a subnet that is attached to an <code>Internet Gateway</code> and <code>NAT Gateway</code> (bastion instance must have a public IP).
- The Gardener created public subnet with the name <code>&lt;shoot-name>-public-utility-&lt;xy></code> can be used.
Please check the created subnets with the cloud provider.</p><p><code>bastion-security-group-id</code>
- Use the id of the created bastion security group.</p><p><code>file-path-to-userdata</code>
- Use the filepath to the <code>user-data</code> file generated in the previous step.</p><ul><li><code>bastion-instance-name</code><ul><li>Optionaly, you can tag the instance.</li><li>Usually <code>&lt;shoot-name>-bastions</code></li></ul></li></ul><ol start=5><li>Create the bastion instance via:</li></ol><pre tabindex=0><code>$ ec2 run-instances --iam-instance-profile Name=&lt;bastion-IAM-instance-profile-name&gt; --image-id &lt;image-id&gt;  --count 1 --instance-type t3.nano --key-name &lt;ssh-public-key-name&gt;  --security-group-ids &lt;bastion-security-group-id&gt; --subnet-id &lt;subnet-id&gt; --associate-public-ip-address --user-data &lt;file-path-to-userdata&gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=&lt;bastion-instance-name&gt;},{Key=component,Value=&lt;mytag&gt;}] ResourceType=volume,Tags=[{Key=component,Value=&lt;mytag&gt;}]&#34;
</code></pre><p>Capture the <code>instance-id</code> from the response and wait until the <code>ec2</code> instance is running and has a public IP address.</p><h3 id=connecting-to-the-target-instance>Connecting to the Target Instance</h3><ol><li>Save the private key of the ssh-key-pair in a temporary local file for later use:</li></ol><pre tabindex=0><code>$ umask 077

$ kubectl get secret &lt;shoot-name&gt;.ssh-keypair -o json | jq -r .data.\&#34;id_rsa\&#34; | base64 -d &gt; id_rsa.key
</code></pre><ol start=2><li>Use the private ssh key to ssh into the bastion instance:</li></ol><pre tabindex=0><code>$ ssh -i &lt;path-to-private-key&gt; gardener@&lt;public-bastion-instance-ip&gt; 
</code></pre><ol start=3><li>If that works, connect from your local terminal to the target instance via the bastion:</li></ol><pre tabindex=0><code>$ ssh  -i &lt;path-to-private-key&gt; -o ProxyCommand=&#34;ssh -W %h:%p -i &lt;private-key&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@&lt;public-ip-bastion&gt;&#34; gardener@&lt;private-ip-target-instance&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no
</code></pre><h2 id=cleanup>Cleanup</h2><p>Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9722c3efab308a9aee16ce3e552f405f>5.4.2 - How to Debug a Pod</h1><div class=lead>Your pod doesn&rsquo;t run as expected. Are there any log files? Where? How could I debug a pod?</div><h2 id=introduction>Introduction</h2><p>Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in
<a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a>
or <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a>.</p><p>In order to identify pods with potential issues, you could e.g. run <code>kubectl get pods --all-namespaces | grep -iv Running</code> to filter
out the pods which are not in the state <code>Running</code>. One of frequent error state is <code>CrashLoopBackOff</code>, which tells that
a pod crashes right after the start. Kubernetes then tries to restart the pod again, but often the pod startup fails again.</p><p>Here is a short list of possible reasons which might lead to a pod crash:</p><ol><li>Error during image pull caused by e.g. wrong/missing secrets or wrong/missing image</li><li>The app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets</li><li>Liveness probe failed</li><li>Too high resource consumption (memory and/or CPU) or too strict quota settings</li><li>Persistent volumes can&rsquo;t be created/mounted</li><li>The container image is not updated</li></ol><p>Basically, the commands <code>kubectl logs ...</code> and <code>kubectl describe ...</code> with different parameters are used to get more
detailed information. By calling e.g. <code>kubectl logs --help</code> you can get more detailed information about the command and its
parameters.</p><p>In the next sections you&rsquo;ll find some basic approaches to get some ideas what went wrong.</p><p>Remarks:</p><ul><li>Even if the pods seem to be running, as the status <code>Running</code> indicates, a high counter of the <code>Restarts</code> shows potential problems</li><li>You can get a good overview of the troubleshooting process with the interactive tutorial <a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Troubleshooting with Kubectl</a> available which explains basic debugging activities</li><li>The examples below are deployed into the namespace <code>default</code>. In case you want to change it, use the optional
parameter <code>--namespace &lt;your-namespace></code> to select the target namespace. The examples require a Kubernetes release ≥ <em>1.8</em>.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren&rsquo;t running.</p><h2 id=error-caused-by-wrong-image-name>Error Caused by Wrong Image Name</h2><p>Start by running <code>kubectl describe pod &lt;your-pod> &lt;your-namespace></code> to get detailed information about the pod startup.</p><p>In the <code>Events</code> section, you should get an error message like <code>Failed to pull image ...</code> and <code>Reason: Failed</code>. The pod is
in state <code>ImagePullBackOff</code>.</p><p>The example below is based on a <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/>demo in the Kubernetes documentation</a>. In all examples, the <code>default</code> namespace is used.</p><p>First, perform a cleanup with:</p><p><code>kubectl delete pod termination-demo</code></p><p>Next, create a resource based on the yaml content below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod 
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: termination-demo-container
</span></span><span style=display:flex><span>    image: debiann
</span></span><span style=display:flex><span>    command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>    args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span></code></pre></div><p><code>kubectl describe pod termination-demo</code> lists in the <code>Event</code> section the content</p><pre tabindex=0><code>Events:
  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason			Message
  ---------	--------	-----	----							-------------					--------	------			-------
  2m		2m		1	default-scheduler											Normal		Scheduled		Successfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal
  2m		2m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-sgccm&#34; 
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling			pulling image &#34;debiann&#34;
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		Failed			Failed to pull image &#34;debiann&#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found
  2m		54s		10	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync		Error syncing pod
  2m		54s		6	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		BackOff			Back-off pulling image &#34;debiann&#34;
</code></pre><p>The error message with <code>Reason: Failed</code> tells you that there is an error during pulling the image. A closer look at the
image name indicates a misspelling.</p><h2 id=the-app-runs-in-an-error-state-caused-eg-by-missing-environmental-variables-configmaps-or-secrets>The App Runs in an Error State Caused e.g. by Missing Environmental Variables (ConfigMaps) or Secrets</h2><p>This example illustrates the behavior in the case when the app expects environment variables but the corresponding Kubernetes artifacts are missing.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, deploy the following manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span></code></pre></div><p>Now, the command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Error</code> or <code>CrashLoopBackOff</code>.
The command <code>kubectl describe pod termination-demo-xxx</code> tells you that there is no error during startup but gives no clue about what caused the crash.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason		Message
</span></span><span style=display:flex><span>  ---------	--------	-----	----							-------------					--------	------		-------
</span></span><span style=display:flex><span>  19m		19m		1	default-scheduler											Normal		Scheduled	Successfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal
</span></span><span style=display:flex><span>  19m		19m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded <span style=color:#00f>for</span> volume <span style=color:#a31515>&#34;default-token-sgccm&#34;</span> 
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling		pulling image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulled		Successfully pulled image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Created		Created container
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Started		Started container
</span></span><span style=display:flex><span>  19m		14m		24	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		BackOff		Back-off restarting failed container
</span></span><span style=display:flex><span>  19m		4m		69	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync	Error syncing pod
</span></span></code></pre></div><p>The command <code>kubectl get logs termination-demo-xxx</code> gives access to the output, the application writes on <code>stderr</code> and
<code>stdout</code>. In this case, you should get an output similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>/bin/sh: 1: cannot open : No such file
</span></span></code></pre></div><p>So you need to have a closer look at the application. In this case, the environmental variable <code>MYFILE</code> is missing. To fix this
issue, you could e.g. add a ConfigMap to your deployment as is shown in the manifest listed below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env 
</span></span></code></pre></div><p>Note that once you fix the error and re-run the scenario, you might still see the pod in a <code>CrashLoopBackOff</code> status.
It is because the container finishes the command <code>sed ...</code> and runs to completion. In order to keep the container in a <code>Running</code> status,
a long running task is required, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>  SLEEP: <span style=color:#a31515>&#34;5&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:green># args: [&#34;-c&#34;, &#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;]</span>
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;while true; do sleep $SLEEP; echo sleeping; done;&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env
</span></span></code></pre></div><h2 id=too-high-resource-consumption-memory-andor-cpu-or-too-strict-quota-settings>Too High Resource Consumption (Memory and/or CPU) or Too Strict Quota Settings</h2><p>You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing,
the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi, which indicate no other limits other than the
ones of the node(s) itself. For more details, e.g. about how to configure limits, see <a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a>.</p><p>In case your application needs more resources, Kubernetes distinguishes between <code>requests</code> and <code>limit</code> settings: <code>requests</code>
specify the guaranteed amount of resource, whereas <code>limit</code> tells Kubernetes the maximum amount of resource the container might
need. Mathematically, both settings could be described by the relation <code>0 &lt;= requests &lt;= limit</code>. For both settings you need to
consider the total amount of resources your nodes provide. For a detailed description of the concept, see <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a>.</p><p>Use <code>kubectl describe nodes</code> to get a first overview of the resource consumption in your cluster. Of special interest are the
figures indicating the amount of CPU and Memory Requests at the bottom of the output.</p><p>The next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, adapt the <code>cpu</code> below in the yaml below to be slightly higher than the remaining CPU resources in your cluster and deploy
this manifest. In this example, <code>600m</code> (milli CPUs) are requested in a Kubernetes system with a single 2 core worker
node which results in an error message.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: <span style=color:#a31515>&#34;600m&#34;</span> 
</span></span></code></pre></div><p>The command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Pending</code>. More details on why this happens
could be found by using the command <code>kubectl describe pod termination-demo-xxx</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Name:           termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Containers:
</span></span><span style=display:flex><span>  termination-demo-container:
</span></span><span style=display:flex><span>    Image:      debian
</span></span><span style=display:flex><span>    Port:       &lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:  &lt;none&gt;
</span></span><span style=display:flex><span>    Command:
</span></span><span style=display:flex><span>      /bin/sh
</span></span><span style=display:flex><span>    Args:
</span></span><span style=display:flex><span>      -c
</span></span><span style=display:flex><span>      sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:        6
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:
</span></span><span style=display:flex><span>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro)
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type           Status
</span></span><span style=display:flex><span>  PodScheduled   False
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason            Age               From               Message
</span></span><span style=display:flex><span>  ----     ------            ----              ----               -------
</span></span><span style=display:flex><span>  Warning  FailedScheduling  9s (x7 over 40s)  default-scheduler  0/2 nodes are available: 2 Insufficient cpu.
</span></span></code></pre></div><p>You can find more details in:</p><ul><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li></ul><p>Remarks:</p><ul><li>This example works similarly when specifying a too high request for memory</li><li>In case you configured an autoscaler range when creating your Kubernetes cluster, another worker node will be spinned up automatically if you didn&rsquo;t reach the maximum number of worker nodes</li><li>In case your app is running out of memory (the memory settings are too small), you will typically find an <code>OOMKilled</code> (Out Of Memory) message in the <code>Events</code> section of the <code>kubectl describe pod ...</code> output</li></ul><h2 id=the-container-image-is-not-updated>The Container Image Is Not Updated</h2><p>You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests, you expected to get the updated app, but the same bug is still in the new deployment present.</p><p>This behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.</p><p>In case you didn&rsquo;t change the image tag, the default image policy <em>IfNotPresent</em> tells Kubernetes to use the cached image (see <a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a>).</p><p>As a best practice, you should not use the tag <code>latest</code> and change the image tag in case you changed anything in your image (see <a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Configuration Best Practices</a>).</p><p>Please have a look at this FAQ <a href=https://github.tools.sap/kubernetes/documentation/blob/master/website/documentation/015-tutorials/image-pull-policy/_index.md>Container Image Not Updating</a> for further details.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a></li><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>Logging Architecture</a></li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Interactive Tutorial Troubleshooting with Kubectl</a></li><li><a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Kubernetes Best Practises</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b5575668e5d9ec9a137723e26819e36b>5.4.3 - tail -f /var/log/my-application.log</h1><div class=lead>Aggregate log files from different pods</div><h2 id=problem>Problem</h2><p>One thing that always bothered me was that I couldn&rsquo;t get logs of several pods at once with <code>kubectl</code>. A simple
<code>tail -f &lt;path-to-logfile></code> isn&rsquo;t possible at all. Certainly, you can use <code>kubectl logs -f &lt;pod-id></code>, but it doesn&rsquo;t
help if you want to monitor more than one pod at a time.</p><p>This is something you really need a lot, at least if you run several instances of a pod behind a <code>deployment</code>.
This is even more so if you don&rsquo;t have a Kibana or a similar setup.</p><img src=/__resources/howto-kubetail_24c471.png width=100%><h2 id=solution>Solution</h2><p>Luckily, there are smart developers out there who always come up with solutions. The <strong>finding of the week</strong> is
a small bash script that allows you to aggregate log files of several pods at the same time in
a simple way. The script is called <code>kubetail</code> and is available at
<a href=https://github.com/johanhaleby/kubetail>GitHub</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-54886e62cc8767ab592a5259b6e86b39>5.5 - Applications</h1></div><div class=td-content><h1 id=pg-cdf7f44ffe002ba4c8c2fba6647dcc99>5.5.1 - Access a Port of a Pod Locally</h1><h2 id=question>Question</h2><p>You have deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How to access this endpoint <strong>without an external load balancer</strong> (e.g. Ingress)?</p><p>This tutorial presents two options:</p><ul><li>Using Kubernetes port forward</li><li>Using Kubernetes apiserver proxy</li></ul><p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to the <a href=https://kubernetes.io/docs/concepts/services-networking/service/>official Kubernetes documentation</a>.</p><h2 id=solution-1-using-kubernetes-port-forward>Solution 1: Using Kubernetes port forward</h2><p>You could use the port forwarding functionality of <code>kubectl</code> to access the pods from your local host <strong>without involving a service</strong>.</p><p>To access any pod follow these steps:</p><ol><li>Run <code>kubectl get pods</code></li><li>Note down the name of the pod in question as <code>&lt;your-pod-name></code></li><li>Run <code>kubectl port-forward &lt;your-pod-name> &lt;local-port>:&lt;your-app-port></code></li><li>Run a web browser or curl locally and enter the URL: <code>http(s)://localhost:&lt;local-port></code></li></ol><p>In addition, <code>kubectl port-forward</code> allows using a resource name, such as a deployment name or service name, to select a matching pod to port forward.
More details can be found in the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>Kubernetes documentation</a>.</p><p>The main drawback of this approach is that the pod&rsquo;s name changes as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes the port forwarding is canceled due to nonobvious reasons. This leads to a kind of shaky approach. A more stable possibility is based on accessing the app via the kube-proxy, which accesses the corresponding service.</p><p><img src=/__resources/howto-port-forward_663827.svg alt=port-forward></p><h2 id=solution-2-using-the-apiserver-proxy-of-your-kubernetes-cluster>Solution 2: Using the apiserver proxy of Your Kubernetes Cluster</h2><p>There are <a href=https://kubernetes.io/docs/concepts/cluster-administration/proxies/>several different proxies</a> in Kubernetes. In this tutorial we will be using <em>apiserver proxy</em> to enable the access to the services in your cluster without Ingress. <strong>Unlike the first solution, here a service is required.</strong></p><p>Use the following format to compose a URL for accessing your service through an existing proxy on the Kubernetes cluster:</p><p><code>https://&lt;your-cluster-master>/api/v1/namespace/&lt;your-namespace>/services/&lt;your-service>:&lt;your-service-port>/proxy/&lt;service-endpoint></code></p><p><strong>Example:</strong></p><table><thead><tr><th>your-main-cluster</th><th style=text-align:center>your-namespace</th><th style=text-align:right>your-service</th><th style=text-align:right>your-service-port</th><th style=text-align:right>your-service-endpoint</th><th style=text-align:right>url to access service</th></tr></thead><tbody><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>nginx-svc</td><td style=text-align:right>80</td><td style=text-align:right>/</td><td style=text-align:right><code>http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/</code></td></tr><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>docker-nodejs-svc</td><td style=text-align:right>4500</td><td style=text-align:right>/cpu?baseNumber=4</td><td style=text-align:right><code>https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4</code></td></tr></tbody></table><p>For more details on the format, please refer to the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>official Kubernetes documentation</a>.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There are applications which do not support relative URLs yet, e.g. <a href=https://github.com/prometheus/prometheus/issues/1583>Prometheus</a> (as of November, 2022).
This typically leads to missing JavaScript objects, which could be investigated with your browser&rsquo;s development tools. If such an issue occurs, please use the <code>port-forward</code> approach <a href=#solution-1-using-kubernetes-port-forward>described above</a>.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-a9c93d6cbc648f7853fd4d0aae5fd5f3>5.5.2 - Auditing Kubernetes for Secure Setup</h1><div class=lead>A few insecure configurations in Kubernetes</div><p><img src=/__resources/teaser_2d8cc0.svg alt=teaser></p><h2 id=increasing-the-security-of-all-gardener-stakeholders>Increasing the Security of All Gardener Stakeholders</h2><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work was to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.</p><h2 id=major-findings>Major Findings</h2><p>From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.</p><p>Alban Crequy (<a href=https://kinvolk.io/>Kinvolk</a>) and Dirk Marwinski (<a href=https://www.sap.com>SAP SE</a>) gave a presentation entitled <a href=https://kccncchina2018english.sched.com/event/H2Hd/hardening-multi-cloud-kubernetes-clusters-as-a-service-dirk-marwinski-sap-se-alban-crequy-kinvolk-gmbh>Hardening Multi-Cloud Kubernetes Clusters as a Service</a> at KubeCon 2018 in Shanghai presenting some of the findings.</p><p>Here is a summary of the findings:</p><ul><li><p>Privilege escalation due to insecure configuration of the Kubernetes
API server</p><ul><li>Root cause: Same certificate authority (CA) is used for both the
API server and the proxy that allows accessing the API server.</li><li>Risk: Users can get access to the API server.</li><li>Recommendation: Always use different CAs.</li></ul></li><li><p>Exploration of the control plane network with malicious
HTTP-redirects</p><ul><li><p>Root cause: See detailed description below.</p></li><li><p>Risk: Provoked error message contains full HTTP payload from an
existing endpoint which can be exploited. The contents of the
payload depends on your setup, but can potentially be user data,
configuration data, and credentials.</p></li><li><p>Recommendation:</p><ul><li>Use the latest version of Gardener</li><li>Ensure the seed cluster&rsquo;s container network supports
network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not
protected as Flannel is used there which doesn&rsquo;t support
network policies.</li></ul></li></ul></li><li><p>Reading private AWS metadata via Grafana</p><ul><li>Root cause: It is possible to configuring a new custom data
source in Grafana, we could send HTTP requests to target the
control</li><li>Risk: Users can get the &ldquo;user-data&rdquo; for the seed cluster from
the metadata service and retrieve a kubeconfig for that
Kubernetes cluster</li><li>Recommendation: Lockdown Grafana features to only what&rsquo;s
necessary in this setup, block all unnecessary outgoing traffic,
move Grafana to a different network, lockdown unauthenticated
endpoints</li></ul></li></ul><h2 id=scenario-1-privilege-escalation-with-insecure-api-server>Scenario 1: Privilege Escalation with Insecure API Server</h2><p>In most configurations, different components connect directly to the Kubernetes API server, often using a <code>kubeconfig</code> with a client
certificate. The API server is started with the flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ...
</span></span></code></pre></div><p>The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.</p><p><img src=/__resources/image3_9bc94e.png alt><em>The API server can have many clients of various kinds</em><br><br><br></p><p>However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt
</span></span><span style=display:flex><span>--requestheader-username-headers=X-Remote-User
</span></span><span style=display:flex><span>--requestheader-group-headers=X-Remote-Group
</span></span></code></pre></div><p><img src=/__resources/image2_e15f3f.png alt><em>API server clients can reach the API server through an authenticating proxy</em><br><br><br></p><p>So far, so good. But what happens if the malicious user “Mallory” tries to connect directly to the API server and reuses
the HTTP headers to pretend to be someone else?</p><p><img src=/__resources/image8_edf260.png alt><em>What happens when a client bypasses the proxy, connecting directly to the API server?</em><br><br><br></p><p>With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority
but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.</p><p>You only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes
client certificate can be used to take the role of different user or group as the API server will accept the user header and
group header.</p><p>The <code>kubectl</code> tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP
requests manually.</p><p>We worked on <a href=https://github.com/kubernetes/website/pull/10093>improving the Kubernetes documentation</a> to make clearer
that this configuration should be avoided.</p><h2 id=scenario-2-exploration-of-the-control-plane-network-with-malicious-http-redirects>Scenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects</h2><p>The API server is a central component of Kubernetes and many components initiate connections to it, including the kubelet
running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services,
deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.</p><p><img src=/__resources/image7_038441.png alt><em>The API server is mostly a component that receives requests</em><br><br><br></p><p>However, there are exceptions. Some <code>kubectl</code> commands will trigger the API server to open a new
connection to the kubelet. <code>kubectl exec</code> is one of those commands. In order to get the standard I/Os from the pod,
the API server will start an HTTP connection to the kubelet on the worker node where the pod is running. Depending on
the container runtime used, it can be done in different ways, but one way to do it is for the kubelet to reply with a
HTTP-302 redirection to the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Container Runtime Interface (CRI)</a>.
Basically, the kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The
redirection from the kubelet will only change the port and path from the URL; the IP address will not be changed because
the kubelet and the CRI component run on the same worker node.</p><p><img src=/__resources/image1_892eee.png alt><em>But the API server also initiates some connections, for example, to worker nodes</em><br><br><br></p><p>It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the kubelet. They
could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods
or even just pods with “host” volumes.</p><p>In contrast, users (even those with “system:masters” permissions or “root” rights) are often not given access to the control plane.
On setups like, for example, GKE or Gardener, the control plane is running on separate nodes, with a different administrative
access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network
in the control plane.</p><p>What would happen if a user was tampering with the kubelet to make it maliciously redirect <code>kubectl exec</code> requests to
a different random endpoint? Most likely the given endpoint would not speak to the streaming server protocol, so there would
be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.</p><p><img src=/__resources/image6_240221.png alt><em>The API server is tricked to connect to other components</em><br><br><br></p><p>The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service
(such as the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html>AWS metadata service</a>)
containing user data, configurations and credentials. The setup we explored had a different AWS account and a different
<a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html>EC2 instance profile</a>
for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the
context of the control plane, which they should not have access to.</p><p>We have reported this issue to the <a href=https://kubernetes.io/docs/reference/issues-security/security/>Kubernetes Security mailing list</a>
and the public pull request that addresses the issue has been merged <a href=https://github.com/kubernetes/kubernetes/pull/66516>PR#66516</a>.
It provides a way to enforce HTTP redirect validation (disabled by default).</p><p>But there are several other ways that users could trigger the API server to generate HTTP requests and get the reply
payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures.
Depending on where the API server runs, it could be with <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes Network Policies</a>, <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html>EC2 Security Groups</a> or just iptables directly. Following the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth principle</a>,
it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.</p><p>In Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does
not need to contact the metadata service. You can see more details in the <a href=https://groups.google.com/forum/#!forum/gardener>announcements on the Gardener mailing list</a>.
This is tracked in <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-2475">CVE-2018-2475</a>.</p><p><em>To be protected from this issue, stakeholders should:</em></p><ul><li><em>Use the latest version of Gardener</em></li><li><em>Ensure the seed cluster’s container network supports network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn’t support network
policies.</em></li></ul><h2 id=scenario-3-reading-private-aws-metadata-via-grafana>Scenario 3: Reading Private AWS Metadata via Grafana</h2><p>For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control
plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control
plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana
via a load balancer. The internal network of the control plane is therefore hidden to users.</p><p><img src=/__resources/image5_f50567.png alt><em>Prometheus and Grafana can be used to monitor worker nodes</em><br><br><br></p><p>Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom
data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata
service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging
console of the Chrome browser.</p><p><img src=/__resources/image9_d39dc7.png alt><em>Credentials can be retrieved from the debugging console of Chrome</em><br><br><br></p><p><img src=/__resources/image4_a248a3.png alt><em>Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets</em><br><br><br></p><p>In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a
kubeconfig for that Kubernetes cluster.</p><p>There are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all
unnecessary outgoing traffic, move Grafana to a different network, or lockdown unauthenticated endpoints, among others.</p><h2 id=conclusion>Conclusion</h2><p>The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes
installation: different cloud providers or different configurations will show different weaknesses. Users should no longer be given access to Grafana.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e8efeb071c3d7a7cfbd4334db1fb8b8>5.5.3 - Container Image Not Pulled</h1><div class=lead>Wrong Container Image or Invalid Registry Permissions</div><h2 id=problem>Problem</h2><p>Two of the most common causes of this problems are specifying the wrong container image or trying to use private images without providing registry credentials.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There is no observable difference in pod status between a missing image and incorrect registry permissions.
In either case, Kubernetes will report an <code>ErrImagePull</code> status for the pods. For this reason, this article deals with
both scenarios.</div><h2 id=example>Example</h2><p>Let&rsquo;s see an example. We&rsquo;ll create a pod named <em>fail</em>, referencing a non-existent Docker image:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl run -i --tty fail --image=tutum/curl:1.123456
</span></span></code></pre></div><p>The command doesn&rsquo;t return and you can terminate the process with <code>Ctrl+C</code>.</p><h2 id=error-analysis>Error Analysis</h2><p>We can then inspect our pods and see that we have one pod with a status of <strong>ErrImagePull</strong> or <strong>ImagePullBackOff</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ (minikube) kubectl get pods
</span></span><span style=display:flex><span>NAME                      READY     STATUS         RESTARTS   AGE
</span></span><span style=display:flex><span>client-5b65b6c866-cs4ch   1/1       Running        1          1m
</span></span><span style=display:flex><span>fail-6667d7685d-7v6w8     0/1       ErrImagePull   0          &lt;invalid&gt;
</span></span><span style=display:flex><span>vuejs-578574b75f-5x98z    1/1       Running        0          1d
</span></span><span style=display:flex><span>$ (minikube) 
</span></span></code></pre></div><p>For some additional information, we can <code>describe</code> the failing pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl describe pod fail-6667d7685d-7v6w8
</span></span></code></pre></div><p>As you can see in the events section, your image can&rsquo;t be pulled:</p><pre tabindex=0><code>Name:		fail-6667d7685d-7v6w8
Namespace:	default
Node:		minikube/192.168.64.10
Start Time:	Wed, 22 Nov 2017 10:01:59 +0100
Labels:		pod-template-hash=2223832418
		run=fail
Annotations:	kubernetes.io/created-by={&#34;kind&#34;:&#34;SerializedReference&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;reference&#34;:{&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;name&#34;:&#34;fail-6667d7685d&#34;,&#34;uid&#34;:&#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f&#34;,&#34;a...
.
.
.
.
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath		Type		Reason			Message
  ---------	--------	-----	----			-------------		--------	------			-------
  1m		1m		1	default-scheduler				Normal		Scheduled		Successfully assigned fail-6667d7685d-7v6w8 to minikube
  1m		1m		1	kubelet, minikube				Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-9fr6r&#34; 
  1m		6s		4	kubelet, minikube	spec.containers{fail}	Normal		Pulling			pulling image &#34;tutum/curl:1.123456&#34;
  1m		5s		4	kubelet, minikube	spec.containers{fail}	Warning		Failed			Failed to pull image &#34;tutum/curl:1.123456&#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found
  1m		&lt;invalid&gt;	10	kubelet, minikube				Warning		FailedSync		Error syncing pod
  1m		&lt;invalid&gt;	6	kubelet, minikube	spec.containers{fail}	Normal		BackOff			Back-off pulling image &#34;tutum/curl:1.123456&#34;
</code></pre><p><strong>Why couldn&rsquo;t Kubernetes pull the image?</strong>
There are three primary candidates besides network connectivity issues:</p><ul><li>The image tag is incorrect</li><li>The image doesn&rsquo;t exist</li><li>Kubernetes doesn&rsquo;t have permissions to pull that image</li></ul><p>If you don&rsquo;t notice a typo in your image tag, then it&rsquo;s time to test using your local machine. I usually start by
running <strong>docker pull on my local development machine</strong> with the exact same image tag. In this case, I would
run <code>docker pull tutum/curl:1.123456</code>.</p><p>If this succeeds, then it probably means that Kubernetes doesn&rsquo;t have the correct permissions to pull that image.</p><p>Add the docker registry user/pwd to your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=&lt;username&gt; --docker-password=&lt;password&gt; --docker-email=&lt;email&gt;
</span></span></code></pre></div><p>If the exact image tag fails, then I will test without an explicit image tag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker pull tutum/curl
</span></span></code></pre></div><p>This command will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn&rsquo;t exist. Go to the Docker registry and check which tags are available for this image.</p><p>If <code>docker pull tutum/curl</code> (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7486173c78c00fc1dfc4e45f04c5c7f>5.5.4 - Container Image Not Updating</h1><div class=lead>Updating images in your cluster during development</div><h2 id=introduction>Introduction</h2><p>A container image should use a fixed tag or the SHA of the image. It should not use the tags <strong>latest</strong>, <strong>head</strong>, <strong>canary</strong>, or other tags that are designed to be <em>floating</em>.</p><h2 id=problem>Problem</h2><p>If you have encountered this issue, you have probably done something along the lines of:</p><ul><li>Deploy anything using an image tag (e.g. <code>cp-enablement/awesomeapp:1.0</code>)</li><li>Fix a bug in awesomeapp</li><li>Build a new image and push it with the <strong>same tag</strong> (<code>cp-enablement/awesomeapp:1.0</code>)</li><li>Update the deployment</li><li>Realize that the bug is still present</li><li>Repeat steps 3-5 without any improvement</li></ul><p>The problem relates to how Kubernetes decides whether to do a <em>docker pull</em> when starting a container.
Since we tagged our image as <em>:1.0</em>, the default pull policy is <strong>IfNotPresent</strong>. The Kubelet already has a local
copy of <code>cp-enablement/awesomeapp:1.0</code>, so it doesn&rsquo;t attempt to do a docker pull. When the new Pods come up,
they&rsquo;re still using the old broken Docker image.</p><p>There are a couple of ways to resolve this, with the recommended one being to <strong>use unique tags</strong>.</p><h2 id=solution>Solution</h2><p>In order to fix the problem, you can use the following bash script that runs anytime the deployment is updated to create a new tag
and push it to the registry.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#00f>#!/usr/bin/env bash
</span></span></span><span style=display:flex><span><span style=color:#00f></span>
</span></span><span style=display:flex><span><span style=color:green># Set the docker image name and the corresponding repository</span>
</span></span><span style=display:flex><span><span style=color:green># Ensure that you change them in the deployment.yml as well.</span>
</span></span><span style=display:flex><span><span style=color:green># You must be logged in with docker login.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># CHANGE THIS TO YOUR Docker.io SETTINGS</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>PROJECT=awesomeapp
</span></span><span style=display:flex><span>REPOSITORY=cp-enablement
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># causes the shell to exit if any subcommand or pipeline returns a non-zero status.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># set debug mode</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build my nodeJS app</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>npm run build
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># get the latest version ID from the Docker.io registry and increment them</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#00f>$(</span>curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags  | sed -e <span style=color:#a31515>&#39;s/[][]//g&#39;</span> -e <span style=color:#a31515>&#39;s/&#34;//g&#39;</span> -e <span style=color:#a31515>&#39;s/ //g&#39;</span> | tr <span style=color:#a31515>&#39;}&#39;</span> <span style=color:#a31515>&#39;\n&#39;</span>  | awk -F: <span style=color:#a31515>&#39;{print $3}&#39;</span> | grep v| tail -n 1<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>${</span>VERSION:1<span style=color:#a31515>}</span>
</span></span><span style=display:flex><span>((VERSION++))
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>&#34;v</span>$VERSION<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build the new docker image</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Building new image&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Push new image&#39;</span>
</span></span><span style=display:flex><span>docker push $REPOSITORY/$PROJECT:$VERSION
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a7727382924a6be0bac7859384e0cf01>5.5.5 - Custom Seccomp Profile</h1><h2 id=overview>Overview</h2><p><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.</p><p>Starting from Kubernetes v1.3.0, the Seccomp feature is in <code>Alpha</code>. To configure it on a <code>Pod</code>, the following annotations can be used:</p><ul><li><code>seccomp.security.alpha.kubernetes.io/pod: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to all containers in a <code>Pod</code>.</li><li><code>container.seccomp.security.alpha.kubernetes.io/&lt;container-name>: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to <code>&lt;container-name></code> in a <code>Pod</code>.</li></ul><p>More details can be found in the <code>PodSecurityPolicy</code> <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp>documentation</a>.</p><h2 id=installation-of-a-custom-profile>Installation of a Custom Profile</h2><p>By default, kubelet loads custom Seccomp profiles from <code>/var/lib/kubelet/seccomp/</code>. There are two ways in which Seccomp profiles can be added to a <code>Node</code>:</p><ul><li>to be baked in the machine image</li><li>to be added at runtime</li></ul><p>This guide focuses on creating those profiles via a <code>DaemonSet</code>.</p><p>Create a file called <code>seccomp-profile.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-profile
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  my-profile.json: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    {
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;defaultAction&#34;: &#34;SCMP_ACT_ALLOW&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;syscalls&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#a31515>        {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;name&#34;: &#34;chmod&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;action&#34;: &#34;SCMP_ACT_ERRNO&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>    }</span>    
</span></span></code></pre></div><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>The policy above is a very simple one and not suitable for complex applications. The <a href=https://github.com/moby/moby/blob/v17.05.0-ce/profiles/seccomp/default.json>default docker profile</a> can be used a reference. Feel free to modify it to your needs.</div><p>Apply the <code>ConfigMap</code> in your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-profile.yaml
</span></span><span style=display:flex><span>configmap/seccomp-profile created
</span></span></code></pre></div><p>The next steps is to create the <code>DaemonSet</code> Seccomp installer. It&rsquo;s going to copy the policy from above in <code>/var/lib/kubelet/seccomp/my-profile.json</code>.</p><p>Create a file called <code>seccomp-installer.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    security: seccomp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      security: seccomp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        security: seccomp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - name: installer
</span></span><span style=display:flex><span>        image: alpine:3.10.0
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>, <span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;cp -r -L /seccomp/*.json /host/seccomp/&#34;</span>]
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: profiles
</span></span><span style=display:flex><span>          mountPath: /seccomp
</span></span><span style=display:flex><span>        - name: hostseccomp
</span></span><span style=display:flex><span>          mountPath: /host/seccomp
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: pause
</span></span><span style=display:flex><span>        image: k8s.gcr.io/pause:3.1
</span></span><span style=display:flex><span>      terminationGracePeriodSeconds: 5
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: hostseccomp
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/seccomp
</span></span><span style=display:flex><span>      - name: profiles
</span></span><span style=display:flex><span>        configMap:
</span></span><span style=display:flex><span>          name: seccomp-profile
</span></span></code></pre></div><p>Create the installer and wait until it&rsquo;s ready on all <code>Nodes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-installer.yaml
</span></span><span style=display:flex><span>daemonset.apps/seccomp-installer created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl -n kube-system get pods -l security=seccomp
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-installer-wjbxq   1/1     Running   0          21s
</span></span></code></pre></div><h2 id=create-a-pod-using-a-custom-seccomp-profile>Create a Pod Using a Custom Seccomp Profile</h2><p>Finally, we want to create a profile which uses our new Seccomp profile <code>my-profile.json</code>.</p><p>Create a file called <code>my-seccomp-pod.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-app
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/my-profile.json&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># you can specify seccomp profile per container. If you add another profile you can configure</span>
</span></span><span style=display:flex><span>    <span style=color:green># it for a specific container - &#39;pause&#39; in this case.</span>
</span></span><span style=display:flex><span>    <span style=color:green># container.seccomp.security.alpha.kubernetes.io/pause: &#34;localhost/some-other-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><p>Create the <code>Pod</code> and see that it&rsquo;s running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f my-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/seccomp-app created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod seccomp-app
</span></span><span style=display:flex><span>NAME         READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-app  1/1     Running   0          42s
</span></span></code></pre></div><h2 id=throubleshooting>Throubleshooting</h2><p>If an invalid or a non-existing profile is used, then the <code>Pod</code> will be stuck in <code>ContainerCreating</code> phase:</p><p><code>broken-seccomp-pod.yaml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: broken-seccomp
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/not-existing-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f broken-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/broken-seccomp created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod broken-seccomp
</span></span><span style=display:flex><span>NAME            READY   STATUS              RESTARTS   AGE
</span></span><span style=display:flex><span>broken-seccomp  1/1     ContainerCreating   0          2m
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl describe pod broken-seccomp
</span></span><span style=display:flex><span>Name:               broken-seccomp
</span></span><span style=display:flex><span>Namespace:          default
</span></span><span style=display:flex><span>....
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason                  Age               From                     Message
</span></span><span style=display:flex><span>  ----     ------                  ----              ----                     -------
</span></span><span style=display:flex><span>  Normal   Scheduled               18s               default-scheduler        Successfully assigned kube-system/broken-seccomp to docker-desktop
</span></span><span style=display:flex><span>  Warning  FailedCreatePodSandBox  4s (x2 over 18s)  kubelet, docker-desktop  Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod &#34;broken-seccomp&#34;: failed to generate sandbox security options
</span></span><span style=display:flex><span>for sandbox &#34;broken-seccomp&#34;: failed to generate seccomp security options for container: cannot load seccomp profile &#34;/var/lib/kubelet/seccomp/not-existing-profile.json&#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a></li><li><a href=https://lwn.net/Articles/656307/>A Seccomp Overview</a></li><li><a href=https://docs.docker.com/engine/security/seccomp>Seccomp Security Profiles for Docker</a></li><li><a href=https://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf>Using Seccomp to Limit the Kernel Attack Surface</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8de2e5545164d701b7df8e8a18ef00b2>5.5.6 - Dockerfile Pitfalls</h1><div class=lead>Common Dockerfile pitfalls</div><h2 id=using-the-latest-tag-for-an-image>Using the <code>latest</code> Tag for an Image</h2><p>Many Dockerfiles use the <code>FROM package:latest</code> pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.</p><h3 id=bad-dockerfile>Bad Dockerfile</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine</span><span>
</span></span></span></code></pre></div><p>While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest), while a build server may fail, because some pipelines make a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn&rsquo;t actually make any changes.</p><h3 id=good-dockerfile>Good Dockerfile</h3><p>A digest takes the place of the tag when pulling an image. This will ensure that your Dockerfile remains immutable.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430</span><span>
</span></span></span></code></pre></div><h2 id=running-aptapkyum-update>Running apt/apk/yum update</h2><p>Running <code>apt-get install</code> is one of those things virtually every Debian-based Dockerfile will have to do in order to satiate some external package requirements your code needs to run. However, using <code>apt-get</code> as an example, this comes with its own problems.</p><p><strong>apt-get upgrade</strong></p><p>This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.</p><p><strong>apt-get update (in a different line than the one running your apt-get install command)</strong></p><p>Running <code>apt-get update</code> as a single line entry will get cached by the build and won&rsquo;t actually run every time you need to run <code>apt-get install</code>. Instead, make sure you run <code>apt-get update</code> in the same line with all the packages to ensure that all are updated correctly.</p><h2 id=avoid-big-container-images>Avoid Big Container Images</h2><p>Building a small container image will reduce the time needed to start or restart pods. An image based on the popular <a href=http://alpinelinux.org/>Alpine Linux project</a> is much smaller than most distribution based images (~5MB). For most popular languages and products, there is usually an official Alpine Linux image, e.g. <a href=https://hub.docker.com/_/golang/>golang</a>, <a href=https://hub.docker.com/_/node/>nodejs</a>, and <a href=https://hub.docker.com/_/postgres/>postgres</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  docker images
</span></span><span style=display:flex><span>REPOSITORY                                                      TAG                     IMAGE ID            CREATED             SIZE
</span></span><span style=display:flex><span>postgres                                                        9.6.9-alpine            6583932564f8        13 days ago         39.26 MB
</span></span><span style=display:flex><span>postgres                                                        9.6                     d92dad241eff        13 days ago         235.4 MB
</span></span><span style=display:flex><span>postgres                                                        10.4-alpine             93797b0f31f4        13 days ago         39.56 MB
</span></span></code></pre></div><p>In addition, for compiled languages such as Go or C++ that do not require build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker&rsquo;s support for <a href=https://docs.docker.com/engine/userguide/eng-image/multistage-build/>multi-stages builds</a>, this can be easily achieved with minimal effort. Such an example can be found at <a href=https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages>Multi-stage builds</a>.</p><p>Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image is also a good base image.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f45e1e3bb19a23ceeebe6d769078b409>5.5.7 - Integrity and Immutability</h1><div class=lead>Ensure that you always get the right image</div><h2 id=introduction>Introduction</h2><p>When transferring data among networked systems, <strong>trust is a central concern</strong>. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the <strong>integrity and immutability</strong> of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a <strong>public registry</strong>.</p><p>This immutability offers you a guarantee that any and all containers that you instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.</p><h2 id=a-lesson-in-deterministic-ops>A Lesson in Deterministic Ops</h2><p>Docker Tags are about as reliable and disposable as this guy down here.</p><p><img src=/__resources/howto-content-trust_5b9a4e.svg alt=docker-labels></p><p>Seems simple enough. You have probably already deployed hundreds of YAML&rsquo;s or started endless counts of Docker containers.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx:1.13.9
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: rss-site
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: web
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: web
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: front-end
</span></span><span style=display:flex><span>          image: nginx:1.13.9
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 80
</span></span></code></pre></div><p><strong>But Tags are mutable and humans are prone to error. Not a good combination.</strong> Here, we’ll dig into why the use of tags can
be dangerous and how to deploy your containers across a pipeline and across environments with <strong>determinism in mind</strong>.</p><p>Let&rsquo;s say that you want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that
you have defined. Any updates or newer versions of an image should be executed as a new deployment. <strong>The solution: digest</strong></p><p>A digest takes the place of the tag when pulling an image. For example, to pull the above image by digest, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de
</span></span></code></pre></div><p>You can now make sure that the same image is always loaded at every deployment. It doesn&rsquo;t matter if the TAG of the image has been changed or not. <strong>This solves the problem of repeatability.</strong></p><h2 id=content-trust>Content Trust</h2><p>However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another
one infected with malware.</p><p><img src=/__resources/howto-content-trust-replace_29e215.svg alt=docker-content-trust></p><p><a href=https://docs.docker.com/engine/security/trust/content_trust/>Docker Content trust</a> gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p><p>Prior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature
called <strong>Docker Content Trust</strong> was introduced to automatically sign and verify the signature of a publisher.</p><p>So, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. <strong>This solves the problem of trust.</strong></p><p>In addition, you should scan all images for known vulnerabilities.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d122ee5ae2c428bfe3d1293dd385721>5.5.8 - Kubernetes Antipatterns</h1><div class=lead>Common antipatterns for Kubernetes and Docker</div><p><img src=/__resources/howto-antipattern_e88da2.png alt=antipattern></p><p>This HowTo covers common Kubernetes antipatterns that we have seen over the past months.</p><h2 id=running-as-root-user>Running as Root User</h2><p>Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes pods and nodes are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the
underlying node.</p><p>Watch the very good presentation by Liz Rice at the KubeCon 2018</p><iframe width=560 height=315 src=https://www.youtube.com/embed/ltrV-Qmh3oY frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>Use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group and add a user to it. Use the <code>USER</code> command to switch to this user. Note that you may also consider to provide <a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user>an explicit UID/GID</a> if required.</p><p>For example:</p><pre tabindex=0><code>ARG GF_UID=&#34;500&#34;
ARG GF_GID=&#34;500&#34;

# add group &amp; user
RUN groupadd -r -g $GF_GID appgroup &amp;&amp; \
   useradd appuser -r -u $GF_UID -g appgroup

USER appuser
</code></pre><h2 id=store-data-or-logs-in-containers>Store Data or Logs in Containers</h2><p>Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the
container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside
of containers. Using an <a href=https://www.elastic.co/de/what-is/elk-stack>ELK stack</a> is another good option for storing and processing logs.</p><h2 id=using-pod-ip-addresses>Using Pod IP Addresses</h2><p>Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application
must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile.</p><p>Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.</p><h2 id=more-than-one-process-in-a-container>More Than One Process in a Container</h2><p>A docker file provides a <code>CMD</code> and <code>ENTRYPOINT</code> to start the image. <code>CMD</code> is often used around a script that makes a configuration and then
starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult.</p><p>You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with <code>PID=1</code>. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.</p><h2 id=creating-images-in-a-running-container>Creating Images in a Running Container</h2><p>A new image can be created with the <code>docker commit</code> command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.</p><h2 id=saving-passwords-in-a-docker-image-->Saving Passwords in a docker Image 💀</h2><p><strong>Do not save passwords in a Docker file!</strong> They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory.</p><p>Always use <a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure>Secrets or ConfigMaps</a> to provision passwords or inject them by mounting a persistent volume.</p><h2 id=using-the-latest-tag>Using the &rsquo;latest&rsquo; Tag</h2><p>Starting an image with <em>tomcat</em> is tempting. If no tags are specified, a container is started with the <code>tomcat:latest</code> image. This image may no longer be up to date and refer to an older version instead. Running a production application requires complete control of the environment with exact versions of the image.</p><p>Make sure you always use a tag or even better the <strong>sha256 hash</strong> of the image e.g. <code>tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f</code>.</p><h3 id=why-use-the-sha256-hash>Why Use the sha256 Hash?</h3><p>Tags are not immutable and can be overwritten by a developer at any time. In this case you don&rsquo;t have complete control over your image - which is bad.</p><h2 id=different-images-per-environment>Different Images per Environment</h2><p>Don&rsquo;t create different images for development, testing, staging and production environments. The image should be the <strong>source of truth</strong> and should only be created once and pushed to the repository. This <code>image:tag</code> should be used for different environments in the future.</p><h2 id=depend-on-start-order-of-pods>Depend on Start Order of Pods</h2><p>Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.</p><h2 id=additional-anti-patterns-and-patterns>Additional Anti-Patterns and Patterns</h2><p>In the community, vast experience has been collected to improve the stability and usability of Docker and Kubernetes.</p><p>Refer to <a href=https://github.com/gravitational/workshop/blob/master/k8sprod.md>Kubernetes Production Patterns</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ffdcb3c3c73c058191fdf52cbca5e94c>5.5.9 - Namespace Isolation</h1><div class=lead>Deny all traffic from other namespaces</div><h2 id=overview>Overview</h2><p>You can configure a <strong>NetworkPolicy</strong> to deny all the traffic from other namespaces while allowing all the traffic
coming from the same namespace the pod was deployed into.</p><img src=/__resources/howto-namespaceisolation_da4a39.png width=100%><p><strong>There are many reasons why you may chose to employ Kubernetes network policies:</strong></p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other</li></ul><p>Kubernetes <strong>network policies</strong> are application centric compared to infrastructure/network centric standard firewalls.
<strong>There are no explicit CIDRs or IP addresses used</strong> for matching source or destination IP’s.
<strong>Network policies build up on labels and selectors</strong> which are key concepts of Kubernetes that are used to organize
(for e.g all DB tier pods of an app) and select subsets of objects.</p><h2 id=example>Example</h2><p>We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are
unable to get content from <em>namespace1</em> if you are sitting in <em>namespace2</em>.</p><h2 id=setup-the-namespaces>Setup the Namespaces</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create two namespaces for test purpose</span>
</span></span><span style=display:flex><span>kubectl create ns customer1
</span></span><span style=display:flex><span>kubectl create ns customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># create a standard HTTP web server</span>
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># expose the port 80 for external access</span>
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2
</span></span></code></pre></div><hr><h2 id=test-without-np>Test Without NP</h2><img src=/__resources/howto-namespaceisolation-without_9cb2f0.png width=80%><p>Create a pod with <em>curl</em> preinstalled inside the namespace <em>customer1</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create a &#34;bash&#34; pod in one namespace</span>
</span></span><span style=display:flex><span>kubectl run -i --tty client --image=tutum/curl -n=customer1
</span></span></code></pre></div><p>Try to <em>curl</em> the exposed nginx server to get the default index.html page. <strong>Execute this in the bash prompt of the pod created above.</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer1&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer1
</span></span><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer2&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer2
</span></span></code></pre></div><p>Both calls are done in a pod within the namespace <em>customer1</em> and both nginx servers are always reachable, no matter in what namespace.</p><hr><h2 id=test-with-np>Test with NP</h2><img src=/__resources/howto-namespaceisolation-with_2c80b5.png width=80%><p>Install the <strong>NetworkPolicy</strong> from your shell:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deny-from-other-namespaces
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector: {}
</span></span></code></pre></div><ul><li>it applies the policy to ALL pods in the named namespace as the <code>spec.podSelector.matchLabels</code> is empty and therefore selects all pods.</li><li>it allows traffic from ALL pods in the named namespace, as <code>spec.ingress.from.podSelector</code> is empty and therefore selects all pods.</li></ul><pre tabindex=0><code>kubectl apply -f ./network-policy.yaml -n=customer1
kubectl apply -f ./network-policy.yaml -n=customer2
</code></pre><p>After this, <code>curl http://nginx.customer2</code> shouldn&rsquo;t work anymore if you are a service inside the namespace <em>customer1</em> and
vice versa<div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>This policy, once applied, will also disable all external traffic to these pods. For example, you can create a service of type <code>LoadBalancer</code> in namespace <code>customer1</code> that match the nginx pod. When you request the service by its <code>&lt;EXTERNAL_IP>:&lt;PORT></code>, then the network policy that will deny the ingress traffic from the service and the request will time out.</div></p><h2 id=related-links>Related Links</h2><p>You can get more information on how to configure the <strong>NetworkPolicies</strong> at:</p><ul><li><a href=https://docs.projectcalico.org/v3.0/getting-started/kubernetes/tutorials/advanced-policy>Calico WebSite</a></li><li><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>Kubernetes NP Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-90f02665c7456e1a24afc104f91bea70>5.5.10 - Orchestration of Container Startup</h1><div class=lead>How to orchestrate a startup sequence of multiple containers</div><h2 id=disclaimer>Disclaimer</h2><p>If an application depends on other services deployed separately, do not rely on a certain start sequence of containers. Instead,
ensure that the application can cope with unavailability of the services it depends on.</p><h2 id=introduction>Introduction</h2><p>Kubernetes offers a feature called <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>InitContainers</a>
to perform some tasks during a pod&rsquo;s initialization.
In this tutorial, we demonstrate how to use <code>InitContainers</code> in order to orchestrate a starting sequence of multiple containers.
The tutorial uses the example app <a href=https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a>url-shortener</a>,
which consists of two components:</p><ul><li>postgresql database</li><li>webapp which depends on the postgresql database and provides two endpoints: <em>create a short url from a given location</em> and <em>redirect from a given short URL to the corresponding target location</em></li></ul><p>This app represents the minimal example where an application relies on another service or database. In this example,
if the application starts before the database is ready, the application will fail as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl logs webapp-958cf5567-h247n
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=info msg=<span style=color:#a31515>&#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\n&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=fatal msg=<span style=color:#a31515>&#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\n&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         1s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     0         2s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     1         3s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   1         4s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     2         18s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   2         29s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     3         43s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   3         56s
</span></span></code></pre></div><p>If the <code>restartPolicy</code> is set to <code>Always</code> (default) in the yaml file, the application will continue to restart the pod with an exponential back-off delay in case of failure.</p><h2 id=using-initcontaniner>Using InitContaniner</h2><p>To avoid such a situation, <code>InitContainers</code> can be defined, which are executed prior to the application container. If one
of the <code>InitContainers</code> fails, the application container won&rsquo;t be triggered.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: webapp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: webapp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: webapp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:  <span style=color:green># check if DB is ready, and only continue when true</span>
</span></span><span style=display:flex><span>      - name: check-db-ready
</span></span><span style=display:flex><span>        image: postgres:9.6.5
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#39;sh&#39;</span>, <span style=color:#a31515>&#39;-c&#39;</span>,  <span style=color:#a31515>&#39;until pg_isready -h postgres -p 5432;  do echo waiting for database; sleep 2; done;&#39;</span>]
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: xcoulon/go-url-shortener:0.1.0
</span></span><span style=display:flex><span>        name: go-url-shortener
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: POSTGRES_HOST
</span></span><span style=display:flex><span>          value: postgres
</span></span><span style=display:flex><span>        - name: POSTGRES_PORT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;5432&#34;</span>
</span></span><span style=display:flex><span>        - name: POSTGRES_DATABASE
</span></span><span style=display:flex><span>          value: url_shortener_db
</span></span><span style=display:flex><span>        - name: POSTGRES_USER
</span></span><span style=display:flex><span>          value: user
</span></span><span style=display:flex><span>        - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>          value: mysecretpassword
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 8080
</span></span></code></pre></div><p>In the above example, the <code>InitContainers</code> use the docker image <code>postgres:9.6.5</code>, which is different from the application container.
This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.</p><p>With introduction of <code>InitContainers</code>, in case the database is not available yet, the pod startup will look like similarly to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-deployment-5cc79d6bfd-t9n8h   1/1       Running   0          5d
</span></span><span style=display:flex><span>privileged-pod                      1/1       Running   0          4d
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         1s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl  logs webapp-fdcb49cbc-4gs4n
</span></span><span style=display:flex><span>Error from server (BadRequest): container <span style=color:#a31515>&#34;go-url-shortener&#34;</span> in pod <span style=color:#a31515>&#34;webapp-fdcb49cbc-4gs4n&#34;</span> is waiting to start: PodInitializing
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-02cf553d7991e467aa6ce1be8575e57c>5.5.11 - Out-Dated HTML and JS Files Delivered</h1><div class=lead>Why is my application always outdated?</div><h2 id=problem>Problem</h2><p><strong>After updating your HTML and JavaScript sources in your web application, the Kubernetes cluster delivers outdated versions - why?</strong></p><h2 id=overview>Overview</h2><p>By default, Kubernetes service pods are not accessible from the external
network, but only from other pods within the same Kubernetes cluster.</p><p>The Gardener cluster has a built-in configuration for HTTP load balancing called <strong>Ingress</strong>,
defining rules for external connectivity to Kubernetes services. Users who want external access
to their Kubernetes services create an ingress resource that defines rules,
including the URI path, backing service name, and other information. The Ingress controller
can then automatically program a frontend load balancer to enable Ingress configuration.</p><p><img src=/__resources/howto-nginx_821fa6.svg alt=nginx></p><h2 id=example-ingress-configuration>Example Ingress Configuration</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div><p>where:</p><ul><li><strong>&lt;GARDENER-CLUSTER></strong>: The cluster name in the Gardener</li><li><strong>&lt;GARDENER-PROJECT></strong>: You project name in the Gardener</li></ul><h2 id=diagnosing-the-problem>Diagnosing the Problem</h2><p>The ingress controller we are using is <strong>NGINX</strong>. NGINX is a software load balancer, web server, and <strong>content cache</strong> built on top of open
source NGINX.</p><p><strong>NGINX caches the content as specified in the HTTP header.</strong> If the HTTP header is missing,
it is assumed that the cache is <strong>forever</strong> and NGINX never updates the content in the
stupidest case.</p><h2 id=solution>Solution</h2><p>In general, you can avoid this pitfall with one of the solutions below:</p><ul><li>Use a cache buster + HTTP-Cache-Control (prefered)</li><li>Use HTTP-Cache-Control with a lower retention period</li><li>Disable the caching in the ingress (just for dev purposes)</li></ul><p>Learning how to set the HTTP header or setup a cache buster is left to you, as an exercise
for your web framework (e.g. Express/NodeJS, SpringBoot, &mldr;)</p><p>Here is an example on how to disable the cache control for your ingress, done with an annotation in your
ingress YAML (during development).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    ingress.kubernetes.io/cache-enable: <span style=color:#a31515>&#34;false&#34;</span>
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-424dc73fc8ed567a9cf1396d57f3cb2c>5.5.12 - Remove Committed Secrets in Github 💀</h1><div class=lead>Never ever commit a kubeconfig.yaml into github</div><h2 id=overview>Overview</h2><p>If you commit sensitive data, such as a <code>kubeconfig.yaml</code> or <code>SSH key</code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&rsquo;s history you can use the git <code>filter-branch</code> command.</p><p>The git <code>filter-branch</code> command rewrites your repository&rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. <strong>Merging or closing all open pull requests before removing files from your repository is recommended.</strong></p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If someone has already checked out the repository, then of course they have the secret on their computer. So ALWAYS revoke the OAuthToken/Password or whatever it was immediately.</div><h2 id=purging-a-file-from-your-repositorys-history>Purging a File from Your Repository&rsquo;s History</h2><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If you run <code>git filter-branch</code> after stashing changes, you won&rsquo;t be able to retrieve your changes with other
stash commands. Before running <code>git filter-branch</code>, we recommend unstashing any changes you&rsquo;ve made. To unstash the
last set of changes you&rsquo;ve stashed, run <code>git stash show -p | git apply -R</code>. For more information, see <a href=https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning>Git Tools - Stashing and Cleaning</a>.</div><p>To illustrate how <code>git filter-branch</code> works, we&rsquo;ll show you how to remove your file with sensitive data from the
history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.</p><p><strong>1. Navigate into the repository&rsquo;s working directory:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd YOUR-REPOSITORY
</span></span></code></pre></div><p><strong>2. Run the following command, replacing <code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code> with the path to the file you want to remove,
not just its filename.</strong></p><p>These arguments will:</p><ul><li>Force Git to process, but not check out, the entire history of every branch and tag</li><li>Remove the specified file, as well as any empty commits generated as a result</li><li>Overwrite your existing tags</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git filter-branch --force --index-filter <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span><span style=color:#a31515>&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--prune-empty --tag-name-filter cat -- --all
</span></span></code></pre></div><p><strong>3. Add your file with sensitive data to <code>.gitignore</code> to ensure that you don&rsquo;t accidentally commit it again:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> echo <span style=color:#a31515>&#34;YOUR-FILE-WITH-SENSITIVE-DATA&#34;</span> &gt;&gt; .gitignore
</span></span></code></pre></div><p>Double-check that you&rsquo;ve removed everything you wanted to from your repository&rsquo;s history, and that all of your
branches are checked out. Once you&rsquo;re happy with the state of your repository, continue to the next step.</p><p><strong>4. Force-push your local changes to overwrite your GitHub repository, as well as all the branches you&rsquo;ve pushed up:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --all
</span></span></code></pre></div><p><strong>4. In order to remove the sensitive file from your tagged releases, you&rsquo;ll also need to force-push against your Git tags:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --tags
</span></span></code></pre></div><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Tell your collaborators to <strong>rebase, not merge</strong>, any branches they created off of your old (tainted) repository history.
One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.</div><h2 id=related-links>Related Links</h2><ul><li><a href=https://help.github.com/articles/removing-sensitive-data-from-a-repository/>Removing Sensitive Data from a Repository</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-77bb872706e47f8307662d7339184bb5>5.5.13 - Using Prometheus and Grafana to Monitor K8s</h1><div class=lead>How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics</div><h2 id=disclaimer>Disclaimer</h2><p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both
applications offer a wide range of flexibility, which needs to be considered in case you have specific requirements.
Such advanced details are not in the scope of this topic.</p><h2 id=introduction>Introduction</h2><p><a href=https://prometheus.io/>Prometheus</a> is an open-source systems monitoring and alerting toolkit for recording numeric
time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented
architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a
particular strength.</p><p>Prometheus is the second hosted project to <a href=https://prometheus.io/blog/2018/08/09/prometheus-graduates-within-cncf/>graduate within CNCF</a>.</p><p>The following characteristics make Prometheus a good match for monitoring Kubernetes clusters:</p><ul><li><p>Pull-based Monitoring<br>Prometheus is a <a href=https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/>pull-based</a> monitoring system,
which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.</p></li><li><p>Labels
Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.<br>Labels are used to identify time series and sets of label matchers can be used in the query language
(<a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a>) to select the time series to be aggregated.</p></li><li><p>Exporters<br>There are many <a href=https://prometheus.io/docs/instrumenting/exporters/>exporters</a> available, which enable integration of
databases or even other monitoring systems not already providing a way to export metrics to Prometheus.
One prominent exporter is the so called <a href=https://github.com/prometheus/node_exporter>node-exporter</a>, which allows to
monitor hardware and OS related metrics of Unix systems.</p></li><li><p>Powerful Query Language<br>The Prometheus query language <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> lets the user
select and aggregate time series data in real time. Results can either be shown as a graph, viewed
as tabular data in the Prometheus expression browser, or consumed by external systems via the <a href=https://prometheus.io/docs/prometheus/latest/querying/api/>HTTP API</a>.</p></li></ul><p>Find query examples on <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>.</p><p>One very popular open-source visualization tool not only for Prometheus is <a href=https://grafana.com>Grafana</a>. Grafana is a
metric analytics and visualization suite. It is popular for visualizing time series data for infrastructure
and application analytics but many use it in other domains including industrial sensors, home automation, weather, and
process control. For more information, see the <a href=http://docs.grafana.org/>Grafana Documentation</a>.</p><p>Grafana accesses data via <a href=https://grafana.com/docs/grafana/latest/basics/>Data Sources</a>. The continuously growing
list of supported backends includes Prometheus.</p><p>Dashboards are created by combining panels, e.g. <a href=http://docs.grafana.org/reference/graph/>Graph</a> and <a href=http://docs.grafana.org/reference/dashlist/>Dashlist</a>.</p><p>In this example, we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring
configuration as the one provided for Kubernetes clusters created by Gardener.</p><p>If you miss elements on the Prometheus web page when accessing it via its service URL <code>https://&lt;your K8s FQN>/api/v1/namespaces/&lt;your-prometheus-namespace>/services/prometheus-prometheus-server:80/proxy</code>,
this is probably caused by a Prometheus issue - <a href=https://github.com/prometheus/prometheus/issues/1583>#1583</a>
To workaround this issue, setup a port forward <code>kubectl port-forward -n &lt;your-prometheus-namespace> &lt;prometheus-pod> 9090:9090</code>
on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant
in case you use the service type <code>LoadBalancer</code>.</p><h2 id=preparation>Preparation</h2><p>The deployment of <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> is based on Helm charts.<br>Make sure to implement the <a href=/docs/guides/client_tools/helm/>Helm settings</a> before deploying the Helm charts.</p><p>The Kubernetes clusters provided by <a href=https://github.com/gardener>Gardener</a> use role based
access control (<a href=https://kubernetes.io/docs/admin/authorization/rbac/>RBAC</a>). To authorize the Prometheus
node-exporter to access hardware and OS relevant metrics of your cluster&rsquo;s worker nodes, specific artifacts need to be
deployed.</p><p>Bind the Prometheus service account to the <code>garden.sapcloud.io:monitoring:prometheus</code> cluster role by running the command
<code>kubectl apply -f crbinding.yaml</code>.</p><p>Content of <code>crbinding.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: garden.sapcloud.io:monitoring:prometheus
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- kind: ServiceAccount
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>  namespace: &lt;your-prometheus-namespace&gt;
</span></span></code></pre></div><h2 id=deployment-of-prometheus-and-grafana>Deployment of Prometheus and Grafana</h2><p>Only minor changes are needed to deploy <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a>
and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> based on Helm charts.</p><p>Copy the following configuration into a file called <code>values.yaml</code> and deploy Prometheus:
<code>helm install &lt;your-prometheus-name> --namespace &lt;your-prometheus-namespace> stable/prometheus -f values.yaml</code></p><p>Typically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this, so feel
free to choose different namespaces.</p><p>Content of <code>values.yaml</code> for Prometheus:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>rbac:
</span></span><span style=display:flex><span>  create: <span style=color:#00f>false</span> <span style=color:green># Already created in Preparation step</span>
</span></span><span style=display:flex><span>nodeExporter:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span> <span style=color:green># The node-exporter is already deployed by default</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  global:
</span></span><span style=display:flex><span>    scrape_interval: 30s
</span></span><span style=display:flex><span>    scrape_timeout: 30s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>serverFiles:
</span></span><span style=display:flex><span>  prometheus.yml:
</span></span><span style=display:flex><span>    rule_files:
</span></span><span style=display:flex><span>      - /etc/config/rules
</span></span><span style=display:flex><span>      - /etc/config/alerts      
</span></span><span style=display:flex><span>    scrape_configs:
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet-cadvisor&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics/cadvisor
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for probing services via the Blackbox Exporter.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/probe`: Only probe services that have a value of `true`</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-services&#39;</span>
</span></span><span style=display:flex><span>      metrics_path: /probe
</span></span><span style=display:flex><span>      params:
</span></span><span style=display:flex><span>        module: [http_2xx]
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: service
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__address__]
</span></span><span style=display:flex><span>          target_label: __param_target
</span></span><span style=display:flex><span>        - target_label: __address__
</span></span><span style=display:flex><span>          replacement: blackbox
</span></span><span style=display:flex><span>        - source_labels: [__param_target]
</span></span><span style=display:flex><span>          target_label: instance
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          target_label: kubernetes_name
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for pods</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape pods that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-pods&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: pod
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          regex: (.+):(?:\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: ${1}:${2}
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_pod_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_pod_name
</span></span><span style=display:flex><span>    <span style=color:green># Scrape config for service endpoints.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># The relabeling allows the actual service scrape endpoint to be configured</span>
</span></span><span style=display:flex><span>    <span style=color:green># via the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape services that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need</span>
</span></span><span style=display:flex><span>    <span style=color:green># to set this to `https` &amp; most likely set the `tls_config` of the scrape config.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: If the metrics are exposed on a different port to the</span>
</span></span><span style=display:flex><span>    <span style=color:green># service then set this appropriately.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-service-endpoints&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: endpoints
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __scheme__
</span></span><span style=display:flex><span>          regex: (https?)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>          regex: (.+)(?::\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: $1:$2
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_name <span style=color:green># Add your additional configuration here...</span>
</span></span></code></pre></div><p>Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set
explicitly in case the default changed.
Deploy Grafana via <code>helm install grafana --namespace &lt;your-prometheus-namespace> stable/grafana -f values.yaml</code>. Here, the same namespace is chosen for Prometheus and for Grafana.</p><p>Content of <code>values.yaml</code> for Grafana:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  service:
</span></span><span style=display:flex><span>    type: ClusterIP
</span></span></code></pre></div><p>Check the running state of the pods on the Kubernetes Dashboard or by running <code>kubectl get pods -n &lt;your-prometheus-namespace></code>.
In case of errors, check the log files of the pod(s) in question.</p><p>The text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user
and password of the Grafana Admin user. The credentials are stored as secrets in the namespace <code>&lt;your-prometheus-namespace></code>
and could be decoded via <code>kubectl get secret --namespace &lt;my-grafana-namespace> grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo</code>.</p><h2 id=basic-functional-tests>Basic Functional Tests</h2><p>To access the web UI of both applications, use port forwarding of port 9090.</p><p>Setup port forwarding for port 9090:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward -n &lt;your-prometheus-namespace&gt; &lt;your-prometheus-server-pod&gt; 9090:9090
</span></span></code></pre></div><p>Open <code>http://localhost:9090</code> in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>):</p><pre tabindex=0><code>100 * (1 - avg by(instance)(irate(node_cpu{mode=&#39;idle&#39;}[5m])))
</code></pre><p>This should show some data in a graph.</p><p>To show the same data in Grafana setup port forwarding for port 3000 for the
Grafana pod and open the Grafana Web UI by opening <code>http://localhost:3000</code> in a browser.
Enter the credentials of the admin user.</p><p>Next, you need to enter the server name of your Prometheus deployment. This name is shown directly after the
installation via helm.</p><p>Run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm status &lt;your-prometheus-name&gt;
</span></span></code></pre></div><p>to find this name. Below, this server name is referenced by <code>&lt;your-prometheus-server-name></code>.</p><p>First, you need to add your Prometheus server as data source:</p><ol><li>Select <em>Dashboards → Data Sources</em></li><li>Select <em>Add data source</em></li><li>Enter
<em>Name</em>: <code>&lt;your-prometheus-datasource-name></code><br><em>Type</em>: Prometheus<br><em>URL</em>: <code>http://&lt;your-prometheus-server-name></code><br><em>Access</em>: <code>proxy</code></li><li>Select <em>Save & Test</em></li></ol><p>In case of failure, check the Prometheus URL in the Kubernetes Dashboard.</p><p>To add a Graph follow these steps:</p><ol><li>In the left corner, select <em>Dashboards → New</em> to create a new dashboard</li><li>Select <em>Graph</em> to create a new graph</li><li>Next, select the <em>Panel Title → Edit</em></li><li>Select your Prometheus Data Source in the drop down list</li><li>Enter the expression <code>100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))</code> in the entry field A</li><li>Select the floppy disk symbol (Save) on top</li></ol><p>Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.</p><p>As a next step you can implement monitoring for your applications by implementing the <a href=https://prometheus.io/docs/instrumenting/clientlibs/>Prometheus client API</a>.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://prometheus.io/>Prometheus</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus Helm Chart</a></li><li><a href=https://www.weave.works/blog/prometheus-kubernetes-perfect-match/>Prometheus and Kubernetes: A Perfect Match</a></li><li><a href=https://grafana.com>Grafana</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana Helm Chart</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-68ec2370d0409cc27325be36693f9368>6 - Tutorials</h1></div><div class=td-content><h1 id=pg-c29db379d9a38d027354d38e2e735f08>6.1 - Authenticating with an Identity Provider</h1><div class=lead>Use OpenID Connect to authenticate users to access shoot clusters</div><h2 id=prerequisites>Prerequisites</h2><p>Please read the following background material on <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>Authenticating</a>.</p><h2 id=overview>Overview</h2><p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:</p><ol><li><a href=#configure-an-identity-provider>Configure an Identity Provider</a> using <strong>OpenID Connect</strong> (OIDC).</li><li><a href=#configure-a-local-kubectl-oidc-login>Configure a local kubectl oidc-login</a> to enable <code>oidc-login</code>.</li><li><a href=#configure-the-shoot-cluster>Configure the shoot cluster</a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.</li><li><a href=#authorize-an-authenticated-user>Authorize an authenticated user</a> using role-based access control (RBAC).</li><li><a href=#verify-the-result>Verify the result</a></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.</div><h2 id=configure-an-identity-provider>Configure an Identity Provider</h2><p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use <em>Auth0</em>, which has a free plan.</p><ol><li><p>In your tenant, create a client application to use authentication with <code>kubectl</code>:</p><p><img src=/__resources/Create-client-application_c47ce0.png alt="Create client application"></p></li><li><p>Provide a <em>Name</em>, choose <em>Native</em> as application type, and choose <em>CREATE</em>.</p><p><img src=/__resources/Choose-application-type_392e78.png alt="Choose application type"></p></li><li><p>In the tab <em>Settings</em>, copy the following parameters to a local text file:</p><ul><li><p><em>Domain</em></p><p>Corresponds to the <strong>issuer</strong> in OIDC. It must be an <code>https</code>-secured endpoint (Auth0 requires a trailing <code>/</code> at the end). For more information, see <a href=https://openid.net/specs/openid-connect-core-1_0.html#Terminology>Issuer Identifier</a>.</p></li><li><p><em>Client ID</em></p></li><li><p><em>Client Secret</em></p><p><img src=/__resources/Basic-information_2f952f.png alt="Basic information"></p></li></ul></li><li><p>Configure the client to have a callback url of <code>http://localhost:8000</code>. This callback connects to your local <code>kubectl oidc-login</code> plugin:</p><p><img src=/__resources/Configure-callback_1a247f.png alt="Configure callback"></p></li><li><p>Save your changes.</p></li><li><p>Verify that <code>https://&lt;Auth0 Domain>/.well-known/openid-configuration</code> is reachable.</p></li><li><p>Choose <em>Users & Roles</em> > <em>Users</em> > <em>CREATE USERS</em> to create a user with a user and password:</p><p><img src=/__resources/Create-user_97a940.png alt="Create user"></p></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Users must have a <em>verified</em> email address.</div><h2 id=configure-a-local-kubectl-oidc-login>Configure a Local <code>kubectl</code> <code>oidc-login</code></h2><ol><li><p>Install the <code>kubectl</code> plugin <a href=https://github.com/int128/kubelogin>oidc-login</a>. We highly recommend the <a href=https://github.com/kubernetes-sigs/krew>krew</a> installation tool, which also makes other plugins easily available.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl krew install oidc-login
</span></span></code></pre></div><p>The response looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Updated the local copy of plugin index.
</span></span><span style=display:flex><span>Installing plugin: oidc-login
</span></span><span style=display:flex><span>CAVEATS:
</span></span><span style=display:flex><span>\
</span></span><span style=display:flex><span>|  You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
</span></span><span style=display:flex><span>|  See https://github.com/int128/kubelogin for more.
</span></span><span style=display:flex><span>/
</span></span><span style=display:flex><span>Installed plugin: oidc-login
</span></span></code></pre></div></li><li><p>Prepare a <code>kubeconfig</code> for later use:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cp ~/.kube/config ~/.kube/config-oidc
</span></span></code></pre></div></li><li><p>Modify the configuration of <code>~/.kube/config-oidc</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: shoot--project--mycluster
</span></span><span style=display:flex><span>    user: my-oidc
</span></span><span style=display:flex><span>  name: shoot--project--mycluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: my-oidc
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    exec:
</span></span><span style=display:flex><span>      apiVersion: client.authentication.k8s.io/v1beta1
</span></span><span style=display:flex><span>      command: kubectl
</span></span><span style=display:flex><span>      args:
</span></span><span style=display:flex><span>      - oidc-login
</span></span><span style=display:flex><span>      - get-token
</span></span><span style=display:flex><span>      - --oidc-issuer-url=https://&lt;Issuer&gt;/ 
</span></span><span style=display:flex><span>      - --oidc-client-id=&lt;Client ID&gt;
</span></span><span style=display:flex><span>      - --oidc-client-secret=&lt;Client Secret&gt;
</span></span><span style=display:flex><span>      - --oidc-extra-scope=email,offline_access,profile
</span></span></code></pre></div></li></ol><p>To test our OIDC-based authentication, the context <code>shoot--project--mycluster</code> of <code>~/.kube/config-oidc</code> is used in a later step. For now, continue to use the configuration <code>~/.kube/config</code> with administration rights for your cluster.</p><h2 id=configure-the-shoot-cluster>Configure the Shoot Cluster</h2><p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: garden.sapcloud.io/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: mycluster
</span></span><span style=display:flex><span>  namespace: garden-project
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientID: &lt;Client ID&gt;
</span></span><span style=display:flex><span>        issuerURL: <span style=color:#a31515>&#34;https://&lt;Issuer&gt;/&#34;</span>
</span></span><span style=display:flex><span>        usernameClaim: email
</span></span></code></pre></div><p>This change of the <code>Shoot</code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It <strong>doesn&rsquo;t</strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.</p><h2 id=authorize-an-authenticated-user>Authorize an Authenticated User</h2><p>In Auth0, you created a user with a verified email address, <code>test@test.com</code> in our example. For simplicity, we authorize a single user identified by this email address with the cluster role <code>view</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: viewer-test
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: view
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: User
</span></span><span style=display:flex><span>  name: test@test.com
</span></span></code></pre></div><p>As administrator, apply the cluster role binding in your shoot cluster.</p><h2 id=verify-the-result>Verify the Result</h2><ol><li><p>To step into the shoes of your user, use the prepared <code>kubeconfig</code> file <code>~/.kube/config-oidc</code>, and switch to the context that uses <code>oidc-login</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cd ~/.kube
</span></span><span style=display:flex><span>export KUBECONFIG=$(pwd)/config-oidc
</span></span><span style=display:flex><span>kubectl config use-context `shoot--project--mycluster`
</span></span></code></pre></div></li><li><p><code>kubectl</code> delegates the authentication to plugin <code>oidc-login</code> the first time the user uses <code>kubectl</code> to contact the API server, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get all
</span></span></code></pre></div><p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.</p></li><li><p>Enter your login credentials.</p><p><img src=/__resources/Login-through-identity-provider_54293b.png alt="Login through identity provider"></p><p>You should get a successful response from the API server:</p><pre tabindex=0><code>Opening in existing browser session.
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   100.64.0.1   &lt;none&gt;        443/TCP   86m
</code></pre></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>After a successful login, <code>kubectl</code> uses a token for authentication so that you don’t have to provide user and password for every new <code>kubectl</code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin <code>oidc-login</code>:</p><ol><li>Delete directory <code>~/.kube/cache/oidc-login</code>.</li><li>Delete the browser cache.</li></ol></div><ol><li><p>To see if your user uses the cluster role <code>view</code>, do some checks with <code>kubectl auth can-i</code>.</p><ul><li><p>The response for the following commands should be <code>no</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i create clusterrolebindings
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i get secrets
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i describe secrets
</span></span></code></pre></div></li><li><p>The response for the following commands should be <code>yes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i list pods
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i get pods
</span></span></code></pre></div></li></ul></li></ol><p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://auth0.com/pricing/>Auth0 Pricing</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7e234a5492bf6ac145bc63720536b8c8>6.2 - Dynamic Volume Provisioning</h1><div class=lead>Running a Postgres database on Kubernetes</div><h2 id=overview>Overview</h2><p>The example shows how to run a Postgres database on Kubernetes and how to dynamically provision and mount the storage
volumes needed by the database</p><h2 id=run-postgres-database>Run Postgres Database</h2><p>Define the following Kubernetes resources in a yaml file:</p><ul><li>PersistentVolumeClaim (PVC)</li><li>Deployment</li></ul><h3 id=persistentvolumeclaim>PersistentVolumeClaim</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: PersistentVolumeClaim
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgresdb-pvc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  accessModes:
</span></span><span style=display:flex><span>    - ReadWriteOnce
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    requests:
</span></span><span style=display:flex><span>      storage: 9Gi
</span></span><span style=display:flex><span>  storageClassName: <span style=color:#a31515>&#39;default&#39;</span>
</span></span></code></pre></div><p>This defines a PVC using the storage class <code>default</code>. Storage classes abstract from the underlying storage provider as well
as other parameters, like disk-type (e.g.; solid-state vs standard disks).</p><p>The default storage class has the annotation <strong>{&ldquo;storageclass.kubernetes.io/is-default-class&rdquo;:&ldquo;true&rdquo;}</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl describe sc default
</span></span><span style=display:flex><span>Name:            default
</span></span><span style=display:flex><span>IsDefaultClass:  Yes
</span></span><span style=display:flex><span>Annotations:     kubectl.kubernetes.io/last-applied-configuration={<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;storage.k8s.io/v1beta1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;StorageClass&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{<span style=color:#a31515>&#34;storageclass.kubernetes.io/is-default-class&#34;</span>:<span style=color:#a31515>&#34;true&#34;</span>},<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;addonmanager.kubernetes.io/mode&#34;</span>:<span style=color:#a31515>&#34;Exists&#34;</span>},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;default&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;parameters&#34;</span>:{<span style=color:#a31515>&#34;type&#34;</span>:<span style=color:#a31515>&#34;gp2&#34;</span>},<span style=color:#a31515>&#34;provisioner&#34;</span>:<span style=color:#a31515>&#34;kubernetes.io/aws-ebs&#34;</span>}
</span></span><span style=display:flex><span>,storageclass.kubernetes.io/is-default-class=true
</span></span><span style=display:flex><span>Provisioner:           kubernetes.io/aws-ebs
</span></span><span style=display:flex><span>Parameters:            type=gp2
</span></span><span style=display:flex><span>AllowVolumeExpansion:  &lt;unset&gt;
</span></span><span style=display:flex><span>MountOptions:          &lt;none&gt;
</span></span><span style=display:flex><span>ReclaimPolicy:         Delete
</span></span><span style=display:flex><span>VolumeBindingMode:     Immediate
</span></span><span style=display:flex><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>A Persistent Volume is automatically created when it is dynamically provisioned. In the following example, the PVC is defined
as &ldquo;postgresdb-pvc&rdquo;, and a corresponding PV &ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&rdquo; is created and associated with the PVC automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create -f .<span style=color:#a31515>\p</span>ostgres_deployment.yaml
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  3s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        8s
</span></span></code></pre></div><p>Notice that the <strong>RECLAIM POLICY</strong> is <strong>Delete</strong> (default value), which is one of the two reclaim policies, the other
one is <strong>Retain</strong>. (A third policy <strong>Recycle</strong> has been deprecated). In the case of <strong>Delete</strong>, the PV is deleted automatically
when the PVC is removed, and the data on the PVC will also be lost.</p><p>On the other hand, a PV with <strong>Retain</strong> policy will not be deleted when the PVC is removed, and moved to <strong>Release</strong> status, so
that data can be recovered by Administrators later.</p><p>You can use the <code>kubectl patch</code> command to change the reclaim policy as described in <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/>Change the Reclaim Policy of a PersistentVolume</a>
or use <code>kubectl edit pv &lt;pv-name></code> to edit it online as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  44m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># change the reclaim policy from &#34;Delete&#34; to &#34;Retain&#34;</span>
</span></span><span style=display:flex><span>$ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb
</span></span><span style=display:flex><span>persistentvolume <span style=color:#a31515>&#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&#34;</span> edited
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># check the reclaim policy afterwards</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  45m
</span></span></code></pre></div><h3 id=deployment>Deployment</h3><p>Once a PVC is created, you can use it in your container via <code>volumes.persistentVolumeClaim.claimName</code>. In the below
example, the PVC <strong>postgresdb-pvc</strong> is mounted as readable and writable, and in <code>volumeMounts</code> two paths in the container are mounted to subfolders in the volume.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgres
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: postgres
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: postgres
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      name: postgres
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: postgres
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: postgres
</span></span><span style=display:flex><span>          image: <span style=color:#a31515>&#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto&#34;</span>
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>            - name: POSTGRES_USER
</span></span><span style=display:flex><span>              value: postgres
</span></span><span style=display:flex><span>            - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>              value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ
</span></span><span style=display:flex><span>            - name: POSTGRES_INITDB_XLOGDIR
</span></span><span style=display:flex><span>              value: <span style=color:#a31515>&#34;/var/log/postgresql/logs&#34;</span>
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 5432
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>            - mountPath: /var/lib/postgresql/data
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: data     <span style=color:green># https://github.com/kubernetes/website/pull/2292.  Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)</span>
</span></span><span style=display:flex><span>            - mountPath: /var/log/postgresql/logs
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: logs
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>        - name: postgre-db
</span></span><span style=display:flex><span>          persistentVolumeClaim:
</span></span><span style=display:flex><span>            claimName: postgresdb-pvc
</span></span><span style=display:flex><span>            readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      imagePullSecrets:
</span></span><span style=display:flex><span>      - name: cpettechregistry
</span></span></code></pre></div><p>To check the mount points in the container:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po
</span></span><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>postgres-7f485fd768-c5jf9   1/1       Running   0          32m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it postgres-7f485fd768-c5jf9 bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/
</span></span><span style=display:flex><span>base    pg_clog       pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    PG_VERSION  postgresql.auto.conf  postmaster.opts
</span></span><span style=display:flex><span>global  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xlog     postgresql.conf       postmaster.pid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/
</span></span><span style=display:flex><span>000000010000000000000001  archive_status
</span></span></code></pre></div><h2 id=deleting-a-persistentvolumeclaim>Deleting a PersistentVolumeClaim</h2><p>In case of a <strong>Delete</strong> policy, deleting a PVC will also delete its associated PV. If <strong>Retain</strong> is the reclaim policy, the
PV will change status from <strong>Bound</strong> to <strong>Released</strong> when the PVC is deleted.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Check pvc and pv before deletion</span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># delete pvc</span>
</span></span><span style=display:flex><span>$ kubectl delete pvc postgresdb-pvc
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> deleted
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># pv changed to status &#34;Released&#34;</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Released   default/postgresdb-pvc   default                  51m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d100c12a1d1b23c6d674069394a6bcf2>6.3 - GPU Enabled Cluster</h1><div class=lead>Setting up a GPU Enabled Cluster for Deep Learning</div><h2 id=disclaimer>Disclaimer</h2><p>Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular,
are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason,
<strong>contributions are highly appreciated</strong> to update this guide.</p><h2 id=create-a-cluster>Create a Cluster</h2><p>First thing first, let’s create a Kubernetes (K8s) cluster with GPU accelerated nodes. In this example we will use an AWS
<strong>p2.xlarge</strong> EC2 instance because it&rsquo;s the cheapest available option at the moment. Use such cheap instances
for learning to limit your resource costs. <strong>This costs around 1€/hour per GPU</strong></p><p><img src=/__resources/howto-gpu_88d839.png alt=gpu-selection></p><h2 id=install-nvidia-driver-as-daemonset>Install NVidia Driver as Daemonset</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-driver-installer
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      name: nvidia-driver-installer
</span></span><span style=display:flex><span>      k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        name: nvidia-driver-installer
</span></span><span style=display:flex><span>        k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972
</span></span><span style=display:flex><span>        name: modulus
</span></span><span style=display:flex><span>        args:
</span></span><span style=display:flex><span>        - compile
</span></span><span style=display:flex><span>        - nvidia
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;410.104&#34;</span>
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: MODULUS_CHROOT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL_DIR
</span></span><span style=display:flex><span>          value: /opt/drivers
</span></span><span style=display:flex><span>        - name: MODULUS_CACHE_DIR
</span></span><span style=display:flex><span>          value: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: MODULUS_LD_ROOT
</span></span><span style=display:flex><span>          value: /root
</span></span><span style=display:flex><span>        - name: IGNORE_MISSING_MODULE_SYMVERS
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;1&#34;</span>          
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: etc-coreos
</span></span><span style=display:flex><span>          mountPath: /etc/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: usr-share-coreos
</span></span><span style=display:flex><span>          mountPath: /usr/share/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: ld-root
</span></span><span style=display:flex><span>          mountPath: /root
</span></span><span style=display:flex><span>        - name: module-cache
</span></span><span style=display:flex><span>          mountPath: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: module-install-dir-base
</span></span><span style=display:flex><span>          mountPath: /opt/drivers
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;gcr.io/google-containers/pause:3.1&#34;</span>
</span></span><span style=display:flex><span>        name: pause
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: etc-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /etc/coreos
</span></span><span style=display:flex><span>      - name: usr-share-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /usr/share/coreos
</span></span><span style=display:flex><span>      - name: ld-root
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /
</span></span><span style=display:flex><span>      - name: module-cache
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/modulus/cache
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      - name: module-install-dir-base
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/drivers
</span></span></code></pre></div><h2 id=install-device-plugin>Install Device Plugin</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>    <span style=color:green>#addonmanager.kubernetes.io/mode: Reconcile</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        scheduler.alpha.kubernetes.io/critical-pod: <span style=color:#a31515>&#39;&#39;</span>
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      priorityClassName: system-node-critical
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: device-plugin
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/device-plugins
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d&#34;</span>
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/usr/bin/nvidia-gpu-device-plugin&#34;</span>, <span style=color:#a31515>&#34;-logtostderr&#34;</span>, <span style=color:#a31515>&#34;-host-path=/opt/drivers/nvidia&#34;</span>]
</span></span><span style=display:flex><span>        name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: device-plugin
</span></span><span style=display:flex><span>          mountPath: /device-plugin
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>  updateStrategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span></code></pre></div><h2 id=test>Test</h2><p>To run an example training on a GPU node, first start a base image with Tensorflow with GPU support & Keras:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deeplearning-workbench
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: deeplearning-workbench
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: deeplearning-workbench
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: deeplearning-workbench
</span></span><span style=display:flex><span>        image: afritzler/deeplearning-workbench
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            nvidia.com/gpu: 1
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span></code></pre></div><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>the <code>tolerations</code> section above is not required if you deploy the <code>ExtendedResourceToleration</code>
admission controller to your cluster. You can do this in the <code>kubernetes</code> section of your Gardener
cluster <code>shoot.yaml</code> as follows:</p><pre tabindex=0><code>  kubernetes:
    kubeAPIServer:
      admissionPlugins:
      - name: ExtendedResourceToleration
</code></pre></div><p>Now exec into the container and start an example Keras training:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash
</span></span><span style=display:flex><span>cd /keras/example
</span></span><span style=display:flex><span>python imdb_cnn.py
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=https://github.com/afritzler/kubernetes-gpu>Andreas Fritzler</a> from the Gardener Core team for the R&D, who has provided this setup.</li><li><a href=https://github.com/squat/modulus>Build and install NVIDIA driver on CoreOS</a></li><li><a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml>Nvidia Device Plugin</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-82db4b0a88ec43d5df912dae1cd481bf>6.4 - Install Knative in Gardener Clusters</h1><div class=lead>A walkthrough the steps for installing Knative in Gardener shoot clusters.</div><h2 id=overview>Overview</h2><p>This guide walks you through the installation of the latest version of Knative
using pre-built images on a <a href=https://gardener.cloud>Gardener</a> created cluster
environment. To set up your own Gardener, see the
<a href=https://github.com/gardener/gardener/blob/master/docs/README.md>documentation</a>
or have a look at the
<a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a>
project. To learn more about this open source project, read the
<a href=https://kubernetes.io/blog/2018/05/17/gardener/>blog on kubernetes.io</a>.</p><h2 id=prerequsites>Prerequsites</h2><p>Knative requires a Kubernetes cluster v1.15 or newer.</p><h2 id=steps>Steps</h2><h3 id=install-and-configure-kubectl>Install and Configure kubectl</h3><ol><li><p>If you already have <code>kubectl</code> CLI, run <code>kubectl version --short</code> to check
the version. You need v1.10 or newer. If your <code>kubectl</code> is older, follow the
next step to install a newer version.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>Install the kubectl CLI</a>.</p></li></ol><h3 id=access-gardener>Access Gardener</h3><ol><li><p>Create a project in the Gardener dashboard. This will essentially create a
Kubernetes namespace with the name <code>garden-&lt;my-project></code>.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#configure-kubectl>Configure access to your Gardener project</a>
using a kubeconfig.</p><p>If you are not the Gardener Administrator already, you
can create a technical user in the Gardener dashboard.
Go to the &ldquo;Members&rdquo; section and add a service account.
You can then download the kubeconfig for your project.
You can skip this step if you create your cluster using the
user interface; it is only needed for programmatic access, make sure you set
<code>export KUBECONFIG=garden-my-project.yaml</code> in your shell.
<img src=/__resources/gardener_service_account_0a4a8a.png alt="Download kubeconfig for Gardener"></p></li></ol><h3 id=creating-a-kubernetes-cluster>Creating a Kubernetes Cluster</h3><p>You can create your cluster using <code>kubectl</code> CLI by providing a cluster
specification yaml file. You can find an example for GCP in the
<a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>gardener/gardener repository</a>.
Make sure the namespace matches that of your project. Then just apply the
prepared so-called &ldquo;shoot&rdquo; cluster CRD with kubectl:</p><pre tabindex=0><code>kubectl apply --filename my-cluster.yaml
</code></pre><p>The easier alternative is to create the cluster following the cluster creation
wizard in the Gardener dashboard:
<img src=/__resources/gardener_shoot_creation_49a4ca.png alt="shoot creation" title="shoot creation via the dashboard"></p><h3 id=configure-kubectl-for-your-cluster>Configure kubectl for Your Cluster</h3><p>You can now download the kubeconfig for your freshly created cluster in the
Gardener dashboard or via the CLI as follows:</p><pre tabindex=0><code>kubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode &gt; my-cluster.yaml
</code></pre><p>This kubeconfig file has full administrators access to you cluster. For the rest
of this guide, be sure you have <code>export KUBECONFIG=my-cluster.yaml</code> set.</p><h2 id=installing-istio>Installing Istio</h2><p>Knative depends on Istio. If your cloud platform offers a managed Istio
installation, we recommend installing Istio that way, unless you need the
ability to customize your installation.</p><p>Otherwise, see the <a href=https://knative.dev/docs/install/installing-istio/>Installing Istio for Knative guide</a>
to install Istio.</p><p>You must install Istio on your Kubernetes cluster before continuing with these
instructions to install Knative.</p><h2 id=installing-cluster-local-gateway-for-serving-cluster-internal-traffic>Installing <code>cluster-local-gateway</code> for Serving Cluster-Internal Traffic</h2><p>If you installed Istio, you can install a <code>cluster-local-gateway</code> within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, <a href=https://knative.dev/docs/admin/install/knative-offerings/>install and use the <code>cluster-local-gateway</code></a>.</p><h2 id=installing-knative>Installing Knative</h2><p>The following commands install all available Knative components as well as the
standard set of observability plugins. Knative&rsquo;s installation guide - <a href=https://knative.dev/docs/admin/install/>Installing Knative</a>.</p><ol><li><p>If you are upgrading from Knative 0.3.x: Update your domain and static IP
address to be associated with the LoadBalancer <code>istio-ingressgateway</code> instead
of <code>knative-ingressgateway</code>. Then run the following to clean up leftover
resources:</p><pre tabindex=0><code>kubectl delete svc knative-ingressgateway -n istio-system
kubectl delete deploy knative-ingressgateway -n istio-system
</code></pre><p>If you have the Knative Eventing Sources component installed, you will also
need to delete the following resource before upgrading:</p><pre tabindex=0><code>kubectl delete statefulset/controller-manager -n knative-sources
</code></pre><p>While the deletion of this resource during the upgrade process will not
prevent modifications to Eventing Source resources, those changes will not be
completed until the upgrade process finishes.</p></li><li><p>To install Knative, first install the CRDs by running the <code>kubectl apply</code>
command once with the <code>-l knative.dev/crd-install=true</code> flag. This prevents
race conditions during the install, which cause intermittent errors:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --selector knative.dev/crd-install=true <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>To complete the installation of Knative and its dependencies, run the
<code>kubectl apply</code> command again, this time without the <code>--selector</code> flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>Monitor the Knative components until all of the components show a <code>STATUS</code> of
<code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --namespace knative-serving
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-eventing
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-monitoring
</span></span></code></pre></div></li></ol><h2 id=set-your-custom-domain>Set Your Custom Domain</h2><ol><li>Fetch the external IP or CNAME of the knative-ingressgateway:</li></ol><pre tabindex=0><code>kubectl --namespace istio-system get service knative-ingressgateway
NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                      AGE
knative-ingressgateway   LoadBalancer   100.70.219.81   35.233.41.212   80:32380/TCP,443:32390/TCP,32400:32400/TCP   4d
</code></pre><ol start=2><li>Create a wildcard DNS entry in your custom domain to point to the above IP or
CNAME:</li></ol><pre tabindex=0><code>*.knative.&lt;my domain&gt; == A 35.233.41.212
# or CNAME if you are on AWS
*.knative.&lt;my domain&gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com
</code></pre><ol start=3><li>Adapt your Knative config-domain (set your domain in the data field):</li></ol><pre tabindex=0><code>kubectl --namespace knative-serving get configmaps config-domain --output yaml
apiVersion: v1
data:
  knative.&lt;my domain&gt;: &#34;&#34;
kind: ConfigMap
  name: config-domain
  namespace: knative-serving
</code></pre><h2 id=whats-next>What&rsquo;s Next</h2><p>Now that your cluster has Knative installed, you can see what Knative has to
offer.</p><p>Deploy your first app with the
<a href=https://knative.dev/docs/serving/getting-started-knative-app/>Getting Started with Knative App Deployment</a>
guide.</p><p>Get started with Knative Eventing by walking through one of the
<a href=https://knative.dev/docs/eventing/samples/>Eventing Samples</a>.</p><p><a href=https://knative.dev/docs/serving/installing-cert-manager/>Install Cert-Manager</a> if you want to use the
<a href=https://knative.dev/docs/serving/using-auto-tls/>automatic TLS cert provisioning feature</a>.</p><h2 id=cleaning-up>Cleaning Up</h2><p>Use the Gardener dashboard to delete your cluster, or execute the following with
kubectl pointing to your <code>garden-my-project.yaml</code> kubeconfig:</p><pre tabindex=0><code>kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true

kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-4588103fb9a692533044ee756a5f863f>7 - FAQ</h1><div class=lead>Commonly asked questions about Gardener</div></div><div class=td-content><h1 id=pg-944cb7b3f3bbe4034568521d98517fee>7.1 - Can I run privileged containers?</h1><p>While it is possible, we highly recommend not to use privileged containers in your productive environment.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d8911a62f1dfeafd561419bd66eb94e4>7.2 - Can Kubernetes upgrade automatically?</h1><p>There is no automatic migration of major/minor versions of Kubernetes. You need to update your clusters manually or press the <em>Upgrade</em> button in the Dashboard.</p><p>Before updating a cluster you should be aware of the potential errors this might cause. The following video will dive into a Kubernetes outage in production that Monzo experienced, its causes and effects, and the architectural and operational lessons learned.</p><iframe width=560 height=315 src=https://www.youtube.com/embed/OUYTNywPk-s frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>It is therefore recommended to first update your test cluster and validate it before performing changes on a productive environment.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8b3de4453774d1b90d502b3e75029e52>7.3 - Can you backup your Kubernetes cluster resources?</h1><p>Backing up your Kubernetes cluster is possible through the use of specialized software like <a href=https://velero.io/>Velero</a>. Velero consists of a server side component and a client tool that allow you to backup or restore all objects in your cluster, as well as the cluster resources and persistent volumes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ea018750e686265b62948a3285d9f6d4>7.4 - Can you migrate the content of one cluster to another cluster?</h1><p>The migration of clusters or content from one cluster to another is out of scope for the Gardener project. For such scenarios you may consider using tools like <a href=https://velero.io/>Velero</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-082d4a0baad178bf10fe6d7ff471d8dc>7.5 - How can you deploy Gardener locally?</h1><p>If you need information on how to deploy your cluster on your local machine, a step-by-step tutorial can be found at <a href=https://gardener.cloud/docs/gardener/development/getting_started_locally/>Running Gardener locally</a>.</p><p>You can also watch the recorded <a href="https://www.youtube.com/watch?v=nV_JI8YWwY4&ab_channel=GardenerProject">Community Call</a> to get a better understanding of the process.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-183a21345f1b25533a5f87a27463f926>7.6 - How can you get the status of a shoot API server?</h1><p>There are two ways to get the health information of a shoot API server.</p><ul><li>Try to reach the public endpoint of the shoot API server via
<code>"https://api.&lt;shoot-name>.&lt;project-name>.shoot.&lt;canary|office|live>.k8s-hana.ondemand.com/healthz"</code></li></ul><p>The endpoint is secured, therefore you need to authenticate via basic auth or client cert. Both are available in the admin kubeconfig of the shoot cluster. Note that with those credentials you have full (admin) access to the cluster, therefore it is highly recommended to create custom credentials with some RBAC rules and bindings which only allow access to the /healthz endpoint.</p><ul><li>Fetch the shoot resource of your cluster via the programmatic API of the Gardener and get the availability information from the status.
You need a kubeconfig for the Garden cluster, which you can get via the Gardener dashboard. Then you could fetch your shoot resource and query for the availability information via:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl get shoot &lt;shoot-name&gt; -o json | jq -r <span style=color:#a31515>&#39;.status.conditions[] | select(.type==&#34;APIServerAvailable&#34;)&#39;</span>
</span></span></code></pre></div><p>The availability information in the second scenario is collected by the Gardener. If you want to collect the information independently from Gardener, you should choose the first scenario.</p><p>If you want to archive a simple pull monitor in the AvS for a shoot cluster, you also need to use the first scenario, because with it you have a stable endpoint for the API server which you can query.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-34933e225ebd68eb383f53ebc3ded996>7.7 - How can you monitor your Kubernetes deployments?</h1><p>Monitoring your Kubernetes deployments is possible through the use of Dynatrace or Prometheus.</p><p><a href=/docs/gardener/development/monitoring-stack/>Here</a> you can also read about monitoring your Kubernetes applications.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-da4d879e5d72d56df0ace67cb1ed54b8>7.8 - How do you configure Multi-AZ worker pools for different extensions?</h1><p>Configuration of Multi-AZ worker pools depends on the infrastructure.</p><p>The zone distribution for the worker pools can be configured generically across all infrastructures. You can find provider-specific details in the <code>InfrastructureConfig</code> section of each extension provider repository:</p><ul><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#infrastructureconfig>AWS</a> (a VPC with a subnet is required in each zone you want to support)</li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/usage-as-end-user/#infrastructureconfig>GCP</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/usage-as-end-user/#infrastructureconfig>Azure</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/usage-as-end-user/#infrastructureconfig>AliCloud</a></li><li><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/usage-as-end-user/#infrastructureconfig>OpenStack</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8e6fe8938311ce746ba7bccc5e0c3413>7.9 - How do you rotate IaaS keys for a running cluster?</h1><p>End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations. These credentials should be regularly rotated.</p><p>How to do so is explained in <a href=/docs/gardener/usage/shoot_credentials_rotation/#cloud-provider-keys>Shoot Credentials Rotation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e08258feb3f39d3778a499e2ffa393d4>7.10 - Reconciliation</h1><h2 id=what-is-impacted-during-a-reconciliation>What is impacted during a reconciliation?</h2><p>Infrastructure and DNSRecord reconciliation are only done during usual reconciliation if there were relevant changes. Otherwise, they are only done during maintenance.</p><h2 id=how-do-you-steer-a-reconciliation>How do you steer a reconciliation?</h2><p>Reconciliation is bound to the maintenance time window of a cluster. This means that your shoot will be reconciled regularly, without need for input.</p><p>Outside of the maintenance time window your shoot will only reconcile if you change the specification or if you explicitly trigger it. To learn how, see <a href=/docs/gardener/usage/shoot_operations/>Trigger shoot operations</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4ff5d6641d61bfaf8eeb7bd73e95a20d>7.11 - What are the meanings of different DNS configuration options?</h1><h2 id=can-you-adapt-a-dns-configuration-to-be-used-by-the-workload-on-the-cluster-coredns-configuration>Can you adapt a DNS configuration to be used by the workload on the cluster (CoreDNS configuration)?</h2><p>Yes, you can. Information on that can be found in <a href=/docs/gardener/usage/custom-dns-config/>Custom DNS Configuration</a>.</p><h2 id=how-to-use-custom-domain-names-using-a-dns-provider>How to use custom domain names using a DNS provider?</h2><h3 id=creating-custom-domain-names-for-the-gardener-infrastructure-dns-records-using-dnsrecords-resources>Creating custom domain names for the Gardener infrastructure DNS records using DNSRecords resources</h3><p>With DNSRecords internal and external domain names of the kube-apiserver are set, as well as the deprecated ingress domain name and an “owner” <a href=/docs/gardener/extensions/dnsrecord/>DNS record</a> for the owning seed.</p><p>For this purpose, you need either a provider extension supporting the needed resource kind <code>DNSRecord/&lt;provider-type></code> or a special extension.</p><p>All main providers support their respective IaaS specific DNS servers:</p><ul><li>AWS => <code>DNSRecord/aws-route53</code></li><li>GCP => <code>DNSRecord/google-cloudns</code></li><li>Azure => <code>DNSRecord/azure-dns</code></li><li>Openstack => <code>DNSRecord/openstack-designate</code></li><li>AliCloud => <code>DNSRecord/alicloud-dns</code></li></ul><p>For Cloudflare there is a <a href=https://github.com/schrodit/gardener-extension-provider-dns-cloudflare>community extension</a> existing.</p><p>For other providers like <a href=https://www.netlify.com/>Netlify</a> and <a href=https://www.infoblox.com/>infoblox</a> there is currently no known supporting extension, however, they are supported for <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/>shoot-dns-service</a>.</p><h3 id=creating-domain-names-for-cluster-resources-like-ingress-or-services-with-services-of-type-loadbalancers-and-for-tls-certificates>Creating domain names for cluster resources like ingress or services with services of type Loadbalancers and for TLS certificates</h3><p>For this purpose, the shoot-dns-service extension is used (DNSProvider and DNSEntry resources).</p><p>You can read more on it in these documents:</p><ul><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/docs/installation/deployment/>Deployment of the Shoot DNS Service Extension</a></li><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_names/>Request DNS Names in Shoot Clusters</a></li><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_providers/>DNS Providers</a></li><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/docs/installation/setup/>Gardener DNS Management for Shoots</a></li><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/>Request X.509 Certificates</a></li><li><a href=https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/docs/installation/setup/>Gardener Certificate Management</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4a13dbd9dc582373befd93279b2c45bb>8 - Other Components</h1><div class=lead>Other components included in the Gardener project</div></div><div class=td-content><h1 id=pg-f95024613e3fc900eec5f206fcaf61c5>8.1 - etcd Druid</h1><div class=lead>A druid for etcd management in Gardener</div><h1 id=etcd-druid>ETCD Druid</h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/etcd-druid><img src=https://goreportcard.com/badge/github.com/gardener/gardener alt="Go Report Card"></a></p><h2 id=background>Background</h2><p><a href=https://github.com/etcd-io/etcd>Etcd</a> in the control plane of Kubernetes clusters which are managed by Gardener is deployed as a StatefulSet. The statefulset has replica of a pod containing two containers namely, etcd and <a href=https://github.com/gardener/etcd-backup-restore>backup-restore</a>. The etcd container calls components in etcd-backup-restore via REST api to perform data validation before etcd is started. If this validation fails etcd data is restored from the latest snapshot stored in the cloud-provider&rsquo;s object store. Once etcd has started, the etcd-backup-restore periodically creates full and delta snapshots. It also performs defragmentation of etcd data periodically.</p><p>The etcd-backup-restore needs as input the cloud-provider information comprising of security credentials to access the object store, the object store bucket name and prefix for the directory used to store snapshots. Currently, for operations like migration and validation, the bash script has to be updated to initiate the operation.</p><h2 id=goals>Goals</h2><ul><li>Deploy etcd and etcd-backup-restore using an etcd CRD.</li><li>Support more than one etcd replica.</li><li>Perform scheduled snapshots.</li><li>Support operations such as restores, defragmentation and scaling with zero-downtime.</li><li>Handle cloud-provider specific operation logic.</li><li>Trigger a full backup on request before volume deletion.</li><li>Offline compaction of full and delta snapshots stored in object store.</li></ul><h2 id=proposal>Proposal</h2><p>The existing method of deploying etcd and backup-sidecar as a StatefulSet alleviates the pain of ensuring the pods are live and ready after node crashes. However, deploying etcd as a Statefulset introduces a plethora of challenges. The etcd controller should be smart enough to handle etcd statefulsets taking into account limitations imposed by statefulsets. The controller shall update the status regarding how to target the K8s objects it has created. This field in the status can be leveraged by <code>HVPA</code> to scale etcd resources eventually.</p><h2 id=crd-specification>CRD specification</h2><p>The etcd CRD should contain the information required to create the etcd and backup-restore sidecar in a pod/statefulset.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - druid.gardener.cloud/etcd
</span></span><span style=display:flex><span>  name: test
</span></span><span style=display:flex><span>  namespace: demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    deltaSnapshotMemoryLimit: 1Gi
</span></span><span style=display:flex><span>    deltaSnapshotPeriod: 300s
</span></span><span style=display:flex><span>    fullSnapshotSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    garbageCollectionPeriod: 43200s
</span></span><span style=display:flex><span>    garbageCollectionPolicy: Exponential
</span></span><span style=display:flex><span>    imageRepository: eu.gcr.io/gardener-project/gardener/etcdbrctl
</span></span><span style=display:flex><span>    imageVersion: v0.12.0
</span></span><span style=display:flex><span>    port: 8080
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 2Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 23m
</span></span><span style=display:flex><span>        memory: 128Mi
</span></span><span style=display:flex><span>    snapstoreTempDir: /var/etcd/data/temp
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    Quota: 8Gi
</span></span><span style=display:flex><span>    clientPort: 2379
</span></span><span style=display:flex><span>    defragmentationSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    enableTLS: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    imageRepository: eu.gcr.io/gardener-project/gardener/etcd
</span></span><span style=display:flex><span>    imageVersion: v3.4.13-bootstrap
</span></span><span style=display:flex><span>    initialClusterState: new
</span></span><span style=display:flex><span>    initialClusterToken: new
</span></span><span style=display:flex><span>    metrics: basic
</span></span><span style=display:flex><span>    pullPolicy: IfNotPresent
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 2500m
</span></span><span style=display:flex><span>        memory: 4Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 1000Mi
</span></span><span style=display:flex><span>    serverPort: 2380
</span></span><span style=display:flex><span>    storageCapacity: 80Gi
</span></span><span style=display:flex><span>    storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  sharedConfig:
</span></span><span style=display:flex><span>    autoCompactionMode: periodic
</span></span><span style=display:flex><span>    autoCompactionRetention: 30m
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  pvcRetentionPolicy: DeleteAll
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  storageCapacity: 80Gi
</span></span><span style=display:flex><span>  storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  store:
</span></span><span style=display:flex><span>    storageContainer: test
</span></span><span style=display:flex><span>    storageProvider: S3
</span></span><span style=display:flex><span>    storePrefix: etcd-test
</span></span><span style=display:flex><span>    storeSecret: etcd-backup
</span></span><span style=display:flex><span>  tlsClientSecret: etcd-client-tls
</span></span><span style=display:flex><span>  tlsServerSecret: etcd-server-tls
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: StatefulSet
</span></span><span style=display:flex><span>    name: etcd-test
</span></span></code></pre></div><h2 id=implementation-agenda>Implementation Agenda</h2><p>As first step implement defragmentation during maintenance windows. Subsequently, we will add zero-downtime upgrades and defragmentation.</p><h2 id=workflow>Workflow</h2><h3 id=deployment-workflow>Deployment workflow</h3><p><img src=/__resources/controller_6d5b8f.png alt=controller-diagram></p><h3 id=defragmentation-workflow>Defragmentation workflow</h3><p><img src=/__resources/defrag_da50bf.png alt=defrag-diagram></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c5520f0940e52e6166ce8932f4dd864>8.2 - Machine Controller Manager</h1><div class=lead>Declarative way of managing machines for Kubernetes cluster</div><h1 id=machine-controller-manager>machine-controller-manager</h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/machine-controller-manager><img src=https://goreportcard.com/badge/github.com/gardener/machine-controller-manager alt="Go Report Card"></a></p><p><strong>Note</strong>
One can add support for a new cloud provider by following <a href=/docs/other-components/machine-controller-manager/docs/development/cp_support_new/>Adding support for new provider</a>.</p><h1 id=overview>Overview</h1><p>Machine Controller Manager aka MCM is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients. MCM gives you the following benefits:</p><ul><li>seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers)</li><li>integrate generically with the cluster autoscaler</li><li>plugin with tools such as the node-problem-detector</li><li>transport the immutability design principle to machine/nodes</li><li>implement e.g. rolling upgrades of machines/nodes</li></ul><p>MCM supports following providers. These provider code is maintained externally (out-of-tree), and the links for the same are linked below:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud>Alicloud</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-aws>AWS</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-azure>Azure</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-equinix-metal>Equinix Metal</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-gcp>GCP</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt</a></li><li><a href=https://github.com/metal-stack/machine-controller-manager-provider-metal>Metal Stack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-openstack>Openstack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-vsphere>V Sphere</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-yandex>Yandex</a></li></ul><p>It can easily be extended to support other cloud providers as well.</p><p>Example of managing machine:</p><pre tabindex=0><code>kubectl create/get/delete machine vm1
</code></pre><h2 id=key-terminologies>Key terminologies</h2><p>Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way</p><ol><li>VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup.</li><li>Node: Native kubernetes node objects. The objects you get to see when you do a <em>&ldquo;kubectl get nodes&rdquo;</em>. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</li><li>Machine: A VM that is provisioned/managed by the Machine Controller Manager.</li></ol><h1 id=design-of-machine-controller-manager>Design of Machine Controller Manager</h1><p>The design of the Machine Controller Manager is influenced by the Kube Controller Manager, where-in multiple sub-controllers are used to manage the Kubernetes clients.</p><h2 id=design-principles>Design Principles</h2><p>It&rsquo;s designed to run in the master plane of a Kubernetes cluster. It follows the best principles and practices of writing controllers, including, but not limited to:</p><ul><li>Reusing code from kube-controller-manager</li><li>leader election to allow HA deployments of the controller</li><li><code>workqueues</code> and multiple thread-workers</li><li><code>SharedInformers</code> that limit to minimum network calls, de-serialization and provide helpful create/update/delete events for resources</li><li>rate-limiting to allow back-off in case of network outages and general instability of other cluster components</li><li>sending events to respected resources for easy debugging and overview</li><li>Prometheus metrics, health and (optional) profiling endpoints</li></ul><h2 id=objects-of-machine-controller-manager>Objects of Machine Controller Manager</h2><p>Machine Controller Manager reconciles a set of Custom Resources namely <code>MachineDeployment</code>, <code>MachineSet</code> and <code>Machines</code> which are managed & monitored by their controllers MachineDeployment Controller, MachineSet Controller, Machine Controller respectively along with another cooperative controller called the Safety Controller.</p><p>Machine Controller Manager makes use of 4 CRD objects and 1 Kubernetes secret object to manage machines. They are as follows:</p><table><thead><tr><th>Custom ResourceObject</th><th>Description</th></tr></thead><tbody><tr><td><code>MachineClass</code></td><td>A <code>MachineClass</code> represents a template that contains cloud provider specific details used to create machines.</td></tr><tr><td><code>Machine</code></td><td>A <code>Machine</code> represents a VM which is backed by the cloud provider.</td></tr><tr><td><code>MachineSet</code></td><td>A <code>MachineSet</code> ensures that the specified number of <code>Machine</code> replicas are running at a given point of time.</td></tr><tr><td><code>MachineDeployment</code></td><td>A <code>MachineDeployment</code> provides a declarative update for <code>MachineSet</code> and <code>Machines</code>.</td></tr><tr><td><code>Secret</code></td><td>A <code>Secret</code> here is a Kubernetes secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.</td></tr></tbody></table><p>See <a href=/docs/other-components/machine-controller-manager/docs/documents/apis/>here</a> for CRD API Documentation</p><h2 id=components-of-machine-controller-manager>Components of Machine Controller Manager</h2><table><thead><tr><th>Controller</th><th>Description</th></tr></thead><tbody><tr><td>MachineDeployment controller</td><td>Machine Deployment controller reconciles the <code>MachineDeployment</code> objects and manages the lifecycle of <code>MachineSet</code> objects. <code>MachineDeployment</code> consumes provider specific <code>MachineClass</code> in its <code>spec.template.spec</code> which is the template of the VM spec that would be spawned on the cloud by MCM.</td></tr><tr><td>MachineSet controller</td><td>MachineSet controller reconciles the <code>MachineSet</code> objects and manages the lifecycle of <code>Machine</code> objects.</td></tr><tr><td>Safety controller</td><td>There is a Safety Controller responsible for handling the unidentified or unknown behaviours from the cloud providers. Safety Controller:<ul><li>freezes the MachineDeployment controller and MachineSet controller if the number of <code>Machine</code> objects goes beyond a certain threshold on top of <code>Spec.replicas</code>. It can be configured by the flag <code>--safety-up</code> or <code>--safety-down</code> and also <code>--machine-safety-overshooting-period`</code>.</li><li>freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li>unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></td></tr></tbody></table><p>Along with the above Custom Controllers and Resources, MCM requires the <code>MachineClass</code> to use K8s <code>Secret</code> that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials. All these controllers work in an co-operative manner. They form a parent-child relationship with <code>MachineDeployment</code> Controller being the grandparent, <code>MachineSet</code> Controller being the parent, and <code>Machine</code> Controller being the child.</p><h2 id=development>Development</h2><p>To start using or developing the Machine Controller Manager, see the documentation in the <code>/docs</code> repository, please <a href=/docs/other-components/machine-controller-manager/docs/>find the index here</a>.</p><h2 id=faq>FAQ</h2><p>An FAQ is available <a href=/docs/other-components/machine-controller-manager/docs/faq/>here</a></p><h2 id=cluster-api-implementation>Cluster-api Implementation</h2><ul><li><code>cluster-api</code> branch of machine-controller-manager implements the machine-api aspect of the <a href=https://github.com/kubernetes-sigs/cluster-api>cluster-api project</a>.</li><li>Link: <a href=https://github.com/gardener/machine-controller-manager/tree/cluster-api>https://github.com/gardener/machine-controller-manager/tree/cluster-api</a></li><li>Once cluster-api project gets stable, we may make <code>master</code> branch of MCM as well cluster-api compliant, with well-defined migration notes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-93836fee9b0589e1016c929a887330c2>8.2.1 - Documentation</h1><h1 id=documentation-index>Documentation Index</h1><h2 id=using-out-of-tree-external-provider-support-recommended>Using Out-of-Tree (External) provider support (Recommended)</h2><h4 id=development>Development</h4><ul><li><a href=/docs/other-components/machine-controller-manager/docs/development/cp_support_new/>Adding support for a new cloud provider</a></li></ul></div><div class=td-content><h1 id=pg-7dc591d336fad8944f749e26822388e1>8.2.1.1 - Deployment</h1></div><div class=td-content><h1 id=pg-b847b52de084bb48512a93c08e3d863e>8.2.1.1.1 - Kubernetes</h1><h1 id=deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</h1><ul><li><a href=#deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</a><ul><li><a href=#prepare-the-cluster>Prepare the cluster</a></li><li><a href=#build-the-docker-image>Build the Docker image</a></li><li><a href=#configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</a></li><li><a href=#usage>Usage</a></li></ul></li></ul><p>As already mentioned, the Machine Controller Manager is designed to run as controller in a Kubernetes cluster. The existing source code can be compiled and tested on a local machine as described in <a href=/docs/other-components/machine-controller-manager/docs/development/local_setup/>Setting up a local development environment</a>. You can deploy the Machine Controller Manager using the steps described below.</p><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using the kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing the cluster info.</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds
</span></span></code></pre></div><h2 id=build-the-docker-image>Build the Docker image</h2><blockquote><p>⚠️ Modify the <code>Makefile</code> to refer to your own registry.</p></blockquote><ul><li>Run the build which generates the binary to <code>bin/machine-controller-manager</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make build
</span></span></code></pre></div><ul><li>Build docker image from latest compiled binary</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make docker-image
</span></span></code></pre></div><ul><li>Push the last created docker image onto the online docker registry.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make push
</span></span></code></pre></div><ul><li>Now you can deploy this docker image to your cluster. A <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml>sample development file</a> is provided. By default, the deployment manages the cluster it is running in. Optionally, the kubeconfig could also be passed as a flag as described in <code>/kubernetes/deployment/out-of-tree/deployment.yaml</code>. This is done when you want your controller running outside the cluster to be managed from.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/deployment.yaml
</span></span></code></pre></div><ul><li>Also deploy the required clusterRole and clusterRoleBindings</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrole.yaml
</span></span><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrolebinding.yaml
</span></span></code></pre></div><h2 id=configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</h2><p>Machine-controller-manager supports several configurable parameters while deploying. Refer to <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L21-L30>the following lines</a>, to know how each parameter can be configured, and what it&rsquo;s purpose is for.</p><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/docs/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a790e8814a7a4fd855d8c9dafea555a5>8.2.1.2 - Development</h1></div><div class=td-content><h1 id=pg-1cb4cbb02ca0bec39e583a80168d8f1a>8.2.1.2.1 - Adding Support for a Cloud Provider</h1><h1 id=adding-support-for-a-new-provider>Adding support for a new provider</h1><p>Steps to be followed while implementing a new (hyperscale) provider are mentioned below. This is the easiest way to add new provider support using a blueprint code.</p><p>However, you may also develop your machine controller from scratch, which would provide you with more flexibility. First, however, make sure that your custom machine controller adheres to the <code>Machine.Status</code> struct defined in the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>MachineAPIs</a>. This will make sure the MCM can act with higher-level controllers like MachineSet and MachineDeployment controller. The key is the <code>Machine.Status.CurrentStatus.Phase</code> key that indicates the status of the machine object.</p><p>Our strong recommendation would be to follow the steps below. This provides the most flexibility required to support machine management for adding new providers. And if you feel to extend the functionality, feel free to update our <a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/util/provider>machine controller libraries</a>.</p><h2 id=setting-up-your-repository>Setting up your repository</h2><ol><li>Create a new empty repository named <code>machine-controller-manager-provider-{provider-name}</code> on GitHub username/project. Do not initialize this repository with a README.</li><li>Copy the remote repository <code>URL</code> (HTTPS/SSH) to this repository displayed once you create this repository.</li><li>Now, on your local system, create directories as required. {your-github-username} given below could also be {github-project} depending on where you have created the new repository.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Navigate to this created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Clone <a href=https://github.com/gardener/machine-controller-manager-provider-sampleprovider>this repository</a> on your local machine.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager-provider-sampleprovider.git
</span></span></code></pre></div></li><li>Rename the directory from <code>machine-controller-manager-provider-sampleprovider</code> to <code>machine-controller-manager-provider-{provider-name}</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mv machine-controller-manager-provider-sampleprovider machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Navigate into the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Update the remote <code>origin</code> URL to the newly created repository&rsquo;s URL you had copied above.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git remote set-url origin git@github.com:{your-github-username}/machine-controller-manager-provider-{provider-name}.git
</span></span></code></pre></div></li><li>Rename GitHub project from <code>gardener</code> to <code>{github-org/your-github-username}</code> wherever you have cloned the repository above. Also, edit all occurrences of the word <code>sampleprovider</code> to <code>{provider-name}</code> in the code. Then, use the hack script given below to do the same.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make rename-project PROJECT_NAME={github-org/your-github-username} PROVIDER_NAME={provider-name}
</span></span><span style=display:flex><span>eg:
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=gardener PROVIDER_NAME=AmazonWebServices (or)
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=githubusername PROVIDER_NAME=AWS
</span></span></code></pre></div></li><li>Now, commit your changes and push them upstream.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git add -A
</span></span><span style=display:flex><span>git commit -m <span style=color:#a31515>&#34;Renamed SampleProvide to {provider-name}&#34;</span>
</span></span><span style=display:flex><span>git push origin master
</span></span></code></pre></div></li></ol><h2 id=code-changes-required>Code changes required</h2><p>The contract between the Machine Controller Manager (MCM) and the Machine Controller (MC) AKA driver has been <a href=/docs/other-components/machine-controller-manager/docs/development/machine_error_codes/>documented here</a> and the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error codes can be found here</a>. You may refer to them for any queries.</p><p>⚠️</p><ul><li>Keep in mind that <strong>there should be a unique way to map between machine objects and VMs</strong>. This can be done by mapping machine object names with VM-Name/ tags/ other metadata.</li><li>Optionally, there should also be a unique way to map a VM to its machine class object. This can be done by tagging VM objects with tags/resource groups associated with the machine class.</li></ul><h4 id=steps-to-integrate>Steps to integrate</h4><ol><li>Update the <code>pkg/provider/apis/provider_spec.go</code> specification file to reflect the structure of the <code>ProviderSpec</code> blob. It typically contains the machine template details in the <code>MachineClass</code> object. Follow the sample spec provided already in the file. A sample provider specification can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/apis/aws_provider_spec.go>here</a>.</li><li>Fill in the methods described at <code>pkg/provider/core.go</code> to manage VMs on your cloud provider. Comments are provided above each method to help you fill them up with desired <code>REQUEST</code> and <code>RESPONSE</code> parameters.<ul><li>A sample provider implementation for these methods can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/core.go>here</a>.</li><li>Fill in the required methods <code>CreateMachine()</code>, and <code>DeleteMachine()</code> methods.</li><li>Optionally fill in methods like <code>GetMachineStatus()</code>, <code>ListMachines()</code>, and <code>GetVolumeIDs()</code>. You may choose to fill these once the working of the required methods seems to be working.</li><li><code>GetVolumeIDs()</code> expects VolumeIDs to be decoded from the volumeSpec based on the cloud provider.</li><li>There is also an OPTIONAL method <code>GenerateMachineClassForMigration()</code> that helps in migration of <code>{ProviderSpecific}MachineClass</code> to <code>MachineClass</code> CR (custom resource). This only makes sense if you have an existing implementation (in-tree) acting on different CRD types. You would like to migrate this. If not, you MUST return an error (machine error UNIMPLEMENTED) to avoid processing this step.</li></ul></li><li>Perform validation of APIs that you have described and make it a part of your methods as required at each request.</li><li>Write unit tests to make it work with your implementation by running <code>make test</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test
</span></span></code></pre></div></li><li>Re-generate the vendors to update any new vendors imported.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make revendor
</span></span></code></pre></div></li><li>Update the sample YAML files on the <code>kubernetes/</code> directory to provide sample files through which the working of the machine controller can be tested.</li><li>Update <code>README.md</code> to reflect any additional changes</li></ol><h2 id=testing-your-code-changes>Testing your code changes</h2><p>Make sure <code>$TARGET_KUBECONFIG</code> points to the cluster where you wish to manage machines. Likewise, <code>$CONTROL_NAMESPACE</code> represents the namespaces where MCM is looking for machine CR objects, and <code>$CONTROL_KUBECONFIG</code> points to the cluster that holds these machine CRs.</p><ol><li>On the first terminal running at <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code>,<ul><li>Run the machine controller (driver) using the command below.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the second terminal pointing to <code>$GOPATH/src/github.com/gardener</code>,<ul><li>Clone the <a href=https://github.com/gardener/machine-controller-manager>latest MCM code</a><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager.git
</span></span></code></pre></div></li><li>Navigate to the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager
</span></span></code></pre></div></li><li>Deploy the required CRDs from the machine-controller-manager repo,<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/crds
</span></span></code></pre></div></li><li>Run the machine-controller-manager in the <code>master</code> branch<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the third terminal pointing to <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code><ul><li>Fill in the object files given below and deploy them as described below.</li><li>Deploy the <code>machine-class</code><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-class.yaml
</span></span></code></pre></div></li><li>Deploy the <code>kubernetes secret</code> if required.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/secret.yaml
</span></span></code></pre></div></li><li>Deploy the <code>machine</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine.yaml
</span></span></code></pre></div></li><li>Once the machine joins, you can test by deploying a machine-deployment.</li><li>Deploy the <code>machine-deployment</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li><li>Make sure to delete both the <code>machine</code> and <code>machine-deployment</code> objects after use.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete -f kubernetes/machine.yaml
</span></span><span style=display:flex><span>kubectl delete -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li></ul></li></ol><h2 id=releasing-your-docker-image>Releasing your docker image</h2><ol><li>Make sure you have logged into gcloud/docker using the CLI.</li><li>To release your docker image, run the following.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    make release IMAGE_REPOSITORY=&lt;link-to-image-repo&gt;
</span></span></code></pre></div><ol start=3><li>A sample kubernetes deploy file can be found at <code>kubernetes/deployment.yaml</code>. Update the same (with your desired MCM and MC images) to deploy your MCM pod.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-95ca541a1e4527eb703416a8714d1f8b>8.2.1.2.2 - Integration Tests</h1><h1 id=integration-tests>Integration tests</h1><h2 id=usage>Usage</h2><h2 id=general-setup--configurations>General setup & configurations</h2><p>Integration tests for <code>machine-controller-manager-provider-{provider-name}</code> can be executed manually by following below steps.</p><ol><li>Clone the repository <code>machine-controller-manager-provider-{provider-name}</code> on the local system.</li><li>Navigate to <code>machine-controller-manager-provider-{provider-name}</code> directory and create a <code>dev</code> sub-directory in it.</li><li>Copy the kubeconfig of Control Cluster from into <code>dev/control-kubeconfig.yaml</code>.</li><li>(optional) Copy the kubeconfig of Target Cluster into <code>dev/target-kubeconfig.yaml</code> and update the <code>Makefile</code> variable <code>TARGET_KUBECONFIG</code> to point to <code>dev/target-kubeconfig.yaml</code>.</li><li>If the tags on instances & associated resources on the provider are of <code>String</code> type (for example, GCP tags on its instances are of type <code>String</code> and not key-value pair) then add <code>TAGS_ARE_STRINGS := true</code> in the <code>Makefile</code> and export it.</li><li>Atleast, one of the two controllers&rsquo; container images must be set in the <code>Makefile</code> variables <code>MCM_IMAGE_TAG</code> and <code>MC_IMAGE_TAG</code> for the controllers to run in the Control Cluster . These images will be used along with <code>kubernetes/deployment.yaml</code> to deploy/update controllers in the Control Cluster . If the intention is to run the controllers locally then unset the variables <code>MCM_IMAGE_TAG</code> and <code>MC_IMAGE_TAG</code> and set variable <code>MACHINE_CONTROLLER_MANAGER_DEPLOYMENT_NAME := machine-controller-manager</code> in the <code>Makefile</code>.</li><li>In order to apply the CRDs when the Control Cluster is a Gardener Shoot or if none of the controller images are specified, <code>machine-controller-manager</code> repository will be cloned automatically. Incase, this repository already exists in local system, then create a softlink as below which helps to test changes in <code>machine-controller-manager</code> quickly.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ln -sf &lt;path-for-machine-controller-manager-repo&gt; dev/mcm
</span></span></code></pre></div></li></ol><h2 id=scenario-based-additional-configurations>Scenario based additional configurations</h2><h3 id=gardener-shoot-as-the-control-cluster>Gardener Shoot as the Control Cluster</h3><p>If the Control Cluster is a Gardener Shoot cluster then,</p><ol><li>Deploy a <code>Secret</code> named <code>test-mc-secret</code> (that contains the provider secret and cloud-config) in the <code>default</code> namespace of the Control Cluster. Refer <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/machine_classes>these</a> <code>MachineClass</code> templates for the same.</li><li>Create a <code>dev/machineclassv1.yaml</code> file in the cloned repository. The name of the <code>MachineClass</code> itself should be <code>test-mc-v1</code>. The value of <code>providerSpec.secretRef.name</code> should be <code>test-mc-secret</code>.</li><li>(Optional) Create an additional <code>dev/machineclassv2.yaml</code> file similar to above but with a bigger machine type and update the <code>Makefile</code> variable <code>MACHINECLASS_V2</code> to point to <code>dev/machineclassv2.yaml</code>.</li></ol><h3 id=gardener-seed-as-the-control-cluster>Gardener Seed as the Control Cluster</h3><p>If the Control Cluster is a Gardener SEED cluster then, the suite ideally employs the already existing <code>MachineClass</code> and Secrets. However,</p><ol><li>(Optional) User can employ a custom <code>MachineClass</code> for the tests using below steps:<ol><li>Deploy a <code>Secret</code> named <code>test-mc-secret</code> (that contains the provider secret and cloud-config) in the shoot namespace of the Control Cluster. That is, the value of <code>metadata.namespace</code> should be <code>technicalID</code> of the Shoot and it will be of the pattern <code>shoot--&lt;project>--&lt;shoot-name></code>. Refer <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/machine_classes>these</a> <code>MachineClass</code> templates for the same.</li><li>Create a <code>dev/machineclassv1.yaml</code> file.<ol><li><code>providerSpec.secretRef.name</code> should refer the secret created in the previous step.</li><li><code>metadata.namespace</code> and <code>providerSpec.secretRef.namespace</code> should be <code>technicalID</code> (<code>shoot--&lt;project>--&lt;shoot-name></code>) of the shoot.</li><li>The name of the <code>MachineClass</code> itself should be <code>test-mc-v1</code>.</li></ol></li></ol></li></ol><h2 id=running-the-tests>Running the tests</h2><ol><li>There is a rule <code>test-integration</code> in the <code>Makefile</code>, which can be used to start the integration test:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make test-integration 
</span></span><span style=display:flex><span>Starting integration tests...
</span></span><span style=display:flex><span>Running Suite: Controller Suite
</span></span><span style=display:flex><span>===============================
</span></span></code></pre></div></li><li>The controllers log files (<code>mcm_process.log</code> and <code>mc_process.log</code>) are stored in <code>.ci/controllers-test/logs</code> repo and can be used later.</li></ol><h2 id=adding-integration-tests-for-new-providers>Adding Integration Tests for new providers</h2><p>For a new provider, Running Integration tests works with no changes. But for the orphan resource test cases to work correctly, the provider-specific API calls and the Resource Tracker Interface (RTI) should be implemented. Please check <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/test/integration/provider/><code>machine-controller-manager-provider-aws</code></a> for reference.</p><h2 id=extending-integration-tests>Extending integration tests</h2><ul><li>Update <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a> to be extend the testcases for all providers. Common testcases for machine|machineDeployment creation|deletion|scaling are packaged into <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a>.</li><li>To extend the provider specfic test cases, the changes should be done in the <code>machine-controller-manager-provider-{provider-name}</code> repository. For example, to extended the testcases for <code>machine-controller-manager-provider-aws</code>, make changes to <code>test/integration/controller/controller_test.go</code> inside the <code>machine-controller-manager-provider-aws</code> repository. <code>commons</code> contains the <code>Cluster</code> and <code>Clientset</code> objects that makes it easy to extend the tests.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-863951ff1cb434dd7fcaea549ea2043f>8.2.1.2.3 - Local Setup</h1><h1 id=preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</h1><ul><li><a href=#preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</a><ul><li><a href=#installing-golang-environment>Installing Golang environment</a></li><li><a href=#installing-docker-optional>Installing <code>Docker</code> (Optional)</a></li><li><a href=#setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</a></li><li><a href=#local-development>Local development</a><ul><li><a href=#installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</a></li></ul></li><li><a href=#prepare-the-cluster>Prepare the cluster</a></li><li><a href=#getting-started>Getting started</a></li><li><a href=#testing-machine-classes>Testing Machine Classes</a></li><li><a href=#usage>Usage</a></li></ul></li></ul><p>Conceptionally, the Machine Controller Manager is designed to run in a container within a Pod inside a Kubernetes cluster. For development purposes, you can run the Machine Controller Manager as a Go process on your local machine. This process connects to your remote cluster to manage VMs for that cluster. That means that the Machine Controller Manager runs outside a Kubernetes cluster which requires providing a <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> in your local filesystem and point the Machine Controller Manager to it when running it (see below).</p><p>Although the following installation instructions are for Mac OS X, similar alternate commands could be found for any Linux distribution.</p><h2 id=installing-golang-environment>Installing Golang environment</h2><p>Install the latest version of Golang (at least <code>v1.8.3</code> is required) by using <a href=https://brew.sh/>Homebrew</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ brew install golang
</span></span></code></pre></div><p>In order to perform linting on the Go source code, install <a href=https://github.com/golang/lint>Golint</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ go get -u golang.org/x/lint/golint
</span></span></code></pre></div><h2 id=installing-docker-optional>Installing <code>Docker</code> (Optional)</h2><p>In case you want to build Docker images for the Machine Controller Manager you have to install Docker itself. We recommend using <a href=https://docs.docker.com/docker-for-mac/>Docker for Mac OS X</a> which can be downloaded from <a href=https://download.docker.com/mac/stable/Docker.dmg>here</a>.</p><h2 id=setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</h2><p>Create a Docker hub account at <a href=https://hub.docker.com/>Docker Hub</a> if you don&rsquo;t already have one.</p><h2 id=local-development>Local development</h2><p>⚠️ Before you start developing, please ensure to comply with the following requirements:</p><ol><li>You have understood the <a href=https://kubernetes.io/docs/concepts/>principles of Kubernetes</a>, and its <a href=https://kubernetes.io/docs/concepts/overview/components/>components</a>, what their purpose is and how they interact with each other.</li><li>You have understood the <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>architecture of the Machine Controller Manager</a></li></ol><p>The development of the Machine Controller Manager could happen by targetting any cluster. You basically need a Kubernetes cluster running on a set of machines. You just need the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> file with the required access permissions attached to it.</p><h3 id=installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</h3><p>Clone the repository from GitHub.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ git clone git@github.com:gardener/machine-controller-manager.git
</span></span><span style=display:flex><span>$ cd machine-controller-manager
</span></span></code></pre></div><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing your cluster info</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><h2 id=getting-started>Getting started</h2><ul><li>Create a <code>dev</code> directory.</li><li>Copy the kubeconfig of kubernetes cluster where you wish to deploy the machines into <code>dev/target-kubeconfig.yaml</code>.</li><li>(optional) Copy the kubeconfig of kubernetes cluster from where you wish to manage the machines into <code>dev/control-kubeconfig.yaml</code>. If you do this, also update the <code>Makefile</code> variable CONTROL_KUBECONFIG to point to <code>dev/control-kubeconfig.yaml</code> and CONTROL_NAMESPACE to the namespace in which your controller watches over.</li><li>There is a rule dev in the <code>Makefile</code> which will automatically start the Machine Controller Manager with development settings:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make start
</span></span><span style=display:flex><span>I1227 11:08:19.963638   55523 controllermanager.go:204] Starting shared informers
</span></span><span style=display:flex><span>I1227 11:08:20.766085   55523 controller.go:247] Starting machine-controller-manager
</span></span></code></pre></div><p>⚠️ The file <code>dev/target-kubeconfig.yaml</code> points to the cluster whose nodes you want to manage. <code>dev/control-kubeconfig.yaml</code> points to the cluster from where you want to manage the nodes from. However, <code>dev/control-kubeconfig.yaml</code> is optional.</p><p>The Machine Controller Manager should now be ready to manage the VMs in your kubernetes cluster.</p><p>⚠️ This is assuming that your MCM is built to manage machines for any in-tree supported providers. There is a new way to deploy and manage out of tree (external) support for providers whose development can be <a href=/docs/other-components/machine-controller-manager/docs/development/cp_support_new/>found here</a></p><h2 id=testing-machine-classes>Testing Machine Classes</h2><p>To test the creation/deletion of a single instance for one particular machine class you can use the <code>managevm</code> cli. The corresponding <code>INFRASTRUCTURE-machine-class.yaml</code> and the <code>INFRASTRUCTURE-secret.yaml</code> need to be defined upfront. To build and run it</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>GO111MODULE=on go build -mod=vendor -o managevm cmd/machine-controller-manager-cli/main.go
</span></span><span style=display:flex><span><span style=color:green># create machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test
</span></span><span style=display:flex><span><span style=color:green># delete machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test --machineid INFRASTRUCTURE:///REGION/INSTANCE_ID
</span></span></code></pre></div><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/docs/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4e5a11d17e9f31f9f3a9d9501e0a39a6>8.2.1.2.4 - Machine Error Codes</h1><h1 id=machine-error-code-handling>Machine Error code handling</h1><h2 id=notational-conventions>Notational Conventions</h2><p>The keywords &ldquo;MUST&rdquo;, &ldquo;MUST NOT&rdquo;, &ldquo;REQUIRED&rdquo;, &ldquo;SHALL&rdquo;, &ldquo;SHALL NOT&rdquo;, &ldquo;SHOULD&rdquo;, &ldquo;SHOULD NOT&rdquo;, &ldquo;RECOMMENDED&rdquo;, &ldquo;NOT RECOMMENDED&rdquo;, &ldquo;MAY&rdquo;, and &ldquo;OPTIONAL&rdquo; are to be interpreted as described in <a href=https://datatracker.ietf.org/doc/html/rfc2119>RFC 2119</a> (Bradner, S., &ldquo;Key words for use in RFCs to Indicate Requirement Levels&rdquo;, BCP 14, RFC 2119, March 1997).</p><p>The key words &ldquo;unspecified&rdquo;, &ldquo;undefined&rdquo;, and &ldquo;implementation-defined&rdquo; are to be interpreted as described in the <a href="https://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf#page=18">rationale for the C99 standard</a>.</p><p>An implementation is not compliant if it fails to satisfy one or more of the MUST, REQUIRED, or SHALL requirements for the protocols it implements.
An implementation is compliant if it satisfies all the MUST, REQUIRED, and SHALL requirements for the protocols it implements.</p><h2 id=terminology>Terminology</h2><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>CR</td><td>Custom Resource (CR) is defined by a cluster admin using the Kubernetes Custom Resource Definition primitive.</td></tr><tr><td>VM</td><td>A Virtual Machine (VM) provisioned and managed by a provider. It could also refer to a physical machine in case of a bare metal provider.</td></tr><tr><td>Machine</td><td>Machine refers to a VM that is provisioned/managed by MCM. It typically describes the metadata used to store/represent a Virtual Machine</td></tr><tr><td>Node</td><td>Native kubernetes <code>Node</code> object. The objects you get to see when you do a &ldquo;kubectl get nodes&rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</td></tr><tr><td>MCM</td><td><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager (MCM)</a> is the controller used to manage higher level Machine Custom Resource (CR) such as machine-set and machine-deployment CRs.</td></tr><tr><td>Provider/Driver/MC</td><td><code>Provider</code> (or) <code>Driver</code> (or) <code>Machine Controller (MC)</code> is the driver responsible for managing machine objects present in the cluster from whom it manages these machines. A simple example could be creation/deletion of VM on the provider.</td></tr></tbody></table><h2 id=pre-requisite>Pre-requisite</h2><h3 id=machineclass-resources>MachineClass Resources</h3><p>MCM introduces the CRD <code>MachineClass</code>. This is a blueprint for creating machines that join a certain cluster as nodes in a certain role. The provider only works with <code>MachineClass</code> resources that have the structure described here.</p><h4 id=providerspec>ProviderSpec</h4><p>The <code>MachineClass</code> resource contains a <code>providerSpec</code> field that is passed in the <code>ProviderSpec</code> request field to CMI methods such as <a href=#createmachine>CreateMachine</a>. The <code>ProviderSpec</code> can be thought of as a machine template from which the VM specification must be adopted. It can contain key-value pairs of these specs. An example for these key-value pairs are given below.</p><table><thead><tr><th>Parameter</th><th>Mandatory</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vmPool</code></td><td>Yes</td><td><code>string</code></td><td>VM pool name, e.g. <code>TEST-WOKER-POOL</code></td></tr><tr><td><code>size</code></td><td>Yes</td><td><code>string</code></td><td>VM size, e.g. <code>xsmall</code>, <code>small</code>, etc. Each size maps to a number of CPUs and memory size.</td></tr><tr><td><code>rootFsSize</code></td><td>No</td><td><code>int</code></td><td>Root (<code>/</code>) filesystem size in GB</td></tr><tr><td><code>tags</code></td><td>Yes</td><td><code>map</code></td><td>Tags to be put on the created VM</td></tr></tbody></table><p>Most of the <code>ProviderSpec</code> fields are not mandatory. If not specified, the provider passes an empty value in the respective <code>Create VM</code> parameter.</p><p>The <code>tags</code> can be used to map a VM to its corresponding machine object&rsquo;s Name</p><p>The <code>ProviderSpec</code> is validated by methods that receive it as a request field for presence of all mandatory parameters and tags, and for validity of all parameters.</p><h4 id=secrets>Secrets</h4><p>The <code>MachineClass</code> resource also contains a <code>secretRef</code> field that contains a reference to a secret. The keys of this secret are passed in the <code>Secrets</code> request field to CMI methods.</p><p>The secret can contain sensitive data such as</p><ul><li><code>cloud-credentials</code> secret data used to authenticate at the provider</li><li><code>cloud-init</code> scripts used to initialize a new VM. The cloud-init script is expected to contain scripts to initialize the Kubelet and make it join the cluster.</li></ul><h4 id=identifying-cluster-machines>Identifying Cluster Machines</h4><p>To implement certain methods, the provider should be able to identify all machines associated with a particular Kubernetes cluster. This can be achieved using one/more of the below mentioned ways:</p><ul><li>Names of VMs created by the provider are prefixed by the cluster ID specified in the ProviderSpec.</li><li>VMs created by the provider are tagged with the special tags like <code>kubernetes.io/cluster</code> (for the cluster ID) and <code>kubernetes.io/role</code> (for the role), specified in the ProviderSpec.</li><li>Mapping <code>Resource Groups</code> to individual cluster.</li></ul><h3 id=error-scheme>Error Scheme</h3><p>All provider API calls defined in this spec MUST return a <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error status</a>, which is very similar to <a href=https://github.com/grpc/grpc/blob/master/src/proto/grpc/status/status.proto>standard machine status</a>.</p><h3 id=machine-provider-interface>Machine Provider Interface</h3><ul><li>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</li><li>The provider SHOULD have a unique way to map the <code>ProviderSpec</code> of a machine-class to a unique <code>Cluster</code>. This avoids deletion of other machines, not backed by the MCM.</li></ul><h4 id=createmachine><code>CreateMachine</code></h4><p>A Provider is REQUIRED to implement this interface method.
This interface method will be called by the MCM to provision a new VM on behalf of the requesting machine object.</p><ul><li><p>This call requests the provider to create a VM backing the machine-object.</p></li><li><p>If VM backing the <code>Machine.Name</code> already exists, and is compatible with the specified <code>Machine</code> object in the <code>CreateMachineRequest</code>, the Provider MUST reply <code>0 OK</code> with the corresponding <code>CreateMachineResponse</code>.</p></li><li><p>The provider can OPTIONALLY make use of the MachineClass supplied in the <code>MachineClass</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the secrets supplied in the <code>Secret</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code>. This could be implicitly provided by the provider by letting you set VM-names (or) could be explicitly specified by the provider using appropriate tags to map the same.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The <code>CreateMachineResponse</code> returned by this method is expected to return</p><ul><li><code>ProviderID</code> that uniquely identifys the VM at the provider. This is expected to match with the node.Spec.ProviderID on the node object.</li><li><code>NodeName</code> that is the expected name of the machine when it joins the cluster. It must match with the node name.</li><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// CreateMachine call is responsible for VM creation on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>CreateMachine(context.Context, <span>*</span>CreateMachineRequest) (<span>*</span>CreateMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineRequest is the create request for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be created
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineResponse is the create response for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=createmachine-errors>CreateMachine Errors</h5><p>If the provider is unable to complete the CreateMachine call successfully, it MUST return a non-ok ginterface method code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in creating/adopting a VM that matches supplied creation request. The <code>CreateMachineResponse</code> is returned with desired values</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and <code>ProviderSpec</code>. Make sure all parameters are in permitted range of values. Exact issue to be given in <code>.message</code></td><td>Update providerSpec to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>6 ALREADY_EXISTS</td><td>Already exists but desired parameters doesn&rsquo;t match</td><td>Parameters of the existing VM don&rsquo;t match the ProviderSpec</td><td>Create machine with a different name</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to create an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>8 RESOURCE_EXHAUSTED</td><td>Resource limits have been reached</td><td>The requestor doesn&rsquo;t have enough resource limits to process this creation request</td><td>Enhance resource limits associated with the user/account to process this</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Resources were out of range</td><td>The requested number of CPUs, memory size, of FS size in ProviderSpec falls outside of the corresponding valid range</td><td>Update request paramaters to request valid resource requests</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=deletemachine><code>DeleteMachine</code></h4><p>A Provider is REQUIRED to implement this driver call.
This driver call will be called by the MCM to deprovision/delete/terminate a VM backed by the requesting machine object.</p><ul><li><p>If a VM corresponding to the specified machine-object&rsquo;s name does not exist or the artifacts associated with the VM do not exist anymore (after deletion), the Provider MUST reply <code>0 OK</code>.</p></li><li><p>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</p></li><li><p>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>DeleteMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALY make use of the <code>Spec.ProviderID</code> map in the <code>Machine</code> object.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The provider must have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</p></li><li><p>The <code>DeleteMachineResponse</code> returned by this method is expected to return</p><ul><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// DeleteMachine call is responsible for VM deletion/termination on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>DeleteMachine(context.Context, <span>*</span>DeleteMachineRequest) (<span>*</span>DeleteMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineRequest is the delete request for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be deleted
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineResponse is the delete response for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=deletemachine-errors>DeleteMachine Errors</h5><p>If the provider is unable to complete the DeleteMachine call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in deleting a VM that matches supplied deletion request.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to delete an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getmachinestatus><code>GetMachineStatus</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This call will be invoked by the MC to get the status of a machine.
This optional driver call helps in optimizing the working of the provider by avoiding unwanted calls to <code>CreateMachine()</code> and <code>DeleteMachine()</code>.</p><ul><li>If a VM corresponding to the specified machine object&rsquo;s <code>Machine.Name</code> exists on provider the <code>GetMachineStatusResponse</code> fields are to be filled similar to the <code>CreateMachineResponse</code>.</li><li>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>GetMachineStatusRequest</code> to communicate with the provider.</li><li>The provider can OPTIONALY make use of the VM unique ID (returned by the provider on machine creation) passed in the <code>ProviderID</code> map in the <code>GetMachineStatusRequest</code>.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetMachineStatus call get&#39;s the status of the VM backing the machine object on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>GetMachineStatus(context.Context, <span>*</span>GetMachineStatusRequest) (<span>*</span>GetMachineStatusResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusRequest is the get request for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM status is to be fetched
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusResponse is the get response for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getmachinestatus-errors>GetMachineStatus Errors</h5><p>If the provider is unable to complete the GetMachineStatus call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in getting machine details for given machine <code>Machine.Name</code></td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>5 NOT_FOUND</td><td>Machine isn&rsquo;t found at provider</td><td>The machine could not be found at provider</td><td>Not required</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to get details for the VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Multiple VMs found</td><td>Multiple VMs found with matching machine object names</td><td>Orphan VM handler to cleanup orphan VMs / Manual intervention maybe required if orphan VM handler isn&rsquo;t enabled.</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=listmachines><code>ListMachines</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
The Provider SHALL return the information about all the machines associated with the <code>MachineClass</code>.
Make sure to use appropriate filters to achieve the same to avoid data transfer overheads.
This optional driver call helps in cleaning up orphan VMs present in the cluster. If not implemented, any orphan VM that might have been created incorrectly by the MCM/Provider (due to bugs in code/infra) might require manual clean up.</p><ul><li>If the Provider succeeded in returning a list of <code>Machine.Name</code> with their corresponding <code>ProviderID</code>, then return <code>0 OK</code>.</li><li>The <code>ListMachineResponse</code> contains a map of <code>MachineList</code> whose<ul><li>Key is expected to contain the <code>ProviderID</code> &</li><li>Value is expected to contain the <code>Machine.Name</code> corresponding to it&rsquo;s kubernetes machine CR object</li></ul></li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>ListMachinesRequest</code> to communicate with the provider.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// ListMachines lists all the machines that might have been created by the supplied machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>ListMachines(context.Context, <span>*</span>ListMachinesRequest) (<span>*</span>ListMachinesResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesRequest is the request object to get a list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesResponse is the response object of the list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineList is the map of list of machines. Format for the map should be &lt;ProviderID, MachineName&gt;.
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineList map[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=listmachines-errors>ListMachines Errors</h5><p>If the provider is unable to complete the ListMachines call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call for listing all VMs associated with <code>ProviderSpec</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>ProviderSpec</code> and make sure that all required fields are present in their desired value format. Exact issue to be given in <code>.message</code></td><td>Update <code>ProviderSpec</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to list VMs and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getvolumeids><code>GetVolumeIDs</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the MCM to get the <code>VolumeIDs</code> for the list of <code>PersistentVolumes (PVs)</code> supplied.
This OPTIONAL (but recommended) driver call helps in serailzied eviction of pods with PVs while draining of machines. This implies applications backed by PVs would be evicted one by one, leading to shorter application downtimes.</p><ul><li>On succesful returnal of a list of <code>Volume-IDs</code> for all supplied <code>PVSpecs</code>, the Provider MUST reply <code>0 OK</code>.</li><li>The <code>GetVolumeIDsResponse</code> is expected to return a repeated list of <code>strings</code> consisting of the <code>VolumeIDs</code> for <code>PVSpec</code> that could be extracted.</li><li>If for any <code>PV</code> the Provider wasn&rsquo;t able to identify the <code>Volume-ID</code>, the provider MAY chose to ignore it and return the <code>Volume-IDs</code> for the rest of the <code>PVs</code> for whom the <code>Volume-ID</code> was found.</li><li>Getting the <code>VolumeID</code> from the <code>PVSpec</code> depends on the Cloud-provider. You can extract this information by parsing the <code>PVSpec</code> based on the <code>ProviderType</code><ul><li><a href=https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339>https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339</a></li><li><a href=https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257>https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257</a></li></ul></li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetVolumeIDsRequest is the request object to get a list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// PVSpecsList is a list of PV specs for whom volume-IDs are required
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Plugin should parse this raw data into pre-defined list of PVSpecs
</span></span></span><span style=display:flex><span><span style=color:green></span>	PVSpecs []<span>*</span>corev1.PersistentVolumeSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetVolumeIDsResponse is the response object of the list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// VolumeIDs is a list of VolumeIDs.
</span></span></span><span style=display:flex><span><span style=color:green></span>	VolumeIDs []<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getvolumeids-errors>GetVolumeIDs Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call getting list of <code>VolumeIDs</code> for the list of <code>PersistentVolumes</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>PVSpecList</code> and make sure that it is in the desired format. Exact issue to be given in <code>.message</code></td><td>Update <code>PVSpecList</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=generatemachineclassformigration><code>GenerateMachineClassForMigration</code></h4><p>A Provider SHOULD implement this driver call, else it MUST return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the Machine Controller to try to perform a machineClass migration for an unknown machineClass Kind. This helps in migration of one kind of machineClass to another kind. For instance an machineClass custom resource of <code>AWSMachineClass</code> to <code>MachineClass</code>.</p><ul><li>On successful generation of machine class the Provider MUST reply <code>0 OK</code> (or) <code>nil</code> error.</li><li><code>GenerateMachineClassForMigrationRequest</code> expects the provider-specific machine class (eg. AWSMachineClass)
to be supplied as the <code>ProviderSpecificMachineClass</code>. The provider is responsible for unmarshalling the golang struct. It also passes a reference to an existing <code>MachineClass</code> object.</li><li>The provider is expected to fill in this<code>MachineClass</code> object based on the conversions.</li><li>An optional <code>ClassSpec</code> containing the <code>type ClassSpec struct</code> is also provided to decode the provider info.</li><li><code>GenerateMachineClassForMigration</code> is only responsible for filling up the passed <code>MachineClass</code> object.</li><li>The task of creating the new <code>CR</code> of the new kind (MachineClass) with the same name as the previous one and also annotating the old machineClass CR with a migrated annotation and migrating existing references is done by the calling library implicitly.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GenerateMachineClassForMigrationRequest is the request for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderSpecificMachineClass is provider specfic machine class object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// E.g. AWSMachineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderSpecificMachineClass interface{}<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass is the machine class object generated that is to be filled up
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ClassSpec contains the class spec object to determine the machineClass kind
</span></span></span><span style=display:flex><span><span style=color:green></span>	ClassSpec <span>*</span>v1alpha1.ClassSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GenerateMachineClassForMigrationResponse is the response for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationResponse struct{}<span>
</span></span></span></code></pre></div><h5 id=migratemachineclass-errors>MigrateMachineClass Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>Migration of provider specific machine class was successful</td><td>Machine reconcilation is retried once the new class has been created</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this provider.</td><td>None</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Might need manual intervension to fix this</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h2 id=configuration-and-operation>Configuration and Operation</h2><h3 id=supervised-lifecycle-management>Supervised Lifecycle Management</h3><ul><li>For Providers packaged in software form:<ul><li>Provider Packages SHOULD use a well-documented container image format (e.g., Docker, OCI).</li><li>The chosen package image format MAY expose configurable Provider properties as environment variables, unless otherwise indicated in the section below.
Variables so exposed SHOULD be assigned default values in the image manifest.</li><li>A Provider Supervisor MAY programmatically evaluate or otherwise scan a Provider Package’s image manifest in order to discover configurable environment variables.</li><li>A Provider SHALL NOT assume that an operator or Provider Supervisor will scan an image manifest for environment variables.</li></ul></li></ul><h4 id=environment-variables>Environment Variables</h4><ul><li>Variables defined by this specification SHALL be identifiable by their <code>MC_</code> name prefix.</li><li>Configuration properties not defined by the MC specification SHALL NOT use the same <code>MC_</code> name prefix; this prefix is reserved for common configuration properties defined by the MC specification.</li><li>The Provider Supervisor SHOULD supply all RECOMMENDED MC environment variables to a Provider.</li><li>The Provider Supervisor SHALL supply all REQUIRED MC environment variables to a Provider.</li></ul><h5 id=logging>Logging</h5><ul><li>Providers SHOULD generate log messages to ONLY standard output and/or standard error.<ul><li>In this case the Provider Supervisor SHALL assume responsibility for all log lifecycle management.</li></ul></li><li>Provider implementations that deviate from the above recommendation SHALL clearly and unambiguously document the following:<ul><li>Logging configuration flags and/or variables, including working sample configurations.</li><li>Default log destination(s) (where do the logs go if no configuration is specified?)</li><li>Log lifecycle management ownership and related guidance (size limits, rate limits, rolling, archiving, expunging, etc.) applicable to the logging mechanism embedded within the Provider.</li></ul></li><li>Providers SHOULD NOT write potentially sensitive data to logs (e.g. secrets).</li></ul><h5 id=available-services>Available Services</h5><ul><li>Provider Packages MAY support all or a subset of CMI services; service combinations MAY be configurable at runtime by the Provider Supervisor.<ul><li>This specification does not dictate the mechanism by which mode of operation MUST be discovered, and instead places that burden upon the VM Provider.</li></ul></li><li>Misconfigured provider software SHOULD fail-fast with an OS-appropriate error code.</li></ul><h5 id=linux-capabilities>Linux Capabilities</h5><ul><li>Providers SHOULD clearly document any additionally required capabilities and/or security context.</li></ul><h5 id=cgroup-isolation>Cgroup Isolation</h5><ul><li>A Provider MAY be constrained by cgroups.</li></ul><h5 id=resource-requirements>Resource Requirements</h5><ul><li>VM Providers SHOULD unambiguously document all of a Provider’s resource requirements.</li></ul><h3 id=deploying>Deploying</h3><ul><li><strong>Recommended:</strong> The MCM and Provider are typically expected to run as two containers inside a common <code>Pod</code>.</li><li>However, for the security reasons they could execute on seperate Pods provided they have a secure way to exchange data between them.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a6210f5d3bf41a2f0b40446a5b76c5f0>8.2.1.2.5 - Testing And Dependencies</h1><h2 id=dependency-management>Dependency management</h2><p>We use golang modules to manage golang dependencies. In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h3 id=updating-dependencies>Updating dependencies</h3><p>The <code>Makefile</code> contains a rule called <code>revendor</code> which performs <code>go mod vendor</code> and <code>go mod tidy</code>.</p><p><code>go mod vendor</code> resets the main module&rsquo;s vendor directory to include all packages needed to build and test all the main module&rsquo;s packages. It does not include test code for vendored packages.</p><p><code>go mod tidy</code> makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make revendor
</span></span></code></pre></div><p>The dependencies are installed into the <code>vendor</code> folder which <strong>should be added</strong> to the VCS.</p><p>⚠️ Make sure you test the code after you have updated the dependencies!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ff5509967d10ad3ac61f9e258aa089ff>8.2.1.3 - Documents</h1></div><div class=td-content><h1 id=pg-c19da9fea261cce3245d5ec5e9021278>8.2.1.3.1 - Apis</h1><h2 id=specification>Specification</h2><h3 id=providerspec-schema>ProviderSpec Schema</h3><br><h3 id=AWSMachineClass><b>AWSMachineClass</b></h3><p><p>AWSMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>AWSMachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#AWSMachineClassSpec>AWSMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>ami</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>blockDevices</code></td><td><em><a href=#AWSBlockDeviceMappingSpec>[]AWSBlockDeviceMappingSpec</a></em></td><td></td></tr><tr><td><code>ebsOptimized</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>iam</code></td><td><em><a href=#AWSIAMProfileSpec>AWSIAMProfileSpec</a></em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>keyName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>monitoring</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>networkInterfaces</code></td><td><em><a href=#AWSNetworkInterfaceSpec>[]AWSNetworkInterfaceSpec</a></em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>spotPrice</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=AlicloudMachineClass><b>AlicloudMachineClass</b></h3><p><p>AlicloudMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>AlicloudMachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#AlicloudMachineClassSpec>AlicloudMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>imageID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>instanceType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>zoneID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>securityGroupID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>vSwitchID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>privateIPAddress</code></td><td><em>string</em></td><td></td></tr><tr><td><code>systemDisk</code></td><td><em><a href=#AlicloudSystemDisk>AlicloudSystemDisk</a></em></td><td></td></tr><tr><td><code>dataDisks</code></td><td><em><a href=#AlicloudDataDisk>[]AlicloudDataDisk</a></em></td><td></td></tr><tr><td><code>instanceChargeType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>internetChargeType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>internetMaxBandwidthIn</code></td><td><em>*int</em></td><td></td></tr><tr><td><code>internetMaxBandwidthOut</code></td><td><em>*int</em></td><td></td></tr><tr><td><code>spotStrategy</code></td><td><em>string</em></td><td></td></tr><tr><td><code>IoOptimized</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>keyPairName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=AzureMachineClass><b>AzureMachineClass</b></h3><p><p>AzureMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>AzureMachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#AzureMachineClassSpec>AzureMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>location</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>properties</code></td><td><em><a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a></em></td><td></td></tr><tr><td><code>resourceGroup</code></td><td><em>string</em></td><td></td></tr><tr><td><code>subnetInfo</code></td><td><em><a href=#AzureSubnetInfo>AzureSubnetInfo</a></em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=GCPMachineClass><b>GCPMachineClass</b></h3><p><p>GCPMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>GCPMachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#GCPMachineClassSpec>GCPMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>canIpForward</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>deletionProtection</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>description</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>disks</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPDisk>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPDisk</a></em></td><td></td></tr><tr><td><code>labels</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>metadata</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPMetadata>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPMetadata</a></em></td><td></td></tr><tr><td><code>networkInterfaces</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPNetworkInterface>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPNetworkInterface</a></em></td><td></td></tr><tr><td><code>scheduling</code></td><td><em><a href=#GCPScheduling>GCPScheduling</a></em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>serviceAccounts</code></td><td><em><a href=#GCPServiceAccount>[]GCPServiceAccount</a></em></td><td></td></tr><tr><td><code>tags</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>zone</code></td><td><em>string</em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=Machine><b>Machine</b></h3><p><p>Machine is the representation of a physical or virtual machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>Machine</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>ObjectMeta for machine object</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSpec>MachineSpec</a></em></td><td><p>Spec contains the specification of the machine</p><br><br><table><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineStatus>MachineStatus</a></em></td><td><p>Status contains fields depicting the status</p></td></tr></tbody></table><br><h3 id=MachineClass><b>MachineClass</b></h3><p><p>MachineClass can be used to templatize and re-use provider configuration
across multiple Machines / MachineSets / MachineDeployments.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplate>NodeTemplate</a></em></td><td><em>(Optional)</em><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>CredentialsSecretRef can optionally store the credentials (in this case the SecretRef does not need to store them).
This might be useful if multiple machine classes with the same credentials but different user-datas are used.</p></td></tr><tr><td><code>providerSpec</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fruntime%23RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Provider-specific configuration to use during node creation.</p></td></tr><tr><td><code>provider</code></td><td><em>string</em></td><td><p>Provider is the combination of name and location of cloud-specific drivers.</p></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef stores the necessary secrets such as credentials or userdata.</p></td></tr></tbody></table><br><h3 id=MachineDeployment><b>MachineDeployment</b></h3><p><p>MachineDeployment enables declarative updates for machines and MachineSets.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineDeployment</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineDeploymentSpec>MachineDeploymentSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the MachineDeployment.</p><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=#RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineDeploymentStatus>MachineDeploymentStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the MachineDeployment.</p></td></tr></tbody></table><br><h3 id=MachineSet><b>MachineSet</b></h3><p><p>MachineSet TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineSet</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSetSpec>MachineSetSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineSetStatus>MachineSetStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=PacketMachineClass><b>PacketMachineClass</b></h3><p><p>PacketMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>PacketMachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#PacketMachineClassSpec>PacketMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>facility</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>billingCycle</code></td><td><em>string</em></td><td></td></tr><tr><td><code>OS</code></td><td><em>string</em></td><td></td></tr><tr><td><code>projectID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>sshKeys</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>userdata</code></td><td><em>string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=AWSBlockDeviceMappingSpec><b>AWSBlockDeviceMappingSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AWSMachineClassSpec>AWSMachineClassSpec</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>deviceName</code></td><td><em>string</em></td><td><p>The device name exposed to the machine (for example, /dev/sdh or xvdh).</p></td></tr><tr><td><code>ebs</code></td><td><em><a href=#AWSEbsBlockDeviceSpec>AWSEbsBlockDeviceSpec</a></em></td><td><p>Parameters used to automatically set up EBS volumes when the machine is
launched.</p></td></tr><tr><td><code>noDevice</code></td><td><em>string</em></td><td><p>Suppresses the specified device included in the block device mapping of the
AMI.</p></td></tr><tr><td><code>virtualName</code></td><td><em>string</em></td><td><p>The virtual device name (ephemeralN). Machine store volumes are numbered
starting from 0. An machine type with 2 available machine store volumes
can specify mappings for ephemeral0 and ephemeral1.The number of available
machine store volumes depends on the machine type. After you connect to
the machine, you must mount the volume.</p><p>Constraints: For M3 machines, you must specify machine store volumes in
the block device mapping for the machine. When you launch an M3 machine,
we ignore any machine store volumes specified in the block device mapping
for the AMI.</p></td></tr></tbody></table><br><h3 id=AWSEbsBlockDeviceSpec><b>AWSEbsBlockDeviceSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AWSBlockDeviceMappingSpec>AWSBlockDeviceMappingSpec</a>)</p><p><p>Describes a block device for an EBS volume.
Please also see <a href=https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/EbsBlockDevice>https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/EbsBlockDevice</a></p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>deleteOnTermination</code></td><td><em>*bool</em></td><td><p>Indicates whether the EBS volume is deleted on machine termination.</p></td></tr><tr><td><code>encrypted</code></td><td><em>bool</em></td><td><p>Indicates whether the EBS volume is encrypted. Encrypted Amazon EBS volumes
may only be attached to machines that support Amazon EBS encryption.</p></td></tr><tr><td><code>iops</code></td><td><em>int64</em></td><td><p>The number of I/O operations per second (IOPS) that the volume supports.
For io1, this represents the number of IOPS that are provisioned for the
volume. For gp2, this represents the baseline performance of the volume and
the rate at which the volume accumulates I/O credits for bursting. For more
information about General Purpose SSD baseline performance, I/O credits,
and bursting, see Amazon EBS Volume Types (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>)
in the Amazon Elastic Compute Cloud User Guide.</p><p>Constraint: Range is 100-20000 IOPS for io1 volumes and 100-10000 IOPS for
gp2 volumes.</p><p>Condition: This parameter is required for requests to create io1 volumes;
it is not used in requests to create gp2, st1, sc1, or standard volumes.</p></td></tr><tr><td><code>kmsKeyID</code></td><td><em>*string</em></td><td><p>Identifier (key ID, key alias, ID ARN, or alias ARN) for a customer managed
CMK under which the EBS volume is encrypted.</p><p>This parameter is only supported on BlockDeviceMapping objects called by
RunInstances (<a href=https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html>https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html</a>),
RequestSpotFleet (<a href=https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotFleet.html>https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotFleet.html</a>),
and RequestSpotInstances (<a href=https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotInstances.html>https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotInstances.html</a>).</p></td></tr><tr><td><code>snapshotID</code></td><td><em>*string</em></td><td><p>The ID of the snapshot.</p></td></tr><tr><td><code>volumeSize</code></td><td><em>int64</em></td><td><p>The size of the volume, in GiB.</p><p>Constraints: 1-16384 for General Purpose SSD (gp2), 4-16384 for Provisioned
IOPS SSD (io1), 500-16384 for Throughput Optimized HDD (st1), 500-16384 for
Cold HDD (sc1), and 1-1024 for Magnetic (standard) volumes. If you specify
a snapshot, the volume size must be equal to or larger than the snapshot
size.</p><p>Default: If you&rsquo;re creating the volume from a snapshot and don&rsquo;t specify
a volume size, the default is the snapshot size.</p></td></tr><tr><td><code>volumeType</code></td><td><em>string</em></td><td><p>The volume type: gp2, io1, st1, sc1, or standard.</p><p>Default: standard</p></td></tr></tbody></table><br><h3 id=AWSIAMProfileSpec><b>AWSIAMProfileSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AWSMachineClassSpec>AWSMachineClassSpec</a>)</p><p><p>Describes an IAM machine profile.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>arn</code></td><td><em>string</em></td><td><p>The Amazon Resource Name (ARN) of the machine profile.</p></td></tr><tr><td><code>name</code></td><td><em>string</em></td><td><p>The name of the machine profile.</p></td></tr></tbody></table><br><h3 id=AWSMachineClassSpec><b>AWSMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AWSMachineClass>AWSMachineClass</a>)</p><p><p>AWSMachineClassSpec is the specification of a AWSMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>ami</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>blockDevices</code></td><td><em><a href=#AWSBlockDeviceMappingSpec>[]AWSBlockDeviceMappingSpec</a></em></td><td></td></tr><tr><td><code>ebsOptimized</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>iam</code></td><td><em><a href=#AWSIAMProfileSpec>AWSIAMProfileSpec</a></em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>keyName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>monitoring</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>networkInterfaces</code></td><td><em><a href=#AWSNetworkInterfaceSpec>[]AWSNetworkInterfaceSpec</a></em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>spotPrice</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></tbody></table><br><h3 id=AWSNetworkInterfaceSpec><b>AWSNetworkInterfaceSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AWSMachineClassSpec>AWSMachineClassSpec</a>)</p><p><p>Describes a network interface.
Please also see <a href=https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/MachineAWSNetworkInterfaceSpecification>https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/MachineAWSNetworkInterfaceSpecification</a></p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>associatePublicIPAddress</code></td><td><em>*bool</em></td><td><p>Indicates whether to assign a public IPv4 address to an machine you launch
in a VPC. The public IP address can only be assigned to a network interface
for eth0, and can only be assigned to a new network interface, not an existing
one. You cannot specify more than one network interface in the request. If
launching into a default subnet, the default value is true.</p></td></tr><tr><td><code>deleteOnTermination</code></td><td><em>*bool</em></td><td><p>If set to true, the interface is deleted when the machine is terminated.
You can specify true only if creating a new network interface when launching
an machine.</p></td></tr><tr><td><code>description</code></td><td><em>*string</em></td><td><p>The description of the network interface. Applies only if creating a network
interface when launching an machine.</p></td></tr><tr><td><code>securityGroupIDs</code></td><td><em>[]string</em></td><td><p>The IDs of the security groups for the network interface. Applies only if
creating a network interface when launching an machine.</p></td></tr><tr><td><code>subnetID</code></td><td><em>string</em></td><td><p>The ID of the subnet associated with the network string. Applies only if
creating a network interface when launching an machine.</p></td></tr></tbody></table><br><h3 id=AlicloudDataDisk><b>AlicloudDataDisk</b></h3><p>(<em>Appears on:</em>
<a href=#AlicloudMachineClassSpec>AlicloudMachineClassSpec</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td></td></tr><tr><td><code>category</code></td><td><em>string</em></td><td></td></tr><tr><td><code>description</code></td><td><em>string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>encrypted</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>deleteWithInstance</code></td><td><em>*bool</em></td><td></td></tr><tr><td><code>size</code></td><td><em>int</em></td><td></td></tr></tbody></table><br><h3 id=AlicloudMachineClassSpec><b>AlicloudMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AlicloudMachineClass>AlicloudMachineClass</a>)</p><p><p>AlicloudMachineClassSpec is the specification of a AlicloudMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>imageID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>instanceType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>zoneID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>securityGroupID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>vSwitchID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>privateIPAddress</code></td><td><em>string</em></td><td></td></tr><tr><td><code>systemDisk</code></td><td><em><a href=#AlicloudSystemDisk>AlicloudSystemDisk</a></em></td><td></td></tr><tr><td><code>dataDisks</code></td><td><em><a href=#AlicloudDataDisk>[]AlicloudDataDisk</a></em></td><td></td></tr><tr><td><code>instanceChargeType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>internetChargeType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>internetMaxBandwidthIn</code></td><td><em>*int</em></td><td></td></tr><tr><td><code>internetMaxBandwidthOut</code></td><td><em>*int</em></td><td></td></tr><tr><td><code>spotStrategy</code></td><td><em>string</em></td><td></td></tr><tr><td><code>IoOptimized</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>keyPairName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></tbody></table><br><h3 id=AlicloudSystemDisk><b>AlicloudSystemDisk</b></h3><p>(<em>Appears on:</em>
<a href=#AlicloudMachineClassSpec>AlicloudMachineClassSpec</a>)</p><p><p>AlicloudSystemDisk describes SystemDisk for Alicloud.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>category</code></td><td><em>string</em></td><td></td></tr><tr><td><code>size</code></td><td><em>int</em></td><td></td></tr></tbody></table><br><h3 id=AzureDataDisk><b>AzureDataDisk</b></h3><p>(<em>Appears on:</em>
<a href=#AzureStorageProfile>AzureStorageProfile</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td></td></tr><tr><td><code>lun</code></td><td><em>*int32</em></td><td></td></tr><tr><td><code>caching</code></td><td><em>string</em></td><td></td></tr><tr><td><code>storageAccountType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>diskSizeGB</code></td><td><em>int32</em></td><td></td></tr></tbody></table><br><h3 id=AzureHardwareProfile><b>AzureHardwareProfile</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureHardwareProfile is specifies the hardware settings for the virtual machine.
Refer github.com/Azure/azure-sdk-for-go/arm/compute/models.go for VMSizes</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vmSize</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureImageReference><b>AzureImageReference</b></h3><p>(<em>Appears on:</em>
<a href=#AzureStorageProfile>AzureStorageProfile</a>)</p><p><p>AzureImageReference is specifies information about the image to use. You can specify information about platform images,
marketplace images, or virtual machine images. This element is required when you want to use a platform image,
marketplace image, or virtual machine image, but is not used in other creation operations.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr><tr><td><code>urn</code></td><td><em>*string</em></td><td><p>Uniform Resource Name of the OS image to be used , it has the format &lsquo;publisher:offer:sku:version&rsquo;</p></td></tr></tbody></table><br><h3 id=AzureLinuxConfiguration><b>AzureLinuxConfiguration</b></h3><p>(<em>Appears on:</em>
<a href=#AzureOSProfile>AzureOSProfile</a>)</p><p><p>AzureLinuxConfiguration is specifies the Linux operating system settings on the virtual machine.<br><br>For a list of
supported Linux distributions, see <a href="https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-endorsed-distros?toc=%2fazure%2fvirtual-machines%2flinux%2ftoc.json">Linux on Azure-Endorsed
Distributions</a><br><br>For running non-endorsed distributions, see <a href="https://docs.microsoft.com/azure/virtual-machines/virtual-machines-linux-create-upload-generic?toc=%2fazure%2fvirtual-machines%2flinux%2ftoc.json">Information for Non-Endorsed
Distributions</a>.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>disablePasswordAuthentication</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>ssh</code></td><td><em><a href=#AzureSSHConfiguration>AzureSSHConfiguration</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureMachineClassSpec><b>AzureMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#AzureMachineClass>AzureMachineClass</a>)</p><p><p>AzureMachineClassSpec is the specification of a AzureMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>location</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>properties</code></td><td><em><a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a></em></td><td></td></tr><tr><td><code>resourceGroup</code></td><td><em>string</em></td><td></td></tr><tr><td><code>subnetInfo</code></td><td><em><a href=#AzureSubnetInfo>AzureSubnetInfo</a></em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureMachineSetConfig><b>AzureMachineSetConfig</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureMachineSetConfig contains the information about the machine set</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr><tr><td><code>kind</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureManagedDiskParameters><b>AzureManagedDiskParameters</b></h3><p>(<em>Appears on:</em>
<a href=#AzureOSDisk>AzureOSDisk</a>)</p><p><p>AzureManagedDiskParameters is the parameters of a managed disk.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr><tr><td><code>storageAccountType</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureNetworkInterfaceReference><b>AzureNetworkInterfaceReference</b></h3><p>(<em>Appears on:</em>
<a href=#AzureNetworkProfile>AzureNetworkProfile</a>)</p><p><p>AzureNetworkInterfaceReference is describes a network interface reference.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr><tr><td><code>properties</code></td><td><em><a href=#AzureNetworkInterfaceReferenceProperties>AzureNetworkInterfaceReferenceProperties</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureNetworkInterfaceReferenceProperties><b>AzureNetworkInterfaceReferenceProperties</b></h3><p>(<em>Appears on:</em>
<a href=#AzureNetworkInterfaceReference>AzureNetworkInterfaceReference</a>)</p><p><p>AzureNetworkInterfaceReferenceProperties is describes a network interface reference properties.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>primary</code></td><td><em>bool</em></td><td></td></tr></tbody></table><br><h3 id=AzureNetworkProfile><b>AzureNetworkProfile</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureNetworkProfile is specifies the network interfaces of the virtual machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>networkInterfaces</code></td><td><em><a href=#AzureNetworkInterfaceReference>AzureNetworkInterfaceReference</a></em></td><td></td></tr><tr><td><code>acceleratedNetworking</code></td><td><em>*bool</em></td><td></td></tr></tbody></table><br><h3 id=AzureOSDisk><b>AzureOSDisk</b></h3><p>(<em>Appears on:</em>
<a href=#AzureStorageProfile>AzureStorageProfile</a>)</p><p><p>AzureOSDisk is specifies information about the operating system disk used by the virtual machine.<br><br>For more
information about disks, see <a href="https://docs.microsoft.com/azure/virtual-machines/virtual-machines-windows-about-disks-vhds?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json">About disks and VHDs for Azure virtual
machines</a>.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td></td></tr><tr><td><code>caching</code></td><td><em>string</em></td><td></td></tr><tr><td><code>managedDisk</code></td><td><em><a href=#AzureManagedDiskParameters>AzureManagedDiskParameters</a></em></td><td></td></tr><tr><td><code>diskSizeGB</code></td><td><em>int32</em></td><td></td></tr><tr><td><code>createOption</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureOSProfile><b>AzureOSProfile</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureOSProfile is specifies the operating system settings for the virtual machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>computerName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>adminUsername</code></td><td><em>string</em></td><td></td></tr><tr><td><code>adminPassword</code></td><td><em>string</em></td><td></td></tr><tr><td><code>customData</code></td><td><em>string</em></td><td></td></tr><tr><td><code>linuxConfiguration</code></td><td><em><a href=#AzureLinuxConfiguration>AzureLinuxConfiguration</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureSSHConfiguration><b>AzureSSHConfiguration</b></h3><p>(<em>Appears on:</em>
<a href=#AzureLinuxConfiguration>AzureLinuxConfiguration</a>)</p><p><p>AzureSSHConfiguration is SSH configuration for Linux based VMs running on Azure</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>publicKeys</code></td><td><em><a href=#AzureSSHPublicKey>AzureSSHPublicKey</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureSSHPublicKey><b>AzureSSHPublicKey</b></h3><p>(<em>Appears on:</em>
<a href=#AzureSSHConfiguration>AzureSSHConfiguration</a>)</p><p><p>AzureSSHPublicKey is contains information about SSH certificate public key and the path on the Linux VM where the public
key is placed.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>path</code></td><td><em>string</em></td><td></td></tr><tr><td><code>keyData</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureStorageProfile><b>AzureStorageProfile</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureStorageProfile is specifies the storage settings for the virtual machine disks.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>imageReference</code></td><td><em><a href=#AzureImageReference>AzureImageReference</a></em></td><td></td></tr><tr><td><code>osDisk</code></td><td><em><a href=#AzureOSDisk>AzureOSDisk</a></em></td><td></td></tr><tr><td><code>dataDisks</code></td><td><em><a href=#AzureDataDisk>[]AzureDataDisk</a></em></td><td></td></tr></tbody></table><br><h3 id=AzureSubResource><b>AzureSubResource</b></h3><p>(<em>Appears on:</em>
<a href=#AzureVirtualMachineProperties>AzureVirtualMachineProperties</a>)</p><p><p>AzureSubResource is the Sub Resource definition.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureSubnetInfo><b>AzureSubnetInfo</b></h3><p>(<em>Appears on:</em>
<a href=#AzureMachineClassSpec>AzureMachineClassSpec</a>)</p><p><p>AzureSubnetInfo is the information containing the subnet details</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vnetName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>vnetResourceGroup</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>subnetName</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=AzureVirtualMachineProperties><b>AzureVirtualMachineProperties</b></h3><p>(<em>Appears on:</em>
<a href=#AzureMachineClassSpec>AzureMachineClassSpec</a>)</p><p><p>AzureVirtualMachineProperties is describes the properties of a Virtual Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>hardwareProfile</code></td><td><em><a href=#AzureHardwareProfile>AzureHardwareProfile</a></em></td><td></td></tr><tr><td><code>storageProfile</code></td><td><em><a href=#AzureStorageProfile>AzureStorageProfile</a></em></td><td></td></tr><tr><td><code>osProfile</code></td><td><em><a href=#AzureOSProfile>AzureOSProfile</a></em></td><td></td></tr><tr><td><code>networkProfile</code></td><td><em><a href=#AzureNetworkProfile>AzureNetworkProfile</a></em></td><td></td></tr><tr><td><code>availabilitySet</code></td><td><em><a href=#AzureSubResource>AzureSubResource</a></em></td><td></td></tr><tr><td><code>identityID</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>zone</code></td><td><em>*int</em></td><td></td></tr><tr><td><code>machineSet</code></td><td><em><a href=#AzureMachineSetConfig>AzureMachineSetConfig</a></em></td><td></td></tr></tbody></table><br><h3 id=ClassSpec><b>ClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetSpec>MachineSetSpec</a>,
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>ClassSpec is the class specification of machine</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiGroup</code></td><td><em>string</em></td><td><p>API group to which it belongs</p></td></tr><tr><td><code>kind</code></td><td><em>string</em></td><td><p>Kind for machine class</p></td></tr><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of machine class</p></td></tr></tbody></table><br><h3 id=ConditionStatus><b>ConditionStatus</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentCondition>MachineDeploymentCondition</a>,
<a href=#MachineSetCondition>MachineSetCondition</a>)</p><p></p><br><h3 id=CurrentStatus><b>CurrentStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineStatus>MachineStatus</a>)</p><p><p>CurrentStatus contains information about the current status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></td><td><em><a href=#MachinePhase>MachinePhase</a></em></td><td></td></tr><tr><td><code>timeoutActive</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current status</p></td></tr></tbody></table><br><h3 id=GCPDisk><b>GCPDisk</b></h3><p><p>GCPDisk describes disks for GCP.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>autoDelete</code></td><td><em>*bool</em></td><td></td></tr><tr><td><code>boot</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>sizeGb</code></td><td><em>int64</em></td><td></td></tr><tr><td><code>type</code></td><td><em>string</em></td><td></td></tr><tr><td><code>interface</code></td><td><em>string</em></td><td></td></tr><tr><td><code>image</code></td><td><em>string</em></td><td></td></tr><tr><td><code>labels</code></td><td><em>map[string]string</em></td><td></td></tr></tbody></table><br><h3 id=GCPMachineClassSpec><b>GCPMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#GCPMachineClass>GCPMachineClass</a>)</p><p><p>GCPMachineClassSpec is the specification of a GCPMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>canIpForward</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>deletionProtection</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>description</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>disks</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPDisk>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPDisk</a></em></td><td></td></tr><tr><td><code>labels</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>metadata</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPMetadata>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPMetadata</a></em></td><td></td></tr><tr><td><code>networkInterfaces</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.GCPNetworkInterface>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPNetworkInterface</a></em></td><td></td></tr><tr><td><code>scheduling</code></td><td><em><a href=#GCPScheduling>GCPScheduling</a></em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>serviceAccounts</code></td><td><em><a href=#GCPServiceAccount>[]GCPServiceAccount</a></em></td><td></td></tr><tr><td><code>tags</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>zone</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=GCPMetadata><b>GCPMetadata</b></h3><p><p>GCPMetadata describes metadata for GCP.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>key</code></td><td><em>string</em></td><td></td></tr><tr><td><code>value</code></td><td><em>*string</em></td><td></td></tr></tbody></table><br><h3 id=GCPNetworkInterface><b>GCPNetworkInterface</b></h3><p><p>GCPNetworkInterface describes network interfaces for GCP</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>disableExternalIP</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>network</code></td><td><em>string</em></td><td></td></tr><tr><td><code>subnetwork</code></td><td><em>string</em></td><td></td></tr></tbody></table><br><h3 id=GCPScheduling><b>GCPScheduling</b></h3><p>(<em>Appears on:</em>
<a href=#GCPMachineClassSpec>GCPMachineClassSpec</a>)</p><p><p>GCPScheduling describes scheduling configuration for GCP.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>automaticRestart</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>onHostMaintenance</code></td><td><em>string</em></td><td></td></tr><tr><td><code>preemptible</code></td><td><em>bool</em></td><td></td></tr></tbody></table><br><h3 id=GCPServiceAccount><b>GCPServiceAccount</b></h3><p>(<em>Appears on:</em>
<a href=#GCPMachineClassSpec>GCPMachineClassSpec</a>)</p><p><p>GCPServiceAccount describes service accounts for GCP.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>email</code></td><td><em>string</em></td><td></td></tr><tr><td><code>scopes</code></td><td><em>[]string</em></td><td></td></tr></tbody></table><br><h3 id=LastOperation><b>LastOperation</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetStatus>MachineSetStatus</a>,
<a href=#MachineStatus>MachineStatus</a>,
<a href=#MachineSummary>MachineSummary</a>)</p><p><p>LastOperation suggests the last operation performed on the object</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></td><td><em>string</em></td><td><p>Description of the current operation</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current operation</p></td></tr><tr><td><code>state</code></td><td><em><a href=#MachineState>MachineState</a></em></td><td><p>State of operation</p></td></tr><tr><td><code>type</code></td><td><em><a href=#MachineOperationType>MachineOperationType</a></em></td><td><p>Type of operation</p></td></tr></tbody></table><br><h3 id=MachineConfiguration><b>MachineConfiguration</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>MachineConfiguration describes the configurations useful for the machine-controller.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>drainTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineDraintimeout is the timeout after which machine is forcefully deleted.</p></td></tr><tr><td><code>healthTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.</p></td></tr><tr><td><code>creationTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineCreationTimeout is the timeout after which machinie creation is declared failed.</p></td></tr><tr><td><code>maxEvictRetries</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>MaxEvictRetries is the number of retries that will be attempted while draining the node.</p></td></tr><tr><td><code>nodeConditions</code></td><td><em>*string</em></td><td><em>(Optional)</em><p>NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentCondition><b>MachineDeploymentCondition</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStatus>MachineDeploymentStatus</a>)</p><p><p>MachineDeploymentCondition describes the state of a MachineDeployment at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineDeploymentConditionType>MachineDeploymentConditionType</a></em></td><td><p>Type of MachineDeployment condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=#ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>The last time this condition was updated.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentConditionType><b>MachineDeploymentConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentCondition>MachineDeploymentCondition</a>)</p><p></p><br><h3 id=MachineDeploymentSpec><b>MachineDeploymentSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentSpec is the specification of the desired behavior of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=#RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStatus><b>MachineDeploymentStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentStatus is the most recently observed status of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The generation observed by the MachineDeployment controller.</p></td></tr><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment (their labels match the selector).</p></td></tr><tr><td><code>updatedReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment that have the desired template spec.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of ready machines targeted by this MachineDeployment.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of available machines (ready for at least minReadySeconds) targeted by this MachineDeployment.</p></td></tr><tr><td><code>unavailableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of unavailable machines targeted by this MachineDeployment. This is the total number of
machines that are still required for the MachineDeployment to have 100% available capacity. They may
either be machines that are running but not yet available or machines that still have not been created.</p></td></tr><tr><td><code>conditions</code></td><td><em><a href=#MachineDeploymentCondition>[]MachineDeploymentCondition</a></em></td><td><p>Represents the latest available observations of a MachineDeployment&rsquo;s current state.</p></td></tr><tr><td><code>collisionCount</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>Count of hash collisions for the MachineDeployment. The MachineDeployment controller uses this
field as a collision avoidance mechanism when it needs to create the name for the
newest MachineSet.</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.MachineSummary>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStrategy><b>MachineDeploymentStrategy</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p><p>MachineDeploymentStrategy describes how to replace existing machines with new ones.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineDeploymentStrategyType>MachineDeploymentStrategyType</a></em></td><td><em>(Optional)</em><p>Type of MachineDeployment. Can be &ldquo;Recreate&rdquo; or &ldquo;RollingUpdate&rdquo;. Default is RollingUpdate.</p></td></tr><tr><td><code>rollingUpdate</code></td><td><em><a href=#RollingUpdateMachineDeployment>RollingUpdateMachineDeployment</a></em></td><td><em>(Optional)</em><p>Rolling update config params. Present only if MachineDeploymentStrategyType =</p><h2>RollingUpdate.</h2><p>TODO: Update this to follow our convention for oneOf, whatever we decide it
to be.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStrategyType><b>MachineDeploymentStrategyType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p></p><br><h3 id=MachineOperationType><b>MachineOperationType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#LastOperation>LastOperation</a>)</p><p><p>MachineOperationType is a label for the operation performed on a machine object.</p></p><br><h3 id=MachinePhase><b>MachinePhase</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#CurrentStatus>CurrentStatus</a>)</p><p><p>MachinePhase is a label for the condition of a machines at the current time.</p></p><br><h3 id=MachineSetCondition><b>MachineSetCondition</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetStatus>MachineSetStatus</a>)</p><p><p>MachineSetCondition describes the state of a machine set at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineSetConditionType>MachineSetConditionType</a></em></td><td><p>Type of machine set condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=#ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>The last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><em>(Optional)</em><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><em>(Optional)</em><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=MachineSetConditionType><b>MachineSetConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineSetCondition>MachineSetCondition</a>)</p><p><p>MachineSetConditionType is the condition on machineset object</p></p><br><h3 id=MachineSetSpec><b>MachineSetSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSet>MachineSet</a>)</p><p><p>MachineSetSpec is the specification of a MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=MachineSetStatus><b>MachineSetStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSet>MachineSet</a>)</p><p><p>MachineSetStatus holds the most recently observed status of MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><p>Replicas is the number of actual replicas.</p></td></tr><tr><td><code>fullyLabeledReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of pods that have labels matching the labels of the pod template of the replicaset.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of ready replicas for this replica set.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of available replicas (ready for at least minReadySeconds) for this replica set.</p></td></tr><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed by the controller.</p></td></tr><tr><td><code>machineSetCondition</code></td><td><em><a href=#MachineSetCondition>[]MachineSetCondition</a></em></td><td><em>(Optional)</em><p>Represents the latest available observations of a replica set&rsquo;s current state.</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>LastOperation performed</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=#%5b%5dgithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.MachineSummary>[]github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=MachineSpec><b>MachineSpec</b></h3><p>(<em>Appears on:</em>
<a href=#Machine>Machine</a>,
<a href=#MachineTemplateSpec>MachineTemplateSpec</a>)</p><p><p>MachineSpec is the specification of a Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></tbody></table><br><h3 id=MachineState><b>MachineState</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#LastOperation>LastOperation</a>)</p><p><p>MachineState is a current state of the machine.</p></p><br><h3 id=MachineStatus><b>MachineStatus</b></h3><p>(<em>Appears on:</em>
<a href=#Machine>Machine</a>)</p><p><p>MachineStatus holds the most recently observed status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodecondition-v1-core>[]Kubernetes core/v1.NodeCondition</a></em></td><td><p>Conditions of this machine, same as node</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>currentStatus</code></td><td><em><a href=#CurrentStatus>CurrentStatus</a></em></td><td><p>Current status of the machine object</p></td></tr><tr><td><code>lastKnownState</code></td><td><em>string</em></td><td><em>(Optional)</em><p>LastKnownState can store details of the last known state of the VM by the plugins.
It can be used by future operation calls to determine current infrastucture state</p></td></tr></tbody></table><br><h3 id=MachineSummary><b>MachineSummary</b></h3><p><p>MachineSummary store the summary of machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of the machine object</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>ownerRef</code></td><td><em>string</em></td><td><p>OwnerRef</p></td></tr></tbody></table><br><h3 id=MachineTemplateSpec><b>MachineTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>,
<a href=#MachineSetSpec>MachineSetSpec</a>)</p><p><p>MachineTemplateSpec describes the data a machine should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object&rsquo;s metadata.
More info: <a href=https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata>https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata</a></p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSpec>MachineSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the machine.
More info: <a href=https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status>https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status</a></p><br><br><table><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr></tbody></table><br><h3 id=NodeTemplate><b>NodeTemplate</b></h3><p>(<em>Appears on:</em>
<a href=#MachineClass>MachineClass</a>)</p><p><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>capacity</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Capacity contains subfields to track all node resources required to scale nodegroup from zero</p></td></tr><tr><td><code>instanceType</code></td><td><em>string</em></td><td><p>Instance type of the node belonging to nodeGroup</p></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td><p>Region of the expected node belonging to nodeGroup</p></td></tr><tr><td><code>zone</code></td><td><em>string</em></td><td><p>Zone of the expected node belonging to nodeGroup</p></td></tr></tbody></table><br><h3 id=NodeTemplateSpec><b>NodeTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>NodeTemplateSpec describes the data a node should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodespec-v1-core>Kubernetes core/v1.NodeSpec</a></em></td><td><em>(Optional)</em><p>NodeSpec describes the attributes that a node is created with.</p><br><br><table><tr><td><code>podCIDR</code></td><td><em>string</em></td><td><em>(Optional)</em><p>PodCIDR represents the pod IP range assigned to the node.</p></td></tr><tr><td><code>podCIDRs</code></td><td><em>[]string</em></td><td><em>(Optional)</em><p>podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node. If this
field is specified, the 0th entry must match the podCIDR field. It may contain at most 1 value for
each of IPv4 and IPv6.</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ID of the node assigned by the cloud provider in the format: <providername>://<providerspecificnodeid></p></td></tr><tr><td><code>unschedulable</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Unschedulable controls node schedulability of new pods. By default, node is schedulable.
More info: <a href=https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration>https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration</a></p></td></tr><tr><td><code>taints</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23taint-v1-core>[]Kubernetes core/v1.Taint</a></em></td><td><em>(Optional)</em><p>If specified, the node&rsquo;s taints.</p></td></tr><tr><td><code>configSource</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodeconfigsource-v1-core>Kubernetes core/v1.NodeConfigSource</a></em></td><td><em>(Optional)</em><p>Deprecated. If specified, the source of the node&rsquo;s configuration.
The DynamicKubeletConfig feature gate must be enabled for the Kubelet to use this field.
This field is deprecated as of 1.22: <a href=https://git.k8s.io/enhancements/keps/sig-node/281-dynamic-kubelet-configuration>https://git.k8s.io/enhancements/keps/sig-node/281-dynamic-kubelet-configuration</a></p></td></tr><tr><td><code>externalID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>Deprecated. Not all kubelets will set this field. Remove field after 1.13.
see: <a href=https://issues.k8s.io/61966>https://issues.k8s.io/61966</a></p></td></tr></table></td></tr></tbody></table><br><h3 id=OpenStackMachineClass><b>OpenStackMachineClass</b></h3><p><p>OpenStackMachineClass TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#OpenStackMachineClassSpec>OpenStackMachineClassSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>imageID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>imageName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>availabilityZone</code></td><td><em>string</em></td><td></td></tr><tr><td><code>flavorName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>keyName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>securityGroups</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>networkID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>networks</code></td><td><em><a href=#OpenStackNetwork>[]OpenStackNetwork</a></em></td><td></td></tr><tr><td><code>subnetID</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>podNetworkCidr</code></td><td><em>string</em></td><td></td></tr><tr><td><code>rootDiskSize</code></td><td><em>int</em></td><td></td></tr><tr><td><code>useConfigDrive</code></td><td><em>*bool</em></td><td><p>in GB</p></td></tr><tr><td><code>serverGroupID</code></td><td><em>*string</em></td><td></td></tr></table></td></tr></tbody></table><br><h3 id=OpenStackMachineClassSpec><b>OpenStackMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#OpenStackMachineClass>OpenStackMachineClass</a>)</p><p><p>OpenStackMachineClassSpec is the specification of a OpenStackMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>imageID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>imageName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td></td></tr><tr><td><code>availabilityZone</code></td><td><em>string</em></td><td></td></tr><tr><td><code>flavorName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>keyName</code></td><td><em>string</em></td><td></td></tr><tr><td><code>securityGroups</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>map[string]string</em></td><td></td></tr><tr><td><code>networkID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>networks</code></td><td><em><a href=#OpenStackNetwork>[]OpenStackNetwork</a></em></td><td></td></tr><tr><td><code>subnetID</code></td><td><em>*string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>podNetworkCidr</code></td><td><em>string</em></td><td></td></tr><tr><td><code>rootDiskSize</code></td><td><em>int</em></td><td></td></tr><tr><td><code>useConfigDrive</code></td><td><em>*bool</em></td><td><p>in GB</p></td></tr><tr><td><code>serverGroupID</code></td><td><em>*string</em></td><td></td></tr></tbody></table><br><h3 id=OpenStackNetwork><b>OpenStackNetwork</b></h3><p>(<em>Appears on:</em>
<a href=#OpenStackMachineClassSpec>OpenStackMachineClassSpec</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td><em>string</em></td><td></td></tr><tr><td><code>name</code></td><td><em>string</em></td><td><p>takes priority before name</p></td></tr><tr><td><code>podNetwork</code></td><td><em>bool</em></td><td></td></tr></tbody></table><br><h3 id=PacketMachineClassSpec><b>PacketMachineClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#PacketMachineClass>PacketMachineClass</a>)</p><p><p>PacketMachineClassSpec is the specification of a PacketMachineClass.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>facility</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>machineType</code></td><td><em>string</em></td><td></td></tr><tr><td><code>billingCycle</code></td><td><em>string</em></td><td></td></tr><tr><td><code>OS</code></td><td><em>string</em></td><td></td></tr><tr><td><code>projectID</code></td><td><em>string</em></td><td></td></tr><tr><td><code>tags</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>sshKeys</code></td><td><em>[]string</em></td><td></td></tr><tr><td><code>userdata</code></td><td><em>string</em></td><td></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr></tbody></table><br><h3 id=RollbackConfig><b>RollbackConfig</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>revision</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The revision to rollback to. If set to 0, rollback to the last revision.</p></td></tr></tbody></table><br><h3 id=RollingUpdateMachineDeployment><b>RollingUpdateMachineDeployment</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p><p>Spec to control the desired behavior of rolling update.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>maxUnavailable</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2futil%2fintstr%23IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be unavailable during the update.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
Absolute number is calculated from percentage by rounding down.
This can not be 0 if MaxSurge is 0.
By default, a fixed value of 1 is used.
Example: when this is set to 30%, the old MC can be scaled down to 70% of desired machines
immediately when the rolling update starts. Once new machines are ready, old MC
can be scaled down further, followed by scaling up the new MC, ensuring
that the total number of machines available at all times during the update is at
least 70% of desired machines.</p></td></tr><tr><td><code>maxSurge</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2futil%2fintstr%23IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be scheduled above the desired number of
machines.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
This can not be 0 if MaxUnavailable is 0.
Absolute number is calculated from percentage by rounding up.
By default, a value of 1 is used.
Example: when this is set to 30%, the new MC can be scaled up immediately when
the rolling update starts, such that the total number of old and new machines do not exceed
130% of desired machines. Once old machines have been killed,
new MC can be scaled up further, ensuring that total number of machines running
at any time during the update is atmost 130% of desired machines.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f633ee4f7382c904ef1d6ec47d65d898>8.2.1.4 - Proposals</h1></div><div class=td-content><h1 id=pg-7bacca9cdc9d809497d59cebbdc7153c>8.2.1.4.1 - Excess Reserve Capacity</h1><h1 id=excess-reserve-capacity>Excess Reserve Capacity</h1><ul><li><a href=#excess-reserve-capacity>Excess Reserve Capacity</a><ul><li><a href=#goal>Goal</a></li><li><a href=#note>Note</a></li><li><a href=#possible-approaches>Possible Approaches</a><ul><li><a href=#approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</a></li><li><a href=#approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</a></li><li><a href=#approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</a></li><li><a href=#approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</a></li></ul></li></ul></li></ul><h2 id=goal>Goal</h2><p>Currently, autoscaler optimizes the number of machines for a given application-workload. Along with effective resource utilization, this feature brings concern where, many times, when new application instances are created - they don&rsquo;t find space in existing cluster. This leads the cluster-autoscaler to create new machines via MachineDeployment, which can take from 3-4 minutes to ~10 minutes, for the machine to really come-up and join the cluster. In turn, application-instances have to wait till new machines join the cluster.</p><p>One of the promising solutions to this issue is Excess Reserve Capacity. Idea is to keep a certain number of machines or percent of resources[cpu/memory] always available, so that new workload, in general, can be scheduled immediately unless huge spike in the workload. Also, the user should be given enough flexibility to choose how many resources or how many machines should be kept alive and non-utilized as this affects the Cost directly.</p><h2 id=note>Note</h2><ul><li>We decided to go with Approach-4 which is based on low priority pods. Please find more details here: <a href=https://github.com/gardener/gardener/issues/254>https://github.com/gardener/gardener/issues/254</a></li><li>Approach-3 looks more promising in long term, we may decide to adopt that in future based on developments/contributions in autoscaler-community.</li></ul><h2 id=possible-approaches>Possible Approaches</h2><p>Following are the possible approaches, we could think of so far.</p><h3 id=approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</h3><ul><li><p>Machine-controller-manager currently takes care of the machines in the shoot cluster starting from creation-deletion-health check to efficient rolling-update of the machines. From the architecture point of view, MachineSet makes sure that X number of machines are always <strong>running and healthy</strong>. MachineDeployment controller smartly uses this facility to perform rolling-updates.</p></li><li><p>We can expand the scope of MachineDeployment controller to maintain excess number of machines by introducing new parallel independent controller named <em>MachineTaint</em> controller. This will result in MCM to include Machine, MachineSet, MachineDeployment, MachineSafety, MachineTaint controllers. MachineTaint controller does not need to introduce any new CRD - analogy fits where taint-controller also resides into kube-controller-manager.</p></li><li><p>Only Job of MachineTaint controller will be:</p><ul><li>List all the Machines under each MachineDeployment.</li><li>Maintain taints of <a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/><em>noSchedule</em> and <em>noExecute</em></a> on <code>X</code> latest MachineObjects.</li><li>There should be an event-based informer mechanism where MachineTaintController gets to know about any Update/Delete/Create event of MachineObjects - in turn, maintains the <em>noSchedule</em> and <em>noExecute</em> taints on all the <em>latest</em> machines.
- Why latest machines?
- Whenever autoscaler decides to add new machines - essentially ScaleUp event - taints from the older machines are removed and newer machines get the taints. This way X number of Machines immediately becomes free for new pods to be scheduled.
- While ScaleDown event, autoscaler specifically mentions which machines should be deleted, and that should not bring any concerns. Though we will have to put proper label/annotation defined by autoscaler on taintedMachines, so that autoscaler does not consider the taintedMachines for deletion while scale-down.
* Annotation on tainted node: <code>"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"</code></li></ul></li><li><p>Implementation Details:</p><ul><li>Expect new <strong>optional field</strong> <em>ExcessReplicas</em> in <code>MachineDeployment.Spec</code>. MachineDeployment controller now adds both <code>Spec.Replicas</code> and <code>Spec.ExcessReplicas</code>[if provided], and considers that as a standard desiredReplicas.
- Current working of MCM will not be affected if ExcessReplicas field is kept nil.</li><li>MachineController currently reads the <em>NodeObject</em> and sets the MachineConditions in MachineObject. Machine-controller will now also read the taints/labels from the MachineObject - and maintains it on the <em>NodeObject</em>.</li></ul></li><li><p>We expect cluster-autoscaler to intelligently make use of the provided feature from MCM.</p><ul><li>CA gets the input of <em>min:max:excess</em> from Gardener. CA continues to set the <code>MachineDeployment.Spec.Replicas</code> as usual based on the application-workload.</li><li>In addition, CA also sets the <code>MachieDeployment.Spec.ExcessReplicas</code> .</li><li>Corner-case:
* CA should decrement the excessReplicas field accordingly when <em>desiredReplicas+excessReplicas</em> on MachineDeployment goes beyond <em>max</em>.</li></ul></li></ul><h3 id=approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</h3><ul><li>There was already an attempt by community to support this feature.<ul><li>Refer for details to: <a href=https://github.com/kubernetes/autoscaler/pull/77/files>https://github.com/kubernetes/autoscaler/pull/77/files</a></li></ul></li></ul><h3 id=approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</h3><ul><li>Forked version of cluster-autoscaler could be improved to plug-in the algorithm for excess-reserve capacity.</li><li>Needs further discussion around upstream support.</li><li>Create golang channel to separate the algorithms to trigger scaling (hard-coded in cluster-autoscaler, currently) from the algorithms about how to to achieve the scaling (already pluggable in cluster-autoscaler). This kind of separation can help us introduce/plug-in new algorithms (such as based node resource utilisation) without affecting existing code-base too much while almost completely re-using the code-base for the actual scaling.</li><li>Also this approach is not specific to our fork of cluster-autoscaler. It can be made upstream eventually as well.</li></ul><h3 id=approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</h3><ul><li>Refer to: <a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/>pod-priority-preemption</a></li><li>TL; DR:<ul><li>High priority pods can preempt the low-priority pods which are already scheduled.</li><li>Pre-create bunch[equivivalent of X shoot-control-planes] of low-priority pods with priority of zero, then start creating the workload pods with better priority which will reschedule the low-priority pods or otherwise keep them in pending state if the limit for max-machines has reached.</li><li>This is still alpha feature.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9b5068d7c93d2b26366fb48394b6419b>8.2.1.4.2 - GRPC Based Implementation of Cloud Providers</h1><h1 id=grpc-based-implementation-of-cloud-providers---wip>GRPC based implementation of Cloud Providers - WIP</h1><h2 id=goal>Goal:</h2><p>Currently the Cloud Providers&rsquo; (CP) functionalities ( Create(), Delete(), List() ) are part of the Machine Controller Manager&rsquo;s (MCM)repository. Because of this, adding support for new CPs into MCM requires merging code into MCM which may not be required for core functionalities of MCM itself. Also, for various reasons it may not be feasible for all CPs to merge their code with MCM which is an Open Source project.</p><p>Because of these reasons, it was decided that the CP&rsquo;s code will be moved out in separate repositories so that they can be maintained separately by the respective teams. Idea is to make MCM act as a GRPC server, and CPs as GRPC clients. The CP can register themselves with the MCM using a GRPC service exposed by the MCM. Details of this approach is discussed below.</p><h2 id=how-it-works>How it works:</h2><p>MCM acts as GRPC server and listens on a pre-defined port 5000. It implements below GRPC services. Details of each of these services are mentioned in next section.</p><ul><li><code>Register()</code></li><li><code>GetMachineClass()</code></li><li><code>GetSecret()</code></li></ul><h2 id=grpc-services-exposed-by-mcm>GRPC services exposed by MCM:</h2><h3 id=register>Register()</h3><p><code>rpc Register(stream DriverSide) returns (stream MCMside) {}</code></p><p>The CP GRPC client calls this service to register itself with the MCM. The CP passes the <code>kind</code> and the <code>APIVersion</code> which it implements, and MCM maintains an internal map for all the registered clients. A GRPC stream is returned in response which is kept open througout the life of both the processes. MCM uses this stream to communicate with the client for machine operations: <code>Create()</code>, <code>Delete()</code> or <code>List()</code>.
The CP client is responsible for reading the incoming messages continuously, and based on the <code>operationType</code> parameter embedded in the message, it is supposed to take the required action. This part is already handled in the package <code>grpc/infraclient</code>.
To add a new CP client, import the package, and implement the <code>ExternalDriverProvider</code> interface:</p><pre tabindex=0><code>type ExternalDriverProvider interface {
	Create(machineclass *MachineClassMeta, credentials, machineID, machineName string) (string, string, error)
	Delete(machineclass *MachineClassMeta, credentials, machineID string) error
	List(machineclass *MachineClassMeta, credentials, machineID string) (map[string]string, error)
}
</code></pre><h3 id=getmachineclass>GetMachineClass()</h3><p><code>rpc GetMachineClass(MachineClassMeta) returns (MachineClass) {}</code></p><p>As part of the message from MCM for various machine operations, the name of the machine class is sent instead of the full machine class spec. The CP client is expected to use this GRPC service to get the full spec of the machine class. This optionally enables the client to cache the machine class spec, and make the call only if the machine calass spec is not already cached.</p><h3 id=getsecret>GetSecret()</h3><p><code>rpc GetSecret(SecretMeta) returns (Secret) {}</code></p><p>As part of the message from MCM for various machine operations, the Cloud Config (CC) and CP credentials are not sent. The CP client is expected to use this GRPC service to get the secret which has CC and CP&rsquo;s credentials from MCM. This enables the client to cache the CC and credentials, and to make the call only if the data is not already cached.</p><h2 id=how-to-add-a-new-cloud-providers-support>How to add a new Cloud Provider&rsquo;s support</h2><p>Import the package <code>grpc/infraclient</code> and <code>grpc/infrapb</code> from MCM (currently in MCM&rsquo;s &ldquo;grpc-driver&rdquo; branch)</p><ul><li>Implement the interface <code>ExternalDriverProvider</code><ul><li><code>Create()</code>: Creates a new machine</li><li><code>Delete()</code>: Deletes a machine</li><li><code>List()</code>: Lists machines</li></ul></li><li>Use the interface <code>MachineClassDataProvider</code><ul><li><code>GetMachineClass()</code>: Makes the call to MCM to get machine class spec</li><li><code>GetSecret()</code>: Makes the call to MCM to get secret containing Cloud Config and CP&rsquo;s credentials</li></ul></li></ul><h3 id=example-implementation>Example implementation:</h3><p>Refer GRPC based implementation for AWS client:
<a href=https://github.com/ggaurav10/aws-driver-grpc>https://github.com/ggaurav10/aws-driver-grpc</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d8d5daa8dcf42211080eccf47432fb74>8.2.1.5 - Usage</h1></div><div class=td-content><h1 id=pg-717c4d2c784b8008555cff40507e0535>8.2.1.5.1 - Machine</h1><h1 id=creatingdeleting-machines-vm>Creating/Deleting machines (VM)</h1><ul><li><a href=#creatingdeleting-machines-vm>Creating/Deleting machines (VM)</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=#important>Important :</a></li><li><a href=#creating-machine>Creating machine</a></li><li><a href=#inspect-status-of-machine>Inspect status of machine</a></li><li><a href=#delete-machine>Delete machine</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/docs/usage/prerequisite/>steps described here</a></li></ul><h2 id=important->Important :</h2><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_objects/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine>Creating machine</h2><ul><li>Modify <code>kubernetes/machine_objects/machine.yaml</code> as per your requirement and create the VM as shown below:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machine by talking to the cloud provider.</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME           STATUS    AGE
</span></span><span style=display:flex><span>test-machine   Running   5m
</span></span></code></pre></div><p>A new machine is created with the name provided in the <code>kubernetes/machine_objects/machine.yaml</code> file.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice a new node joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS     AGE     VERSION
</span></span><span style=display:flex><span>ip-10-250-14-52.eu-east-1.compute.internal.  Ready      1m      v1.8.0
</span></span></code></pre></div><p>This shows that a new node has successfully joined the cluster.</p><h2 id=inspect-status-of-machine>Inspect status of machine</h2><p>To inspect the status of any created machine, run the command given below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine test-machine -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: Machine
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;Machine&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T06:58:21Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    node: ip-10-250-14-52.eu-east-1.compute.internal
</span></span><span style=display:flex><span>    test-label: test-label
</span></span><span style=display:flex><span>  name: test-machine
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12616948&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine
</span></span><span style=display:flex><span>  uid: 535e596c-ead3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  class:
</span></span><span style=display:flex><span>    kind: AWSMachineClass
</span></span><span style=display:flex><span>    name: test-aws
</span></span><span style=display:flex><span>  providerID: aws:///eu-east-1/i-00bef3f2618ffef23
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient disk space available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientDisk
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: OutOfDisk
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient memory available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientMemory
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: MemoryPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has no disk pressure
</span></span><span style=display:flex><span>    reason: KubeletHasNoDiskPressure
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: DiskPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    message: kubelet is posting ready status
</span></span><span style=display:flex><span>    reason: KubeletReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Ready
</span></span><span style=display:flex><span>  currentStatus:
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    phase: Running
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Machine is now ready
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    state: Successful
</span></span><span style=display:flex><span>    type: Create
</span></span><span style=display:flex><span>  node: ip-10-250-14-52.eu-west-1.compute.internal
</span></span></code></pre></div><h2 id=delete-machine>Delete machine</h2><p>To delete the VM using the <code>kubernetes/machine_objects/machine.yaml</code> as shown below</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to delete the existing VM by talking to the cloud provider. The node should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3c49f9a5cce5d9bb509c617fb8070953>8.2.1.5.2 - Machine Deployment</h1><h1 id=maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</h1><ul><li><a href=#maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a><ul><li><a href=#important-warning>Important ⚠️</a></li></ul></li><li><a href=#creating-machine-deployment>Creating machine-deployment</a></li><li><a href=#inspect-status-of-machine-deployment>Inspect status of machine-deployment</a></li><li><a href=#health-monitoring>Health monitoring</a></li><li><a href=#update-your-machines>Update your machines</a><ul><li><a href=#inspect-existing-cluster-configuration>Inspect existing cluster configuration</a></li><li><a href=#perform-a-rolling-update>Perform a rolling update</a></li><li><a href=#re-check-cluster-configuration>Re-check cluster configuration</a></li><li><a href=#more-variants-of-updates>More variants of updates</a></li></ul></li><li><a href=#undo-an-update>Undo an update</a></li><li><a href=#pause-an-update>Pause an update</a></li><li><a href=#delete-machine-deployment>Delete machine-deployment</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><p>Follow the <a href=/docs/other-components/machine-controller-manager/docs/usage/prerequisite/>steps described here</a></p><h3 id=important->Important ⚠️</h3><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine-deployment.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-deployment>Creating machine-deployment</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-deployment.yaml</code> as per your requirement. Modify the number of replicas to the desired number of machines. Then, create an machine-deployment.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-deployments in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>NAME                      READY   DESIRED   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>test-machine-deployment   3       3         3            0           10m
</span></span></code></pre></div><p>You will notice a new machine-deployment with your given name</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         0       10m
</span></span></code></pre></div><p>You will notice a new machine-set backing your machine-deployment</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                                       STATUS    AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-5d24b   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-6mpn4   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-dpt2q   Pending   5m
</span></span></code></pre></div><p>Now you will notice N (number of replicas specified in the manifest) new machines whose name are prefixed with the machine-deployment object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you would see that new nodes have joined the cluster. You can see this using</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-deployment>Inspect status of machine-deployment</h2><p>To inspect the status of any created machine-deployment run the command below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment test-machine-deployment -o yaml
</span></span></code></pre></div><p>You should get the following output.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;MachineDeployment&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine-deployment&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;minReadySeconds&#34;</span>:200,<span style=color:#a31515>&#34;replicas&#34;</span>:3,<span style=color:#a31515>&#34;selector&#34;</span>:{<span style=color:#a31515>&#34;matchLabels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;strategy&#34;</span>:{<span style=color:#a31515>&#34;rollingUpdate&#34;</span>:{<span style=color:#a31515>&#34;maxSurge&#34;</span>:1,<span style=color:#a31515>&#34;maxUnavailable&#34;</span>:1},<span style=color:#a31515>&#34;type&#34;</span>:<span style=color:#a31515>&#34;RollingUpdate&#34;</span>},<span style=color:#a31515>&#34;template&#34;</span>:{<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:55:56Z
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-deployment
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12634168&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-deployment
</span></span><span style=display:flex><span>  uid: c0b488f7-eae3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    message: Deployment has minimum availability.
</span></span><span style=display:flex><span>    reason: MinimumReplicasAvailable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  updatedReplicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><p>Health monitor is also applied similar to how it&rsquo;s described for <a href=/docs/other-components/machine-controller-manager/docs/usage/machine_set/>machine-sets</a></p><h2 id=update-your-machines>Update your machines</h2><p>Let us consider the scenario where you wish to update all nodes of your cluster from t2.xlarge machines to m5.xlarge machines. Assume that your current <em>test-aws</em> has its <strong>spec.machineType: t2.xlarge</strong> and your deployment <em>test-machine-deployment</em> points to this AWSMachineClass.</p><h4 id=inspect-existing-cluster-configuration>Inspect existing cluster configuration</h4><ul><li>Check Nodes present in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice one machine-set backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         3       10m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge.</li></ul><h4 id=perform-a-rolling-update>Perform a rolling update</h4><p>To update this machine-deployment VMs to <code>m5.xlarge</code>, we would do the following:</p><ul><li>Copy your existing aws-machine-class.yaml</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp kubernetes/machine_classes/aws-machine-class.yaml kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Modify aws-machine-class-new.yaml, and update its <em>metadata.name: test-aws2</em> and <em>spec.machineType: m5.xlarge</em></li><li>Now create this modified MachineClass</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Edit your existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Update from <em>spec.template.spec.class.name: test-aws</em> to <em>spec.template.spec.class.name: test-aws2</em></li></ul><h4 id=re-check-cluster-configuration>Re-check cluster configuration</h4><p>After a few minutes (~3mins)</p><ul><li>Check nodes present in cluster now. They are different nodes.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-11-171.eu-west-1.compute.internal   Ready     4m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-17-213.eu-west-1.compute.internal   Ready     5m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-81.eu-west-1.compute.internal    Ready     5m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice two machine-sets backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   0         0         0       1h
</span></span><span style=display:flex><span>test-machine-deployment-86ff45cc5    3         3         3       20m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge in terminated state, and N new VMs of type m5.xlarge in running state.</li></ul><p>This shows how a rolling update of a cluster from nodes with t2.xlarge to m5.xlarge went through.</p><h4 id=more-variants-of-updates>More variants of updates</h4><ul><li>The above demonstration was a simple use case. This could be more complex like - updating the system disk image versions/ kubelet versions/ security patches etc.</li><li>You can also play around with the maxSurge and maxUnavailable fields in machine-deployment.yaml</li><li>You can also change the update strategy from rollingupdate to recreate</li></ul><h2 id=undo-an-update>Undo an update</h2><ul><li>Edit the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Edit the deployment to have this new field of <em>spec.rollbackTo.revision: 0</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></li><li>This will undo your update to the previous version.</li></ul><h2 id=pause-an-update>Pause an update</h2><ul><li>You can also pause the update while update is going on by editing the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li><p>Edit the deployment to have this new field of <em>spec.paused: true</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></p></li><li><p>This will pause the rollingUpdate if it&rsquo;s in process</p></li><li><p>To resume the update, edit the deployment as mentioned above and remove the field <em>spec.paused: true</em> updated earlier</p></li></ul><h2 id=delete-machine-deployment>Delete machine-deployment</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-deployment.yaml</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>The Machine Controller Manager picks up the manifest and starts to delete the existing VMs by talking to the cloud provider. The nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5c4d6530edef0894ced455a919342044>8.2.1.5.3 - Machine Set</h1><h1 id=maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</h1><ul><li><a href=#maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=#important-warning>Important ⚠️</a></li><li><a href=#creating-machine-set>Creating machine-set</a></li><li><a href=#inspect-status-of-machine-set>Inspect status of machine-set</a></li><li><a href=#health-monitoring>Health monitoring</a></li><li><a href=#delete-machine-set>Delete machine-set</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/docs/usage/prerequisite/>steps described here</a></li></ul><h2 id=important->Important ⚠️</h2><blockquote><p>Make sure that the <code>kubernetes/machines_objects/machine-set.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-set>Creating machine-set</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-set.yaml</code> as per your requirement. You can modify the number of replicas to the desired number of machines. Then, create an machine-set:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-set.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME               DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-set   3         3         0       1m
</span></span></code></pre></div><p>You will see a new machine-set with your given name</p><ul><li>Check Machine Controller Manager machines in the cluster:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                     STATUS    AGE
</span></span><span style=display:flex><span>test-machine-set-b57zs   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-c4bg8   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-kvskg   Pending   5m
</span></span></code></pre></div><p>Now you will see N (number of replicas specified in the manifest) new machines whose names are prefixed with the machine-set object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice new nodes joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-0-234.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-15-98.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-6-21.eu-west-1.compute.internal    Ready     2m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-set>Inspect status of machine-set</h2><ul><li>To inspect the status of any created machine-set run the following command:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset test-machine-set -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;MachineSet&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine-set&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>,<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;minReadySeconds&#34;</span>:200,<span style=color:#a31515>&#34;replicas&#34;</span>:3,<span style=color:#a31515>&#34;selector&#34;</span>:{<span style=color:#a31515>&#34;matchLabels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;template&#34;</span>:{<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:37:42Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-set
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12630893&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-set
</span></span><span style=display:flex><span>  uid: 3469faaa-eae1-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineClass: {}
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  fullyLabeledReplicas: 3
</span></span><span style=display:flex><span>  machineSetCondition: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  observedGeneration: 0
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><ul><li>If you try to delete/terminate any of the machines backing the machine-set by either talking to the Machine Controller Manager or from the cloud provider, the Machine Controller Manager recreates a matching healthy machine to replace the deleted machine.</li><li>Similarly, if any of your machines are unreachable or in an unhealthy state (kubelet not ready / disk pressure) for longer than the configured timeout (~ 5mins), the Machine Controller Manager recreates the nodes to replace the unhealthy nodes.</li></ul><h2 id=delete-machine-set>Delete machine-set</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-set.yaml</code>:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine-set.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager has immediately picked up your manifest and started to delete the existing VMs by talking to the cloud provider. Your nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d24f54393560b4c4b5017ff2719c5747>8.2.1.5.4 - Prerequisite</h1><h1 id=setting-up-the-usage-environment>Setting up the usage environment</h1><ul><li><a href=#setting-up-the-usage-environment>Setting up the usage environment</a><ul><li><a href=#important-warning>Important ⚠️</a></li><li><a href=#set-kubeconfig>Set KUBECONFIG</a></li><li><a href=#replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</a></li><li><a href=#deploy-required-crds-and-objects>Deploy required CRDs and Objects</a></li><li><a href=#check-current-cluster-state>Check current cluster state</a></li></ul></li></ul><h2 id=important->Important ⚠️</h2><blockquote><p>All paths are relative to the root location of this project repository.</p></blockquote><blockquote><p>Run the Machine Controller Manager either as described in <a href=/docs/other-components/machine-controller-manager/docs/development/local_setup/>Setting up a local development environment</a> or <a href=/docs/other-components/machine-controller-manager/docs/deployment/kubernetes/>Deploying the Machine Controller Manager into a Kubernetes cluster</a>.</p></blockquote><blockquote><p>Make sure that the following steps are run before managing machines/ machine-sets/ machine-deploys.</p></blockquote><h2 id=set-kubeconfig>Set KUBECONFIG</h2><p>Using the existing <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a>, open another Terminal panel/window with the <code>KUBECONFIG</code> environment variable pointing to this Kubeconfig file as shown below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ export KUBECONFIG=&lt;PATH_TO_REPO&gt;/dev/kubeconfig.yaml
</span></span></code></pre></div><h2 id=replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</h2><p>Open <code>kubernetes/machine_classes/aws-machine-class.yaml</code> and replace required values there with the desired VM configurations.</p><p>Similarily open <code>kubernetes/secrets/aws-secret.yaml</code> and replace - <em>userData, providerAccessKeyId, providerSecretAccessKey</em> with base64 encoded values of cloudconfig file, AWS access key id, and AWS secret access key respectively. Use the following command to get the base64 encoded value of your details</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ echo <span style=color:#a31515>&#34;sample-cloud-config&#34;</span> | base64
</span></span><span style=display:flex><span>base64-encoded-cloud-config
</span></span></code></pre></div><p>Do the same for your access key id and secret access key.</p><h2 id=deploy-required-crds-and-objects>Deploy required CRDs and Objects</h2><p>Create all the required CRDs in the cluster using <code>kubernetes/crds.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><p>Create the class template that will be used as an machine template to create VMs using <code>kubernetes/machine_classes/aws-machine-class.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_classes/aws-machine-class.yaml
</span></span></code></pre></div><p>Create the secret used for the cloud credentials and cloudconfig using <code>kubernetes/secrets/aws-secret.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/secrets/aws-secret.yaml
</span></span></code></pre></div><h2 id=check-current-cluster-state>Check current cluster state</h2><p>Get to know the current cluster state using the following commands,</p><ul><li>Checking aws-machine-class in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get awsmachineclass
</span></span><span style=display:flex><span>NAME       MACHINE TYPE   AMI          AGE
</span></span><span style=display:flex><span>test-aws   t2.large       ami-123456   5m
</span></span></code></pre></div><ul><li>Checking kubernetes secrets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get secret
</span></span><span style=display:flex><span>NAME                  TYPE                                  DATA      AGE
</span></span><span style=display:flex><span>test-secret           Opaque                                3         21h
</span></span></code></pre></div><ul><li>Checking kubernetes nodes in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span></code></pre></div><p>Lists the default set of nodes attached to your cluster</p><ul><li>Checking Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-deploys in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ae558d79151c72695cd5e6821e146fe7>8.2.1.6 - Machine Controller Manager FAQ</h1><div class=lead>Commonly asked questions about MCM</div><h1 id=frequently-asked-questions>Frequently Asked Questions</h1><p>The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If
you&rsquo;re using an older version of MCM please refer to corresponding version of
this document. Few of the answers assume that the MCM being used is in conjuction with <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>:</p><h1 id=table-of-contents>Table of Contents:</h1><ul><li><p><a href=#basics>Basics</a></p><ul><li><a href=#what-is-machine-controller-manager>What is Machine Controller Manager?</a></li><li><a href=#Why-is-my-machine-deleted>Why is my machine deleted?</a></li><li><a href=#What-are-the-different-sub-controllers-in-MCM>What are the different sub-controllers in MCM?</a></li><li><a href=#What-is-safety-controller-in-MCM>What is Safety Controller in MCM?</a></li></ul></li><li><p><a href=#how-to>How to?</a></p><ul><li><a href=#How-to-install-MCM-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</a></li><li><a href=#How-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</a></li><li><a href=#How-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</a></li><li><a href=#How-to-force-delete-a-machine>How to force delete a machine?</a></li><li><a href=#How-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</a></li><li><a href=#How-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</a></li></ul></li><li><p><a href=#internals>Internals</a></p><ul><li><a href=#What-is-the-high-level-design-of-MCM>What is the high level design of MCM?</a></li><li><a href=#What-are-the-different-configuration-options-in-MCM>What are the different configuration options in MCM?</a></li><li><a href=#What-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</a></li><li><a href=#How-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</a></li><li><a href=#How-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</a></li><li><a href=#How-does-maxEvictRetries-configuration-work-with-drainTimeout-configuration>How does maxEvictRetries configuration work with drainTimeout configuration?</a></li><li><a href=#What-are-the-different-phases-of-a-machine>What are the different phases of a machine?</a></li></ul></li><li><p><a href=#troubleshooting>Troubleshooting</a></p><ul><li><a href=#My-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</a></li><li><a href=#My-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</a></li></ul></li><li><p><a href=#developer>Developer</a></p><ul><li><a href=#How-should-I-test-my-code-before-submitting-a-PR>How should I test my code before submitting a PR?</a></li><li><a href=#I-need-to-change-the-APIs-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</a></li><li><a href=#How-can-I-update-the-dependencies-of-MCM>How can I update the dependencies of MCM?</a></li></ul></li><li><p><a href=#in-the-context-of-gardener>In the context of Gardener</a></p><ul><li><a href=#How-can-I-configure-MCM-using-Shoot-resource>How can I configure MCM using Shoot resource?</a></li><li><a href=#How-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</a></li></ul></li></ul><h1 id=basics>Basics</h1><h3 id=what-is-machine-controller-manager>What is Machine Controller Manager?</h3><p>Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as <code>Machine</code>, <code>MachineSet</code>, <code>MachineDeployment</code> which depicts the functionality of <code>Pod</code>, <code>Replicaset</code>, <code>Deployment</code> of the core Kubernetes respectively. Read more about it at <a href=https://github.com/gardener/machine-controller-manager/tree/master/docs>README</a>.</p><ul><li>Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.</li></ul><h3 id=why-is-my-machine-deleted>Why is my machine deleted?</h3><p>A machine is deleted by MCM generally for 2 reasons-</p><ul><li><p>Machine is unhealthy for at least <code>MachineHealthTimeout</code> period. The default <code>MachineHealthTimeout</code> is 10 minutes.</p><ul><li>By default, a machine is considered unhealthy if any of the following node conditions - <code>DiskPressure</code>, <code>KernelDeadlock</code>, <code>FileSystem</code>, <code>Readonly</code> is set to <code>true</code>, or <code>KubeletReady</code> is set to <code>false</code>. However, this is something that is configurable using the following <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L30>flag</a>.</li></ul></li><li><p>Machine is scaled down by the <code>MachineDeployment</code> resource.</p><ul><li>This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the <code>MachineDeployment</code>. Read more about cluster-autoscaler&rsquo;s scale down behavior <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#how-does-scale-down-work>here</a>.</li></ul></li></ul><h3 id=what-are-the-different-sub-controllers-in-mcm>What are the different sub-controllers in MCM?</h3><p>MCM mainly contains the following sub-controllers:</p><ul><li><code>MachineDeployment Controller</code>: Responsible for reconciling the <code>MachineDeployment</code> objects. It manages the lifecycle of the <code>MachineSet</code> objects.</li><li><code>MachineSet Controller</code>: Responsible for reconciling the <code>MachineSet</code> objects. It manages the lifecycle of the <code>Machine</code> objects.</li><li><code>Machine Controller</code>: responsible for reconciling the <code>Machine</code> objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - <a href=https://github.com/gardener/machine-controller-manager-provider-gcp>link</a>.</li><li>Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality <a href=#what-is-safety-controller>below</a>.</li></ul><h3 id=what-is-safety-controller-in-mcm>What is Safety Controller in MCM?</h3><p><code>Safety Controller</code> contains following functions:</p><ul><li>Orphan VM handler:<ul><li>It lists all the VMs in the cloud matching the <code>tag</code> of given cluster name and maps the VMs with the <code>machine</code> objects using the <code>ProviderID</code> field. VMs without any backing <code>machine</code> objects are logged and deleted after confirmation.</li><li>This handler runs every 30 minutes and is configurable via <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L112>machine-safety-orphan-vms-period</a> flag.</li></ul></li><li>Freeze mechanism:<ul><li><code>Safety Controller</code> freezes the <code>MachineDeployment</code> and <code>MachineSet</code> controller if the number of <code>machine</code> objects goes beyond a certain threshold on top of <code>Spec.Replicas</code>. It can be configured by the flag <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L102-L103>&ndash;safety-up or &ndash;safety-down</a> and also <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L113>machine-safety-overshooting-period</a>.</li><li><code>Safety Controller</code> freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li><code>Safety Controller</code> unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></li></ul><h1 id=how-to>How to?</h1><h3 id=how-to-install-mcm-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</h3><p>MCM can be installed in a cluster with following steps:</p><ul><li><p>Apply all the CRDs from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/crds>here</a></p></li><li><p>Apply all the deployment, role-related objects from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/deployment/out-of-tree>here</a>.</p><ul><li>Control cluster is the one where the <code>machine-*</code> objects are stored. Target cluster is where all the node objects are registered.</li></ul></li></ul><h3 id=how-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</h3><p>MCM allows configuring the rollout of the worker machines using <code>maxSurge</code> and <code>maxUnavailable</code> fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios.
The overall process is very similar to how the <code>Deployment Controller</code> manages pods during <code>RollingUpdate</code>.</p><ul><li><code>maxSurge</code> refers to the number of additional machines that can be added on top of the <code>Spec.Replicas</code> of MachineDeployment <em>during rollout process</em>.</li><li><code>maxUnavailable</code> refers to the number of machines that can be deleted from <code>Spec.Replicas</code> field of the MachineDeployment <em>during rollout process</em>.</li></ul><h3 id=how-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</h3><p>During scale down, triggered via <code>MachineDeployment</code>/<code>MachineSet</code>, MCM prefers to delete the <code>machine/s</code> which have the least priority set.
Each <code>machine</code> object has an annotation <code>machinepriority.machine.sapcloud.io</code> set to <code>3</code> by default. Admin can reduce the priority of the given machines by changing the annotation value to <code>1</code>. The next scale down by <code>MachineDeployment</code> shall delete the machines with the least priority first.</p><h3 id=how-to-force-delete-a-machine>How to force delete a machine?</h3><p>A machine can be force deleted by adding the label <code>force-deletion: "True"</code> on the <code>machine</code> object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.</p><h3 id=how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</h3><p>An ongoing rolling-update of the machine-deployment can be paused by using <code>spec.paused</code> field. See the example below:</p><pre tabindex=0><code>apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineDeployment
metadata:
  name: test-machine-deployment
spec:
  paused: true
</code></pre><p>It can be unpaused again by removing the <code>Paused</code> field from the machine-deployment.</p><h3 id=how-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</h3><p>MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM.
However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs.
To do so they should remove/not-use tags on their VMs containing the following strings:</p><ol><li><code>kubernetes.io/cluster/</code></li><li><code>kubernetes.io/role/</code></li><li><code>kubernetes-io-cluster-</code></li><li><code>kubernetes-io-role-</code></li></ol><h1 id=internals>Internals</h1><h3 id=what-is-the-high-level-design-of-mcm>What is the high level design of MCM?</h3><p>Please refer the following <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>document</a>.</p><h3 id=what-are-the-different-configuration-options-in-mcm>What are the different configuration options in MCM?</h3><p>MCM allows configuring many knobs to fine-tune its behavior according to the user&rsquo;s need.
Please refer to the <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go>link</a> to check the exact configuration options.</p><h3 id=what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</h3><p>A machine&rsquo;s lifecycle is governed by mainly following timeouts, which can be configured <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34>here</a>.</p><ul><li><code>MachineDrainTimeout</code>: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours.</li><li><code>MachineHealthTimeout</code>: Amount of time after which an unhealthy machine is declared <code>Failed</code> and the machine is replaced by <code>MachineSet</code> controller.</li><li><code>MachineCreationTimeout</code>: Amount of time after which a machine creation is declared <code>Failed</code> and the machine is replaced by the <code>MachineSet</code> controller.</li><li><code>NodeConditions</code>: List of node conditions which if set to true for <code>MachineHealthTimeout</code> period, the machine is declared <code>Failed</code> and replaced by <code>MachineSet</code> controller.</li><li><code>MaxEvictRetries</code>: An integer number depicting the number of times a failed <em>eviction</em> should be retried on a pod during drain process. A pod is <em>deleted</em> after <code>max-retries</code>.</li></ul><h3 id=how-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</h3><p>MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications.
Drain internally uses the <code>EvictionAPI</code> to evict the pods and triggers the <code>Deletion</code> of pods after <code>MachineDrainTimeout</code>. Please note:</p><ul><li>Stateless pods are evicted in parallel.</li><li>Stateful applications (with PVCs) are serially evicted. Please find more info in this <a href=#how-are-the-stateful-applications-drained-during-machine-deletion>answer below</a>.</li></ul><h3 id=how-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</h3><p>Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways.
It is implemented in the following manner:</p><ul><li>Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object&rsquo;s <code>.status.volumesAttached</code> to be removed by KCM. It does the same for all the stateful-pods.</li><li>It waits for <code>PvDetachTimeout</code> (default 2 minutes) for a given pod&rsquo;s PVC to be removed, else moves forward.</li></ul><h3 id=how-does-maxevictretries-configuration-work-with-draintimeout-configuration>How does <code>maxEvictRetries</code> configuration work with <code>drainTimeout</code> configuration?</h3><p>It is recommended to only set <code>MachineDrainTimeout</code>. It satisfies the related requirements. <code>MaxEvictRetries</code> is auto-calculated based on <code>MachineDrainTimeout</code>, if <code>maxEvictRetries</code> is not provided. Following will be the overall behavior of both configurations together:</p><ul><li>If <code>maxEvictRetries</code> isn&rsquo;t set and only <code>maxDrainTimeout</code> is set:<ul><li>MCM auto calculates the <code>maxEvictRetries</code> based on the <code>drainTimeout</code>.</li></ul></li><li>If <code>drainTimeout</code> isn&rsquo;t set and only <code>maxEvictRetries</code> is set:<ul><li>Default <code>drainTimeout</code> and user provided <code>maxEvictRetries</code> for each pod is considered.</li></ul></li><li>If both <code>maxEvictRetries</code> and <code>drainTimoeut</code> are set:<ul><li>Then both will be respected.</li></ul></li><li>If none are set:<ul><li>Defaults are respected.</li></ul></li></ul><h3 id=what-are-the-different-phases-of-a-machine>What are the different phases of a machine?</h3><p>A phase of a <code>machine</code> can be identified with <code>Machine.Status.CurrentStatus.Phase</code>. Following are the possible phases of a <code>machine</code> object:</p><ul><li><code>Pending</code>: Machine creation call has succeeded. MCM is waiting for machine to join the cluster.</li><li><code>CrashLoopBackOff</code>: Machine creation call has failed. MCM will retry the operation after a minor delay.</li><li><code>Running</code>: Machine creation call has succeeded. Machine has joined the cluster successfully.</li><li><code>Unknown</code>: Machine health checks are failing, eg <code>kubelet</code> has stopped posting the status.</li><li><code>Failed</code>: Machine health checks have failed for a prolonged time. Hence it is declared failed. <code>MachineSet</code> controller will replace such machines immediately.</li><li><code>Terminating</code>: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the <code>machine</code> object. It also includes time when it&rsquo;s being drained.</li></ul><h1 id=troubleshooting>Troubleshooting</h1><h3 id=my-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be deleted.
Though following could be the reasons but not limited to:</p><ul><li>Pod/s with misconfigured PDBs block the drain operation. PDBs with <code>maxUnavailable</code> set to 0, doesn&rsquo;t allow the eviction of the pods. Hence, drain/eviction is retried till <code>MachineDrainTimeout</code>. Default <code>MachineDrainTimeout</code> could be as large as ~2hours. Hence, blocking the machine deletion.<ul><li>Short term: User can manually delete the pod in the question, <em>with caution</em>.</li><li>Long term: Please set more appropriate PDBs which allow disruption of at least one pod.</li></ul></li><li>Expired cloud credentials can block the deletion of the machine from infrastructure.</li><li>Cloud provider can&rsquo;t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.</li></ul><h3 id=my-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be created.
It could possibly be debugged with following steps:</p><ul><li>Verify if the machine is actually created in the cloud. User can use the <code>Machine.Spec.ProviderId</code> to query the machine in cloud.</li><li>A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if <code>MachineDeployment</code> is pointing the correct <code>MachineClass</code>, and <code>MachineClass</code> is pointing to the correct <code>Secret</code>. The secret object contains the actual cloud-config in <code>base64</code> format which will be used to boot the machine.</li><li>User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.</li></ul><h1 id=developer>Developer</h1><h3 id=how-should-i-test-my-code-before-submitting-a-pr>How should I test my code before submitting a PR?</h3><ul><li>Developer can locally setup the MCM using following <a href=/docs/other-components/machine-controller-manager/docs/development/local_setup/>guide</a></li><li>Developer must also enhance the unit tests related to the incoming changes.</li><li>Developer can locally run the unit test by executing:</li></ul><pre tabindex=0><code>make test-unit
</code></pre><h3 id=i-need-to-change-the-apis-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</h3><p>Developer should add/update the API fields at both of the following places:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go</a></li><li><a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1>https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1</a></li></ul><p>Once API changes are done, auto-generate the code using following command:</p><pre tabindex=0><code>./hack/generate-code
</code></pre><p>Please ignore the API-violation errors for now.</p><h3 id=how-can-i-update-the-dependencies-of-mcm>How can I update the dependencies of MCM?</h3><p>MCM uses <code>gomod</code> for depedency management.
Developer should add/udpate depedency in the go.mod file. Please run following command to automatically revendor the dependencies.</p><pre tabindex=0><code>make revendor
</code></pre><h1 id=in-the-context-of-gardener>In the context of Gardener</h1><h3 id=how-can-i-configure-mcm-using-shoot-resource>How can I configure MCM using Shoot resource?</h3><p>All of the knobs of MCM can be configured by the <code>workers</code> <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>section</a> of the shoot resource.</p><ul><li>Gardener creates a <code>MachineDeployment</code> per zone for each worker-pool under <code>workers</code> section.</li><li><code>workers.dataVolumes</code> allows to attach multiple disks to a machine during creation. Refer the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>link</a>.</li><li><code>workers.machineControllerManager</code> allows configuration of multiple knobs of the <code>MachineDeployment</code> from the shoot resource.</li></ul><h3 id=how-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</h3><p>Shoot resource allows the worker-pool to spread across multiple zones using the field <code>workers.zones</code>. Refer <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L115>link</a>.</p><ul><li>Gardener creates one <code>MachineDeployment</code> per zone. Each <code>MachineDeployment</code> is initiated with the following replica:</li></ul><pre tabindex=0><code>MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availibility zones)
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-7a191e685b90cf1ddfdb3a9230a2ed0c>8.2.1.7 - Outline</h1><h1 id=machine-controller-manager>Machine Controller Manager</h1><p>CORE &ndash; ./machine-controller-manager(provider independent)
Out of tree : Machine controller (provider specific)
MCM is a set controllers:</p><ul><li><p>Machine Deployment Controller</p></li><li><p>Machine Set Controller</p></li><li><p>Machine Controller</p></li><li><p>Machine Safety Controller</p></li></ul><h2 id=questions-and-refactoring-suggestions>Questions and refactoring Suggestions</h2><h3 id=refactoring>Refactoring</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>ConcurrentNodeSyncs” bad name - nothing to do with node syncs actually.<br>If its value is ’10’ then it will start 10 goroutines (workers) per resource type (machine, machinist, machinedeployment, provider-specific-class, node - study the different resource types.</td><td>cmd/machine-controller-manager/app/options/options.go</td><td>pending</td></tr><tr><td>LeaderElectionConfiguration is very similar to the one present in “client-go/tools/leaderelection/leaderelection.go” - can we simply used the one in client-go instead of defining again?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>Have all userAgents as constant. Right now there is just one.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t run function be defined on MCMServer struct itself?</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>clientcmd.BuildConfigFromFlags fallsback to inClusterConfig which will surely not work as that is not the target. Should it not check and exit early?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>A more direct way to create an in cluster config is using <code>k8s.io/client-go/rest</code> -> rest.InClusterConfig instead of using clientcmd.BuildConfigFromFlags passing empty arguments and depending upon the implementation to fallback to creating a inClusterConfig. If they change the implementation that you get affected.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Introduce a method on MCMServer which gets a target KubeConfig and controlKubeConfig or alternatively which creates respective clients.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Why can’t we use Kubernetes.NewConfigOrDie also for kubeClientControl?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>I do not see any benefit of client builders actually. All you need to do is pass in a config and then directly use client-go functions to create a client.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Function: getAvailableResources - rename this to getApiServerResources</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Move the method which waits for API server to up and ready to a separate method which returns a discoveryClient when the API server is ready.</td><td>cmd/app/controllermanager.go - getAvailableResources function</td><td>pending</td></tr><tr><td>Many methods in client-go used are now deprecated. Switch to the ones that are now recommended to be used instead.</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>This method needs a general overhaul</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>If the design is influenced/copied from KCM then its very different. There are different controller structs defined for deployment, replicaset etc which makes the code much more clearer. You can see “kubernetes/cmd/kube-controller-manager/apps.go” and then follow the trail from there. - agreed needs to be changed in future (if time permits)</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>I am not sure why “MachineSetControlInterface”, “RevisionControlInterface”, “MachineControlInterface”, “FakeMachineControl” are defined in this file?</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td><code>IsMachineActive</code> - combine the first 2 conditions into one with OR.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Minor change - correct the comment, first word should always be the method name. Currently none of the comments have correct names.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>There are too many deep copies made. What is the need to make another deep copy in this method? You are not really changing anything here.</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>Why can&rsquo;t these validations be done as part of a validating webhook?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>Small change to the following <code>if</code> condition. <code>else if</code> is not required a simple <code>else</code> is sufficient. <a href=#1.1-code1>Code1</a></td><td></td><td></td></tr><tr><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td><td></td></tr><tr><td>Why call these <code>inactiveMachines</code>, these are live and running and therefore active.</td><td>pkg/controller/machineset.go - terminateMachines</td><td>pending</td></tr></tbody></table><h3 id=clarification>Clarification</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>Why are there 2 versions - internal and external versions?</td><td>General</td><td>pending</td></tr><tr><td>Safety controller freezes MCM controllers in the following cases:<br>* Num replicas go beyond a threshold (above the defined replicas)<br>* Target API service is not reachable<br>There seems to be an overlap between DWD and MCM Safety controller. In the meltdown scenario why is MCM being added to DWD, you could have used Safety controller for that.</td><td>General</td><td>pending</td></tr><tr><td>All machine resources are v1alpha1 - should we not promote it to beta. V1alpha1 has a different semantic and does not give any confidence to the consumers.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t controller manager use context.Context instead of creating a stop channel? - Check if signals (<code>os.Interrupt</code> and <code>SIGTERM</code> are handled properly. Do not see code where this is handled currently.)</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>What is the rationale behind a timeout of 10s? If the API server is not up, should this not just block as it can anyways not do anything. Also, if there is an error returned then you exit the MCM which does not make much sense actually as it will be started again and you will again do the poll for the API server to come back up. Forcing an exit of MCM will not have any impact on the reachability of the API server in anyway so why exit?</td><td>cmd/app/controllermanager.go - getAvailableResources</td><td>pending</td></tr><tr><td>There is a very weird check - <code>availableResources[machineGVR] || availableResources[machineSetGVR] || availableResources[machineDeploymentGVR]</code><br>Shouldn’t this be conjunction instead of disjunction?<br>* What happens if you do not find one or all of these resources?<br>Currently an error log is printed and nothing else is done. MCM can be used outside gardener context where consumers can directly create MachineClass and Machine and not create MachineSet / Maching Deployment. There is no distinction made between context (gardener or outside-gardener).<br></td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Instead of having an empty select {} to block forever, isn’t it better to wait on the stop channel?</td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Do we need provider specific queues and syncs and listers</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>Why are resource types prefixed with “Cluster”? - not sure , check PR</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>When will forgetAfterSuccess be false and why? - as per the current code this is never the case. - Himanshu will check</td><td>cmd/app/controllermanager.go - createWorker</td><td>pending</td></tr><tr><td>What is the use of “ExpectationsInterface” and “UIDTrackingContExpectations”?<br>* All expectations related code should be in its own file “expectations.go” and not in this file.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Why do we not use lister but directly use the controlMachingClient to get the deployment? Is it because you want to avoid any potential delays caused by update of the local cache held by the informer and accessed by the lister? What is the load on API server due to this?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why is this conversion needed? <a href=#1.2-code2>code2</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>A deep copy of <code>machineDeployment</code> is already passed and within the function another deepCopy is made. Any reason for it?</td><td>pkg/controller/deployment.go - addMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>What is an <code>Status.ObservedGeneration</code>?<br>*<em>Read more about generations and observedGeneration at:<br><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata>https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata</a><br></em><a href=https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792>https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792</a><br>Ideally the update to the <code>ObservedGeneration</code> should only be made after successful reconciliation and not before. I see that this is just copied from <code>deployment_controller.go</code> as is</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why and when will a <code>MachineDeployment</code> be marked as frozen and when will it be un-frozen?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Shoudn&rsquo;t the validation of the machine deployment be done during the creation via a validating webhook instead of allowing it to be stored in etcd and then failing the validation during sync? I saw the checks and these can be done via validation webhook.</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>RollbackTo has been marked as deprecated. What is the replacement? <a href=#1.3-code3>code3</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>What is the max machineSet deletions that you could process in a single run? The reason for asking this question is that for every machineSetDeletion a new goroutine spawned.<br>* Is the <code>Delete</code> call a synchrounous call? Which means it blocks till the machineset deletion is triggered which then also deletes the machines (due to cascade-delete and blockOwnerDeletion= true)?</td><td>pkg/controller/deployment.go - terminateMachineSets</td><td>pending</td></tr><tr><td>If there are validation errors or error when creating label selector then a nil is returned. In the worker reconcile loop if the return value is nil then it will remove it from the queue (forget + done). What is the way to see any errors? Typically when we describe a resource the errors are displayed. Will these be displayed when we discribe a <code>MachineDeployment</code>?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>If an error is returned by <code>updateMachineSetStatus</code> and it is <code>IsNotFound</code> error then returning an error will again queue the <code>MachineSet</code>. Is this desired as <code>IsNotFound</code> indicates the <code>MachineSet</code> has been deleted and is no longer there?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>is <code>machineControl.DeleteMachine</code> a synchronous operation which will wait till the machine has been deleted? Also where is the <code>DeletionTimestamp</code> set on the <code>Machine</code>? Will it be automatically done by the API server?</td><td>pkg/controller/deployment.go - prepareMachineForDeletion</td><td>pending</td></tr></tbody></table><h3 id=bugsenhancements>Bugs/Enhancements</h3><table><thead><tr><th>Statement + TODO</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>This defines QPS and Burst for its requests to the KAPI. Check if it would make sense to explicitly define a FlowSchema and PriorityLevelConfiguration to ensure that the requests from this controller are given a well-defined preference. What is the rational behind deciding these values?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>In function “validateMachineSpec” fldPath func parameter is never used.</td><td>pkg/apis/machine/validation/machine.go</td><td>pending</td></tr><tr><td>If there is an update failure then this method recursively calls itself without any sort of delays which could lead to a LOT of load on the API server. (opened: <a href=https://github.com/gardener/machine-controller-manager/issues/686>https://github.com/gardener/machine-controller-manager/issues/686</a>)</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>We are updating <code>filteredMachines</code> by invoking <code>syncMachinesNodeTemplates</code>, <code>syncMachinesConfig</code> and <code>syncMachinesClassKind</code> but we do not create any deepCopy here. Everywhere else the general principle is when you mutate always make a deepCopy and then mutate the copy instead of the original as a lister is used and that changes the cached copy.<br><code>Fix</code>: <code>SatisfiedExpectations</code> check has been commented and there is a TODO there to fix it. Is there a PR for this?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr></tbody></table><p>Code references</p><h1 id=11-code1>1.1 code1</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>       <span style=color:#00f>if</span> machineSet.DeletionTimestamp == <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// manageReplicas is the core machineSet method where scale up/down occurs
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// It is not called when deletion timestamp is set
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		manageReplicasErr = c.manageReplicas(ctx, filteredMachines, machineSet)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span>​</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        	} <span style=color:#00f>else</span> <span style=color:#00f>if</span> machineSet.DeletionTimestamp != <span style=color:#00f>nil</span> { 
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            <span style=color:green>//FIX: change this to simple else without the if
</span></span></span></code></pre></div><h1 id=12-code2>1.2 code2</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>    <span style=color:#00f>defer</span> dc.enqueueMachineDeploymentAfter(deployment, 10*time.Minute)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    *  <span style=color:#a31515>`Clarification`</span>:  Why  is  this  conversion  needed<span>?</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    err = v1alpha1.Convert_v1alpha1_MachineDeployment_To_machine_MachineDeployment(deployment, internalMachineDeployment, <span style=color:#00f>nil</span>)
</span></span></code></pre></div><h1 id=13-code3>1.3 code3</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// rollback is not re-entrant in case the underlying machine sets are updated with a new
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// revision so we should ensure that we won&#39;t proceed to update machine sets until we
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:#00f>if</span> d.Spec.RollbackTo != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#00f>return</span> dc.rollback(ctx, d, machineSets, machineMap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	}
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6ce2ea9aa86ba7547491965abbb81343>9 - Curated Links</h1><div class=lead>Interesting and useful content on Kubernetes</div><p>A curated list of awesome Kubernetes sources.
Inspired by <a href=https://github.com/sindresorhus/awesome>@sindresorhus&rsquo; awesome</a></p><h3 id=setup>Setup</h3><ul><li><a href=https://docs.docker.com/docker-for-mac/>Install Docker for Mac</a></li><li><a href=https://docs.docker.com/docker-for-windows/install/>Install Docker for Windows</a></li><li><a href=https://kubernetes.io/docs/tasks/tools/install-minikube/>Run a Kubernetes Cluster on your local machine</a></li></ul><h3 id=a-place-that-marks-the-beginning-of-a-journey>A Place That Marks the Beginning of a Journey</h3><ul><li><a href="https://docs.google.com/presentation/d/1JqcALpsg07eH665ZXQrIvOcin6SzzsIUjMRRVivrZMg/edit?usp=sharing">Kubernetes Community Overview and Contributions Guide</a> by <a href=https://twitter.com/idvoretskyi/>Ihor Dvoretskyi</a></li><li><a href=http://www.ctl.io/developers/blog/post/what-is-kubernetes-and-how-to-use-it/>An Intro to Google’s Kubernetes and How to Use It</a> by <a href=https://twitter.com/rhein_wein>Laura Frank</a></li><li><a href=http://containertutorials.com/get_started_kubernetes/index.html>Getting Started on Kubernetes</a> by <a href=https://twitter.com/rajdeepdua>Rajdeep Dua</a></li><li><a href=https://github.com/meteorhacks/meteorhacks.github.io/blob/master/_posts/2015-04-22-learn-kubernetes-the-future-of-the-cloud.md>Kubernetes: The Future of Cloud Hosting</a> by <a href=https://twitter.com/meteorhacks>Meteorhacks</a></li><li><a href=http://thevirtualizationguy.wordpress.com/tag/kubernetes/>Kubernetes by Google</a> by <a href=https://twitter.com/GastonPantana>Gaston Pantana</a></li><li><a href=https://keithtenzer.com/containers/application-containers-kubernetes-and-docker-from-scratch/>Application Containers: Kubernetes and Docker from Scratch</a> by <a href=https://twitter.com/keithtenzer>Keith Tenzer</a></li><li><a href=http://omerio.com/2015/12/18/learn-the-kubernetes-key-concepts-in-10-minutes/>Learn the Kubernetes Key Concepts in 10 Minutes</a> by <a href=https://twitter.com/omerio>Omer Dawelbeit</a></li><li><a href=http://deis.com/blog/2016/kubernetes-illustrated-guide/>The Children&rsquo;s Illustrated Guide to Kubernetes</a> by <a href=https://github.com/deis>Deis</a> :-)</li><li><a href=http://medium.com/@mhausenblas/the-kubectl-run-command-27c68de5cb76#.mlwi5an7o>The ‘kubectl run’ command</a> by <a href=https://twitter.com/mhausenblas>Michael Hausenblas</a></li><li><a href=https://github.com/xiaopeng163/docker-k8s-lab>Docker Kubernetes Lab Handbook</a> by <a href=https://twitter.com/xiaopeng163>Peng Xiao</a></li></ul><h2 id=interactive-learning-environments>Interactive Learning Environments</h2><p><em>Learn Kubernetes using an interactive environment without requiring downloads or configuration</em></p><ul><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/cluster-interactive/>Interactive Tutorial</a></li><li><a href=http://labs.play-with-k8s.com/>Play with Kubernetes</a></li><li><a href=http://kubernetesbootcamp.github.io/kubernetes-bootcamp/>Kubernetes Bootcamp</a></li></ul><h2 id=massive-open-online-courses--tutorials>Massive Open Online Courses / Tutorials</h2><p><em>List of available free online courses(<a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOC</a>) and tutorials</em></p><ul><li><a href=https://devopswithkubernetes.com/>DevOps with Kubernetes</a></li><li><a href=https://www.my-mooc.com/en/mooc/introduction-to-kubernetes/>Introduction to Kubernetes</a></li></ul><h3 id=courses>Courses</h3><ul><li><a href=http://in.udacity.com/course/scalable-microservices-with-kubernetes--ud615>Scalable Microservices with Kubernetes at Udacity</a></li><li><a href=http://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x>Introduction to Kubernetes at edX</a></li></ul><h3 id=tutorials>Tutorials</h3><ul><li><a href=http://kubernetes.io/docs/tutorials/>Kubernetes Tutorials by Kubernetes Team</a></li><li><a href=https://kubebyexample.com/>Kubernetes By Example by OpenShift Team</a></li><li><a href=http://www.tutorialspoint.com/kubernetes/>Kubernetes Tutorial by Tutorialspoint</a></li></ul><h2 id=package-managers>Package Managers</h2><ul><li><a href=http://helm.sh>Helm</a></li><li><a href=https://github.com/coreos/kpm>KPM</a></li></ul><h2 id=rpc>RPC</h2><ul><li><a href=http://grpc.io>gRPC</a></li></ul><h2 id=secret-generation-and-management>Secret Generation and Management</h2><ul><li><a href=http://www.vaultproject.io/docs/auth/kubernetes.html>Vault auth plugin backend: Kubernetes</a></li><li><a href=https://github.com/kelseyhightower/vault-controller>Vault controller</a></li><li><a href=https://github.com/jetstack/kube-lego>kube-lego</a></li><li><a href=https://github.com/dtan4/k8sec>k8sec</a></li><li><a href=https://github.com/Boostport/kubernetes-vault>kubernetes-vault</a></li><li><a href=https://github.com/shyiko/kubesec>kubesec</a> - Secure Secret management</li></ul><h2 id=machine-learning>Machine Learning</h2><ul><li><a href=https://github.com/tensorflow/k8s>TensorFlow k8s</a></li><li><a href=https://github.com/deepinsight/mxnet-operator>mxnet-operator</a> - Tools for ML/MXNet on Kubernetes.</li><li><a href=https://github.com/google/kubeflow>kubeflow</a> - Machine Learning Toolkit for Kubernetes.</li><li><a href=https://github.com/SeldonIO/seldon-core>seldon-core</a> - Open source framework for deploying machine learning models on Kubernetes</li></ul><h2 id=raspberry-pi>Raspberry Pi</h2><p><em>Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.</em></p><ul><li><a href=http://kubecloud.io>Kubecloud</a></li><li><a href=https://kubecloud.io/setting-up-a-kubernetes-on-arm-cluster-on-raspberry-pis-f7f64065138c>Setting up a Kubernetes on ARM cluster</a></li><li><a href=https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/>Setup Kubernetes on a Raspberry Pi Cluster easily the official way!</a> by <a href=https://blog.hypriot.com/crew/>Mathias Renner and Lucas Käldström</a></li><li><a href=https://www.hanselman.com/blog/HowToBuildAKubernetesClusterWithARMRaspberryPiThenRunNETCoreOnOpenFaas.aspx>How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas</a> by <a href=https://twitter.com/shanselman>Scott Hanselman</a></li></ul><h2 id=contributing>Contributing</h2><p>Contributions are most welcome!</p><p>This list is just getting started, please contribute to make it super awesome.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4985cb55ddfb184639d767ec54b9f0f7>10 - Contribute</h1><div class=lead>The project&rsquo;s contributor guide for code and documentation</div><h1 id=contributing-to-gardener>Contributing to Gardener</h1><h2 id=welcome>Welcome</h2><p>Welcome to the Contributor section of Gardener. Here you can learn how it is possible for you to contribute your ideas and expertise to the project and have it grow even more.</p><h2 id=prerequisites>Prerequisites</h2><p>Before you begin contributing to Gardener, there are a couple of things you should become familiar with and complete first.</p><h3 id=code-of-conduct>Code of Conduct</h3><p>All members of the Gardener community must abide by the
<a href=https://github.com/cncf/foundation/blob/master/code-of-conduct.md>CNCF Code of Conduct</a>.
Only by respecting each other can we develop a productive, collaborative community.
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting <a href=mailto:gardener.opensource@sap.com>gardener.opensource@sap.com</a> and/or a Gardener project maintainer.</p><h3 id=developer-certificate-of-origin>Developer Certificate of Origin</h3><p>Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use <a href=https://developercertificate.org/>the standard DCO text of the Linux Foundation</a>.</p><h3 id=license>License</h3><p>Your contributions to Gardener must be licensed properly:</p><ul><li>Code contributions must be licensed under the <a href=http://www.apache.org/licenses/LICENSE-2.0>Apache 2.0 License</a></li><li>Documentation contributions must be licensed under the <a href=https://creativecommons.org/licenses/by/4.0/legalcode>Creative Commons Attribution 4.0 International License</a></li></ul><h2 id=contributing>Contributing</h2><p>Gardener uses GitHub to manage reviews of pull requests.</p><ul><li><p>If you are a new contributor see: <a href=#steps-to-contribute>Steps to Contribute</a></p></li><li><p>If you have a trivial fix or improvement, go ahead and create a pull request.</p></li><li><p>If you plan to do something more involved, first discuss your ideas
on our <a href=https://groups.google.com/forum/?fromgroups#!forum/gardener>mailing list</a>.
This will avoid unnecessary work and surely give you and us a good deal
of inspiration.</p></li><li><p>Relevant coding style guidelines are the <a href=https://github.com/golang/go/wiki/CodeReviewComments>Go Code Review
Comments</a>
and the <em>Formatting and style</em> section of Peter Bourgon&rsquo;s <a href=http://peter.bourgon.org/go-in-production/#formatting-and-style>Go: Best
Practices for Production
Environments</a>.</p></li></ul><h2 id=steps-to-contribute>Steps to Contribute</h2><p>Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.</p><p>If you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.</p><p>We kindly ask you to follow the <a href=#pull-request-checklist>Pull Request Checklist</a> to ensure reviews can happen accordingly.</p><h2 id=pull-request-checklist>Pull Request Checklist</h2><ul><li><p>Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn&rsquo;t merge cleanly with master you may be asked to rebase your changes.</p></li><li><p>Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).</p></li><li><p>Test your changes as thoroughly as possible before your commit them. Preferably, automate your testing with <a href=/docs/gardener/development/testing/>unit / integration tests</a>. If tested manually, provide information about the test scope in the PR description (e.g. “Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.”).</p></li><li><p>Create <em>Work In Progress [WIP]</em> pull requests only if you need a clarification or an explicit review before you can continue your work item.</p></li><li><p>If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our <a href=https://groups.google.com/forum/?fromgroups#!forum/gardener>mailing list</a>.</p></li><li><p>If you add new features, make sure that they are documented in the <a href=https://github.com/gardener/documentation>Gardener documentation</a>.</p></li><li><p>If your changes are relevant for operators, consider to update the <a href=https://github.com/gardener/ops-toolbelt>ops toolbelt image</a>.</p></li><li><p>Post review:</p><ul><li>If a review requires you to change your commit(s), please test the changes again.</li><li>Amend the affected commit(s) and force push onto your branch.</li><li>Set respective comments in your GitHub review to resolved.</li><li>Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.</li></ul></li></ul><h2 id=contributing-bigger-changes>Contributing Bigger Changes</h2><p>If you want to contribute bigger changes to Gardener, such as when introducing new API resources and their corresponding controllers, or implementing an approved <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>Gardener Enhancement Proposal</a>, follow the guidelines outlined in <a href=/docs/contribute/10_code/10_contributing_bigger_changes/>Contributing Bigger Changes</a>.</p><h2 id=issues-and-planning>Issues and Planning</h2><p>We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren&rsquo;t restricted to the issue template provided by the Gardener maintainers.</p><p>ZenHub is used for planning:</p><ul><li>Install the <a href=https://chrome.google.com/webstore/detail/zenhub-for-github/ogcgkffhplmphkaahpmffcafajaocjbd>ZenHub Chrome plugin</a></li><li>Login to <a href=https://www.zenhub.com/>ZenHub</a></li><li>Open the <a href=https://app.zenhub.com/workspace/o/gardener/gardener>Gardener ZenHub workspace</a></li></ul><h2 id=security-release-process>Security Release Process</h2><p>See <a href=https://github.com/gardener/documentation/blob/master/security-release-process.md>Security Release Process</a>.</p><h2 id=community>Community</h2><h3 id=slack-channel>Slack Channel</h3><p><a href=https://kubernetes.slack.com/messages/gardener>#gardener</a>, sign up <a href=http://slack.k8s.io/>here</a>.</p><h3 id=mailing-list>Mailing List</h3><p><a href=https://groups.google.com/forum/?fromgroups#!forum/gardener>gardener@googlegroups.com</a></p><p>The mailing list is hosted through Google Groups. To receive the lists&rsquo; emails, <a href=https://support.google.com/groups/answer/1067205>join the group</a> as you would any other Google Group.</p><h3 id=other>Other</h3><p>For additional channels where you can reach us, as well as links to our bi-weekly meetings, visit the <a href=/community>Community page</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a5fa4df509236d74f89bbc22f343d978>10.1 - Contributing Code</h1><p>You are welcome to <strong>contribute code</strong> to Gardener in order to fix a bug or to implement a new feature.</p><p>The following rules govern code contributions:</p><ul><li>Contributions must be licensed under the <a href=http://www.apache.org/licenses/LICENSE-2.0>Apache 2.0 License</a></li><li>You need to sign the Contributor License Agreement. We are using <em><a href=https://cla-assistant.io/>CLA assistant</a></em> providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-90f91c19c664d27748e99faef9e875a9>10.1.1 - Contributing Bigger Changes</h1><h2 id=contributing-bigger-changes>Contributing Bigger Changes</h2><p>Here are the guidelines you should follow when contributing larger changes to Gardener:</p><ul><li><p>Avoid proposing a big change in one single PR. Instead, split your work into multiple stages which are independently mergeable and create one PR for each stage. For example, if introducing a new API resource and its controller, these stages could be:</p><ul><li>API resource types, including defaults and generated code.</li><li>API resource validation.</li><li>API server storage.</li><li>Admission plugin(s), if any.</li><li>Controller(s), including changes to existing controllers. Split this phase further into different functional subsets if appropriate.</li></ul></li><li><p>If you realize later that changes to artifacts introduced in a previous stage are required, by all means make them and explain in the PR why they were needed.</p></li><li><p>Consider splitting a big PR further into multiple commits to allow for more focused reviews. For example, you could add unit tests / documentation in separate commits from the rest of the code. If you have to adapt your PR to review feedback, prefer doing that also in a separate commit to make it easier for reviewers to check how their feedback has been addressed.</p></li><li><p>To make the review process more efficient and avoid too many long discussions in the PR itself, ask for a &ldquo;main reviewer&rdquo; to be assigned to your change, then work with this person to make sure he or she understands it in detail, and agree together on any improvements that may be needed. If you can&rsquo;t reach an agreement on certain topics, comment on the PR and invite other people to join the discussion.</p></li><li><p>Even if you have a &ldquo;main reviewer&rdquo; assigned, you may still get feedback from other reviewers. In general, these &ldquo;non-main reviewers&rdquo; are advised to focus more on the design and overall approach rather than the implementation details. Make sure that you address any concerns on this level appropriately.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2525203726f294062263705771e54f3f>10.1.2 - CI/CD</h1><h1 id=cicd>CI/CD</h1><p>As an execution environment for CI/CD workloads, we use <a href=https://concourse-ci.org>Concourse</a>.
We however abstract from the underlying &ldquo;build executor&rdquo; and instead offer a
<code>Pipeline Definition Contract</code>, through which components declare their build pipelines as
required.</p><p><img src=/__resources/overview_55d06f.png alt=Overview></p><p>In order to run continuous delivery workloads for all components contributing to the
<a href=https://github.com/gardener>Gardener</a> project, we operate a central service.</p><p>Typical workloads encompass the execution of tests and builds of a variety of technologies,
as well as building and publishing container images, typically containing build results.</p><p>We are building our CI/CD offering around some principles:</p><ul><li><em>container-native</em> - each workload is executed within a container environment. Components may customise used container images</li><li><em>automation</em> - pipelines are generated without manual interaction</li><li><em>self-service</em> - components customise their pipelines by changing their sources</li><li><em>standardisation</em></li></ul><p><strong>Learn more on our: <a href=https://gardener.github.io/cc-utils/>Build Pipeline Reference Manual</a></strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-1cf3a14a3a218c3fe1bbf3356a18e887>10.1.3 - Dependencies</h1><h1 id=testing>Testing</h1><p>We follow the BDD-style testing principles and are leveraging the <a href=https://onsi.github.io/ginkgo/>Ginkgo</a> framework along with <a href=http://onsi.github.io/gomega/>Gomega</a> as matcher library. In order to execute the existing tests, you can use</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test         <span style=color:green># runs tests</span>
</span></span><span style=display:flex><span>make verify       <span style=color:green># runs static code checks and test</span>
</span></span></code></pre></div><p>There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the <a href=https://blog.golang.org/cover>Go Cover Tool</a>. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-cov
</span></span><span style=display:flex><span>open gardener.coverage.html
</span></span><span style=display:flex><span>make test-cov-clean
</span></span></code></pre></div><h3 id=sigsk8siocontroller-runtime-env-test>sigs.k8s.io/controller-runtime env test</h3><p>Some of the integration tests in Gardener are using the <code>sigs.k8s.io/controller-runtime/pkg/envtest</code> package.
It sets up a temporary control plane (etcd + kube-apiserver) against the integration tests can run.
The <code>test</code> and <code>test-cov</code> rules in the <code>Makefile</code> prepare this env test automatically by downloading the respective binaries (if not yet present) and set the necessary environment variables.</p><p>You can also run <code>go test</code> or <code>ginkgo</code> without the <code>test</code>/<code>test-cov</code> rules.
In this case you have to set the <code>KUBEBUILDER_ASSETS</code> environment variable to the path that contains the etcd + kube-apiserver binaries or you need to have the binaries pre-installed under <code>/usr/local/kubebuilder/bin</code>.</p><h2 id=dependency-management>Dependency Management</h2><p>We are using <a href=https://github.com/golang/go/wiki/Modules>go modules</a> for depedency management.
In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h3 id=updating-dependencies>Updating Dependencies</h3><p>The <code>Makefile</code> contains a rule called <code>revendor</code> which performs <code>go mod vendor</code> and <code>go mod tidy</code>.
<code>go mod vendor</code> resets the main module&rsquo;s vendor directory to include all packages needed to build and test all the main module&rsquo;s packages. It does not include test code for vendored packages.
<code>go mod tidy</code> makes sure <code>go.mod</code> matches the source code in the module. It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make revendor
</span></span></code></pre></div><p>The dependencies are installed into the <code>vendor</code> folder which <strong>should be added</strong> to the VCS.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Make sure that you test the code after you have updated the dependencies!</div></div><div class=td-content style=page-break-before:always><h1 id=pg-e295840fd7d673d0ce1ea67515a01b0e>10.1.4 - Security Release Process</h1><h1 id=gardener-security-release-process>Gardener Security Release Process</h1><p>Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.</p><h2 id=gardener-security-team>Gardener Security Team</h2><p>Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits.
The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:</p><ul><li>Olaf Beier, (<strong><a href=https://github.com/olafbeier>@olafbeier</a></strong>)</li><li>Vasu Chandrasekhara, (<strong><a href=https://github.com/vasu1124>@vasu1124</a></strong>)</li><li>Alban Crequy, (<strong><a href=https://github.com/alban>@alban</a></strong>)</li><li>Norbert Hamann, (<strong><a href=https://github.com/norberthamann>@norberthamann</a></strong>)</li><li>Claudia Hölters, (<strong><a href=https://github.com/hoeltcl>@hoeltcl</a></strong>)</li><li>Oliver Kling, (<strong><a href=https://github.com/oliverkling>@oliverkling</a></strong>)</li><li>Vedran Lerenc, (<strong><a href=https://github.com/vlerenc>@vlerenc</a></strong>)</li><li>Dirk Marwinski, (<strong><a href=https://github.com/marwinski>@marwinski</a></strong>)</li><li>Michael Schubert, (<strong><a href=https://github.com/schu>@schu</a></strong>)</li><li>Matthias Sohn, (<strong><a href=https://github.com/msohn>@msohn</a></strong>)</li><li>Frederik Thormaehlen, (<strong><a href=https://github.com/ThormaehlenFred>@ThormaehlenFred</a></strong>)</li><li>Christian Cwienk (<strong><a href=https://github.com/ccwienk>@ccwienk</a></strong>)</li></ul><h2 id=disclosures>Disclosures</h2><h3 id=private-disclosure-processes>Private Disclosure Processes</h3><p>The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you&rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to <a href=mailto:secure@sap.com>secure@sap.com</a>. We&rsquo;ll send a confirmation e-mail to acknowledge your report, and we&rsquo;ll send an additional e-mail when we&rsquo;ve identified the issue positively or negatively.</p><h3 id=public-disclosure-processes>Public Disclosure Processes</h3><p>If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to <a href=mailto:secure@sap.com>secure@sap.com</a> to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.</p><p>If possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a <a href=#private-disclosure-process>private disclosure process</a> (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn&rsquo;t necessary and is unlikely to make a public disclosure less damaging.</p><h2 id=patch-release-and-public-communication>Patch, Release, and Public Communication</h2><p>For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the &ldquo;Fix Team&rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the &ldquo;Fix Lead.&rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the &ldquo;Fix team.&rdquo; (i.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score >= 7; see below). If the fix relies on another upstream project&rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.</p><h3 id=fix-team-organization>Fix Team Organization</h3><p>The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team.
The Fix Lead will give the Fix Team access to a private security repository to develop the fix.</p><h3 id=fix-development-process>Fix Development Process</h3><p>The Fix Lead and the Fix Team will create a <a href=https://www.first.org/cvss/specification-document>CVSS</a> using the <a href=https://www.first.org/cvss/calculator/3.0>CVSS Calculator</a>. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect.
The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers.
If the CVSS score is under 7.0 (a <a href=https://www.first.org/cvss/specification-document#i5>medium severity score</a>) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private <a href=#communication-channel>Gardener Security mailing list</a>.</p><h3 id=fix-disclosure-process>Fix Disclosure Process</h3><p>With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the <a href=#communication-channel>Gardener mailing list</a> that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.</p><h3 id=fix-release-day>Fix Release Day</h3><p>The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date.
The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue.
The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will <a href=https://github.com/lgtmco/lgtm>LGTM</a> and merge.
The Release Managers will merge these PRs as quickly as possible. Changes shouldn&rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches.
The Fix Lead will request a CVE from the SAP Product Security Response Team via email to <a href=mailto:cna@sap.com>cna@sap.com</a> with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the <a href=#communication-channel>Gardener mailing list</a> and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.</p><p>As much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia.
The Fix Lead will remove the Fix Team from the private security repository.</p><h3 id=retrospective>Retrospective</h3><p>These steps should be completed after the Release Date. The retrospective process <a href=https://landing.google.com/sre/book/chapters/postmortem-culture.html>should be blameless</a>.</p><p>The Fix Lead will send a retrospective of the process to the <a href=#communication-channel>Gardener mailing list</a> including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process.
The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the <a href=#communication-channel>Gardener mailing list</a>. Honest critique is the only way we are going to get good at this as a community.</p><h3 id=communication-channel>Communication Channel</h3><p>The <a href=#private-disclosure-process>private</a> or <a href=#public-disclosure-process>public disclosure process</a> should be triggered exclusively by writing an e-mail to <a href=mailto:secure@sap.com>secure@sap.com</a>.</p><p>Gardener security announcements will be communicated by the Fix Lead sending an e-mail to the <a href=https://groups.google.com/forum/#!forum/gardener>Gardener mailing list</a> (reachable via <a href=mailto:gardener@googlegroups.com>gardener@googlegroups.com</a>) as well as posting a link in the <a href=https://kubernetes.slack.com/messages/CB57N0BFG/details/>Gardener Slack channel</a>. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (<a href="https://support.google.com/groups/answer/1067205?hl=en">how to find and join a group</a>)</p><p>The members of the <a href=#gardener-security-team>Gardener Security Team</a> are subscribed to the private <a href=https://groups.google.com/forum/#!forum/gardener-security>Gardener Security mailing list</a> (reachable via <a href=mailto:gardener-security@googlegroups.com>gardener-security@googlegroups.com</a>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-20f6c6ae2bde2a86d198747232bac9f7>10.2 - Contributing Documentation</h1><p>You are welcome to <strong>contribute documentation</strong> to Gardener.</p><p>The following rules govern documentation contributions:</p><ul><li>Contributions must be licensed under the <a href=https://creativecommons.org/licenses/by/4.0/legalcode>Creative Commons Attribution 4.0 International License</a></li><li>You need to sign the Contributor License Agreement. We are using <em><a href=https://cla-assistant.io/>CLA assistant</a></em> providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ed220fc8e42f8f812a449fc99dc8d493>10.2.1 - Working with Images</h1><p>Using images on the website has to contribute to the aestethics and comprehendability of the materials, with uncompromised experience when loading and browsing pages. That concerns crisp clear images, their consistent layout and color scheme, dimensions and aspect ratios, flicker-free and fast loading or the feeling of it, even on unreliable mobile networks and devices.</p><h2 id=image-production-guidelines>Image Production Guidelines</h2><p>A good, detailed reference for optimal use of images for the web can be found at web.dev&rsquo;s <a href="https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/image-optimization?hl=en">Fast Load Times</a> topic. The following summarizes some key points plus suggestions for tools support.</p><p>You are strongly encouraged to use <strong>vector images</strong> (SVG) as much as possible. They scale seamlessly without compromising the quality and are easier to maintain.</p><p>If you are just now starting with SVG authoring, here are some tools suggestions: <a href=https://www.figma.com/>Figma</a> (online/Win/Mac), <a href=https://www.sketch.com/>Sketch</a> (Mac only).</p><p>For <strong>raster images</strong> (JPG, PNG, GIF), consider the following requirements and choose a tool that enables you to conform to them:</p><ul><li>Be mindful about image size, the total page size and loading times.</li><li>Larger images (>10K) need to support <em>progressive rendering</em>. Consult with your favorite authoring tool&rsquo;s documentation to find out if and how it supports that.</li><li>The site delivers the optimal media content format and size depending on the device screen size. You need to provide several variants (large screen, laptop, tablet, phone). Your authoring tool should be able to resize and resample images. Always save the largest size first and then downscale from it to avoid image quality loss.</li></ul><p>If you are looking for a tool that conforms to those guidelines, <a href=https://www.irfanview.com/>IrfanView</a> is a very good option.</p><p><strong>Screenshots</strong> can be taken with whatever tool you have available. A simple Alt+PrtSc (Win) and paste into an image processing tool to save it does the job. If you need to add emphasized steps (1,2,3) when you describe a process on a screeshot, you can use <a href=https://www.techsmith.com/screen-capture.html>Snaggit</a>. Use red color and numbers. Mind the requirements for raster images laid out above.</p><p><strong>Diagrams</strong> can be exported as PNG/JPG from a diagraming tool such as Visio or even PowerPoint. Pick whichever you are comfortable with to design the diagram and make sure you comply with the requirements for the raster images production above. Diagrams produced as SVG are welcome too if your authoring tool supports exporting in that format. In any case, ensure that your diagrams &ldquo;blend&rdquo; with the content on the site - use the same color scheme and geometry style. Do not complicate diagrams too much. The site also supports <a href=https://mermaid-js.github.io/mermaid/#/>Mermaid</a> diagrams produced with markdown and rendered as SVG. You don&rsquo;t need special tools for them, but for more complex ones you might want to prototype your diagram wth Mermaid&rsquo;s <a href=https://mermaidjs.github.io/mermaid-live-editor>online live editor</a>, before encoding it in your markdown. More tips on using Mermaid can be found in the <a href=/docs/contribute/20_documentation/30_shortcodes/#mermaid>Shortcodes</a> documentation.</p><h2 id=using-images-in-markdown>Using Images in Markdown</h2><p>The standard for adding images to a topic is to use markdown&rsquo;s <code>![caption](image-path)</code>. If the image is not showing properly, or if you wish to serve images close to their natural size and avoid scaling, then you can use HTML5&rsquo;s <code>&lt;picture></code> tag.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;picture&gt;
</span></span><span style=display:flex><span>    <span style=color:green>&lt;!-- default, laptop-width-L max 1200px --&gt;</span>
</span></span><span style=display:flex><span>    &lt;source srcset=<span style=color:#a31515>&#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-XL.png&#34;</span>
</span></span><span style=display:flex><span>            media=<span style=color:#a31515>&#34;(min-width: 1000px)&#34;</span>&gt;
</span></span><span style=display:flex><span>    <span style=color:green>&lt;!-- default, laptop-width max 1000px --&gt;</span>
</span></span><span style=display:flex><span>    &lt;source srcset=<span style=color:#a31515>&#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-L.png&#34;</span>
</span></span><span style=display:flex><span>            media=<span style=color:#a31515>&#34;(min-width: 1400px)&#34;</span>&gt;
</span></span><span style=display:flex><span>    <span style=color:green>&lt;!-- default, tablets-width max 750px --&gt;</span>
</span></span><span style=display:flex><span>    &lt;source srcset=<span style=color:#a31515>&#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-M.png&#34;</span>
</span></span><span style=display:flex><span>            media=<span style=color:#a31515>&#34;(min-width: 750px)&#34;</span>&gt;
</span></span><span style=display:flex><span>    <span style=color:green>&lt;!-- default, phones-width max 450px --&gt;</span>
</span></span><span style=display:flex><span>    &lt;img src=<span style=color:#a31515>&#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview.png&#34;</span> /&gt;
</span></span><span style=display:flex><span>&lt;/picture&gt;
</span></span></code></pre></div><p>When deciding on image sizes, consider the breakpoints in the example above as maximum widths for each image variant you provide. Note that the site is designed for maximum width 1200px. There is no point to create images larger than that, since they will be scaled down.</p><p>For a nice overview on making the best use of responsive images with HTML5, please refer to the <a href=https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images>Responsive Images</a> guide.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7d75cb7b553f6e474656481ba3abb5b>10.2.2 - Formatting Guide</h1><p>This page gives writing formatting guidelines for the Gardener documentation. For style guidelines, see the <a href=/docs/contribute/20_documentation/40_style_guide/>Style Guide</a>.</p><p>These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.</p><ul><li><a href=#formatting-of-inline-elements>Formatting of Inline Elements</a></li><li><a href=#code-snippet-formatting>Code Snippet Formatting</a></li><li><a href=#related-links>Related Links</a></li></ul><h2 id=formatting-of-inline-elements>Formatting of Inline Elements</h2><table><thead><tr><th style=text-align:left>Type of Text</th><th style=text-align:left>Formatting</th><th style=text-align:left>Markdown Syntax</th></tr></thead><tbody><tr><td style=text-align:left><a href=#user-interface-elements>User Interface Elements</a></td><td style=text-align:left><em>italics</em></td><td style=text-align:left><code>Choose *CLUSTERS*</code>.</td></tr><tr><td style=text-align:left><a href=#new-terms-and-emphasis>New Terms and Emphasis</a></td><td style=text-align:left><strong>bold</strong></td><td style=text-align:left><code>Do **not** stop it.</code></td></tr><tr><td style=text-align:left><a href=#technical-names>Technical Names</a></td><td style=text-align:left><code>code</code></td><td style=text-align:left><code>Open file `root.yaml`.</code></td></tr><tr><td style=text-align:left><a href=#api-objects-and-technical-components>API Objects and Technical Components</a></td><td style=text-align:left><code>code</code></td><td style=text-align:left><code>Deploy a `Pod`.</code></td></tr><tr><td style=text-align:left><a href=#inline-code-and-inline-commands>Inline Code and Inline Commands</a></td><td style=text-align:left><code>code</code></td><td style=text-align:left><code>For declarative management, use `kubectl apply`.</code></td></tr><tr><td style=text-align:left><a href=#object-field-names-and-field-values>Object Field Names and Field Values</a></td><td style=text-align:left><code>code</code></td><td style=text-align:left><code>Set the value of `image` to `nginx:1.8`.</code></td></tr><tr><td style=text-align:left><a href=#links-and-references>Links and References</a></td><td style=text-align:left><a href=/docs/contribute/20_documentation/20_formatting_guide/>link</a></td><td style=text-align:left><code>Visit the [Gardener website](https://gardener.cloud/)</code></td></tr><tr><td style=text-align:left><a href=#headers>Headers</a></td><td style=text-align:left>various</td><td style=text-align:left><code># API Server</code></td></tr></tbody></table><h3 id=user-interface-elements>User Interface Elements</h3><p>When referring to UI elements, refrain from using verbs like &ldquo;Click&rdquo; or &ldquo;Select with right mouse button&rdquo;. This level of detail is hardly ever needed and also invalidates a procedure if other devices are used. For example, for a tablet you&rsquo;d say &ldquo;Tap on&rdquo;.</p><p>Use <em>italics</em> when you refer to UI elements.</p><table><thead><tr><th style=text-align:left>UI Element</th><th style=text-align:left>Standard Formulation</th><th style=text-align:left>Markdown Syntax</th></tr></thead><tbody><tr><td style=text-align:left>Button, Menu path</td><td style=text-align:left>Choose <em>UI Element</em>.</td><td style=text-align:left><code>Choose *UI Element*.</code></td></tr><tr><td style=text-align:left>Menu path, context menu, navigation path</td><td style=text-align:left>Choose <em>System</em> > <em>User Profile</em> > <em>Own Data</em>.</td><td style=text-align:left><code>Choose *System* \> *User Profile* \> *Own Data*.</code></td></tr><tr><td style=text-align:left>Entry fields</td><td style=text-align:left>Enter your password.</td><td style=text-align:left><code>Enter your password.</code></td></tr><tr><td style=text-align:left>Checkbox, radio button</td><td style=text-align:left>Select <em>Filter</em>.</td><td style=text-align:left><code>Select *Filter*.</code></td></tr><tr><td style=text-align:left>Expandable screen elements</td><td style=text-align:left>Expand <em>User Settings</em>.<br>Collapse <em>User Settings</em>.</td><td style=text-align:left><code>Expand *User Settings*</code>.<br><code>Collapse *User Settings*.</code></td></tr></tbody></table><h3 id=new-terms-and-emphasis>New Terms and Emphasis</h3><p>Use <strong>bold</strong> to emphasize something or to introduce a new term.</p><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>A <strong>cluster</strong> is a set of nodes &mldr;</td><td style=text-align:left>A &ldquo;cluster&rdquo; is a set of nodes &mldr;</td></tr><tr><td style=text-align:left>The system does <strong>not</strong> delete your objects.</td><td style=text-align:left>The system does not(!) delete your objects.</td></tr></tbody></table><h3 id=technical-names>Technical Names</h3><p>Use code style (using backticks) for filenames, technical componentes, directories, and paths.</p><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>Open file <code>envars.yaml</code>.</td><td style=text-align:left>Open the envars.yaml file.</td></tr><tr><td style=text-align:left>Go to directory <code>/docs/tutorials</code>.</td><td style=text-align:left>Go to the /docs/tutorials directory.</td></tr><tr><td style=text-align:left>Open file <code>/_data/concepts.yaml</code>.</td><td style=text-align:left>Open the /_data/concepts.yaml file.</td></tr></tbody></table><h3 id=api-objects-and-technical-components>API Objects and Technical Components</h3><p>When you refer to an API object, use the same uppercase and lowercase letters
that are used in the actual object name, and use backticks to format them. Typically, the names of API
objects use
<a href=https://en.wikipedia.org/wiki/Camel_case>camel case</a>.</p><p>Don&rsquo;t split the API object name into separate words. For example, use
<code>PodTemplateList</code>, not Pod Template List.</p><p>Refer to API objects without saying &ldquo;object,&rdquo; unless omitting &ldquo;object&rdquo;
leads to an awkward construction.</p><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>The <code>Pod</code> has two containers.</td><td style=text-align:left>The pod has two containers.</td></tr><tr><td style=text-align:left>The <code>Deployment</code> is responsible for &mldr;</td><td style=text-align:left>The Deployment object is responsible for &mldr;</td></tr><tr><td style=text-align:left>A <code>PodList</code> is a list of Pods.</td><td style=text-align:left>A Pod List is a list of pods.</td></tr><tr><td style=text-align:left>The <code>gardener-control-manager</code> has control loops&mldr;</td><td style=text-align:left>The gardener-control-manager has control loops&mldr;</td></tr><tr><td style=text-align:left>The <code>gardenlet</code> starts up with a bootstrap <code>kubeconfig</code> having a bootstrap token that allows to create <code>CertificateSigningRequest</code> (CSR) resources.</td><td style=text-align:left>The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.</td></tr></tbody></table><h3 id=inline-code-and-inline-commands>Inline Code and Inline Commands</h3><p>Use backticks (`) for inline code.</p><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>The <code>kubectl run</code> command creates a <code>Deployment</code>.</td><td style=text-align:left>The &ldquo;kubectl run&rdquo; command creates a Deployment.</td></tr><tr><td style=text-align:left>For declarative management, use <code>kubectl apply</code>.</td><td style=text-align:left>For declarative management, use &ldquo;kubectl apply&rdquo;.</td></tr></tbody></table><h3 id=object-field-names-and-field-values>Object Field Names and Field Values</h3><p>Use backticks (`) for field names, and field values.</p><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>Set the value of the <code>replicas</code> field in the configuration file.</td><td style=text-align:left>Set the value of the &ldquo;replicas&rdquo; field in the configuration file.</td></tr><tr><td style=text-align:left>The value of the <code>exec</code> field is an <code>ExecAction</code> object.</td><td style=text-align:left>The value of the &ldquo;exec&rdquo; field is an ExecAction object.</td></tr><tr><td style=text-align:left>Set the value of <code>imagePullPolicy</code> to <code>Always</code>.</td><td style=text-align:left>Set the value of <code>imagePullPolicy</code> to &ldquo;Always&rdquo;.</td></tr><tr><td style=text-align:left>Set the value of <code>image</code> to <code>nginx:1.8</code>.</td><td style=text-align:left>the value of <code>image</code> to nginx:1.8.</td></tr></tbody></table><h3 id=links-and-references>Links and References</h3><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left>Use a descriptor of the link&rsquo;s destination: &ldquo;For more information, visit <a href=/docs/contribute/20_documentation/20_formatting_guide/>Gardener&rsquo;s website</a>.&rdquo;</td><td style=text-align:left>Use a generic placeholder: &ldquo;For more information, go <a href=/docs/contribute/20_documentation/20_formatting_guide/>here</a>.&rdquo;</td></tr></tbody></table><p>Another thing to keep in mind is that markdown links do not work in <a href=/docs/contribute/20_documentation/30_shortcodes/>shortcodes</a>. To circumvent this problem, you can use HTML links.</p><h3 id=headers>Headers</h3><ul><li>Use H1 for the title of the topic.</li><li>Use H2 for each main section.</li><li>Use H3 for any sub-section in the main sections.</li><li>Avoid using H4-H6. Try moving the additional information to a new topic instead.</li></ul><h2 id=code-snippet-formatting>Code Snippet Formatting</h2><h3 id=dont-include-the-command-prompt>Don&rsquo;t Include the Command Prompt</h3><table><thead><tr><th style=text-align:left>Do</th><th style=text-align:left>Don&rsquo;t</th></tr></thead><tbody><tr><td style=text-align:left><code>kubectl get pods</code></td><td style=text-align:left><code>$ kubectl get pods</code></td></tr></tbody></table><h3 id=separate-commands-from-output>Separate Commands from Output</h3><code>Verify that the pod is running on your chosen node:<pre tabindex=0><code>kubectl get pods --output=wide
</code></pre><p>The output is similar to:</p><pre tabindex=0><code>NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
</code></pre></code><h3 id=placeholders>Placeholders</h3><p>Use angle brackets for placeholders. Tell the reader what a placeholder
represents, for example:</p><code><p>Display information about a pod:</p><pre tabindex=0><code>kubectl describe pod &lt;pod-name&gt;
</code></pre><p><code>&lt;pod-name></code> is the name of one of your pods.</p></code><h3 id=versioning-kubernetes-examples>Versioning Kubernetes examples</h3><p>Make code examples and configuration examples that include version information consistent with the accompanying text. Identify the Kubernetes version in the <strong>Prerequisites</strong> section.</p><h2 id=related-links>Related links</h2><ul><li><a href=/docs/contribute/20_documentation/40_style_guide/>Style Guide</a></li><li><a href=/docs/contribute/20_documentation/>Contributors Guide</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d62441ff72901a18cf74b4dd2be084ee>10.2.3 - Markdown</h1><p>Hugo uses <a href=https://www.markdownguide.org/>Markdown</a> for its simple content format. However, there are a lot of things that Markdown
doesn&rsquo;t support well. You could use pure HTML to expand possibilities. A typical example is reducing
the original dimensions of an image.</p><p>However, use HTML judicially and to the minimum extent possible. Using HTML in markdowns makes it
harder to maintain and publish coherent documentation bundles. This is a job typically performed by
a publishing platform mechanisms, such as Hugo&rsquo;s layouts. Considering that the source documentation
might be published by multiple platforms you should be considerate in using markup that may bind it
to a particular one.</p><p>For the same reason, avoid inline scripts and styles in your content. If you absolutely need to use them and they are not working as expected, please create a documentation issue and describe your case.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Tip</h4>Markdown is great for its simplicity but may be also constraining for the same reason. Before looking at HTML to make up for that, first check the <a href=/docs/contribute/20_documentation/30_shortcodes/>shortcodes</a> for alternatives.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-0e5702fc9886e66dce242f397b3ca73e>10.2.4 - Organization</h1><p>The Gardener project implements the <em>documentation-as-code</em> paradigm. Essentially this means that:</p><ul><li>Documentation resides close to the code it describes - in the corresponding GitHub repositories. Only documentation with regards to cross-cutting concerns that cannot be affiliated to a specific component repository is hosted in the general <a href=https://github.com/gardener/documentation>gardener/documentation</a> repository.</li><li>We use tools to develop, validate and integrate documentation <em>sources</em></li><li>The change management process is largely automated with automatic validation, integration and deployment using <a href=https://github.com/gardener/docforge>docforge</a> and <a href=https://github.com/gardener/docs-toolbelt>docs-toolbelt</a>.</li><li>The documentation sources are intended for <em>reuse</em> and <em>not bound</em> to a specific publishing platform.</li><li>The physical organization in a repository is irrelevant for the tool support. What needs to be maintained is the intended result in a <a href=https://github.com/gardener/docforge>docforge</a> documentation bundle manifest configuration, very much like virtual machines configurations, that docforge can reliably recreate in any case.</li><li>We use GitHub as distributed, versioning storage system and <a href=https://github.com/gardener/docforge>docforge</a> to pull sources in their desired state to forge documentation bundles according to a desired specification provided as a manifest.</li></ul><h2 id=content-organization>Content Organization</h2><p>Documentation that can be affiliated to component is hosted and maintained in the component repository.</p><p>A good way to organize your documentation is to place it in a &lsquo;docs&rsquo; folder and create separate subfolders per role activity. For example:</p><pre tabindex=0><code>repositoryX
|_ docs
   |_ usage
   |  |_ images
   |  |_ 01.png
   |  |_ hibernation.md
   |_ operations
   |_ deployment
</code></pre><p>Do not use folders just because they are in the template. Stick to the predefined roles and corresponding activities for naming convention. A system makes it easier to maintain and get oriented. While recommended, this is not a mandatory way of organizing the documentation.</p><ul><li>User: <code>usage</code></li><li>Operator: <code>operations</code></li><li>Gardener (service) provider: <code>deployment</code></li><li>Gardener Developer: <code>development</code></li><li>Gardener Extension Developer: <code>extensions</code></li></ul><h2 id=publishing-on-gardenercloud>Publishing on gardener.cloud</h2><p>The Gardener website is one of the multiple optional publishing channels where the source material might end up as documentation. We use docforge and automated integration and publish process to enable transparent change
management.</p><p>To have documentation published on the website it is necessary to use the docforge manifests available at <a href=https://github.com/gardener/documentation/tree/master/.docforge>gardener/documentation/.docforge</a> and register a reference to your documentation.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>This is work in progress and we are transitioning to a more transparent way of integrating component documentation. This guide will be updated as we progress.</div><p>These manifests describe a particular publishing goal, i.e. using Hugo to publish on the website, and you will find out that they contain Hugo-specific front-matter properties.
Consult with the documentation maintainers for details. Use the gardener channel in <a href=https://kubernetes.slack.com/messages/gardener>slack</a> or <a href=https://github.com/gardener/documentation/pulls>open a PR</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1f274f33c0a57768d9b92bf09aa7d0ac>10.2.5 - Shortcodes</h1><p>Shortcodes are the Hugo way to extend the limitations of Markdown before resorting to HTML. There are a number of built-in shortcodes available from Hugo. This list is extended with Gardener website shortcodes designed specifically for its content.
Find a complete reference to the Hugo built-in shortcodes on its <a href=https://gohugo.io/content-management/shortcodes/>website</a>.</p><p>Below is a reference to the shortcodes developed for the Gardener website.</p><h2 id=alert>alert</h2><pre tabindex=0><code>{{% alert color=&#34;info&#34; title=&#34;Notice&#34; %}}
text
{{% /alert %}}
</code></pre><p>produces<div class="alert alert-info" role=alert><h4 class=alert-heading>Notice</h4>A notice disclaimer</div></p><p>All the color options are <code>info</code>|<code>warning</code>|<code>primary</code></p><p>You can also omit the title section from an alert, useful when creating notes.</p><p>It is important to note that the text that the &ldquo;alerts&rdquo; shortcode wraps will not be processed during site building. Do not use shortcodes in it.</p><p>You should also avoid mixing HTML and markdown formatting in shortcodes, since it won&rsquo;t render correctly when the site is built.</p><h3 id=alert-examples>Alert Examples</h3><div class="alert alert-info" role=alert>Info color</div><div class="alert alert-warning" role=alert>Warning color</div><div class="alert alert-primary" role=alert>Primary color</div><h2 id=mermaid>mermaid</h2><p>The <a href=https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/>GitHub mermaid</a> fenced code block syntax is used. You can find additional documentation at <a href=https://mermaid-js.github.io/mermaid/#/README>mermaid&rsquo;s official website</a>.</p><pre tabindex=0><code>```mermaid
graph LR;
    A[Hard edge] --&gt;|Link text| B(Round edge)
    B --&gt; C{Decision}
    C --&gt;|One| D[Result one]
    C --&gt;|Two| E[Result two]
```
</code></pre><p>produces:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR;
    A[Hard edge] --&gt;|Link text| B(Round edge)
    B --&gt; C{Decision}
    C --&gt;|One| D[Result one]
    C --&gt;|Two| E[Result two]
</code></pre><p>Default settings can be overridden using the %%init%% header at the start of the diagram definition.
See the <a href="https://mermaid-js.github.io/mermaid/#/theming?id=themes-at-the-local-or-current-level">mermaid theming documentation</a>.</p><pre tabindex=0><code>```mermaid
%%{init: {&#39;theme&#39;: &#39;neutral&#39;, &#39;themeVariables&#39;: { &#39;mainBkg&#39;: &#39;#eee&#39;}}}%%
graph LR;
    A[Hard edge] --&gt;|Link text| B(Round edge)
    B --&gt; C{Decision}
    C --&gt;|One| D[Result one]
    C --&gt;|Two| E[Result two]
```
</code></pre><p>produces:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;theme&#39;: &#39;neutral&#39;, &#39;themeVariables&#39;: { &#39;mainBkg&#39;: &#39;#eee&#39;}}}%%
graph LR;
    A[Hard edge] --&gt;|Link text| B(Round edge)
    B --&gt; C{Decision}
    C --&gt;|One| D[Result one]
    C --&gt;|Two| E[Result two]
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-d784a625afde0ebf3c15e17013143234>10.2.6 - Style Guide</h1><p>This page gives writing style guidelines for the Gardener documentation. For formatting guidelines, see the <a href=/docs/contribute/20_documentation/20_formatting_guide/>Formatting Guide</a>.</p><p>These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a Pull Request.</p><ul><li><a href=#structure>Structure</a></li><li><a href=#language-and-grammar>Language and Grammar</a></li><li><a href=#related-links>Related Links</a></li></ul><h2 id=structure>Structure</h2><h3 id=documentation-types-overview>Documentation Types Overview</h3><p>The following table summarizes the types of documentation and their mapping to the SAP UA taxonomy. Every topic you create will fall into one of these categories.</p><table><thead><tr><th style=text-align:left>Gardener Content Type</th><th style=text-align:left>Definition</th><th style=text-align:left>Example</th><th style=text-align:left>Content</th><th style=text-align:left>Comparable UA Content Type</th></tr></thead><tbody><tr><td style=text-align:left>Concept</td><td style=text-align:left>Introduce a functionality or concept; covers background information.</td><td style=text-align:left><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Services</a></td><td style=text-align:left><a href=/docs/contribute/20_documentation/40_style_guide/concept_template/>Overview, Relevant headings</a></td><td style=text-align:left>Concept</td></tr><tr><td style=text-align:left>Reference</td><td style=text-align:left>Provide a reference, for example, list all command line options of <code>gardenctl</code> and what they are used for.</td><td style=text-align:left><a href=https://kubernetes.io/docs/reference/kubectl/overview/>Overview of kubectl</a></td><td style=text-align:left><a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Relevant headings</a></td><td style=text-align:left>Reference</td></tr><tr><td style=text-align:left>Task</td><td style=text-align:left>A step-by-step description that allows users to complete a specific task.</td><td style=text-align:left><a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a></td><td style=text-align:left><a href=/docs/contribute/20_documentation/40_style_guide/task_template/>Overview, Prerequisites, Steps, Result</a></td><td style=text-align:left>Complex Task</td></tr><tr><td style=text-align:left>Trail</td><td style=text-align:left>Collection of all other content types to cover a big topic.</td><td style=text-align:left><a href=https://docs.oracle.com/javase/tutorial/networking/TOC.html>Custom Networking</a></td><td style=text-align:left>None</td><td style=text-align:left>Maps</td></tr><tr><td style=text-align:left>Tutorial</td><td style=text-align:left>A combination of many tasks that allows users to complete an example task with the goal to learn the details of a given feature.</td><td style=text-align:left><a href=https://kubernetes.io/docs/tutorials/stateful-application/cassandra/>Deploying Cassandra with a StatefulSet</a></td><td style=text-align:left>Overview, Prerequisites, Tasks, Result</td><td style=text-align:left>Tutorial</td></tr></tbody></table><p>See the <a href=/docs/contribute/>Contributors Guide</a> for more details on how to produce and contribute documentation.</p><h3 id=topic-structure>Topic Structure</h3><p>When creating a topic, you will need to follow a certain structure. A topic generally comprises of, in order:</p><ul><li><p><a href=#front-matter>Metadata</a> (Specific for <code>.md</code> files in Gardener) - Additional information about the topic</p></li><li><p>Title - A short, descriptive name for the topic</p></li><li><p>Content - The main part of the topic. It contains all the information relevant to the user</p><ul><li>Concept content: <a href=/docs/contribute/20_documentation/40_style_guide/concept_template/>Overview, Relevant headings</a></li><li>Task content: <a href=/docs/contribute/20_documentation/40_style_guide/task_template/>Overview, Prerequisites, Steps, Result</a></li><li>Reference content: <a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Relevant headings</a></li></ul></li><li><p>Related Links (Optional) - A part after the main content that contains links that are not a part of the topic, but are still connected to it</p></li></ul><p>You can use the provided content description files as a template for your own topics.</p><h3 id=front-matter>Front Matter</h3><p><a href=https://gohugo.io/content-management/front-matter>Front matter</a> is metadata applied at the head of each content Markdown file. It is used to instruct the static site generator build process. The format is YAML and it must be enclosed in leading and trailing comment dashes (<code>---</code>).</p><p>Sample codeblock:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>title: Getting Started
</span></span><span style=display:flex><span>Description: Guides to get you accustomed with Gardener
</span></span><span style=display:flex><span>Weight: 10
</span></span><span style=display:flex><span>---
</span></span></code></pre></div><p>There are a number of <a href=https://gohugo.io/content-management/front-matter#predefined>predefined</a> front matter properites, but not all of them are considered by the layouts developed for the website. The most essential ones to consider are:</p><ul><li><code>title</code> the content title that will be used as page title and in navigation structures.</li><li><code>weight</code> a positive integer number that controls the ordering of the content in navigation structures.</li><li><code>description</code> describes the content. For some content types such as documentation guides, it may be rendered in the UI.</li><li><code>url</code> if specified, it will override the default url constructed from the file path to the content. Make sure the url you specify is consistent and meaningful. Prefer short paths. Do not provide redundant URLs!</li><li><code>categories</code> specifies the type of user the topic is aimed towards. Currently only used in the public website.<pre tabindex=0><code>categories:
- Users
- Operators
- Developers
</code></pre></li></ul><p>While this section will be automatically generated if your topic has a title header, adding more detailed information helps other users, developers and technical writers better sort, classify and understand the topic.</p><p>By using a metadata section you can also skip adding a title header or overwrite it in the navigation section.</p><h3 id=alerts>Alerts</h3><p>If you want to add a note, tip, or a warning to your topic, use the templates provides in the <a href=/docs/contribute/20_documentation/30_shortcodes/#alert>Shortcodes</a> documentation.</p><h3 id=images>Images</h3><p>If you want to add an image to your topic, it is recommended to follow the guidelines outlined in the <a href=/docs/contribute/20_documentation/45_images/>Images</a> documentation.</p><h3 id=general-tips>General Tips</h3><ul><li>Try to create a succint title and an informative description for your topics</li><li>If a topic feels too long, it might be better to split it into a few different ones</li><li>Avoid having have more than ten steps in one a task topic</li><li>When writing a tutorial, link the tasks used in it instead of copying their content</li></ul><h2 id=language-and-grammar>Language and Grammar</h2><h3 id=language>Language</h3><ul><li>Gardener documentation uses US English</li><li>Keep it simple and use words that non-native English speakers are also familiar with</li><li>Use the <a href=https://www.merriam-webster.com/>Merriam-Webster Dictionary</a> when checking the spelling of words</li></ul><h3 id=writing-style>Writing Style</h3><ul><li>Write in a conversational manner and use simple present tense</li><li>Be friendly and refer to the person reading your content as &ldquo;you&rdquo;, instead of standard terms such as &ldquo;user&rdquo;</li><li>Use an active voice - make it clear who is performing the action</li></ul><h3 id=creating-titles-and-headers>Creating Titles and Headers</h3><ul><li>Use <a href=https://titlecaseconverter.com/words-to-capitalize/>title case</a> when creating titles or headers</li><li>Avoid adding additional formatting to the title or header</li><li>Concept and reference topic titles should be simple and succint</li><li>Task and tutorial topic titles begin with a verb</li></ul><h2 id=related-links>Related links</h2><ul><li><a href=/docs/contribute/20_documentation/20_formatting_guide/>Formatting Guide</a></li><li><a href=/docs/contribute/20_documentation/>Contributors Guide</a></li><li><a href=/docs/contribute/20_documentation/30_shortcodes/>Shortcodes</a></li><li><a href=/docs/contribute/20_documentation/45_images/>Images</a></li><li><a href=https://www.sapterm.com/>SAPterm</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4d90c01eca4bb797bc03d5873e8908c4>10.2.6.1 - Concept Topic Structure</h1><div class=lead>Describes the contents of a concept topic</div><h1 id=concept-title>Concept Title</h1><p>(the topic title can also be placed in the frontmatter)</p><h2 id=overview>Overview</h2><p>This section provides an overview of the topic and the information provided in it.</p><h2 id=relevant-heading-1>Relevant heading 1</h2><p>This section gives the user all the information needed in order to understand the topic.</p><h3 id=relevant-subheading>Relevant subheading</h3><p>This adds additional information that belongs to the topic discussed in the parent heading.</p><h2 id=relevant-heading-2>Relevant heading 2</h2><p>This section gives the user all the information needed in order to understand the topic.</p><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/contribute/20_documentation/40_style_guide/concept_template/>Link 1</a></li><li><a href=/docs/contribute/20_documentation/40_style_guide/concept_template/>Link 2</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4bc07ea257dfc8dbfa10101cb0bdf610>10.2.6.2 - Reference Topic Structure</h1><div class=lead>Describes the contents of a reference topic</div><h1 id=topic-title>Topic Title</h1><p>(the topic title can also be placed in the frontmatter)</p><h2 id=content>Content</h2><p>This section gives the user all the information needed in order to understand the topic.</p><table><thead><tr><th style=text-align:left>Content Type</th><th style=text-align:left>Definition</th><th style=text-align:left>Example</th></tr></thead><tbody><tr><td style=text-align:left>Name 1</td><td style=text-align:left>Definition of Name 1</td><td style=text-align:left><a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Relevant link</a></td></tr><tr><td style=text-align:left>Name 2</td><td style=text-align:left>Definition of Name 2</td><td style=text-align:left><a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Relevant link</a></td></tr></tbody></table><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Link 1</a></li><li><a href=/docs/contribute/20_documentation/40_style_guide/reference_template/>Link 2</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-65612b82d7bea886a6582cf824767781>10.2.6.3 - Task Topic Structure</h1><div class=lead>Describes the contents of a task topic</div><h1 id=task-title>Task Title</h1><p>(the topic title can also be placed in the frontmatter)</p><h2 id=overview>Overview</h2><p>This section provides an overview of the topic and the information provided in it.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Prerequisite 1</li><li>Prerequisite 2</li></ul><h2 id=steps>Steps</h2><p>Avoid nesting headings directly on top of each other with no text inbetween.</p><ol><li>Describe step 1 here</li><li>Describe step 2 here</li></ol><h3 id=you-can-use-smaller-sections-within-sections-for-related-tasks>You can use smaller sections within sections for related tasks</h3><p>Avoid nesting headings directly on top of each other with no text inbetween.</p><ol><li>Describe step 1 here</li><li>Describe step 2 here</li></ol><h2 id=result>Result</h2><p>Screenshot of the final status once all the steps have been completed.</p><h2 id=related-links>Related Links</h2><p>Provide links to other relevant topics, if applicable. Once someone has completed these steps, what might they want to do next?</p><ul><li><a href=/docs/contribute/20_documentation/40_style_guide/task_template/>Link 1</a></li><li><a href=/docs/contribute/20_documentation/40_style_guide/task_template/>Link 2</a></li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>