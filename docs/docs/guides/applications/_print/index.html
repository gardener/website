<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/applications/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/applications/index.xml><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=icon type=image/x-icon href=https://gardener.cloud/images/favicon.ico><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-16x16.png sizes=16x16><title>Applications | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:title" content="Applications"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/applications/"><meta itemprop=name content="Applications"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary"><meta name=twitter:title content="Applications"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.122f9effc36493edf1f25030c2ce7965b16b3b0eaeb02528b9a50e0fa9110c15.css as=style><link href=/scss/main.min.122f9effc36493edf1f25030c2ce7965b16b3b0eaeb02528b9a50e0fa9110c15.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.48b7b817e77eb3e654cce060c6f03cab.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/applications/>Return to the regular view of this page</a>.</p></div><h1 class=title>Applications</h1><div class=content></div></div><div class=td-content><h1 id=pg-cdf7f44ffe002ba4c8c2fba6647dcc99>1 - Access a port of a pod locally</h1><h2 id=question>Question</h2><p>You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access
this endpoint <strong>without an external load balancer</strong> (e.g. Ingress)?
This tutorial presents two options:</p><ul><li>Using Kubernetes port forward</li><li>Using Kubernetes apiserver proxy</li></ul><p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to <a href=/docs/guides/applications/service-access>Access my service</a></p><h2 id=solution-1-using-kubernetes-port-forward>Solution 1: Using Kubernetes port forward</h2><p>You could use the port forwarding functionality of <code>kubectl</code> to access the pods from your
local host <strong>without involving a service</strong>.</p><p>To access any pod follow these steps:</p><ul><li>Run <code>kubectl get pods</code></li><li>Note down the name of the pod in question as <code>&lt;your-pod-name></code></li><li>Run <code>kubectl port-forward &lt;your-pod-name> &lt;local-port>:&lt;your-app-port></code></li><li>Run a web browser or curl locally and enter the URL <code>http(s)://localhost:&lt;local-port></code></li></ul><p>In addition, <code>kubectl port-forward</code> allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward.
Find more details in the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>Kubernetes documentation</a>.</p><p>The main drawback of this approach is that the pod&rsquo;s name will change as soon as it is restarted. Moreover, you need
to have a web browser on your client and you need to make sure that the local port is not already used by an
application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons.
This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.</p><p><img src=/__resources/howto-port-forward_521e20.svg alt=port-forward></p><h2 id=solution-2-using-apiserver-proxy>Solution 2: Using apiserver proxy</h2><p>There are several different proxies used with Kubernetes, <a href=https://kubernetes.io/docs/concepts/cluster-administration/proxies/>the official documentation</a> provides a good overview.</p><p>In this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. <strong>Different from the first solution, a service is required for this solution</strong> .</p><p>Use the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>Discovering builtin services</a></p><p><code>https://&lt;cluster-master>/api/v1/namespace/&lt;namespace>/services/&lt;service>:&lt;service-port>/proxy/&lt;service-endpoint></code></p><p><strong>Example:</strong></p><table><thead><tr><th>cluster-master</th><th style=text-align:center>namespace</th><th style=text-align:right>service</th><th style=text-align:right>service-port</th><th style=text-align:right>service-endpoint</th><th style=text-align:right>url to access service</th></tr></thead><tbody><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>nginx-svc</td><td style=text-align:right>80</td><td style=text-align:right>/</td><td style=text-align:right><code>http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/</code></td></tr><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>docker-nodejs-svc</td><td style=text-align:right>4500</td><td style=text-align:right>/cpu?baseNumber=4</td><td style=text-align:right><code>https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4</code></td></tr></tbody></table><p>There are applications, which do <strong>not</strong> yet support relative URLs like <a href=https://github.com/prometheus/prometheus/issues/1583>Prometheus</a> (as of end of November, 2017).
This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the
<code>port-forward</code> approach described above.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a7ec86eaf7d4bebd0f35775f37c982c6>2 - Access service from outside Kubernetes cluster</h1><div class=lead>Is there an ingress deployed and how is it configured</div><h2 id=tldr>TL;DR</h2><p>To expose your application / service for access from outside the cluster, following options exist:</p><ul><li>Kubernetes Service of type <code>LoadBalancer</code></li><li>Kubernetes Service of type &lsquo;NodePort&rsquo; + Ingress</li></ul><p>This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called
North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there
are many examples, <a href=https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/>here</a> is one brief example.</p><h2 id=service-types>Service Types</h2><p>A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.<br>Services can be exposed in different ways by specifying a <strong>type</strong> in the service spec,
and different types determine accessibility from inside and outside of cluster.</p><ul><li>ClusterIP</li><li>NodePort</li><li>LoadBalancer</li></ul><p>Type <code>ExternalName</code> is a special case of service and not discussed here.</p><h3 id=type-clusterip>Type ClusterIP</h3><p>A service of type <code>ClusterIP</code> exposes a service on an internal IP in the cluster, which makes the service <strong>only reachable</strong>
from within the cluster. This is the default value if no type is specified.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP </span><span class=w> </span><span class=c># use ClusterIP as type here</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span></code></pre></div><p>Execute following commands to create deployment and service</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl create -f &lt;Your yaml file name&gt;
</code></pre></div><p>Checking the service status</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get svc nginx-svc
NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>   AGE
nginx-svc   ClusterIP   100.66.125.61   &lt;none&gt;        80/TCP    45m
</code></pre></div><p>As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file.<br>You can test the service like this:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># list all existing pods in cluster</span>
$ kubectl get pods
NAME                                           READY     STATUS        RESTARTS   AGE
docker-nodejs-app-76b77494-vwv4d               1/1       Running       <span class=m>0</span>          11d
nginx-deployment-74d949bf69-nvdzs              1/1       Running       <span class=m>0</span>          1h
privileged-pod                                 1/1       Running       <span class=m>0</span>          11d

<span class=c1># test service from within the cluster on the same pod</span>
$ kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-nvdzs  curl 100.66.125.61:80
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
<span class=m>100</span>   <span class=m>612</span>  <span class=m>100</span>   <span class=m>612</span>    <span class=m>0</span>     <span class=m>0</span>  1006k      <span class=m>0</span> --:--:-- --:--:-- --:--:--  597k
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body <span class=o>{</span>
        width: 35em<span class=p>;</span>
        margin: <span class=m>0</span> auto<span class=p>;</span>
        font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
    <span class=o>}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
...
</code></pre></div><blockquote><p><i class="fa fa-gittip" aria-hidden=true></i> Tip</p><ul><li>The service is also accessible from any other container (even from different pods) within the same cluster, e.g. <code>kubectl -it exec &lt;another POD_NAME> curl &lt;YourServiceClusterIP:YourPort></code>.
You need to make sure command <code>curl</code> is installed in the container.</li><li>You can also find out the dns name of the ClusterIP by command <code>kubectl exec -it &lt;POD_NAME> nslookup &lt;ClusterIP></code>,
replace the IP address with the resolved name in your test.
The resolved name typically looks like <code>nginx-svc.default.svc.cluster.local</code> where <code>nginx-svc</code> is the name of your
service defined in the configuration file.</li></ul></blockquote><h3 id=type-nodeport>Type NodePort</h3><p>Follow the previous example, just replace the type with <code>NodePort</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>...</span><span class=w>
</span><span class=w> </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>   </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>   </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>     </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><p>A service of type <code>NodePort</code> is a ClusterIP service with an additional capability: it is reachable at the IP address
of the node as well as at the assigned cluster IP on the services network.
The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates
a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”).
Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run
<code>kubectl get svc &lt;your-service></code>, we can see the NodePort that has been allocated for it.</p><p>Note that in the in following example, in addition to port 80, port <strong>32521</strong> has been opened as well on the node, in contrast to
the output of &ldquo;ClusterIP&rdquo; case where only port 80 is opened.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get svc nginx-svc
NAME        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>        AGE
nginx-svc   NodePort   100.70.105.182   &lt;none&gt;        80:32521/TCP   16m
</code></pre></div><p>Therefore you can access the service <em>from within the cluster</em> in two ways:</p><ul><li>Access via ClusterIP:port</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1>#via ClusterIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80

<span class=c1>#via internal name of ClusterIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80
</code></pre></div><ul><li>Access via NodeIP:NodePort</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>
<span class=c1># First find out the Node IP address</span>
$ kubectl describe node
Name:               ip-10-250-20-203.eu-central-1.compute.internal
Roles:              node
Addresses:
  InternalIP:   10.250.20.203
  InternalDNS:  ip-10-250-20-203.eu-central-1.compute.internal
  Hostname:     ip-10-250-20-203.eu-central-1.compute.internal
...


<span class=c1>#via NodeIP:NodePort</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521

<span class=c1>#via internal name of NodeIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521
</code></pre></div><h3 id=type-loadbalancer>Type LoadBalancer</h3><p>The <code>LoadBalancer</code> type is the simplest approach, which is created by specifying type as <code>LoadBalancer</code>.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer </span><span class=w> </span><span class=c># use LoadBalancer as type here</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span></code></pre></div><p>Once the service is created, it has an external IP address as shown here:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get services -l <span class=nv>app</span><span class=o>=</span>nginx-app -o wide
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP                                                                  PORT<span class=o>(</span>S<span class=o>)</span>        AGE       SELECTOR
nginx-svc   LoadBalancer   100.67.182.148   a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com   80:31196/TCP   9m        <span class=nv>app</span><span class=o>=</span>nginx-app
</code></pre></div><p>A service of type LoadBalancer <strong>combines the capabilities of a NodePort with the ability to setup a complete ingress path</strong>.<br>Hence the service can be accessible from outside the cluster without the need for additional components like an Ingress.</p><p>To test the external IP run this curl command from your local machine:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>
$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com

StatusCode        : <span class=m>200</span>
StatusDescription : OK
Content           : &lt;!DOCTYPE html&gt;
                    &lt;html&gt;
                    &lt;head&gt;
                    &lt;title&gt;Welcome to nginx!&lt;/title&gt;
                    &lt;style&gt;
                        body <span class=o>{</span>
                            width: 35em<span class=p>;</span>
                            margin: <span class=m>0</span> auto<span class=p>;</span>
                            font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
                        <span class=o>}</span>
                    &lt;/style&gt;
                    &lt;...
RawContent        : HTTP/1.1 <span class=m>200</span> OK
...
</code></pre></div><p>Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section <code>NodePort</code>.</p><h2 id=loadbalancer-vs-ingress>LoadBalancer vs. Ingress</h2><p>As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster.
However this approach has its own limitation.
You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2
a separate resource called <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives>Ingress</a> is
introduced for this purpose.</p><p>You might need to enable the <code>Nginx Ingress</code> add-ons in your gardener dashboard to use some of those functionnality.</p><h3 id=why-an-ingress>Why an Ingress</h3><p>LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a
a separate resource that configures a LoadBalancer in a more flexible way.
The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to
handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the
external load balancer is forwarded to the kube-proxy that in turn forwards
the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected
pods which is more efficient.</p><p>Typically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them
you already pay 400$ per month just for load balancing.</p><h3 id=how-to-use-the-ingress>How to use the ingress?</h3><p>In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS
record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:</p><ul><li><strong>k8s-hana.ondemand.com</strong></li></ul><p><code>&lt;gardener_cluster_name>.&lt;gardener_project_name>.shoot.canary.k8s-hana.ondemand.com</code>.</p><p>Both <code>&lt;gardener_cluster_name></code> and <code>&lt;gardener_project_name></code> are defined in Gardener which can be determined on Gardener dashboard.</p><p>This results in the following default DNS endpoints:</p><ul><li><code>api.&lt;cluster_domain></code> Kubernetes API</li><li><code>*.ingress.&lt;cluster_domain></code> Internal nginx ingress</li></ul><h3 id=example-configure-an-ingress-resource-with-service-type-nodeport>Example: Configure an Ingress resource with Service type: NodePort</h3><p>With the configuration below you can reach your service <strong>nginx-svc</strong> with:</p><p><code>http://test.ingress.&amp;lt;GARDENER-CLUSTER-NAME&amp;gt;.&amp;lt;GARDENER-PROJECT-NAME&amp;gt;.shoot.canary.k8s-hana.ondemand.com</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginxsvc-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></code></pre></div><p>Show the newly created ingress and test it :</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get ingress
NAME                    HOSTS                                                               ADDRESS         PORTS     AGE
nginxsvc-ingress        nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com  10.250.20.203   <span class=m>80</span>        29s

$ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com

StatusCode        : <span class=m>200</span>
StatusDescription : OK
Content           : &lt;!DOCTYPE html&gt;
                    &lt;html&gt;
                    &lt;head&gt;
                    &lt;title&gt;Welcome to nginx!&lt;/title&gt;
                    &lt;style&gt;
                        body <span class=o>{</span>
                            width: 35em<span class=p>;</span>
                            margin: <span class=m>0</span> auto<span class=p>;</span>
                            font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
...
</code></pre></div><h2 id=reference>Reference:</h2><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types>Concepts: Kubernetes Service</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-the-service>Concepts: Connecting Applications with Services</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/>Tutorial: Using a Service to Expose Your App</a></li><li><a href=https://kubernetes.io/docs/tutorials/services/source-ip>Tutorial: Using Source IP</a></li><li><a href=https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727>Kubernetes Networking</a></li><li><a href=http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/>Accessing Kubernetes Pods from Outside of the Cluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a9c93d6cbc648f7853fd4d0aae5fd5f3>3 - Auditing Kubernetes for Secure Setup</h1><div class=lead>A few insecure configurations in Kubernetes</div><h1 id=auditing-kubernetes-for-secure-setup>Auditing Kubernetes for Secure Setup</h1><p><img src=/__resources/teaser_2d8cc0.svg alt=teaser></p><h2 id=increasing-the-security-of-all-gardener-stakeholders>Increasing the Security of all Gardener Stakeholders</h2><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of
all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a
Gardener managed shoot cluster resides in the corresponding seed cluster. This is a
<a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with
a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation,
as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service
architecture.</p><h2 id=major-findings>Major Findings</h2><p>From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes
installation and how to fix them.</p><p>Alban Crequy (<a href=https://kinvolk.io/>Kinvolk</a>) and Dirk Marwinski (<a href=https://www.sap.com>SAP SE</a>) gave a presentation entitled <a href=https://kccncchina2018english.sched.com/event/H2Hd/hardening-multi-cloud-kubernetes-clusters-as-a-service-dirk-marwinski-sap-se-alban-crequy-kinvolk-gmbh>Hardening Multi-Cloud Kubernetes Clusters as a Service</a> at KubeCon 2018 in Shanghai presenting some of the findings.</p><p>Here is a summary of the findings:</p><ul><li><p>Privilege escalation due to insecure configuration of the Kubernetes
API server</p><ul><li>Root cause: Same certificate authority (CA) is used for both the
API server and the proxy that allows accessing the API server.</li><li>Risk: Users can get access to the API server.</li><li>Recommendation: Always use different CAs.</li></ul></li><li><p>Exploration of the control plane network with malicious
HTTP-redirects</p><ul><li><p>Root cause: See detailed description below.</p></li><li><p>Risk: Provoked error message contains full HTTP payload from an
existing endpoint which can be exploited. The contents of the
payload depends on your setup, but can potentially be user data,
configuration data, and credentials.</p></li><li><p>Recommendation:</p><ul><li>Use the latest version of Gardener</li><li>Ensure the seed cluster&rsquo;s container network supports
network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not
protected as Flannel is used there which doesn&rsquo;t support
network policies.</li></ul></li></ul></li><li><p>Reading private AWS metadata via Grafana</p><ul><li>Root cause: It is possible to configuring a new custom data
source in Grafana, we could send HTTP requests to target the
control</li><li>Risk: Users can get the &ldquo;user-data&rdquo; for the seed cluster from
the metadata service and retrieve a kubeconfig for that
Kubernetes cluster</li><li>Recommendation: Lockdown Grafana features to only what&rsquo;s
necessary in this setup, block all unnecessary outgoing traffic,
move Grafana to a different network, lockdown unauthenticated
endpoints</li></ul></li></ul><h2 id=scenario-1-privilege-escalation-with-insecure-api-server>Scenario 1: Privilege Escalation with Insecure API Server</h2><p>In most configurations, different components connect directly to the Kubernetes API server, often using a <code>kubeconfig</code> with a client
certificate. The API server is started with the flag:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>/hyperkube apiserver --client-ca-file<span class=o>=</span>/srv/kubernetes/ca/ca.crt ...
</code></pre></div><p>The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component
is really signed by the configured certificate authority for clients.</p><p><img src=/__resources/image3_9bc94e.png alt><em>The API server can have many clients of various kinds</em><br><br><br></p><p>However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The
proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with
additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>--requestheader-client-ca-file<span class=o>=</span>/srv/kubernetes/ca/ca-proxy.crt
--requestheader-username-headers<span class=o>=</span>X-Remote-User
--requestheader-group-headers<span class=o>=</span>X-Remote-Group
</code></pre></div><p><img src=/__resources/image2_e15f3f.png alt><em>API server clients can reach the API server through an authenticating proxy</em><br><br><br></p><p>So far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses
the HTTP headers to pretend to be someone else?</p><p><img src=/__resources/image8_edf260.png alt><em>What happens when a client bypasses the proxy, connecting directly to the API server?</em><br><br><br></p><p>With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority
but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header
“X-Remote-Group: system:masters”.</p><p>You only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes
client certificate can be used to take the role of different user or group as the API server will accept the user header and
group header.</p><p>The <code>kubectl</code> tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP
requests manually.</p><p>We worked on <a href=https://github.com/kubernetes/website/pull/10093>improving the Kubernetes documentation</a> to make clearer
that this configuration should be avoided.</p><h2 id=scenario-2-exploration-of-the-control-plane-network-with-malicious-http-redirects>Scenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects</h2><p>The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet
running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services,
deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.</p><p><img src=/__resources/image7_038441.png alt><em>The API server is mostly a component that receives requests</em><br><br><br></p><p>However, there are exceptions. Some <code>kubectl</code> commands will trigger the API server to open a new
connection to the Kubelet. <code>Kubectl exec</code> is one of those commands. In order to get the standard I/Os from the pod,
the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on
the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a
HTTP-302 redirection to the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Container Runtime Interface (CRI)</a>.
Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The
redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because
the Kubelet and the CRI component run on the same worker node.</p><p><img src=/__resources/image1_892eee.png alt><em>But the API server also initiates some connections, for example, to worker nodes</em><br><br><br></p><p>It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They
could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods
or even just pods with “host” volumes.</p><p>In contrast, users — even those with “system:masters” permissions or “root” rights — are often not given access to the control plane.
On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative
access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network
in the control plane.</p><p>What would happen if a user was tampering with the Kubelet to make it maliciously redirect <code>kubectl exec</code> requests to
a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would
be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.</p><p><img src=/__resources/image6_240221.png alt><em>The API server is tricked to connect to other components</em><br><br><br></p><p>The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service
(such as the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html>AWS metadata service</a>)
containing user data, configurations and credentials. The setup we explored had a different AWS account and a different
<a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html>EC2 instance profile</a>
for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the
context of the control plane, which they should not have access to.</p><p>We have reported this issue to the <a href=https://kubernetes.io/docs/reference/issues-security/security/>Kubernetes Security mailing list</a>
and the public pull request that addresses the issue has been merged <a href=https://github.com/kubernetes/kubernetes/pull/66516>PR#66516</a>.
It provides a way to enforce HTTP redirect validation (disabled by default).</p><p>But there are several other ways that users could trigger the API server to generate HTTP requests and get the reply
payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures.
Depending on where the API server runs, it could be with <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes Network Policies</a>,
<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html>EC2 Security Groups</a> or just
iptables directly. Following the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth principle</a>,
it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.</p><p>In Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does
not need to contact the metadata service. You can see more details in the <a href=https://groups.google.com/forum/#!forum/gardener>announcements on the Gardener mailing list</a>.
This is tracked in <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-2475">CVE-2018-2475</a>.</p><p><em>To be protected from this issue, stakeholders should:</em></p><ul><li><em>Use the latest version of Gardener</em></li><li><em>Ensure the seed cluster’s container network supports network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn’t support network
policies.</em></li></ul><h2 id=scenario-3-reading-private-aws-metadata-via-grafana>Scenario 3: Reading Private AWS Metadata via Grafana</h2><p>For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control
plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control
plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana
via a load balancer. The internal network of the control plane is therefore hidden to users.</p><p><img src=/__resources/image5_f50567.png alt><em>Prometheus and Grafana can be used to monitor worker nodes</em><br><br><br></p><p>Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom
data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata
service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging
console of the Chrome browser.</p><p><img src=/__resources/image9_d39dc7.png alt><em>Credentials can be retrieved from the debugging console of Chrome</em><br><br><br></p><p><img src=/__resources/image4_a248a3.png alt><em>Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets</em><br><br><br></p><p>In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a
kubeconfig for that Kubernetes cluster.</p><p>There are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all
unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.</p><h2 id=conclusion>Conclusion</h2><p>The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes
installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e8efeb071c3d7a7cfbd4334db1fb8b8>4 - Container image not pulled</h1><div class=lead>Wrong Container Image or Invalid Registry Permissions</div><h2 id=problem>Problem</h2><p>Two of the most common problems are specifying the wrong container image or trying to use
private images without providing registry credentials.</p><p><strong>Note:</strong> There is no observable difference in Pod status between a missing image and incorrect registry permissions.
In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with
both scenarios.</p><h2 id=example>Example</h2><p>Let&rsquo;s see an example. We&rsquo;ll create a pod named <em>fail</em> referencing a non-existent Docker image:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl run -i --tty fail --image<span class=o>=</span>tutum/curl:1.123456
</code></pre></div><p>the command prompt doesn&rsquo;t return and you can press <code>ctrl+c</code></p><h2 id=error-analysis>Error analysis</h2><p>We can then inspect our Pods and see that we have one Pod with a status of <strong>ErrImagePull</strong> or <strong>ImagePullBackOff</strong>.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>$ <span class=o>(</span>minikube<span class=o>)</span> kubectl get pods
NAME                      READY     STATUS         RESTARTS   AGE
client-5b65b6c866-cs4ch   1/1       Running        <span class=m>1</span>          1m
fail-6667d7685d-7v6w8     0/1       ErrImagePull   <span class=m>0</span>          &lt;invalid&gt;
vuejs-578574b75f-5x98z    1/1       Running        <span class=m>0</span>          1d
$ <span class=o>(</span>minikube<span class=o>)</span> 

</code></pre></div><p>For some additional information, we can <code>describe</code> the failing Pod.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl describe pod fail-6667d7685d-7v6w8
</code></pre></div><p>As you can see in the events section, your image can&rsquo;t be pulled</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>Name:		fail-6667d7685d-7v6w8
Namespace:	default
Node:		minikube/192.168.64.10
Start Time:	Wed, 22 Nov 2017 10:01:59 +0100
Labels:		pod-template-hash=2223832418
		run=fail
Annotations:	kubernetes.io/created-by={&#34;kind&#34;:&#34;SerializedReference&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;reference&#34;:{&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;name&#34;:&#34;fail-6667d7685d&#34;,&#34;uid&#34;:&#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f&#34;,&#34;a...
.
.
.
.
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath		Type		Reason			Message
  ---------	--------	-----	----			-------------		--------	------			-------
  1m		1m		1	default-scheduler				Normal		Scheduled		Successfully assigned fail-6667d7685d-7v6w8 to minikube
  1m		1m		1	kubelet, minikube				Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-9fr6r&#34; 
  1m		6s		4	kubelet, minikube	spec.containers{fail}	Normal		Pulling			pulling image &#34;tutum/curl:1.123456&#34;
  1m		5s		4	kubelet, minikube	spec.containers{fail}	Warning		Failed			Failed to pull image &#34;tutum/curl:1.123456&#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found
  1m		&lt;invalid&gt;	10	kubelet, minikube				Warning		FailedSync		Error syncing pod
  1m		&lt;invalid&gt;	6	kubelet, minikube	spec.containers{fail}	Normal		BackOff			Back-off pulling image &#34;tutum/curl:1.123456&#34;
</code></pre></div><p><strong>Why couldn&rsquo;t Kubernetes pull the image?</strong>
There are three primary candidates besides network connectivity issues:</p><ul><li>The image tag is incorrect</li><li>The image doesn&rsquo;t exist</li><li>Kubernetes doesn&rsquo;t have permissions to pull that image</li></ul><p>If you don&rsquo;t notice a typo in your image tag, then it&rsquo;s time to test using your local machine. I usually start by
running <strong>docker pull on my local development machine</strong> with the exact same image tag. In this case, I would
run <code>docker pull tutum/curl:1.123456</code></p><p>If this succeeds, then it probably means that Kubernetes doesn&rsquo;t have correct permissions to pull that image.</p><p>Add the docker registry user/pwd to your cluster</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl create secret docker-registry dockersecret --docker-server<span class=o>=</span>https://index.docker.io/v1/ --docker-username<span class=o>=</span>&lt;username&gt; --docker-password<span class=o>=</span>&lt;password&gt; --docker-email<span class=o>=</span>&lt;email&gt;
</code></pre></div><p>If the exact image tag fails, then I will test without an explicit image tag - <code>docker pull tutum/curl</code> - which will attempt to pull the latest tag. If this succeeds, then that means
the originally specified tag doesn&rsquo;t exist. Go to the Docker registry and check which tags are available for this image.</p><p>If <code>docker pull tutum/curl</code> (without an exact tag) fails, then we have a bigger problem -
that image does not exist at all in our image registry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7486173c78c00fc1dfc4e45f04c5c7f>5 - Container image not updating</h1><div class=lead>Updating Images in your cluster during development</div><h2 id=preface>Preface</h2><p>A container image should use a fixed tag or the content hash of the image. It should not use the tags <strong>latest</strong>,
<strong>head</strong>, <strong>canary</strong>, or other tags that are designed to be <em>floating</em>.</p><h2 id=problem>Problem</h2><p>Many Kubernetes users have run into this problem.
The story goes something like this:</p><ul><li>Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0)</li><li>Fix a bug in awesomeapp</li><li>Build a new image and push it with the <strong>same tag</strong> (cp-enablement/awesomeapp:1.0)</li><li>Update your deployment</li><li>Realize that the bug is still present</li><li>Rinse and repeat steps 3 to 5 until you recognize this doesn&rsquo;t work</li></ul><p>The problem relates to how Kubernetes decides whether to do a <em>docker pull</em> when starting a container.
Since we tagged our image as <em>:1.0</em>, the default pull policy is <strong>IfNotPresent</strong>. The Kubelet already has a local
copy of cp-enablement/awesomeapp:1.0, hence it doesn&rsquo;t attempt to do a docker pull. When the new Pods come up,
they still use the old broken Docker image.</p><p>There are three ways to resolve this:</p><ul><li><del>Switch to using the tag :latest</del> (DO NOT DO THIS!)</li><li>Specify ImagePullPolicy: always (not recomended).</li><li><strong>Use unique tags (best practice)</strong></li></ul><h2 id=solution>Solution</h2><p>In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag
and push the build result to the registry.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=cp>#!/usr/bin/env bash
</span><span class=cp></span>
<span class=c1># Set the docker image name and the corresponding repository</span>
<span class=c1># Ensure that you change them in the deployment.yml as well.</span>
<span class=c1># You must be logged in with docker login…</span>
<span class=c1>#</span>
<span class=c1># CHANGE THIS TO YOUR Docker.io SETTINGS</span>
<span class=c1>#</span>
<span class=nv>PROJECT</span><span class=o>=</span>awesomeapp
<span class=nv>REPOSITORY</span><span class=o>=</span>cp-enablement

<span class=c1># exit if any subcommand or pipeline returns a non-zero status.</span>
<span class=nb>set</span> -e

<span class=c1># set debug mode</span>
<span class=c1>#set -x</span>

<span class=c1># build my nodeJS app</span>
<span class=c1>#</span>
npm run build

<span class=c1># get latest version IDs from the Docker.io registry and increment them</span>
<span class=c1>#</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=k>$(</span>curl https://registry.hub.docker.com/v1/repositories/<span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>/tags  <span class=p>|</span> sed -e <span class=s1>&#39;s/[][]//g&#39;</span> -e <span class=s1>&#39;s/&#34;//g&#39;</span> -e <span class=s1>&#39;s/ //g&#39;</span> <span class=p>|</span> tr <span class=s1>&#39;}&#39;</span> <span class=s1>&#39;\n&#39;</span>  <span class=p>|</span> awk -F: <span class=s1>&#39;{print $3}&#39;</span> <span class=p>|</span> grep v<span class=p>|</span> tail -n 1<span class=k>)</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=si>${</span><span class=nv>VERSION</span><span class=p>:</span><span class=nv>1</span><span class=si>}</span>
<span class=o>((</span>VERSION++<span class=o>))</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=s2>&#34;v</span><span class=nv>$VERSION</span><span class=s2>&#34;</span>


<span class=c1># build a new docker image</span>
<span class=c1>#</span>
<span class=nb>echo</span> <span class=s1>&#39;&gt;&gt;&gt; Building new image&#39;</span>
<span class=c1># Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875)</span>
docker build --no-cache<span class=o>=</span><span class=nb>true</span> -t <span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>:<span class=nv>$VERSION</span> . <span class=p>|</span> tee /tmp/docker_build_result.log
<span class=nv>RESULT</span><span class=o>=</span><span class=k>$(</span>cat /tmp/docker_build_result.log <span class=p>|</span> tail -n 1<span class=k>)</span>
<span class=k>if</span> <span class=o>[[</span> <span class=s2>&#34;</span><span class=nv>$RESULT</span><span class=s2>&#34;</span> !<span class=o>=</span> *Successfully* <span class=o>]]</span><span class=p>;</span>
<span class=k>then</span>
  <span class=nb>exit</span> -1
<span class=k>fi</span>


<span class=nb>echo</span> <span class=s1>&#39;&gt;&gt;&gt; Push new image&#39;</span>
docker push <span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>:<span class=nv>$VERSION</span>


</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a7727382924a6be0bac7859384e0cf01>6 - Custom Seccomp profile</h1><h1 id=custom-seccomp-profile>Custom Seccomp profile</h1><h2 id=context>Context</h2><p><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.</p><p>Starting from Kubernetes v1.3.0 the Seccomp feature is in <code>Alpha</code>. To configure it on a <code>Pod</code>, the following annotations can be used:</p><ul><li><code>seccomp.security.alpha.kubernetes.io/pod: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to all containers in a <code>Pod</code>.</li><li><code>container.seccomp.security.alpha.kubernetes.io/&lt;container-name>: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to <code>&lt;container-name></code> in a <code>Pod</code>.</li></ul><p>More details can be found in the <code>PodSecurityPolicy</code> <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp>documentation</a>.</p><h2 id=installation-of-custom-profile>Installation of custom profile</h2><p>By default, kubelet loads custom Seccomp profiles from <code>/var/lib/kubelet/seccomp/</code>. There are two ways in which Seccomp profiles can be added to a <code>Node</code>:</p><ul><li>to be baked in the machine image</li><li>to be added at runtime.</li></ul><p>This guide focuses on creating those profiles via a <code>DaemonSet</code>.</p><p>Create a file called <code>seccomp-profile.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ConfigMap</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-profile</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>my-profile.json</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span><span class=sd>    {
</span><span class=sd>      &#34;defaultAction&#34;: &#34;SCMP_ACT_ALLOW&#34;,
</span><span class=sd>      &#34;syscalls&#34;: [
</span><span class=sd>        {
</span><span class=sd>          &#34;name&#34;: &#34;chmod&#34;,
</span><span class=sd>          &#34;action&#34;: &#34;SCMP_ACT_ERRNO&#34;
</span><span class=sd>        }
</span><span class=sd>      ]
</span><span class=sd>    }</span><span class=w>    
</span></code></pre></div><blockquote><p>The policy above is a very simple one and not siutable for complex applications. The <a href=https://github.com/moby/moby/blob/v17.05.0-ce/profiles/seccomp/default.json>default docker profile</a> can be used a reference. Feel free to modify it to your needs.</p></blockquote><p>Apply the <code>ConfigMap</code> in your cluster:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f seccomp-profile.yaml
configmap/seccomp-profile created
</code></pre></div><p>The next steps is to create the <code>DaemonSet</code> seccomp installer. It&rsquo;s going to copy the policy from above in <code>/var/lib/kubelet/seccomp/my-profile.json</code>.</p><p>Create a file called <code>seccomp-installer.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DaemonSet</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>initContainers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>installer</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>alpine:3.10.0</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;cp -r -L /seccomp/*.json /host/seccomp/&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>volumeMounts</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>profiles</span><span class=w>
</span><span class=w>          </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/seccomp</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hostseccomp</span><span class=w>
</span><span class=w>          </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/host/seccomp</span><span class=w>
</span><span class=w>          </span><span class=nt>readOnly</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span><span class=w>      </span><span class=nt>terminationGracePeriodSeconds</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span><span class=w>      </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hostseccomp</span><span class=w>
</span><span class=w>        </span><span class=nt>hostPath</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/var/lib/kubelet/seccomp</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>profiles</span><span class=w>
</span><span class=w>        </span><span class=nt>configMap</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-profile</span><span class=w>
</span></code></pre></div><p>Create the installer and wait until it&rsquo;s ready on all <code>Nodes</code>:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f seccomp-installer.yaml
daemonset.apps/seccomp-installer created

$ kubectl -n kube-system get pods -l security=seccomp
NAME                      READY   STATUS    RESTARTS   AGE
seccomp-installer-wjbxq   1/1     Running   0          21s
</code></pre></div><h2 id=create-a-pod-using-custom-seccomp-profile>Create a Pod using custom Seccomp profile</h2><p>Finally we want to create a profile which uses our new Seccomp profile <code>my-profile.json</code>.</p><p>Create a file called <code>my-seccomp-pod.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-app</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>seccomp.security.alpha.kubernetes.io/pod</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;localhost/my-profile.json&#34;</span><span class=w>
</span><span class=w>    </span><span class=c># you can specify seccomp profile per container. If you add another profile you can configure</span><span class=w>
</span><span class=w>    </span><span class=c># it for a specific container - &#39;pause&#39; in this case.</span><span class=w>
</span><span class=w>    </span><span class=c># container.seccomp.security.alpha.kubernetes.io/pause: &#34;localhost/some-other-profile.json&#34;</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span></code></pre></div><p>Create the <code>Pod</code> and see that&rsquo;s running:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f my-seccomp-pod.yaml
pod/seccomp-app created

$ kubectl get pod seccomp-app
NAME         READY   STATUS    RESTARTS   AGE
seccomp-app  1/1     Running   0          42s
</code></pre></div><h2 id=throubleshooting>Throubleshooting</h2><p>If an invalid or not existing profile is used then the <code>Pod</code> will be stuck in <code>ContainerCreating</code> phase:</p><p><code>broken-seccomp-pod.yaml</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>broken-seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>seccomp.security.alpha.kubernetes.io/pod</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;localhost/not-existing-profile.json&#34;</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span></code></pre></div><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f broken-seccomp-pod.yaml
pod/broken-seccomp created

$ kubectl get pod broken-seccomp
NAME            READY   STATUS              RESTARTS   AGE
broken-seccomp  1/1     ContainerCreating   0          2m

$ kubectl describe pod broken-seccomp
Name:               broken-seccomp
Namespace:          default
....
Events:
  Type     Reason                  Age               From                     Message
  ----     ------                  ----              ----                     -------
  Normal   Scheduled               18s               default-scheduler        Successfully assigned kube-system/broken-seccomp to docker-desktop
  Warning  FailedCreatePodSandBox  4s (x2 over 18s)  kubelet, docker-desktop  Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod &#34;broken-seccomp&#34;: failed to generate sandbox security options
for sandbox &#34;broken-seccomp&#34;: failed to generate seccomp security options for container: cannot load seccomp profile &#34;/var/lib/kubelet/seccomp/not-existing-profile.json&#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory
</code></pre></div><h2 id=further-reading>Further reading</h2><ul><li><a href=https://en.wikipedia.org/wiki/Seccomp>https://en.wikipedia.org/wiki/Seccomp</a></li><li><a href=https://docs.docker.com/engine/security/seccomp>https://docs.docker.com/engine/security/seccomp</a></li><li><a href=https://lwn.net/Articles/656307/>https://lwn.net/Articles/656307/</a></li><li><a href=http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf>http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8de2e5545164d701b7df8e8a18ef00b2>7 - Dockerfile pitfalls</h1><div class=lead>Common Dockerfile pitfalls</div><h2 id=using-latest-tag-for-an-image>Using latest tag for an image</h2><p>Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest
image from a Docker registry.</p><h3 id=bad-dockerfile>Bad Dockerfile</h3><div class=highlight><pre class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=k>FROM</span><span class=s> alpine</span><span class=err>
</span></code></pre></div><p>While simple, using the latest tag for an image means that your build
can suddenly break if that image gets updated. This can lead to problems where everything builds fine
locally (because your local cache thinks it is the latest) while a build server may fail, because some
Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be
difficult, since the maintainer of the Dockerfile didn&rsquo;t actually make any changes.</p><h3 id=good-dockerfile>Good Dockerfile</h3><p>A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.</p><div class=highlight><pre class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=k>FROM</span><span class=s> alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430</span><span class=err>
</span></code></pre></div><h2 id=running-aptapkyum-update>Running apt/apk/yum update</h2><p>Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to
satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with
its own problems.</p><p><strong>apt-get upgrade</strong></p><p>This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile
from creating consistent, immutable builds.</p><p><strong>apt-get update in a different line than running your apt-get install command.</strong></p><p>Running apt-get update as a single line entry will get cached by the build and won&rsquo;t actually run every
time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all
the packages to ensure all are updated correctly.</p><h2 id=avoid-big-container-images>Avoid big container images</h2><p>Building small container image will reduce the time needed to start or restart pods. An image based on the popular
<a href=http://alpinelinux.org/>Alpine Linux project</a> is much smaller
than most distribution based images (~5MB). For most popular languages
and products, there are usually an official Alpine Linux image, e.g. <a href=https://hub.docker.com/_/golang/>golang</a>,
<a href=https://hub.docker.com/_/node/>nodejs</a> and <a href=https://hub.docker.com/_/postgres/>postgres</a>.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$  docker images
REPOSITORY                                                      TAG                     IMAGE ID            CREATED             SIZE
postgres                                                        9.6.9-alpine            6583932564f8        <span class=m>13</span> days ago         39.26 MB
postgres                                                        9.6                     d92dad241eff        <span class=m>13</span> days ago         235.4 MB
postgres                                                        10.4-alpine             93797b0f31f4        <span class=m>13</span> days ago         39.56 MB
</code></pre></div><p>In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it
is recommended to avoid build time tooling in the final images. With Docker&rsquo;s support for
<a href=https://docs.docker.com/engine/userguide/eng-image/multistage-build/>multi-stages builds</a>
this can be easily achieved with minimal effort. Such an example can be found <a href=https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages>here</a>.<br>Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image is also a good base image.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d65e7a77c3c4bf464f24d76782f40592>8 - HTTPS with self Signed Certificate</h1><h2 id=configuring-ingress-with-front-end-tls>Configuring ingress with front-end TLS</h2><p>It is alyways recommended to enable encryption for services to prevent traffic interception and
man-in-the-middle attacks - even in DEV environments.</p><p><img src="https://github.com/freegroup/kube-https/raw/master/images/ingress-https.png?raw=true" alt=Screen title=Screenshot></p><p>You should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure
access to a service from the client to the load balancer by using HTTPS.</p><p>We will use basic procedure here. If your configuration requires advanced security options, please refer
to official CloudFlare&rsquo;s <a href=https://github.com/cloudflare/cfssl>cfssl</a> documentation.</p><h2 id=before-you-begin>Before you begin</h2><p>At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate
with your cluster. If you do not already have a cluster, you can create one by using Gardener</p><h3 id=install-cfssl>Install CFSSL</h3><p>The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.</p><h3 id=initialize-a-ca>Initialize a CA</h3><p>Before we can generate any certs we need to initialize a CA.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>mkdir cfssl
<span class=nb>cd</span> cfssl
cfssl print-defaults config &gt; ca-config.json
cfssl print-defaults csr &gt; ca-csr.json
</code></pre></div><h3 id=configure-ca-options>Configure CA options</h3><p>Now we can configure signing options inside ca-config.json config file. Default options
contain following preconfigured fields:</p><ul><li>profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension.</li><li>expiry: with 8760h default value (or 365 days)</li></ul><p>For compliance let&rsquo;s edit the <code>ca-config.json</code> file and rename <em><strong>www</strong></em> profile into <strong>server</strong></p><p>Edit the <code>ca-csr.json</code> to your needs. See example below. Keep in mind that the <strong>hosts</strong> entries must match
all your ingress entries.</p><p>example <code>ca-csr.json</code></p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>
</code></pre></div><p>And generate CA with defined options:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -initca ca-csr.json <span class=p>|</span> cfssljson -bare ca -
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>ca-key.pem</li><li>ca.csr</li><li>ca.pem</li></ul><p><strong>Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.</strong></p><h2 id=generate-server-certificate>Generate server certificate</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl print-defaults csr &gt; server.json
</code></pre></div><p>Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:</p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>

</code></pre></div><p>Now we are ready to generate server certificate and private key:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -ca<span class=o>=</span>ca.pem -ca-key<span class=o>=</span>ca-key.pem -config<span class=o>=</span>ca-config.json -profile<span class=o>=</span>server server.json <span class=p>|</span> cfssljson -bare server
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>server-key.pem</li><li>server.csr</li><li>server.pem</li></ul><h2 id=configure-kubernetes-ingress-with-tls>Configure Kubernetes ingress with TLS</h2><p>To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update
applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.</p><h2 id=create-kubernetes-secret>Create Kubernetes secret</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem
</code></pre></div><h2 id=create-service--ingress>Create Service / Ingress</h2><p>now you can referenc ethe TLS secret within your ingress definition</p><p>example ingress definition</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>extensions/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>tls-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f45e1e3bb19a23ceeebe6d769078b409>9 - Integrity and Immutability</h1><div class=lead>Ensure that you get always the right image</div><h2 id=introduction>Introduction</h2><p>When transferring data among networked systems, <strong>trust is a central concern</strong>. In particular, when communicating over an
untrusted medium such as the internet, it is critical to ensure the <strong>integrity and immutability</strong> of all the data a
system operates on. Especially if you use Docker Engine to push and pull images (data) to a <strong>public registry</strong>.</p><p>This immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical
at inception. Surprise surprise, deterministic operations.</p><h2 id=a-lesson-in-deterministic-ops>A Lesson in Deterministic Ops</h2><p>Docker Tags are about as reliable and disposable as this guy down here.</p><p><img src=/__resources/howto-content-trust_866756.svg alt=docker-labels></p><p>Seems simple enough. You have probably already deployed hundreds of YAML&rsquo;s or started endless count of Docker container.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker run --name mynginx1 -P -d nginx:1.13.9
</code></pre></div><p>or</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>rss-site</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>web</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>web</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>front-end</span><span class=w>
</span><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.9</span><span class=w>
</span><span class=w>          </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>            </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></code></pre></div><p><strong>But Tags are mutable and humans are prone to error. Not a good combination.</strong> Here we’ll dig into why the use of tags can
be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with
<strong>determinism in mind</strong>.</p><p>I want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that
I defined. Any updates or newer versions of an image should be executed as a new deployment. <strong>The solution: digest</strong></p><p>A digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the
following command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de
</code></pre></div><p>You can now make sure that the same image is always loaded at every deployment. It doesn&rsquo;t matter if the TAG of the
image has been changed or not. <strong>This solves the problem of repeatability.</strong></p><h2 id=content-trust>Content Trust</h2><p>However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another
one infected with malware.</p><p><img src=/__resources/howto-content-trust-replace_615330.svg alt=docker-content-trust></p><p><a href=https://docs.docker.com/engine/security/trust/content_trust/>Docker Content trust</a> gives
you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p><p>Prior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature
called <strong>Docker Content Trust</strong> was introduced to automatically sign and verify the signature of a publisher.</p><p>So, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see
if someone tampered with it in any way. <strong>This solves the problem of trust.</strong></p><p>In addition you should scan all images for known vulnerabilities, this can fill another book</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d122ee5ae2c428bfe3d1293dd385721>10 - Kubernetes Antipatterns</h1><div class=lead>Common Antipatterns for Kubernetes and Docker</div><p><img src=/__resources/howto-antipattern_faca4e.png alt=antipattern></p><p>This HowTo covers common kubernetes antipatterns that we have seen over the past months.</p><h2 id=running-as-root-user>Running as root user.</h2><p>Whenever possible, do not run containers as root user. One could be
tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it
share the same kernel. If a container is compromised, the root user in the container has full control over the
underlying node.</p><p>Watch the very good presentation by Liz Rice at the KubeCon 2018</p><iframe width=560 height=315 src=https://www.youtube.com/embed/ltrV-Qmh3oY frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>Use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group
and add a user to it. Use the <code>USER</code> command to switch to this user. Note that you may also consider to provide
<a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user>an explicit UID/GID</a> if required.</p><p>For Example:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>ARG GF_UID=&#34;500&#34;
ARG GF_GID=&#34;500&#34;

# add group &amp; user
RUN groupadd -r -g $GF_GID appgroup &amp;&amp; \
   useradd appuser -r -u $GF_UID -g appgroup

USER appuser

</code></pre></div><h2 id=store-data-or-logs-in-containers>Store data or logs in containers</h2><p>Containers are ideal for stateless applications
and should be transient. This means that no data or logs should be stored in the
container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside
of containers. Using an <a href=https://www.elastic.co/de/what-is/elk-stack>ELK stack</a> is another good option for storing and processing logs.</p><h2 id=using-pod-ip-addresses>Using pod IP addresses</h2><p>Each pod is assigned an IP address. It is necessary
for pods to communicate with each other to build an application, e.g. an application
must communicate with a database. Existing pods are terminated and new pods are
constantly started. If you would rely on the IP address of a pod or container, you would need to update the application
configuration constantly. This makes the application fragile. Create
services instead. They provide a logical name that can be assigned independently of the
varying number and IP addresses of containers. Services are the basic concept for load
balancing within Kubernetes.</p><h2 id=more-than-one-process-in-a-container>More than one process in a container</h2><p>A docker file provides a <code>CMD</code> and <code>ENTRYPOINT</code> to
start the image. <code>CMD</code> is often used around a script that makes a configuration and then
starts the container. Do not try to start multiple processes with this script. It is
important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes
managing your containers, collecting logs and updating each process more difficult.
You can split the image into multiple containers and manage them independently - even in one pod.
Bear in mind that Kubernetes only monitors the process with PID=1. If more than
one process is started within a container, then these no longer fall under the control of Kubernetes.</p><h2 id=creating-images-in-a-running-container>Creating images in a running container</h2><p>A new image can be created with the <code>docker commit</code>
command. This is useful if changes have been made to the container and you want to persist
them for later error analysis. However, images created like this are not reproducible and
completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize
which components the image contains. Instead, always make changes to the docker file, close
existing containers and start a new container with the updated image.</p><h2 id=saving-passwords-in-docker-image-->Saving passwords in docker image 💀</h2><p>Do not save passwords in a Docker file. They are in plain
text and are checked into a repository. That makes them completely vulnerable even if you are using
a private repository like the Artifactory.
Always use <a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure>Secrets or ConfigMaps</a>
to provision passwords or inject them by mounting a persistent volume.</p><h2 id=using-the-latest-tag>Using the &lsquo;latest&rsquo; tag</h2><p>Starting an image with <em>tomcat</em> is tempting. If no tags are specified, a container is
started with the tomcat:latest image. This image may no longer be up to date and refers to an
older version instead. Running a production application requires complete control of the environment
with exact versions of the image. Make sure you always use a tag or even better the <strong>sha256 hash</strong>
of the image e.g. <code>tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f</code>.
Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case
you don&rsquo;t have complete control over your image - which is bad.</p><h2 id=different-images-per-environment>Different images per environment</h2><p>Don&rsquo;t create different images for development, testing, staging
and production environments. The image should be the <strong>source of truth</strong> and should only be created once
and pushed to the repository. This image:tag should be used for different environments in the future.</p><h2 id=depend-on-start-order-of-pods>Depend on start order of pods</h2><p>Applications often depend on containers being started in a certain order.
For example, a database container must be up and running before an application can connect to it. The application
should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The
application container should be able to handle such situations without terminating or crashing.</p><h2 id=additional-anti-patterns-and-patterns>Additional anti-patterns and patterns&mldr;</h2><p>In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes.
Refer to the following link for more information</p><ul><li><a href=https://github.com/gravitational/workshop/blob/master/k8sprod.md>Kubernetes Production Patterns</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ffdcb3c3c73c058191fdf52cbca5e94c>11 - Namespace Isolation</h1><p>&mldr;or <strong>DENY all traffic from other namespaces</strong></p><p>You can configure a <strong>NetworkPolicy</strong> to deny all the traffic from other namespaces while allowing all the traffic
coming from the same namespace the pod was deployed into.</p><img src=/__resources/howto-namespaceisolation_00dff7.png width=100%><p><strong>There are many reasons why you may chose to employ Kubernetes network policies:</strong></p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other</li></ul><p>Kubernetes <strong>network policies</strong> are application centric compared to infrastructure/network centric standard firewalls.
<strong>There are no explicit CIDRs or IP addresses used</strong> for matching source or destination IP’s. <strong>Network policies build up on labels
and selectors</strong> which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and
select subsets of objects.</p><h2 id=example>Example</h2><p>We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are
unable to get content from <em>namespace1</em> if you are sitting in <em>namespace2</em>.</p><h2 id=setup-the-namespaces>Setup the namespaces</h2><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># create two namespaces for test purpose</span>
kubectl create ns customer1
kubectl create ns customer2

<span class=c1># create a standard HTTP web server</span>
kubectl run nginx --image<span class=o>=</span>nginx --replicas<span class=o>=</span><span class=m>1</span> --port<span class=o>=</span><span class=m>80</span> -n<span class=o>=</span>customer1
kubectl run nginx --image<span class=o>=</span>nginx --replicas<span class=o>=</span><span class=m>1</span> --port<span class=o>=</span><span class=m>80</span> -n<span class=o>=</span>customer2

<span class=c1># expose the port 80 for external access</span>
kubectl expose deployment nginx --port<span class=o>=</span><span class=m>80</span> --type<span class=o>=</span>NodePort -n<span class=o>=</span>customer1
kubectl expose deployment nginx --port<span class=o>=</span><span class=m>80</span> --type<span class=o>=</span>NodePort -n<span class=o>=</span>customer2

</code></pre></div><hr><h2 id=test-without-np>Test without NP</h2><img src=/__resources/howto-namespaceisolation-without_8a0a23.png width=80%><p>Create a pod with <em>curl</em> preinstalled inside the namespace <em>customer1</em></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># create a &#34;bash&#34; pod in one namespace</span>
kubectl run -i --tty client --image<span class=o>=</span>tutum/curl -n<span class=o>=</span>customer1
</code></pre></div><p>try to <em>curl</em> the exposed nginx server to get the default index.html page. <strong>Execute this in the bash prompt of the
pod created above.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># get the index.html from the nginx of the namespace &#34;customer1&#34; =&gt; success</span>
curl http://nginx.customer1
<span class=c1># get the index.html from the nginx of the namespace &#34;customer2&#34; =&gt; success</span>
curl http://nginx.customer2
</code></pre></div><p>Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.</p><hr><h2 id=test-with-np>Test with NP</h2><img src=/__resources/howto-namespaceisolation-with_3a6dd1.png width=80%><p>Install the <strong>NetworkPolicy</strong> from your shell</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>NetworkPolicy</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>deny-from-other-namespaces</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>podSelector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>from</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>podSelector</span><span class=p>:</span><span class=w> </span>{}<span class=w>
</span></code></pre></div><ul><li>it applies the policy to ALL pods in the named namespace as the <code>spec.podSelector.matchLabels</code> is empty and therefore selects all pods.</li><li>it allows traffic from ALL pods in the named namespace, as <code>spec.ingress.from.podSelector</code> is empty and therefore selects all pods.</li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl apply -f ./network-policy.yaml -n=customer1
kubectl apply -f ./network-policy.yaml -n=customer2
</code></pre></div><p>after this <code>curl http://nginx.customer2</code> shouldn&rsquo;t work anymore if you are a service inside the namespace <em>customer1</em> and
vice versa</p><p><em>Note</em>: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type <code>LoadBalancer</code> in namespace <code>customer1</code> that match the nginx pod. When you request the service by its <code>&lt;EXTERNAL_IP>:&lt;PORT></code>, then the network policy will deny the ingress traffic from the service and the request will time out.</p><h2 id=more>More</h2><p>You can get more information how to configure the <strong>NetworkPolicies</strong> on:</p><ul><li><a href=https://docs.projectcalico.org/v3.0/getting-started/kubernetes/tutorials/advanced-policy>Calico WebSite</a></li><li><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>Kubernetes NP Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-90f02665c7456e1a24afc104f91bea70>12 - Orchestration of container startup</h1><div class=lead>How to orchestrate startup sequence of multiple containers</div><h2 id=disclaimer>Disclaimer</h2><p>If an application depends on other services deployed separately do not rely on a certain start sequence of containers
but ensure that the application can cope with unavailability of the services it depends on.</p><h2 id=introduction>Introduction</h2><p>Kubernetes offers a feature called <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>InitContainers</a> to perform some tasks during a pod&rsquo;s initialization.
In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app <a href=https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a>url-shortener</a> which consists of two components:</p><ul><li>postgresql database</li><li>webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.</li></ul><p>This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl logs webapp-958cf5567-h247n
<span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2018-06-12T11:02:42Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\n&#34;</span>
<span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2018-06-12T11:02:42Z&#34;</span> <span class=nv>level</span><span class=o>=</span>fatal <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\n&#34;</span>


$ kubectl get po -w
NAME                                READY     STATUS    RESTARTS   AGE
webapp-958cf5567-h247n   0/1       Pending   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       Pending   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       ContainerCreating   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       ContainerCreating   <span class=m>0</span>         1s
webapp-958cf5567-h247n   0/1       Error     <span class=m>0</span>         2s
webapp-958cf5567-h247n   0/1       Error     <span class=m>1</span>         3s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>1</span>         4s
webapp-958cf5567-h247n   0/1       Error     <span class=m>2</span>         18s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>2</span>         29s
webapp-958cf5567-h247n   0/1       Error     <span class=m>3</span>         43s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>3</span>         56s

</code></pre></div><p>If the <code>restartPolicy</code> is set to <code>Always</code> (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.</p><h2 id=using-initcontaniner>Using InitContaniner</h2><p>To avoid such situation, <code>InitContainers</code> can be defined which are executed prior to the application container. If one InitContainers fails, the application container won&rsquo;t be triggered.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>initContainers</span><span class=p>:</span><span class=w>  </span><span class=c># check if DB is ready, and only continue when true</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>check-db-ready</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>postgres:9.6.5</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s1>&#39;sh&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;-c&#39;</span><span class=p>,</span><span class=w>  </span><span class=s1>&#39;until pg_isready -h postgres -p 5432;  do echo waiting for database; sleep 2; done;&#39;</span><span class=p>]</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>xcoulon/go-url-shortener:0.1.0</span><span class=w>
</span><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>go-url-shortener</span><span class=w>
</span><span class=w>        </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_HOST</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>postgres</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_PORT</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;5432&#34;</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_DATABASE</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>url_shortener_db</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_USER</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>user</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_PASSWORD</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>mysecretpassword</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><p>In above example, the <code>InitContainers</code> uses docker image <code>postgres:9.6.5</code> which is different from the application container.
This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.</p><p>With introduction of <code>InitContainers</code>, the pod startup will look like following in case database is not available yet:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get po -w
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-5cc79d6bfd-t9n8h   1/1       Running   <span class=m>0</span>          5d
privileged-pod                      1/1       Running   <span class=m>0</span>          4d
webapp-fdcb49cbc-4gs4n   0/1       Pending   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Pending   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   <span class=m>0</span>         1s


$ kubectl  logs webapp-fdcb49cbc-4gs4n
Error from server <span class=o>(</span>BadRequest<span class=o>)</span>: container <span class=s2>&#34;go-url-shortener&#34;</span> in pod <span class=s2>&#34;webapp-fdcb49cbc-4gs4n&#34;</span> is waiting to start: PodInitializing
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-02cf553d7991e467aa6ce1be8575e57c>13 - Out-Dated HTML and JS files delivered</h1><div class=lead>Why is my application always outdated?</div><h2 id=problem>Problem</h2><p><strong>After updating your HTML and JavaScript sources in your web application,
the kubernetes cluster delivers outdated versions - why?</strong></p><h2 id=preamble>Preamble</h2><p>By default, Kubernetes service pods are not accessible from the external
network, but only from other pods within the same Kubernetes cluster.</p><p>The Gardener cluster has a built-in configuration for HTTP load balancing called <strong>Ingress</strong>,
defining rules for external connectivity to Kubernetes services. Users who want external access
to their Kubernetes services create an ingress resource that defines rules,
including the URI path, backing service name, and other information. The Ingress controller
can then automatically program a frontend load balancer to enable Ingress configuration.</p><p><img src=/__resources/howto-nginx_f7c046.svg alt=nginx></p><h2 id=example-ingress-configuration>Example Ingress Configuration</h2><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><p>where:</p><ul><li><strong>&lt;GARDENER-CLUSTER></strong>: The cluster name in the Gardener</li><li><strong>&lt;GARDENER-PROJECT></strong>: You project name in the Gardener</li></ul><h2 id=what-is-the-underlying-problem>What is the underlying problem?</h2><p>The ingress controller we are using is <strong>NGINX</strong>.</p><blockquote><p>NGINX is a software load balancer, web server, and <strong>content cache</strong> built on top of open
source NGINX.</p></blockquote><p><strong>NGINX caches the content as specified in the HTTP header.</strong> If the HTTP header is missing,
it is assumed that the cache is <strong>forever</strong> and NGINX never updates the content in the
stupidest case.</p><h2 id=solution>Solution</h2><p>In general you can avoid this pitfall with one of the solutions below:</p><ul><li>use a cache buster + HTTP-Cache-Control(prefered)</li><li>use HTTP-Cache-Control with a lower retention period</li><li>disable the caching in the ingress (just for dev purpose)</li></ul><p>Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise
for your web framework (e.g. Express/NodeJS, SpringBoot,&mldr;)</p><p>Here an example how to disable the cache control for your ingress done with an annotation in your
ingress YAML (during development).</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>ingress.kubernetes.io/cache-enable</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;false&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-424dc73fc8ed567a9cf1396d57f3cb2c>14 - Storing secrets in git 💀</h1><div class=lead>Never ever commit a kubeconfig.yaml into github</div><h2 id=problem>Problem</h2><p>If you commit sensitive data, such as a <code>kubeconfig.yaml</code> or <code>SSH key</code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&rsquo;s history you can use the git <code>filter-branch</code> command.</p><p>The git filter-branch command rewrite your repository&rsquo;s history, which changes the SHAs for
existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests
in your repository. <strong>I recommend merging or closing all open pull requests before removing files from your repository.</strong></p><blockquote><p><strong>Warning:</strong> - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.</p></blockquote><h2 id=purging-a-file-from-your-repositorys-history>Purging a file from your repository&rsquo;s history</h2><blockquote><p><strong>Warning:</strong> If you run <code>git filter-branch</code> after stashing changes, you won&rsquo;t be able to retrieve your changes with other
stash commands. Before running git filter-branch, we recommend unstashing any changes you&rsquo;ve made. To unstash the
last set of changes you&rsquo;ve stashed, run <code>git stash show -p | git apply -R</code>. For more information, see Git Tools Stashing.</p></blockquote><p>To illustrate how <code>git filter-branch</code> works, we&rsquo;ll show you how to remove your file with sensitive data from the
history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.</p><p><strong>Navigate into the repository&rsquo;s working directory.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=nb>cd</span> YOUR-REPOSITORY
</code></pre></div><p><strong>Run the following command, replacing <code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code> with the path to the file you want to remove,
not just its filename.</strong></p><p>These arguments will:</p><ul><li>Force Git to process, but not check out, the entire history of every branch and tag</li><li>Remove the specified file, as well as any empty commits generated as a result</li><li>Overwrite your existing tags</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git filter-branch --force --index-filter <span class=se>\
</span><span class=se></span><span class=s1>&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39;</span> <span class=se>\
</span><span class=se></span>--prune-empty --tag-name-filter cat -- --all

</code></pre></div><p><strong>Add your file with sensitive data to <code>.gitignore</code> to ensure that you don&rsquo;t accidentally commit it again.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash> <span class=nb>echo</span> <span class=s2>&#34;YOUR-FILE-WITH-SENSITIVE-DATA&#34;</span> &gt;&gt; .gitignore
</code></pre></div><p><strong>Double-check that you&rsquo;ve removed everything you wanted to from your repository&rsquo;s history, and that all of your
branches are checked out.</strong></p><p><strong>Once you&rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository,
as well as all the branches you&rsquo;ve pushed up:</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git push origin --force --all
</code></pre></div><p><strong>In order to remove the sensitive file from your tagged releases, you&rsquo;ll also need to force-push against your Git tags:</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git push origin --force --tags
</code></pre></div><blockquote><p><strong>Warning:</strong> Tell your collaborators to <strong>rebase, not merge</strong>, any branches they created off of your old (tainted) repository history.
One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.</p></blockquote><p>References:</p><ul><li><a href=https://help.github.com/articles/removing-sensitive-data-from-a-repository/>https://help.github.com/articles/removing-sensitive-data-from-a-repository/</a></li></ul><style>blockquote{border:1px solid red;padding:10px;margin-top:40px;margin-bottom:40px}blockquote p{font-size:1.5rem;color:#000}</style></div><div class=td-content style=page-break-before:always><h1 id=pg-77bb872706e47f8307662d7339184bb5>15 - Using Prometheus and Grafana to monitor K8s</h1><div class=lead>How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics</div><h2 id=disclaimer>Disclaimer</h2><p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both
applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments.
Such advanced details are not in the scope of this post.</p><h2 id=introduction>Introduction</h2><p><a href=https://prometheus.io/>Prometheus</a> is an open-source systems monitoring and alerting toolkit for recording numeric
time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented
architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a
particular strength.</p><p>Prometheus graduates within CNCF <a href=https://prometheus.io/blog/2018/08/09/prometheus-graduates-within-cncf/>second hosted project</a>.</p><p>The following characteristics make Prometheus a good match for monitoring Kubernetes clusters:</p><ul><li><p>Pull-based monitoring<br>Prometheus is a <a href=https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/>pull-based</a> monitoring system,
which means that the Prometheus server dynamically discovers and pulls metrics from your services running in
Kubernetes.</p></li><li><p>Labels
Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.<br>Labels are used to identify time series and sets of label matchers can be used in the query language
( <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> ) to select the time series to be aggregated..</p></li><li><p>Exporters<br>There are many <a href=https://prometheus.io/docs/instrumenting/exporters/>exporters</a> available which enable integration of
databases or even other monitoring systems not already providing a way to export metrics to Prometheus.
One prominent exporter is the so called <a href=https://github.com/prometheus/node_exporter>node-exporter</a>, which allows to
monitor hardware and OS related metrics of Unix systems.</p></li><li><p>Powerful query language<br>The Prometheus query language <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> lets the user
select and aggregate time series data in real time. Results can either be shown as a graph, viewed
as tabular data in the Prometheus expression browser, or consumed by external systems via the <a href=https://prometheus.io/docs/prometheus/latest/querying/api/>HTTP API</a>.</p></li></ul><p>Find query examples on <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>.</p><p>One very popular open-source visualization tool not only for Prometheus is <a href=https://grafana.com>Grafana</a>. Grafana is a
metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure
and application analytics but many use it in other domains including industrial sensors, home automation, weather, and
process control [see <a href=http://docs.grafana.org/>Grafana Documentation</a>].</p><p>Grafana accesses data via <a href=http://docs.grafana.org/guides/basic_concepts/>Data Sources</a>. The continuously growing
list of supported backends includes Prometheus.</p><p>Dashboards are created by combining panels, e.g. <a href=http://docs.grafana.org/reference/graph/>Graph</a> and <a href=http://docs.grafana.org/reference/dashlist/>Dashlist</a>.</p><p>In this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring
configuration as the one provided for Kubernetes clusters created by Gardener.</p><p>If you miss elements on the Prometheus web page when accessing it via its service URL <code>https://&lt;your K8s FQN>/api/v1/namespaces/&lt;your-prometheus-namespace>/services/prometheus-prometheus-server:80/proxy</code>
this is probably caused by Prometheus issue <a href=https://github.com/prometheus/prometheus/issues/1583>#1583</a>
To workaround this issue setup a port forward <code>kubectl port-forward -n &lt;your-prometheus-namespace> &lt;prometheus-pod> 9090:9090</code>
on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant
in case you use the service type <code>LoadBalancer</code>.</p><h2 id=preparation>Preparation</h2><p>The deployment of <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> is based on Helm charts.<br>Make sure to implement the <a href=/docs/guides/client_tools/helm>Helm settings</a> before deploying the Helm charts.</p><p>The Kubernetes clusters provided by <a href=https://github.com/gardener>Gardener</a> use role based
access control (<a href=https://kubernetes.io/docs/admin/authorization/rbac/>RBAC</a>). To authorize the Prometheus
node-exporter to access hardware and OS relevant metrics of your cluster&rsquo;s worker nodes specific artifacts need to be
deployed.</p><p>Bind the prometheus service account to the <code>garden.sapcloud.io:monitoring:prometheus</code> cluster role by running the command
<code>kubectl apply -f crbinding.yaml</code>.</p><p>Content of <code>crbinding.yaml</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRoleBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-name&gt;-server</span><span class=w>
</span><span class=w></span><span class=nt>roleRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>apiGroup</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io</span><span class=w>
</span><span class=w>  </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRole</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>garden.sapcloud.io:monitoring:prometheus</span><span class=w>
</span><span class=w></span><span class=nt>subjects</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-name&gt;-server</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-namespace&gt;</span><span class=w>
</span></code></pre></div><h2 id=deployment-of-prometheus-and-grafana>Deployment of Prometheus and Grafana</h2><p>Only minor changes are needed to deploy <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a>
and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> based on Helm charts.</p><p>Copy the following configuration into a file called values.yaml and deploy Prometheus: <code>helm install &lt;your-prometheus-name> --namespace &lt;your-prometheus-namespace> stable/prometheus -f values.yaml</code></p><p>Typically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel
free to choose different namespaces.</p><p>Content of <code>values.yaml</code> for Prometheus:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>rbac</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>create</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w> </span><span class=c># Already created in Preparation step</span><span class=w>
</span><span class=w></span><span class=nt>nodeExporter</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w> </span><span class=c># The node-exporter is already deployed by default</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nt>server</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>global</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_timeout</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nt>serverFiles</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>prometheus.yml</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>rule_files</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>/etc/config/rules</span><span class=w>
</span><span class=w>      </span>- <span class=l>/etc/config/alerts      </span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_configs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kube-kubelet&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>honor_labels</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>scheme</span><span class=p>:</span><span class=w> </span><span class=l>https</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>tls_config</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># This is needed because the kubelets&#39; certificates are not generated</span><span class=w>
</span><span class=w>      </span><span class=c># for a specific pod IP</span><span class=w>
</span><span class=w>        </span><span class=nt>insecure_skip_verify</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>      </span><span class=nt>bearer_token_file</span><span class=p>:</span><span class=w> </span><span class=l>/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>node</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>        </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>/metrics</span><span class=w>
</span><span class=w>      </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_node_address_InternalIP]</span><span class=w>
</span><span class=w>        </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>      </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>        </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_node_label_(.+)</span><span class=w>
</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kube-kubelet-cadvisor&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>honor_labels</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>scheme</span><span class=p>:</span><span class=w> </span><span class=l>https</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>tls_config</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># This is needed because the kubelets&#39; certificates are not generated</span><span class=w>
</span><span class=w>      </span><span class=c># for a specific pod IP</span><span class=w>
</span><span class=w>        </span><span class=nt>insecure_skip_verify</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>      </span><span class=nt>bearer_token_file</span><span class=p>:</span><span class=w> </span><span class=l>/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>node</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>        </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>/metrics/cadvisor</span><span class=w>
</span><span class=w>      </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_node_address_InternalIP]</span><span class=w>
</span><span class=w>        </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>      </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>        </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_node_label_(.+)</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c># Example scrape config for probing services via the Blackbox Exporter.</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/probe`: Only probe services that have a value of `true`</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-services&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>metrics_path</span><span class=p>:</span><span class=w> </span><span class=l>/probe</span><span class=w>
</span><span class=w>      </span><span class=nt>params</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>module</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>http_2xx]</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>service</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_probe]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__param_target</span><span class=w>
</span><span class=w>        </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>blackbox</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__param_target]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_service_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_name</span><span class=w>
</span><span class=w>    </span><span class=c># Example scrape config for pods</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scrape`: Only scrape pods that have a value of `true`</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-pods&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>pod</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_annotation_prometheus_io_scrape]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_annotation_prometheus_io_path]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+):(?:\d+);(\d+)</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>${1}:${2}</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_pod_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_pod_name</span><span class=w>
</span><span class=w>    </span><span class=c># Scrape config for service endpoints.</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># The relabeling allows the actual service scrape endpoint to be configured</span><span class=w>
</span><span class=w>    </span><span class=c># via the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scrape`: Only scrape services that have a value of `true`</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need</span><span class=w>
</span><span class=w>    </span><span class=c># to set this to `https` &amp; most likely set the `tls_config` of the scrape config.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/port`: If the metrics are exposed on a different port to the</span><span class=w>
</span><span class=w>    </span><span class=c># service then set this appropriately.</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-service-endpoints&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>endpoints</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_scrape]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_scheme]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__scheme__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(https?)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_path]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__, __meta_kubernetes_service_annotation_prometheus_io_port]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)(?::\d+);(\d+)</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>$1:$2</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_service_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_name</span><span class=w> </span><span class=c># Add your additional configuration here...</span><span class=w>
</span></code></pre></div><p>Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set
explicitly in case the default changed.
Deploy Grafana via <code>helm install grafana --namespace &lt;your-prometheus-namespace> stable/grafana -f values.yaml</code>. Here, the same namespace is chosen for Prometheus and for Grafana.</p><p>Content of <code>values.yaml</code> for Grafana:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>server</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>  </span><span class=nt>service</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP</span><span class=w>
</span></code></pre></div><p>Check the running state of the pods on the Kubernetes Dashboard or by running <code>kubectl get pods -n &lt;your-prometheus-namespace></code>.
In case of errors check the log files of the pod(s) in question.</p><p>The text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user
and password of the Grafana Admin user. The credentials are stored as secrets in the namespace <code>&lt;your-prometheus-namespace></code>
and could be decoded via <code>kubectl get secret --namespace &lt;my-grafana-namespace> grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo</code>.</p><h2 id=basic-functional-tests>Basic functional tests</h2><p>To access the web UI of both applications use port forwarding of port 9090.</p><p>Setup port forwarding for port 9090:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl port-forward -n &lt;your-prometheus-namespace&gt; &lt;your-prometheus-server-pod&gt; 9090:9090
</code></pre></div><p>Open <code>http://localhost:9090</code> in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server
(see <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>)</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>100 * (1 - avg by(instance)(irate(node_cpu{mode=&#39;idle&#39;}[5m])))
</code></pre></div><p>This should show some data in a graph.</p><p>To show the same data in Grafana setup port forwarding for port 3000 for the
Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser.
Enter the credentials of the admin user.</p><p>Next, you need to enter the server name of your Prometheus deployment. This name is shown directly after the
installation via helm.</p><p>Run</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>helm status &lt;your-prometheus-name&gt;
</code></pre></div><p>to find this name. Below this server name is referenced by <code>&lt;your-prometheus-server-name></code>.</p><p>First, you need to add your Prometheus server as data source.</p><ul><li>select <em>Dashboards → Data Sources</em></li><li>select <em>Add data source</em></li><li>enter
<em>Name</em>: <code>&lt;your-prometheus-datasource-name></code><br><em>Type</em>: Prometheus<br><em>URL</em>: <code>http://&lt;your-prometheus-server-name></code><br>_Access: <code>proxy</code></li><li>select <em>Save & Test</em></li></ul><p>In case of failure check the Prometheus URL in the Kubernetes Dashboard.</p><p>To add a Graph follow these steps:</p><ul><li>in the left corner, select <em>Dashboards → New</em> to create a new dashboard</li><li>select <em>Graph</em> to create a new graph</li><li>next, select the <em>Panel Title → Edit</em></li><li>select your Prometheus Data Source in the drop down list</li><li>enter the expression <code>100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))</code> in the entry field A</li><li>select the floppy disk symbol (Save) on top</li></ul><p>Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.</p><p>As a next step you can implement monitoring for your applications by implementing the <a href=https://prometheus.io/docs/instrumenting/clientlibs/>Prometheus client API</a>.</p><h2 id=links>Links</h2><ul><li><a href=https://prometheus.io/>Prometheus</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus Helm Chart</a></li><li><a href=https://www.weave.works/blog/prometheus-kubernetes-perfect-match/>Prometheus and Kubernetes: A Perfect Match</a></li><li><a href=https://grafana.com>Grafana</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana Helm Chart</a></li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=/blog/>Blogs</a></li><li><a href=/community/>Community</a></li><li><a href=/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2021 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script><script src=/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js integrity="sha256-OxcsE7YsK+qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin=anonymous></script></body></html>