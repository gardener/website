<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/networking/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/networking/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Networking | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/docs/gardener/networking/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Networking"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Networking"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Networking"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css as=style integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><link href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css rel=stylesheet integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009F76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.d98ac06a3bf30aebcb55e1fbd51858ed.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/networking/>Return to the regular view of this page</a>.</p></div><h1 class=title>Networking</h1><div class=content></div></div><div class=td-content><h1 id=pg-75ac105b3ef1370a62e6782a421a1288>1 - Custom DNS Configuration</h1><h1 id=custom-dns-configuration>Custom DNS Configuration<a class=td-heading-self-link href=#custom-dns-configuration aria-label="Heading self-link"></a></h1><p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.</p><p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.</p><p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &ldquo;undo&rdquo;, we utilize the <code>import</code> plugin from CoreDNS [1].
which enables in-line configuration changes.</p><h2 id=how-to-use>How to use<a class=td-heading-self-link href=#how-to-use aria-label="Heading self-link"></a></h2><p>To customize your CoreDNS cluster config, you can simply edit a <code>ConfigMap</code> named <code>coredns-custom</code> in the <code>kube-system</code> namespace.
By editing, this <code>ConfigMap</code>, you are modifying CoreDNS configuration, therefore care is advised.</p><p>For example, to apply new config to CoreDNS that would point all <code>.global</code> DNS requests to another DNS pod, simply edit the configuration as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: coredns-custom
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  istio.server: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    global:8053 {
</span></span></span><span style=display:flex><span><span style=color:#a31515>            errors
</span></span></span><span style=display:flex><span><span style=color:#a31515>            cache 30
</span></span></span><span style=display:flex><span><span style=color:#a31515>            forward . 1.2.3.4
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }</span>
</span></span><span style=display:flex><span>  corefile.override: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>         # &lt;some-plugin&gt; &lt;some-plugin-config&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>         debug
</span></span></span><span style=display:flex><span><span style=color:#a31515>         whoami</span>
</span></span></code></pre></div><p>The port number 8053 in <code>global:8053</code> is the specific port that CoreDNS is bound to and cannot be changed to any other port if it should act on ordinary name resolution requests from pods. Otherwise, CoreDNS will open a second port, but you are responsible to direct the traffic to this port. <code>kube-dns</code> service in <code>kube-system</code> namespace will direct name resolution requests within the cluster to port 8053 on the CoreDNS pods.
Moreover, additional network policies are needed to allow corresponding ingress traffic to CoreDNS pods.
In order for the destination DNS server to be reachable, it must listen on port 53 as it is required by network policies. Other ports are only possible if additional network policies allow corresponding egress traffic from CoreDNS pods.</p><p>It is important to have the <code>ConfigMap</code> keys ending with <code>*.server</code> (if you would like to add a new server) or <code>*.override</code>
if you want to customize the current server configuration (it is optional setting both).</p><h2 id=warning>Warning<a class=td-heading-self-link href=#warning aria-label="Heading self-link"></a></h2><p>Be careful when overriding plugins <code>log</code>, <code>forward</code> or <code>cache</code>.</p><ul><li>Increasing log level can lead to increased load/reduced throughput.</li><li>Changing the forward target may lead to unexpected results.</li><li>Playing with the cache settings can impact the timeframe how long it takes for changes to become visible.</li></ul><p><code>*.override</code> and <code>*.server</code> data points from <code>coredns-custom</code> <code>ConfigMap</code> are imported into Corefile as follows.
Please consult <code>coredns</code> <a href=https://coredns.io/plugins/>plugin documentation</a> for potential side-effects.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>.:8053 {
</span></span><span style=display:flex><span>  health {
</span></span><span style=display:flex><span>      lameduck 15s
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  ready
</span></span><span style=display:flex><span>  [search-rewrites]
</span></span><span style=display:flex><span>  kubernetes[clusterDomain]in-addr.arpa ip6.arpa {
</span></span><span style=display:flex><span>      pods insecure
</span></span><span style=display:flex><span>      fallthrough in-addr.arpa ip6.arpa
</span></span><span style=display:flex><span>      ttl 30
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  prometheus :9153
</span></span><span style=display:flex><span>  loop
</span></span><span style=display:flex><span>  import custom/*.override
</span></span><span style=display:flex><span>  errors
</span></span><span style=display:flex><span>  log . {
</span></span><span style=display:flex><span>      class error
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  forward . /etc/resolv.conf
</span></span><span style=display:flex><span>  cache 30
</span></span><span style=display:flex><span>  reload
</span></span><span style=display:flex><span>  loadbalance round_robin
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>import custom/*.server
</span></span></code></pre></div><h2 id=optional-reload-coredns>[Optional] Reload CoreDNS<a class=td-heading-self-link href=#optional-reload-coredns aria-label="Heading self-link"></a></h2><p>As Gardener is configuring the <code>reload</code> <a href=https://coredns.io/plugins/reload/>plugin</a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate <code>ConfigMap</code> changes. However, if you don&rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system rollout restart deploy coredns
</span></span></code></pre></div><p>This will reload the config into CoreDNS.</p><p>The approach we follow here was inspired by AKS&rsquo;s approach [2].</p><h2 id=anti-pattern>Anti-Pattern<a class=td-heading-self-link href=#anti-pattern aria-label="Heading self-link"></a></h2><p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).</p><p>If incompatible changes are applied by mistake, simply delete the content of the <code>ConfigMap</code> and re-apply.
This should bring the cluster DNS back to functioning state.</p><h2 id=node-local-dns>Node Local DNS<a class=td-heading-self-link href=#node-local-dns aria-label="Heading self-link"></a></h2><p>Custom DNS configuration] may not work as expected in conjunction with <code>NodeLocalDNS</code>.
With <code>NodeLocalDNS</code>, ordinary DNS queries targeted at the upstream DNS servers, i.e. non-kubernetes domains,
will not end up at CoreDNS, but will instead be directly sent to the upstream DNS server. Therefore, configuration
applying to non-kubernetes entities, e.g. the <code>istio.server</code> block in the
<a href=/docs/gardener/networking/custom-dns-config/>custom DNS configuration</a> example, may not have any effect with <code>NodeLocalDNS</code> enabled.
If this kind of custom configuration is required, forwarding to upstream DNS has to be disabled.
This can be done by setting the option (<code>spec.systemComponents.nodeLocalDNS.disableForwardToUpstreamDNS</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      disableForwardToUpstreamDNS: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><p>[1] <a href=https://github.com/coredns/coredns/tree/master/plugin/import>Import plugin</a>
[2] <a href=https://docs.microsoft.com/en-us/azure/aks/coredns-custom>AKS Custom DNS</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b65b1134e1b41326ea6a72f017a192cc>2 - DNS Search Path Optimization</h1><h1 id=dns-search-path-optimization>DNS Search Path Optimization<a class=td-heading-self-link href=#dns-search-path-optimization aria-label="Heading self-link"></a></h1><h2 id=dns-search-path>DNS Search Path<a class=td-heading-self-link href=#dns-search-path aria-label="Heading self-link"></a></h2><p>Using fully qualified names has some downsides, e.g., it may become harder to move deployments from one landscape to the
next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context
they are used in.</p><p>The DNS search path allows for the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local
names to create a fully qualified name.</p><p>If a short/local name should be resolved, each entry is appended to it one by one to check whether it can be resolved. The
process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search
path, the short/local name is attempted to be resolved on it own.</p><h2 id=dns-option-ndots>DNS Option <code>ndots</code><a class=td-heading-self-link href=#dns-option-ndots aria-label="Heading self-link"></a></h2><p>As explained in the <a href=/docs/gardener/networking/dns-search-path-optimization/#dns-search-path>section above</a>, the DNS search path is used for short/local names to create fully
qualified names. The DNS option <code>ndots</code> specifies how many dots (<code>.</code>) a name needs to have to be considered fully qualified.
For names with less than <code>ndots</code> dots (<code>.</code>), the <a href=/docs/gardener/networking/dns-search-path-optimization/#dns-search-path>DNS search path</a> will be applied.</p><h2 id=dns-search-path-ndots-and-kubernetes>DNS Search Path, <code>ndots</code>, and Kubernetes<a class=td-heading-self-link href=#dns-search-path-ndots-and-kubernetes aria-label="Heading self-link"></a></h2><p>Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a
service, most notably by its name directly, using the namespace as suffix, utilizing <code>&lt;namespace>.svc</code> as suffix or as a
fully qualified name as <code>&lt;service>.&lt;namespace>.svc.cluster.local</code> (assuming <code>cluster.local</code> to be the cluster domain).</p><p>This is why the DNS search path is fairly long in Kubernetes, usually consisting of <code>&lt;namespace>.svc.cluster.local</code>,
<code>svc.cluster.local</code>, <code>cluster.local</code>, and potentially some additional entries coming from the local network of the cluster.
For various reasons, the default <code>ndots</code> value in the context of Kubernetes is with <code>5</code>, also fairly large. See
<a href=https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056>this comment</a> for a more detailed description.</p><h2 id=dns-search-pathndots-problem-in-kubernetes>DNS Search Path/<code>ndots</code> Problem in Kubernetes<a class=td-heading-self-link href=#dns-search-pathndots-problem-in-kubernetes aria-label="Heading self-link"></a></h2><p>As the DNS search path is long and <code>ndots</code> is large, a lot of DNS queries might traverse the DNS search path. This results
in an explosion of DNS requests.</p><p>For example, consider the name resolution of the default kubernetes service <code>kubernetes.default.svc.cluster.local</code>. As this
name has only four dots, it is not considered a fully qualified name according to the default <code>ndots=5</code> setting. Therefore,
the DNS search path is applied, resulting in the following queries being created</p><ul><li><code>kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.network-domain</code></li><li>&mldr;</li></ul><p>In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.</p><h2 id=general-workaroundsmitigations>General Workarounds/Mitigations<a class=td-heading-self-link href=#general-workaroundsmitigations aria-label="Heading self-link"></a></h2><p>Kubernetes provides the capability to set the DNS options for each pod (see
<a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config>Pod DNS config</a> for details).
However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be
useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general
solution may difficult to impossible.</p><p>Another approach is to use always fully qualified names and append a dot (<code>.</code>) to the name to prevent the name resolution
system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the
trailing dot (<code>.</code>). Furthermore, it makes moving to different landscapes more difficult/error-prone.</p><h2 id=gardener-specific-workaroundsmitigations>Gardener Specific Workarounds/Mitigations<a class=td-heading-self-link href=#gardener-specific-workaroundsmitigations aria-label="Heading self-link"></a></h2><p>Gardener allows users to <a href=/docs/gardener/networking/custom-dns-config/>customize their DNS configuration</a>. CoreDNS allows several approaches to deal with
the requests generated by the DNS search path. <a href=https://coredns.io/plugins/cache/>Caching</a> is possible as well as
<a href=https://coredns.io/plugins/rewrite/>query rewriting</a>. There are also several other <a href=https://coredns.io/plugins/>plugins</a>
available, which may mitigate the situation.</p><h2 id=gardener-dns-query-rewriting>Gardener DNS Query Rewriting<a class=td-heading-self-link href=#gardener-dns-query-rewriting aria-label="Heading self-link"></a></h2><p>As explained <a href=/docs/gardener/networking/dns-search-path-optimization/#dns-search-path-ndots-and-kubernetes>above</a>, the application of the DNS search path may lead to the undesired
creation of DNS requests. Especially with the default setting of <code>ndots=5</code>, seemingly fully qualified names pointing to
services in the cluster may trigger the DNS search path application.</p><p>Gardener allows to automatically rewrite some obviously incorrect DNS names, which stem from an application of the DNS search
path to the most likely desired name. This will automatically rewrite requests like <code>service.namespace.svc.cluster.local.svc.cluster.local</code> to
<code>service.namespace.svc.cluster.local</code>.</p><p>In case the applications also target services for name resolution, which are outside of the cluster and have less than <code>ndots</code> dots,
it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the
<code>commonSuffixes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      rewriting:
</span></span><span style=display:flex><span>        commonSuffixes:
</span></span><span style=display:flex><span>        - gardener.cloud
</span></span><span style=display:flex><span>        - example.com
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>DNS requests containing a common suffix and ending in <code>.svc.cluster.local</code> are assumed to be incorrect application of the DNS
search path. Therefore, they are rewritten to everything ending in the common suffix. For example, <code>www.gardener.cloud.svc.cluster.local</code>
would be rewritten to <code>www.gardener.cloud</code>.</p><p>Please note that the common suffixes should be long enough and include enough dots (<code>.</code>) to prevent random overlap with
other DNS queries. For example, it would be a bad idea to simply put <code>com</code> on the list of common suffixes, as there may be
services/namespaces which have <code>com</code> as part of their name. The effect would be seemingly random DNS requests. Gardener
requires that common suffixes contain at least one dot (.) and adds a second dot at the beginning. For instance, a common
suffix of <code>example.com</code> in the configuration would match <code>*.example.com</code>.</p><p>Since some clients verify the host in the response of a DNS query, the host must also be rewritten.
For that reason, we can&rsquo;t rewrite a query for <code>service.dst-namespace.svc.cluster.local.src-namespace.svc.cluster.local</code> or
<code>www.example.com.src-namespace.svc.cluster.local</code>, as for an answer rewrite <code>src-namespace</code> would not be known.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-844c26030dafcb9e94d1600709e07b34>3 - Dual-stack network migration</h1><div class=lead>Migrate IPv4 shoots to dual-stack IPv4,IPv6 network</div><h1 id=dual-stack-network-migration>Dual-Stack Network Migration<a class=td-heading-self-link href=#dual-stack-network-migration aria-label="Heading self-link"></a></h1><p>This document provides a guide for migrating IPv4-only or IPv6-only Gardener shoot clusters to dual-stack networking (IPv4 and IPv6).</p><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Dual-stack networking allows clusters to operate with both IPv4 and IPv6 protocols. This configuration is controlled via the <code>spec.networking.ipFamilies</code> field, which accepts the following values:</p><ul><li><code>[IPv4]</code></li><li><code>[IPv6]</code></li><li><code>[IPv4, IPv6]</code></li><li><code>[IPv6, IPv4]</code></li></ul><h3 id=key-considerations>Key Considerations<a class=td-heading-self-link href=#key-considerations aria-label="Heading self-link"></a></h3><ul><li>Adding a new protocol is only allowed as the second element in the array, ensuring the primary protocol remains unchanged.</li><li>Migration involves multiple reconciliation runs to ensure a smooth transition without disruptions.</li></ul><h2 id=preconditions>Preconditions<a class=td-heading-self-link href=#preconditions aria-label="Heading self-link"></a></h2><p>Gardener supports multiple different network configurations, including running with pod overlay network or native routing. Currently, there is only native routing as supported operating mode for dual-stack networking in Gardener. This means that the pod overlay network needs to be disabled before starting the dual-stack migration. Otherwise, pod-to-pod cross-node communication may not work as expected after the migration.</p><p>At the moment, this only affects IPv4-only clusters, which should be migrated to dual-stack networking. IPv6-only clusters always use native routing.</p><p>You can check whether your cluster uses overlay network or native routing by looking for <code>spec.networking.providerConfig.overlay.enabled</code> in your cluster&rsquo;s manifest. If it is set to <code>true</code> or not present, the cluster is using the pod overlay network. If it is set to <code>false</code>, the cluster is using native routing.</p><p>Please note that there are infrastructure-specific limitations with regards to cluster size due to one route being added per node. Therefore, please consult the documentation of your infrastructure if your cluster should grow beyond 50 nodes and adapt the route limit quotas accordingly before switching to native routing.</p><p>To disable the pod overlay network and thereby switch to native routing, adjust your cluster specification as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=migration-process>Migration Process<a class=td-heading-self-link href=#migration-process aria-label="Heading self-link"></a></h2><p>The migration process should usually take place during the corresponding shoot maintenance time window. If you wish to run the migration process earlier, then you need to roll the nodes yourself and then trigger a reconcile so that the status of the <code>DualStackNodesMigrationReady</code> constraint is set to <code>true</code>. Once this is the case a new reconcile needs to be triggered to update the final components as described in step 5.</p><h3 id=step-1-update-networking-configuration>Step 1: Update Networking Configuration<a class=td-heading-self-link href=#step-1-update-networking-configuration aria-label="Heading self-link"></a></h3><p>Modify the <code>spec.networking.ipFamilies</code> field to include the desired dual-stack configuration. For example, change <code>[IPv4]</code> to <code>[IPv4, IPv6]</code>.</p><h3 id=step-2-infrastructure-reconciliation>Step 2: Infrastructure Reconciliation<a class=td-heading-self-link href=#step-2-infrastructure-reconciliation aria-label="Heading self-link"></a></h3><p>Changing the <code>ipFamilies</code> field triggers an infrastructure reconciliation. This step applies necessary changes to the underlying infrastructure to support dual-stack networking.</p><h3 id=step-3-control-plane-updates>Step 3: Control Plane Updates<a class=td-heading-self-link href=#step-3-control-plane-updates aria-label="Heading self-link"></a></h3><p>Depending on the infrastructure, control plane components will be updated or reconfigured to support dual-stack networking.</p><h3 id=step-4-node-rollout>Step 4: Node Rollout<a class=td-heading-self-link href=#step-4-node-rollout aria-label="Heading self-link"></a></h3><p>Nodes must support the new network protocol. However, node rollout is a manual step and is not triggered automatically. It should be performed during a maintenance window to minimize disruptions. Over time, this step may occur automatically, for example, during Kubernetes minor version updates that involve node replacements.</p><p>Cluster owners can monitor the progress of this step by checking the <code>DualStackNodesMigrationReady</code> constraint in the shoot status. During shoot reconciliation, the system verifies if all nodes support dual-stack networking and updates the migration state accordingly.</p><h3 id=step-5-final-reconciliation>Step 5: Final Reconciliation<a class=td-heading-self-link href=#step-5-final-reconciliation aria-label="Heading self-link"></a></h3><p>Once all nodes are migrated, the remaining control plane components and the Container Network Interface (CNI) are configured for dual-stack networking. The migration constraint is removed at the end of this step.</p><h2 id=post-migration-behavior>Post-Migration Behavior<a class=td-heading-self-link href=#post-migration-behavior aria-label="Heading self-link"></a></h2><p>After completing the migration:</p><ul><li>The shoot cluster supports dual-stack networking.</li><li>New pods will receive IP addresses from both address families.</li><li>Existing pods will only receive a second IP address upon recreation.</li><li>If full dual-stack networking is required all pods need to be rolled.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6dfa059798de2f667ed451972c157fe5>4 - ExposureClasses</h1><h1 id=exposureclasses>ExposureClasses<a class=td-heading-self-link href=#exposureclasses aria-label="Heading self-link"></a></h1><p>The Gardener API server provides a cluster-scoped <code>ExposureClass</code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ, etc.</p><h2 id=background>Background<a class=td-heading-self-link href=#background aria-label="Heading self-link"></a></h2><p>The <code>ExposureClass</code> resource is based on the concept for the <code>RuntimeClass</code> resource in Kubernetes.</p><p>A <code>RuntimeClass</code> abstracts the installation of a certain container runtime (e.g., gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>Runtime Class</a> for more information.</p><p>In contrast, an <code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g., corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.</p><p>Example: <code>RuntimeClass</code> and <code>ExposureClass</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: node.k8s.io/v1
</span></span><span style=display:flex><span>kind: RuntimeClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gvisor
</span></span><span style=display:flex><span>handler: gvisorconfig
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   nodeSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     env: prod</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: internet
</span></span><span style=display:flex><span>handler: internet-config
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   seedSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     matchLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#       network/env: internet</span>
</span></span></code></pre></div><p>Similar to <code>RuntimeClasses</code>, <code>ExposureClasses</code> also define a <code>.handler</code> field reflecting the name reference for the corresponding CRI configuration of the <code>RuntimeClass</code> and the control plane exposure configuration for the <code>ExposureClass</code>.</p><p>The CRI handler for <code>RuntimeClasses</code> is usually installed by an administrator (e.g., via a <code>DaemonSet</code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for <code>ExposureClasses</code> will be also provided by an administrator.
This exposure configuration is part of the gardenlet configuration, as this component is responsible to configure the control plane accordingly.
See the <a href=/docs/gardener/networking/exposureclasses/#gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers</a> section for more information.</p><p>The <code>RuntimeClass</code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its <code>.scheduling</code> section.
The <code>ExposureClass</code> also supports the selection of a subset of available Seed clusters whose gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its <code>.scheduling</code> section.</p><h2 id=usage-by-a-shoot>Usage by a <code>Shoot</code><a class=td-heading-self-link href=#usage-by-a-shoot aria-label="Heading self-link"></a></h2><p>A <code>Shoot</code> can reference an <code>ExposureClass</code> via the <code>.spec.exposureClassName</code> field.</p><blockquote><p>&#9888;&#xfe0f; When creating a <code>Shoot</code> resource, the Gardener scheduler will try to assign the <code>Shoot</code> to a <code>Seed</code> which will host its control plane.</p></blockquote><p>The scheduling behaviour can be influenced via the <code>.spec.seedSelectors</code> and/or <code>.spec.tolerations</code> fields in the <code>Shoot</code>.
<code>ExposureClass</code>es can also contain scheduling instructions.
If a <code>Shoot</code> is referencing an <code>ExposureClass</code>, then the scheduling instructions of both will be merged into the <code>Shoot</code>.
Those unions of scheduling instructions might lead to a selection of a <code>Seed</code> which is not able to deal with the <code>handler</code> of the <code>ExposureClass</code> and the <code>Shoot</code> creation might end up in an error.
In such case, the <code>Shoot</code> scheduling instructions should be revisited to check that they are not interfering with the ones from the <code>ExposureClass</code>.
If this is not feasible, then the combination with the <code>ExposureClass</code> might not be possible and you need to contact your Gardener administrator.</p><details><summary>Example: Shoot and ExposureClass scheduling instructions merge flow</summary><ol><li>Assuming there is the following <code>Shoot</code> which is referencing the <code>ExposureClass</code> below:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>handler: abc
</span></span><span style=display:flex><span>scheduling:
</span></span><span style=display:flex><span>  seedSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=2><li>Both <code>seedSelectors</code> would be merged into the <code>Shoot</code>. The result would be the following:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=3><li>Now the Gardener Scheduler would try to find a <code>Seed</code> with those labels.</li></ol><ul><li>If there are <strong>no</strong> Seeds with matching labels for the seed selector, then the <code>Shoot</code> will be unschedulable.</li><li>If there are Seeds with matching labels for the seed selector, then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see <a href=/docs/gardener/concepts/scheduler/#algorithm-overview>Gardener Scheduler</a>.<ul><li>If the <code>Seed</code> is <strong>not</strong> able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the Shoot will end up in error state.</li><li>If the <code>Seed</code> is able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the <code>Shoot</code> will be created.</li></ul></li></ul></details><h2 id=gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers<a class=td-heading-self-link href=#gardenlet-configuration-exposureclass-handlers aria-label="Heading self-link"></a></h2><p>The gardenlet is responsible to realize the control plane exposure strategy defined in the referenced <code>ExposureClass</code> of a <code>Shoot</code>.</p><p>Therefore, the <code>GardenletConfiguration</code> can contain an <code>.exposureClassHandlers</code> list with the respective configuration.</p><p>Example of the <code>GardenletConfiguration</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>exposureClassHandlers:
</span></span><span style=display:flex><span>- name: internet-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internet
</span></span><span style=display:flex><span>- name: internal-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internal
</span></span><span style=display:flex><span>  sni:
</span></span><span style=display:flex><span>    ingress:
</span></span><span style=display:flex><span>      namespace: ingress-internal
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        network: internal
</span></span></code></pre></div><p>Each gardenlet can define how the handler of a certain <code>ExposureClass</code> needs to be implemented for the Seed(s) where it is responsible for.</p><p>The <code>.name</code> is the name of the handler config and it must match to the <code>.handler</code> in the <code>ExposureClass</code>.</p><p>All control planes on a <code>Seed</code> are exposed via a load balancer, either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified, which can be done via the <code>.loadBalancerService</code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.</p><p>The control planes on a <code>Seed</code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the <code>Seed</code>.
The configuration of the ingress gateways can be controlled via the <code>.sni</code> section in the same way like for the default ingress gateways.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-89eadf4fffab70d5caac218992722852>5 - KUBERNETES_SERVICE_HOST Environment Variable Injection</h1><h1 id=kubernetes_service_host-environment-variable-injection><code>KUBERNETES_SERVICE_HOST</code> Environment Variable Injection<a class=td-heading-self-link href=#kubernetes_service_host-environment-variable-injection aria-label="Heading self-link"></a></h1><p>In each Shoot cluster&rsquo;s <code>kube-system</code> namespace a <code>DaemonSet</code> called <code>apiserver-proxy</code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>APIServer SNI GEP</a> for more details.</p><p>To skip this extra network hop, a <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>mutating webhook</a> called <code>apiserver-proxy.networking.gardener.cloud</code> is deployed next to the API server in the Seed. It adds a <code>KUBERNETES_SERVICE_HOST</code> environment variable to each container and init container that do not specify it. See the webhook <a href=https://github.com/gardener/apiserver-proxy/>repository</a> for more information.</p><h2 id=opt-out-of-pod-injection>Opt-Out of Pod Injection<a class=td-heading-self-link href=#opt-out-of-pod-injection aria-label="Heading self-link"></a></h2><p>In some cases it&rsquo;s desirable to opt-out of Pod injection:</p><ul><li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.</li><li>Want to test the <code>kube-proxy</code> and <code>kubelet</code> in-cluster discovery.</li></ul><h3 id=opt-out-of-pod-injection-for-specific-pods>Opt-Out of Pod Injection for Specific Pods<a class=td-heading-self-link href=#opt-out-of-pod-injection-for-specific-pods aria-label="Heading self-link"></a></h3><p>To opt out of the injection, the Pod should be labeled with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: nginx
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: nginx
</span></span><span style=display:flex><span>        apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: nginx
</span></span><span style=display:flex><span>        image: nginx:1.14.2
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 80
</span></span></code></pre></div><h3 id=opt-out-of-pod-injection-on-namespace-level>Opt-Out of Pod Injection on Namespace Level<a class=td-heading-self-link href=#opt-out-of-pod-injection-on-namespace-level aria-label="Heading self-link"></a></h3><p>To opt out of the injection of <strong>all</strong> Pods in a namespace, you should label your namespace with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Namespace
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>  name: my-namespace
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a namespace level and enable it for individual pods in it.</p></blockquote><h3 id=opt-out-of-pod-injection-for-the-entire-cluster>Opt-Out of Pod Injection for the Entire Cluster<a class=td-heading-self-link href=#opt-out-of-pod-injection-for-the-entire-cluster aria-label="Heading self-link"></a></h3><p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the <code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector</code> annotation with value <code>disable</code> on the <code>Shoot</code> resource itself:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: <span style=color:#a31515>&#39;disable&#39;</span>
</span></span><span style=display:flex><span>  name: my-cluster
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a cluster level and enable it for individual pods in it.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-06c68004ceb8c3011e8fbba935ebe29a>6 - NodeLocalDNS Configuration</h1><h1 id=nodelocaldns-configuration>NodeLocalDNS Configuration<a class=td-heading-self-link href=#nodelocaldns-configuration aria-label="Heading self-link"></a></h1><p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.</p><h2 id=background>Background<a class=td-heading-self-link href=#background aria-label="Heading self-link"></a></h2><p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)</li><li>and more &mldr;</li></ul><p>To workaround the issues described above, <code>node-local-dns</code> was introduced. The architecture is described below. The idea is simple:</p><ul><li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.</li><li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.</li></ul><p><img src=/docs/gardener/networking/images/node-local-dns.png alt=node-local-dns-architecture></p><h2 id=configuring-nodelocaldns>Configuring NodeLocalDNS<a class=td-heading-self-link href=#configuring-nodelocaldns aria-label="Heading self-link"></a></h2><p>All that needs to be done to enable the usage of the <code>node-local-dns</code> feature is to set the corresponding option (<code>spec.systemComponents.nodeLocalDNS.enabled</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>It is worth noting that:</p><ul><li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.</li><li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.</li><li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, DNS requests are repeated for some time as UDP is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.</li><li>Enabling or disabling node-local-dns triggers a rollout of all shoot worker nodes, see also <a href=/docs/gardener/shoot-operations/shoot_updates/#rolling-update-triggers>this document</a>.</li></ul><p>For more information about <code>node-local-dns</code>, please refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md>KEP</a> or to the <a href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/>usage documentation</a>.</p><h2 id=known-issues>Known Issues<a class=td-heading-self-link href=#known-issues aria-label="Heading self-link"></a></h2><p>Custom DNS configuration may not work as expected in conjunction with <code>NodeLocalDNS</code>.
Please refer to <a href=/docs/gardener/networking/custom-dns-config/#node-local-dns>Custom DNS Configuration</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c07e51600cc12694d1b150dd745e822c>7 - Shoot Networking Configurations</h1><div class=lead>Configuring Pod network. Maximum number of Nodes and Pods per Node</div><h1 id=shoot-networking-configurations>Shoot Networking Configurations<a class=td-heading-self-link href=#shoot-networking-configurations aria-label="Heading self-link"></a></h1><p>This document contains network related information for Shoot clusters.</p><h2 id=pod-network>Pod Network<a class=td-heading-self-link href=#pod-network aria-label="Heading self-link"></a></h2><p>A Pod network is imperative for any kind of cluster communication with Pods not started within the Node&rsquo;s host network.
More information about the Kubernetes network model can be found in the <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>Cluster Networking</a> topic.</p><p>Gardener allows users to configure the Pod network&rsquo;s CIDR during Shoot creation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: &lt;some-network-extension-name&gt; <span style=color:green># {calico,cilium}</span>
</span></span><span style=display:flex><span>    pods: 100.96.0.0/16
</span></span><span style=display:flex><span>    nodes: ...
</span></span><span style=display:flex><span>    services: ...
</span></span></code></pre></div><blockquote><p>&#9888;&#xfe0f; The <code>networking.pods</code> IP configuration is immutable and cannot be changed afterwards.
Please consider the following paragraph to choose a configuration which will meet your demands.</p></blockquote><p>One of the network plugin&rsquo;s (CNI) tasks is to assign IP addresses to Pods started in the Pod network.
Different network plugins come with different IP address management (IPAM) features, so we can&rsquo;t give any definite advice how IP ranges should be configured.
Nevertheless, we want to outline the standard configuration.</p><p>Information in <code>.spec.networking.pods</code> matches the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>&ndash;cluster-cidr flag</a> of the Kube-Controller-Manager of your Shoot cluster.
This IP range is divided into smaller subnets, also called <code>podCIDRs</code> (default mask <code>/24</code>) and assigned to Node objects <code>.spec.podCIDR</code>.
Pods get their IP address from this smaller node subnet in a default IPAM setup.
Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.</p><p><em><strong>Example 1</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/16
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 256 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>256 nodes</strong> which are ready to run workload in the Pod network.</p><p><em><strong>Example 2</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 16 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>16 nodes</strong> which are ready to run workload in the Pod network.</p><p>Beside the configuration in <code>.spec.networking.pods</code>, users can tune the <code>nodeCIDRMaskSize</code> used by Kube-Controller-Manager on shoot creation.
A smaller IP range per node means more <code>podCIDRs</code> and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      nodeCIDRMaskSize: 24 <span style=color:green># (default)</span>
</span></span></code></pre></div><blockquote><p>&#9888;&#xfe0f; The <code>nodeCIDRMaskSize</code> configuration is immutable and cannot be changed afterwards.</p></blockquote><p><em><strong>Example 3</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /25
-------------------------

Number of podCIDRs: 32 --&gt; max. Node count 
Number of IPs per podCIDRs: 128
</code></pre><p>With the configuration above, a Shoot cluster can at most have <strong>32 nodes</strong> which are ready to run workload in the Pod network.</p><h2 id=reserved-networks>Reserved Networks<a class=td-heading-self-link href=#reserved-networks aria-label="Heading self-link"></a></h2><p>Some network ranges are reserved for specific use-cases in the communication between seeds and shoots.</p><table><thead><tr><th>IPv</th><th>CIDR</th><th>Name</th><th>Purpose</th></tr></thead><tbody><tr><td>IPv6</td><td>fd8f:6d53:b97a:1::/96</td><td>Default VPN Range</td><td></td></tr><tr><td>IPv4</td><td>240.0.0.0/8</td><td>Kube-ApiServer Mapping Range</td><td>Used for the <code>kubernetes.default.svc.cluster.local</code> service in a shoot</td></tr><tr><td>IPv4</td><td>241.0.0.0/8</td><td>Seed Pod Mapping Range</td><td>Used for allowing overlapping IPv4 networks between shoot and seed. Requires non-HA control plane. Only used within the vpn pods</td></tr><tr><td>IPv4</td><td>242.0.0.0/8</td><td>Shoot Node Mapping Range</td><td>Used for allowing overlapping IPv4 networks between shoot and seed. Requires non-HA control plane. Only used within the vpn pods</td></tr><tr><td>IPv4</td><td>243.0.0.0/8</td><td>Shoot Service Mapping Range</td><td>Used for allowing overlapping IPv4 networks between shoot and seed. Requires non-HA control plane. Only used within the vpn pods</td></tr><tr><td>IPv4</td><td>244.0.0.0/8</td><td>Shoot Pod Mapping Range</td><td>Used for allowing overlapping IPv4 networks between shoot and seed. Requires non-HA control plane. Only used within the vpn pods</td></tr></tbody></table><blockquote><p>&#9888;&#xfe0f; Do not use any of the CIDR ranges mentioned above for any of the node, pod or service networks.
Gardener will prevent their creation. Pre-existing shoots using reserved ranges will still work, though it is recommended
to recreate them with compatible network ranges.</p></blockquote><h2 id=overlapping-ipv4-networks-between-seed-and-shoot>Overlapping IPv4 Networks between Seed and Shoot<a class=td-heading-self-link href=#overlapping-ipv4-networks-between-seed-and-shoot aria-label="Heading self-link"></a></h2><p>IPv4 or dual-stack shoot clusters can have overlapping network ranges with their seed cluster.
Gardener will disentangle the overlapping ranges by mapping them to reserved ranges within the VPN network using double network address translation (NAT).
Notice that none of the reserved ranges will show up outside the VPN network, i.e., they are not routable in the seed or shoot cluster.</p><blockquote><p><strong>Note:</strong> single-stack IPv6 shoots are usually not affected due to their vastly larger address space. However,
Gardener still enforces the non-overlapping condition for IPv6 networks to avoid any potential issues.</p></blockquote></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://demo.gardener.cloud>Demo</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors<div style=margin-top:2%><a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/privacy.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/impressum.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></div></span></div></footer></div><script src=/js/main.min.403ff095218c662472dab60ed98ecbb19431682de5ac7c6159891241cd366af5.js integrity="sha256-QD/wlSGMZiRy2rYO2Y7LsZQxaC3lrHxhWYkSQc02avU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script><script src=/js/community-index.js></script></body></html>